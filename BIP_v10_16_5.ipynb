{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell_0"
   },
   "source": "# BIP v10.16.5 - Bond Invariant Principle\n\n**Extracting moral knowledge from 5,000 years of human ethical reasoning**\n\nThis notebook implements a complete pipeline for:\n1. Loading multi-lingual ancient and modern ethical texts\n2. Extracting moral bonds (agent, patient, obligation type)\n3. Training cross-cultural moral embeddings\n4. Analyzing ethical patterns across traditions\n\n**Bond Extraction Training Data (NEW in v10.14.4):**\n- [ETHICS](https://github.com/hendrycks/ethics): 130K scenarios across 5 categories\n- [Scruples](https://github.com/allenai/scruples): 32K real-life anecdotes with ethical judgments\n- [EthicsSuite](https://github.com/llm-ethics/ethicssuite): 20K complex contextualized moral situations\n\n**Corpus Coverage:**\n\n*Ancient & Classical:*\n- Hebrew (Biblical, Mishnaic, Talmudic) - Sefaria (88 texts)\n- Aramaic (Talmud Bavli) - Sefaria\n- Classical Chinese (Confucian, Daoist, Legalist, Buddhist) - ctext.org, CBETA\n- Arabic (Quranic) - Tanzil\n- Sanskrit (Dharmashastra, Upanishads, Itihasa) - GitHub\n- Pali (Theravada Canon) - SuttaCentral\n- Greek & Latin (Stoic, Platonic, Aristotelian) - Perseus Digital Library\n\n*Western Philosophy & Religion:*\n- English: Kant, Mill, Spinoza, Aristotle, Plato, Epictetus, Marcus Aurelius (Gutenberg)\n- Bible KJV: Complete (80 books incl. Apocrypha)\n- Luther's Catechisms (Small & Large)\n- French: Montaigne, Voltaire, Rousseau (Gutenberg)\n- Spanish: Cervantes Don Quixote (Gutenberg)\n- Italian: Machiavelli, Dante (Gutenberg)\n\n*Modern Ethics:*\n- Dear Abby advice columns (68K letters)\n- hendrycks/ethics dataset (134K scenarios)\n- Folklore & Native American traditions (Ashliman Folktexts)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bf0c9ff3-b4d0-4af4-d148-46ec29b5a0b1",
    "id": "cell_1"
   },
   "outputs": [],
   "source": "# @title 1. Configuration & Setup { display-mode: \"form\" }\n\n# @markdown ---\n# @markdown ### Version\nBIP_VERSION = \"10.16.5\"  # @param {type:\"string\"}\n# @markdown Central version number - change to update all references\n# @markdown ## Data Source Configuration\n\nDATA_MODE = \"Update missing\"  # @param [\"Refresh all\", \"Update missing\", \"Cache only\"]\n# @markdown - **Refresh all**: Re-download everything from source (slow, ~2hrs)\n# @markdown - **Update missing**: Use cache, download only what's missing (recommended)\n# @markdown - **Cache only**: Use only cached data, fail if missing\n\nDRIVE_FOLDER = f\"BIP_v{BIP_VERSION}\"  # @param {type:\"string\"}\n# @markdown **Folder name for persistent storage** (edit above to change)\n\n# Derive flags from DATA_MODE\nUSE_DRIVE_DATA = True  # Always use Drive for caching\nREFRESH_DATA_FROM_SOURCE = DATA_MODE == \"Refresh all\"\nCACHE_ONLY = DATA_MODE == \"Cache only\"\n# @markdown ---\n# @markdown ## Model Backbone\nBACKBONE = \"LaBSE\"  # @param [\"MiniLM\", \"LaBSE\", \"XLM-R-base\", \"XLM-R-large\"]\n# @markdown - **MiniLM**: Fast, 118M params, good baseline\n# @markdown - **LaBSE**: Best cross-lingual alignment, 471M params (recommended)\n# @markdown - **XLM-R-base**: Strong multilingual, 270M params\n# @markdown - **XLM-R-large**: Strongest representations, 550M params\n\n# @markdown ---\n# @markdown ## Output Options\nCREATE_DOWNLOAD_ZIP = False  # @param {type:\"boolean\"}\n# @markdown - **CREATE_DOWNLOAD_ZIP**: Create and download a zip file of results (optional)\n# @markdown - Results are always persisted to Google Drive regardless of this setting\n\nSKIP_TRAINING = True  # @param {type:\"boolean\"}\n# @markdown - **SKIP_TRAINING**: Skip Cell 7 training, load models from Drive instead\n# @markdown - Use this to run evaluation (Cell 8+) on previously trained models\n\n# @markdown ---\n# @markdown ## Training Hyperparameters\nFREEZE_ENCODER = False  # @param {type:\"boolean\"} # v10.16.2: Unfreeze for invariance\n# @markdown - **FREEZE_ENCODER**: Only train probe head (recommended for stability)\n\nUSE_AMP = False  # @param {type:\"boolean\"} # DISABLED: gradient reversal causes NaN in float16\n# @markdown - **USE_AMP**: Use Automatic Mixed Precision (float16). Disable if you get NaN errors.\n# @markdown - Unfrozen: Fine-tune entire encoder (471M params, risk of catastrophic forgetting)\n\nLEARNING_RATE = 1e-5  # @param {type:\"number\"}\n# @markdown - **Frozen encoder**: 1e-4 to 1e-3 works well\n# @markdown - **Unfrozen encoder**: Use 1e-5 to 5e-6 (lower = more stable)\n\nWARMUP_RATIO = 0.1  # @param {type:\"number\"}\n# @markdown - Fraction of training for learning rate warmup (0.0 to 0.2)\n\nGRADIENT_CLIP = 1.0  # @param {type:\"number\"}\n# @markdown - Max gradient norm (prevents exploding gradients, 0 = disabled)\n\nNUM_EPOCHS = 15  # @param {type:\"integer\"} # v10.16.2: More epochs for fine-tuning\n# @markdown - Number of training epochs per split\n\nEARLY_STOPPING_PATIENCE = 5  # @param {type:\"integer\"}\n# @markdown - Stop if no improvement for N epochs (0 = disabled)\n\nADV_WARMUP_EPOCHS = 2  # @param {type:\"integer\"}\n\n# @markdown ---\n# @markdown ### v10.15.1.3: Per-Split Parameter Tuning\nPER_SPLIT_TUNING = True  # @param {type:\"boolean\"}\nSPLIT_ADV_LAMBDA = {\n    # v10.16.4: Increased all values for stronger adversarial training\n    # Previous values were too weak (0.35 = only 23% of max strength)\n    \"mixed_baseline\": 1.5,      # Was 0.35 - now full strength\n    \"ancient_to_modern\": 1.2,   # Was 0.30\n    \"stoic_to_confucian\": 1.5,  # Was 0.50\n    \"hebrew_to_arabic\": 1.5,    # Was 0.40\n    \"chinese_to_greek\": 1.5,    # Was 0.40\n}\n# @markdown ---\n# @markdown ### v10.15.1.4: Encoder Fine-Tuning (KEY CHANGE)\n# @markdown Previous versions froze the encoder. Now we can fine-tune it.\nUNFREEZE_ENCODER = True  # @param {type:\"boolean\"} # v10.16.2: Enable fine-tuning\n# @markdown - True: Fine-tune LaBSE to learn language-invariant moral structure\n\nUNFREEZE_AFTER_EPOCHS = 3  # @param {type:\"integer\"} # v10.16.2: Earlier unfreeze\n# @markdown - Epochs before unfreezing (if UNFREEZE_ENCODER=True)\n\nUNFREEZE_LAYERS = 4  # @param {type:\"integer\"} # v10.16.2: More layers\n# @markdown - Only unfreeze top N transformer layers (0=all)\n\nENCODER_LR_SCALE = 0.1  # @param {type:\"number\"}\n# @markdown - Learning rate multiplier for encoder (0.1 = 10x smaller)\n# @markdown - False: Probe-only mode (test pretrained representations)\n\nENCODER_LR = 1e-6  # @param {type:\"number\"} # v10.16.2: Slightly higher for fine-tuning\n# @markdown Learning rate for encoder (1000x lower than head LR to prevent NaN)\n\nHEAD_LR = 1e-3  # @param {type:\"number\"}\n# @markdown Learning rate for classification/adversarial heads\n\nUNFREEZE_AFTER_EPOCHS = 2  # @param {type:\"integer\"}\n# @markdown Epochs to train heads before unfreezing encoder (warmup)\n\nGRADIENT_ACCUMULATION_STEPS = 4  # @param {type:\"integer\"}\n# @markdown Accumulate gradients to simulate larger batch (memory efficiency)\n\n# @markdown ---\n# @markdown ### v10.15.1.4: Stronger Adversarial Heads\nADV_HIDDEN_DIM = 1024  # @param {type:\"integer\"}\n# @markdown Hidden dimension for adversarial classifier (was 256)\n\nADV_NUM_LAYERS = 4  # @param {type:\"integer\"}\n# @markdown Number of layers in adversarial head (was 2)\n\nADV_DROPOUT = 0.4  # @param {type:\"number\"}\n# @markdown Dropout in adversarial heads for regularization\n\n# @markdown - Epochs to ramp adversarial strength (longer = more stable)\n\nADV_MAX_LAMBDA = 1.5  # @param {type:\"number\"} # v10.16.2: Stronger adversarial # REDUCED from 1.0 for stability\n# @markdown - Max adversarial weight (0.7 recommended for strong disentanglement)\n\n# @markdown ### v10.16.5: Confusion Loss (KEY FIX)\n# @markdown Forces embeddings where NO classifier can predict language/period\nUSE_CONFUSION_LOSS = True  # @param {type:\"boolean\"}\nCONFUSION_WEIGHT = 0.5  # @param {type:\"number\"}\n# @markdown - Weight for entropy maximization (forces uniform predictions)\n# @markdown - This prevents adversarial heads from \"learning to fail\"\n\n# @markdown ### v10.15.1: Surface Invariance Training\nCONTRASTIVE_WEIGHT = 0.5  # @param {type:\"number\"} # v10.16.1: Increased\n# @markdown - Weight for contrastive loss (same moral, different surface)\n\nCONTRASTIVE_TEMPERATURE = 0.07  # @param {type:\"number\"}\n# @markdown - InfoNCE temperature (lower = harder negatives)\n\nSURFACE_AUGMENT = True  # @param {type:\"boolean\"}\n# @markdown - Create surface-perturbed training pairs\n\nAUGMENT_SIMILARITY_WEIGHT = 0.2  # @param {type:\"number\"}\n# @markdown - Weight for augmented pair similarity loss\n\n# @markdown ---\n# @markdown ### v10.16.2: Encoder Fine-Tuning Strategy (KEY CHANGE)\n# @markdown The frozen encoder preserves language info (99.6% lang acc).\n# @markdown Unfreezing allows the model to learn language-invariant representations.\n\nUSE_GRADIENT_CHECKPOINTING = True  # @param {type:\"boolean\"}\n# @markdown - Save memory during encoder fine-tuning (slower but fits in VRAM)\n\nENCODER_WARMUP_EPOCHS = 3  # @param {type:\"integer\"}\n# @markdown - Epochs to warm up encoder LR after unfreezing\n\nMIN_LANG_ACC_TARGET = 0.20  # @param {type:\"number\"}\n# @markdown - Target language accuracy (0.125 = random for 8 languages)\n\n# @markdown ---\n# @markdown ### v10.16.1: Structural Contrastive Training (NEW)\nUSE_STRUCTURAL_CONTRASTIVE = True  # @param {type:\"boolean\"}\n# @markdown - Enable structural perturbation contrastive loss\n\nSTRUCTURAL_CONTRASTIVE_WEIGHT = 0.4  # @param {type:\"number\"}\n# @markdown - Weight for structural contrastive loss (push apart)\n\nSTRUCTURAL_CONTRASTIVE_MARGIN = 0.8  # @param {type:\"number\"}\n# @markdown - Minimum distance for structural perturbations\n\n# @markdown ---\n# @markdown ### v10.16.1: Triplet Loss (NEW)\nUSE_TRIPLET_LOSS = True  # @param {type:\"boolean\"}\n# @markdown - Enable triplet loss (anchor, surface+, structural-)\n\nTRIPLET_MARGIN = 0.5  # @param {type:\"number\"}\n# @markdown - Triplet loss margin\n\nTRIPLET_WEIGHT = 0.3  # @param {type:\"number\"}\n# @markdown - Weight for triplet loss\n\n# @markdown ---\n# @markdown ### v10.16.1: Ratio Regularization (NEW)\nUSE_RATIO_LOSS = True  # @param {type:\"boolean\"}\n# @markdown - Encourage structural distance > surface distance\n\nTARGET_RATIO = 2.0  # @param {type:\"number\"}\n# @markdown - Target ratio: structural/surface > this value\n\nRATIO_LOSS_WEIGHT = 0.2  # @param {type:\"number\"}\n# @markdown - Weight for ratio loss\n\nZ_DIM = 64  # @param {type:\"integer\"} # v10.16.1: Reduced for stronger abstraction\n# @markdown - Bond embedding dimension (smaller = more abstraction)\n\n# Backbone configurations\nBACKBONE_CONFIGS = {\n    \"MiniLM\": {\n        \"model_name\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n        \"hidden_size\": 384,\n        \"recommended_batch\": {\n            \"L4/A100\": 4096,\n            \"T4\": 512,\n            \"2xT4\": 1024,\n            \"SMALL\": 128,\n            \"MINIMAL/CPU\": 64,\n        },\n    },\n    \"LaBSE\": {\n        \"model_name\": \"sentence-transformers/LaBSE\",\n        \"hidden_size\": 768,\n        \"recommended_batch\": {\n            \"L4/A100\": 4096,  # Increased: only using 2.1/22.5GB at 256\n            \"T4\": 512,\n            \"2xT4\": 1024,\n            \"SMALL\": 128,\n            \"MINIMAL/CPU\": 64,\n        },\n    },\n    \"XLM-R-base\": {\n        \"model_name\": \"xlm-roberta-base\",\n        \"hidden_size\": 768,\n        \"recommended_batch\": {\n            \"L4/A100\": 2048,  # Increased for better GPU utilization\n            \"T4\": 256,\n            \"2xT4\": 512,\n            \"SMALL\": 128,\n            \"MINIMAL/CPU\": 64,\n        },\n    },\n    \"XLM-R-large\": {\n        \"model_name\": \"xlm-roberta-large\",\n        \"hidden_size\": 1024,\n        \"recommended_batch\": {\n            \"L4/A100\": 256,\n            \"T4\": 64,\n            \"2xT4\": 128,\n            \"SMALL\": 32,\n            \"MINIMAL/CPU\": 16,\n        },\n    },\n}\n\nBACKBONE_CONFIG = BACKBONE_CONFIGS[BACKBONE]\nMODEL_NAME = BACKBONE_CONFIG[\"model_name\"]\nBACKBONE_HIDDEN = BACKBONE_CONFIG[\"hidden_size\"]\n\n\n# @markdown ---\n# @markdown ## Run Setup\n\nimport os\nimport sys\nimport time\n\nEXPERIMENT_START = time.time()\n\nprint(\"=\" * 60)\nprint(\"BIP v10.9 - ENVIRONMENT DETECTION\")\nprint(\"=\" * 60)\n\n# ===== ENVIRONMENT DETECTION =====\n# Detect which cloud platform we're running on\n\nENV_NAME = \"UNKNOWN\"\nENV_GPU_QUOTA = \"Unknown\"\nPERSISTENT_STORAGE = None\nDATA_DIR = \"/content\"  # Default\n\n\ndef detect_environment():\n    \"\"\"Detect cloud environment and return (name, gpu_quota, storage_path, data_dir)\"\"\"\n\n    # 1. Google Colab\n    try:\n        import google.colab\n\n        return (\"COLAB\", \"Free: T4 ~12h/day, Pro: L4/A100\", \"/content/drive/MyDrive\", \"/content\")\n    except ImportError:\n        pass\n\n    # 2. Kaggle Kernels\n    if os.path.exists(\"/kaggle\"):\n        # Kaggle has /kaggle/input for datasets, /kaggle/working for output\n        return (\"KAGGLE\", \"Free: 2xT4 30h/week, TPU 30h/week\", \"/kaggle/working\", \"/kaggle/working\")\n\n    # 3. Lightning.ai Studios\n    if os.environ.get(\"LIGHTNING_CLOUDSPACE_HOST\") or os.path.exists(\"/teamspace\"):\n        # Lightning.ai has /teamspace/studios for persistent storage\n        return (\n            \"LIGHTNING_AI\",\n            \"Free: 22h/month GPU, Pro: A10G/H100\",\n            \"/teamspace/studios\",\n            \"/teamspace/studios\",\n        )\n\n    # 4. Paperspace Gradient\n    if os.environ.get(\"PAPERSPACE_NOTEBOOK_REPO_ID\") or os.path.exists(\"/notebooks\"):\n        return (\"PAPERSPACE\", \"Free: M4000 6h, Pro: A100/H100\", \"/storage\", \"/notebooks\")\n\n    # 5. Saturn Cloud\n    if os.environ.get(\"SATURN_RESOURCE_ID\") or \"saturn\" in os.environ.get(\"HOSTNAME\", \"\").lower():\n        return (\n            \"SATURN_CLOUD\",\n            \"Free: T4 10h/month, Pro: A10G/A100\",\n            \"/home/jovyan/workspace\",\n            \"/home/jovyan\",\n        )\n\n    # 6. HuggingFace Spaces\n    if os.environ.get(\"SPACE_ID\") or os.environ.get(\"HF_SPACE_ID\"):\n        return (\n            \"HUGGINGFACE_SPACES\",\n            \"Free: CPU only, ZeroGPU: A10G/A100 quota\",\n            \"/data\",\n            \"/home/user/app\",\n        )\n\n    # 7. AWS SageMaker Studio Lab\n    if os.path.exists(\"/home/studio-lab-user\"):\n        return (\n            \"SAGEMAKER_STUDIO_LAB\",\n            \"Free: T4 4h/session, 24h max/day\",\n            \"/home/studio-lab-user\",\n            \"/home/studio-lab-user\",\n        )\n\n    # 8. Deepnote\n    if os.environ.get(\"DEEPNOTE_PROJECT_ID\"):\n        return (\"DEEPNOTE\", \"Free: CPU, Pro: T4/A10G\", \"/work\", \"/work\")\n\n    # 9. Local/Unknown\n    return (\"LOCAL\", \"Depends on local hardware\", os.getcwd(), os.getcwd())\n\n\nENV_NAME, ENV_GPU_QUOTA, PERSISTENT_STORAGE, DATA_DIR = detect_environment()\n\nprint(f\"\\nEnvironment: {ENV_NAME}\")\nprint(f\"GPU Quota:   {ENV_GPU_QUOTA}\")\nprint(f\"Storage:     {PERSISTENT_STORAGE}\")\nprint(f\"Data Dir:    {DATA_DIR}\")\n\n# Environment-specific setup\nENV_TIPS = {\n    \"COLAB\": [\n        \"Tip: Use GPU runtime (Runtime -> Change runtime type -> T4 GPU)\",\n        \"Tip: Colab Pro gives L4 GPU access (~2x faster than T4)\",\n    ],\n    \"KAGGLE\": [\n        \"Tip: Enable GPU (Settings -> Accelerator -> GPU T4 x2)\",\n        \"Tip: 30h/week GPU quota resets every Saturday\",\n        \"Tip: Upload data as a Kaggle Dataset for persistence\",\n    ],\n    \"LIGHTNING_AI\": [\n        \"Tip: Select GPU studio (A10G recommended for this workload)\",\n        \"Tip: /teamspace/studios persists across sessions\",\n    ],\n    \"PAPERSPACE\": [\n        \"Tip: Use /storage for persistent data across runs\",\n        \"Tip: Free tier has 6h/month GPU limit\",\n    ],\n    \"SATURN_CLOUD\": [\n        \"Tip: Start a T4 instance from the Resources tab\",\n        \"Tip: 10h/month free GPU quota\",\n    ],\n    \"HUGGINGFACE_SPACES\": [\n        \"Tip: ZeroGPU provides A10G/A100 access with quota system\",\n        \"Tip: Use Gradio/Streamlit for interactive demos\",\n    ],\n    \"SAGEMAKER_STUDIO_LAB\": [\n        \"Tip: Request GPU runtime from the launcher\",\n        \"Tip: Sessions timeout after 4h, max 24h/day\",\n    ],\n    \"LOCAL\": [\"Tip: Running locally - ensure CUDA is installed for GPU support\"],\n}\n\nprint(\"\\n\" + \"-\" * 60)\nprint(\"ENVIRONMENT TIPS:\")\nfor tip in ENV_TIPS.get(ENV_NAME, [\"No specific tips for this environment\"]):\n    print(f\"  {tip}\")\nprint(\"-\" * 60)\n\n# ===== INSTALL DEPENDENCIES =====\nimport subprocess\n\nprint(\"\\nInstalling dependencies...\")\nfor pkg in [\n    \"transformers\",\n    \"sentence-transformers\",\n    \"pandas\",\n    \"tqdm\",\n    \"scikit-learn\",\n    \"pyyaml\",\n    \"psutil\",\n    \"datasets\",\n]:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n\nimport psutil\nimport torch\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"GPU DETECTION & RESOURCE ALLOCATION\")\nprint(\"=\" * 60)\n\n# Detect hardware\nif torch.cuda.is_available():\n    GPU_NAME = torch.cuda.get_device_name(0)\n    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n    GPU_COUNT = torch.cuda.device_count()\nelse:\n    GPU_NAME = \"CPU\"\n    VRAM_GB = 0\n    GPU_COUNT = 0\n\nRAM_GB = psutil.virtual_memory().total / 1e9\n\nprint(\"\\nDetected Hardware:\")\nprint(f\"  GPU:  {GPU_NAME}\" + (f\" (x{GPU_COUNT})\" if GPU_COUNT > 1 else \"\"))\nprint(\n    f\"  VRAM: {VRAM_GB:.1f} GB\"\n    + (f\" (total: {VRAM_GB * GPU_COUNT:.1f} GB)\" if GPU_COUNT > 1 else \"\")\n)\nprint(f\"  RAM:  {RAM_GB:.1f} GB\")\n\n# Set optimal parameters based on hardware\nif VRAM_GB >= 22:  # L4 (24GB) or A100\n    GPU_TIER = \"L4/A100\"\nelif VRAM_GB >= 14:  # T4 (16GB)\n    GPU_TIER = \"T4\"\nelif VRAM_GB >= 10:\n    GPU_TIER = \"SMALL\"\nelse:\n    GPU_TIER = \"MINIMAL/CPU\"\n\n# Kaggle with 2xT4 can use larger batch\nif ENV_NAME == \"KAGGLE\" and GPU_COUNT >= 2:\n    GPU_TIER = \"2xT4\"\n    print(\"  ** Kaggle 2xT4 detected **\")\n\n# Get backbone-specific batch size\nBATCH_SIZE = BACKBONE_CONFIG[\"recommended_batch\"].get(GPU_TIER, 64)\n# Eval can use larger batch (no gradients)\nEVAL_BATCH_SIZE = min(BATCH_SIZE * 4, 512) if VRAM_GB >= 20 else BATCH_SIZE * 2\nprint(f\"  Backbone: {BACKBONE} -> batch size {BATCH_SIZE}\")\n\nMAX_PER_LANG = 50000  # Language sample limit\nCPU_CORES = os.cpu_count() or 2\nNUM_WORKERS = min(4, CPU_CORES - 1) if RAM_GB >= 24 and VRAM_GB >= 14 else 0\nMAX_TEST_SAMPLES = 20000\n# Use LEARNING_RATE from UI, or scale with batch size\nif LEARNING_RATE and LEARNING_RATE != 1e-5:  # 1e-5 is the default\n    LR = LEARNING_RATE\nelse:\n    LR = 2e-5 * (BATCH_SIZE / 256)  # Linear scaling with batch size\n\nprint(\"\\n\" + \"-\" * 60)\nprint(\"OPTIMAL SETTINGS:\")\nprint(\"-\" * 60)\nprint(f\"  Environment:     {ENV_NAME}\")\nprint(f\"  GPU Tier:        {GPU_TIER}\")\nprint(f\"  Backbone:        {BACKBONE}\")\nprint(f\"  Batch size:      {BATCH_SIZE}\")\nprint(f\"  Eval batch:     {EVAL_BATCH_SIZE}\")\nprint(f\"  Max per lang:    {MAX_PER_LANG:,}\")\nprint(f\"  DataLoader workers: {NUM_WORKERS}\")\nprint(f\"  Learning rate:   {LR:.2e}\")\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# USE_AMP is set by form field above\nUSE_AMP = USE_AMP and torch.cuda.is_available()  # Only enable if GPU available\nscaler = torch.amp.GradScaler(\"cuda\") if USE_AMP else None\n\n# ===== PERSISTENT STORAGE SETUP =====\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PERSISTENT STORAGE SETUP\")\nprint(\"=\" * 60)\n\nSAVE_DIR = None\nDRIVE_HAS_DATA = False\nDRIVE_FILES = set()  # Use set for O(1) lookup\n\nif ENV_NAME == \"COLAB\":\n    # Google Colab - mount Drive\n    try:\n        from google.colab import drive\n\n        DRIVE_MOUNT_PATH = \"/content/drive\"\n\n        if os.path.exists(f\"{DRIVE_MOUNT_PATH}/MyDrive\"):\n            print(\"Google Drive already mounted\")\n        else:\n            try:\n                drive.mount(DRIVE_MOUNT_PATH, force_remount=False)\n                print(\"Google Drive mounted successfully\")\n            except Exception as e:\n                print(f\"Drive mount issue: {e}\")\n                try:\n                    drive.mount(DRIVE_MOUNT_PATH, force_remount=True)\n                    print(\"Google Drive mounted (force remount)\")\n                except Exception as e2:\n                    print(f\"WARNING: Could not mount Drive: {e2}\")\n                    print(\"Falling back to local storage\")\n                    PERSISTENT_STORAGE = DATA_DIR\n\n        SAVE_DIR = f\"{DRIVE_MOUNT_PATH}/MyDrive/{DRIVE_FOLDER}\"\n    except Exception as e:\n        print(f\"Colab Drive setup failed: {e}\")\n        SAVE_DIR = f\"{DATA_DIR}/{DRIVE_FOLDER}\"\n\nelif ENV_NAME == \"KAGGLE\":\n    # Kaggle - use working directory\n    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n    print(f\"Using Kaggle working directory: {SAVE_DIR}\")\n    print(\"Note: Data persists until kernel is reset\")\n    # Check for uploaded datasets\n    if os.path.exists(\"/kaggle/input\"):\n        datasets = os.listdir(\"/kaggle/input\")\n        if datasets:\n            print(f\"Available datasets: {datasets[:5]}\")\n\nelif ENV_NAME == \"LIGHTNING_AI\":\n    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n    print(f\"Using Lightning.ai studio storage: {SAVE_DIR}\")\n\nelif ENV_NAME == \"PAPERSPACE\":\n    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n    print(f\"Using Paperspace /storage: {SAVE_DIR}\")\n\nelif ENV_NAME == \"HUGGINGFACE_SPACES\":\n    # HF Spaces has limited persistent storage\n    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n    print(f\"Using HuggingFace Spaces storage: {SAVE_DIR}\")\n    print(\"Warning: HF Spaces storage is limited\")\n\nelse:\n    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n    print(f\"Using local storage: {SAVE_DIR}\")\n\n# Check if folder exists BEFORE creating it\nfolder_existed = os.path.exists(SAVE_DIR)\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# Check what's available in storage - use BOTH listdir AND direct exists checks\n# (Google Drive can have sync issues where listdir misses files)\nif os.path.exists(SAVE_DIR):\n    DRIVE_FILES = set(os.listdir(SAVE_DIR))  # O(1) membership test\n\n    # Direct existence checks for key files (bypasses listdir caching issues)\n    key_files = [\"passages.jsonl\", \"bonds.jsonl\", \"dear_abby.csv\", \"all_splits.json\"]\n    for kf in key_files:\n        kf_path = os.path.join(SAVE_DIR, kf)\n        if os.path.exists(kf_path) and kf not in DRIVE_FILES:\n            print(f\"  [Drive sync fix] Found {kf} via os.path.exists() but not listdir()\")\n            DRIVE_FILES.add(kf)\n\n    DRIVE_HAS_DATA = \"passages.jsonl\" in DRIVE_FILES and \"bonds.jsonl\" in DRIVE_FILES\n\nprint(\"\\n\" + \"-\" * 60)\nprint(\"STORAGE STATUS:\")\nprint(\"-\" * 60)\nprint(f\"  Folder: {SAVE_DIR}\")\nprint(f\"  Folder existed: {folder_existed}\")\nprint(f\"  Files found: {len(DRIVE_FILES)}\")\n\n# If folder was empty/new, show what folders exist in parent to help debug\nif not DRIVE_FILES and ENV_NAME == \"COLAB\":\n    parent = os.path.dirname(SAVE_DIR)  # e.g., /content/drive/MyDrive\n    if os.path.exists(parent):\n        siblings = [d for d in os.listdir(parent) if \"bip\" in d.lower() or \"BIP\" in d]\n        if siblings:\n            print(f\"  ** Similar folders in {parent}: {siblings}\")\n        else:\n            print(f\"  ** No BIP folders found in {parent}\")\nif DRIVE_FILES:\n    for f in sorted(DRIVE_FILES)[:10]:  # sorted() converts to list for slicing\n        print(f\"    - {f}\")\n    if len(DRIVE_FILES) > 10:\n        print(f\"    ... and {len(DRIVE_FILES) - 10} more\")\nprint(f\"  Pre-processed data available: {DRIVE_HAS_DATA}\")\n\n# Decide data loading strategy\nLOAD_FROM_DRIVE = USE_DRIVE_DATA and DRIVE_HAS_DATA and not REFRESH_DATA_FROM_SOURCE\n\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"DATA LOADING STRATEGY: {DATA_MODE}\")\nprint(\"-\" * 60)\nif DATA_MODE == \"Refresh all\":\n    print(\"  -> Will re-download ALL data from online sources\")\n    print(\"     (This takes ~2 hours, use 'Update missing' to save time)\")\nelif DATA_MODE == \"Cache only\":\n    if LOAD_FROM_DRIVE:\n        print(\"  -> Using cached data only (no downloads)\")\n    else:\n        print(\"  -> ERROR: Cache-only mode but no cached data found!\")\n        print(\"     Change DATA_MODE to 'Update missing'\")\nelse:  # Update missing (default)\n    if LOAD_FROM_DRIVE:\n        print(\"  -> Using cached processed data from Drive\")\n        print(\"     (v10.9 corpora will be added if missing)\")\n    else:\n        print(\"  -> Will download missing data, use cached where available\")\n        print(\n            f\"     Sefaria: {'cached' if os.path.exists(f'{SAVE_DIR}/Sefaria-Export-json.tar.gz') else 'will download'}\"\n        )\nprint(\"=\" * 60)\n\n# Create local directories\nfor d in [\"data/processed\", \"data/splits\", \"data/raw\", \"models/checkpoints\", \"results\"]:\n    os.makedirs(d, exist_ok=True)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SETUP COMPLETE\")\nprint(\"=\" * 60)\nprint(f\"  Environment: {ENV_NAME}\")\nprint(f\"  GPU:         {GPU_NAME} ({GPU_TIER})\")\nprint(f\"  Storage:     {SAVE_DIR}\")\nprint(\"  Ready to run: Cell 2 (Imports)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "423888ac-20af-4048-88f9-b12bd48c3548",
    "id": "cell_2"
   },
   "outputs": [],
   "source": [
    "# @title 2. Load Corpora (v10.12 - Self-Contained) { display-mode: \"form\" }\n",
    "# @markdown Downloads from verified external sources - fully self-contained, no external imports\n",
    "# @markdown\n",
    "# @markdown **Sources (9 categories):**\n",
    "# @markdown - Sanskrit: Itihasa (93K shlokas)\n",
    "# @markdown - Pali: SuttaCentral API (Full Canon)\n",
    "# @markdown - Arabic: Tanzil.net (Quran)\n",
    "# @markdown - Hebrew/Aramaic: Sefaria GitHub\n",
    "\n",
    "INCLUDE_RESPONSA = False  # @param {type:\"boolean\"}\n",
    "# @markdown - **INCLUDE_RESPONSA**: Include Responsa texts (requires 30-50 min git clone)\n",
    "# @markdown - Set to True only if you need the full Responsa collection\n",
    "# @markdown - Chinese: ctext.org API\n",
    "# @markdown - Greek/Latin: Perseus Digital Library\n",
    "# @markdown - Romance: Don Quijote, Montaigne, Voltaire, Rousseau, Machiavelli, Dante\n",
    "# @markdown - Folklore: Ashliman Folktexts (incl. Native American)\n",
    "# @markdown - English: Gutenberg philosophy, Dear Abby (68K), hendrycks/ethics (134K)\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "# Global session for all HTTP requests (shared across loaders)\n",
    "ctext_session = requests.Session()\n",
    "ctext_session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; BIP-Corpus-Loader/1.0; +https://github.com/)\",\n",
    "    \"Accept\": \"application/json, text/plain, */*\",\n",
    "})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING CORPORA (v10.12 - Self-Contained)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "DATA_DIR = Path(\"data/raw/v10.12\")\n",
    "CACHE_DIR = DATA_DIR / \"cache\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# v10.15.1: Immediate Drive caching helper\n",
    "def cache_to_drive(cache_file: Path, quiet: bool = False) -> None:\n",
    "    \"\"\"Immediately copy cache file to Drive for persistence.\"\"\"\n",
    "    try:\n",
    "        if '_save_dir' in globals() and _save_dir and os.path.exists(_save_dir):\n",
    "            drive_cache = Path(_save_dir) / 'corpus_cache'\n",
    "            drive_cache.mkdir(exist_ok=True)\n",
    "            dest = drive_cache / cache_file.name\n",
    "            if not dest.exists():\n",
    "                import shutil\n",
    "                shutil.copy(cache_file, dest)\n",
    "                if not quiet:\n",
    "                    print(f\"      -> Cached {cache_file.name} to Drive\")\n",
    "    except Exception:\n",
    "        pass  # Drive cache is optional\n",
    "\n",
    "# Get settings from Cell 1\n",
    "try:\n",
    "    _cache_only = CACHE_ONLY\n",
    "except NameError:\n",
    "    _cache_only = False\n",
    "\n",
    "try:\n",
    "    _save_dir = SAVE_DIR\n",
    "except NameError:\n",
    "    _save_dir = \"data/processed\"\n",
    "\n",
    "# Memory limits per language (L4 GPU safe)\n",
    "MAX_PASSAGES_PER_LANG = {\n",
    "    \"sanskrit\": 15000,\n",
    "    \"pali\": 10000,\n",
    "    \"arabic\": 10000,\n",
    "    \"classical_chinese\": 10000,\n",
    "    \"hebrew\": 15000,\n",
    "    \"aramaic\": 10000,\n",
    "    \"english\": 50000,  # Increased for folklore + ethics\n",
    "    \"greek\": 10000,\n",
    "    \"latin\": 10000,\n",
    "    \"spanish\": 5000,\n",
    "    \"french\": 5000,\n",
    "    \"italian\": 5000,\n",
    "    \"default\": 5000,\n",
    "}\n",
    "\n",
    "MIN_PASSAGES = 500  # For 6-sigma confidence\n",
    "\n",
    "# ============================================================================\n",
    "# RESTORE CACHE FROM DRIVE (if available)\n",
    "# ============================================================================\n",
    "# In hybrid mode, check if Drive has cached corpus files and restore them\n",
    "# This avoids re-downloading on every Colab restart\n",
    "\n",
    "if _save_dir and os.path.exists(_save_dir):\n",
    "    drive_cache = Path(_save_dir) / \"corpus_cache\"\n",
    "    if drive_cache.exists():\n",
    "        import shutil\n",
    "\n",
    "        restored = 0\n",
    "        for cache_file in drive_cache.glob(\"*.json\"):\n",
    "            local_cache = CACHE_DIR / cache_file.name\n",
    "            if not local_cache.exists():\n",
    "                shutil.copy(cache_file, local_cache)\n",
    "                restored += 1\n",
    "        if restored:\n",
    "            print(f\"Restored {restored} cache files from Drive\")\n",
    "    # Note: sefaria.json IS cached to Drive after first successful load\n",
    "    # Git clone is faster than Drive copy for many small files\n",
    "\n",
    "# ============================================================================\n",
    "# RATE LIMITING\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, calls_per_minute: int = 20):\n",
    "        self.min_interval = 60.0 / calls_per_minute\n",
    "        self.last_call = 0.0\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def wait(self):\n",
    "        with self.lock:\n",
    "            elapsed = time.time() - self.last_call\n",
    "            if elapsed < self.min_interval:\n",
    "                time.sleep(self.min_interval - elapsed)\n",
    "            self.last_call = time.time()\n",
    "\n",
    "\n",
    "GITHUB_LIMITER = RateLimiter(calls_per_minute=60)\n",
    "SUTTACENTRAL_LIMITER = RateLimiter(calls_per_minute=120)\n",
    "CTEXT_LIMITER = RateLimiter(calls_per_minute=30)\n",
    "\n",
    "# ============================================================================\n",
    "# SANSKRIT - Itihasa from GitHub (VERIFIED)\n",
    "# https://github.com/rahular/itihasa - 93K shlokas\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_itihasa_github() -> list[dict]:\n",
    "    \"\"\"Load Itihasa Sanskrit shlokas from GitHub.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"itihasa.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            if passages and \"time_periods\" in passages[0]:\n",
    "                print(f\"  Itihasa: {len(passages):,} passages (cached)\")\n",
    "                return passages\n",
    "            print(\"  Itihasa cache missing time_periods - rebuilding...\")\n",
    "            passages = []\n",
    "\n",
    "    print(\"  Downloading Itihasa from GitHub...\")\n",
    "    data_path = DATA_DIR / \"itihasa\"\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = [\n",
    "        (\"train.sn\", \"https://raw.githubusercontent.com/rahular/itihasa/main/data/train.sn\"),\n",
    "        (\"dev.sn\", \"https://raw.githubusercontent.com/rahular/itihasa/main/data/dev.sn\"),\n",
    "        (\"test.sn\", \"https://raw.githubusercontent.com/rahular/itihasa/main/data/test.sn\"),\n",
    "    ]\n",
    "\n",
    "    for name, url in files:\n",
    "        local_file = data_path / name\n",
    "        if not local_file.exists():\n",
    "            try:\n",
    "                GITHUB_LIMITER.wait()\n",
    "                resp = ctext_session.get(url, timeout=120)\n",
    "                if resp.status_code == 200:\n",
    "                    with open(local_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(resp.text)\n",
    "                    print(f\"    Downloaded {name}: {len(resp.text) // 1024}KB\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Failed {name}: {e}\")\n",
    "\n",
    "    # Parse .sn files\n",
    "    for sn_file in data_path.glob(\"*.sn\"):\n",
    "        with open(sn_file, encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f, 1):\n",
    "                text = line.strip()\n",
    "                if text and len(text) > 10:\n",
    "                    passages.append(\n",
    "                        {\n",
    "                            \"id\": f\"itihasa_{sn_file.stem}_{i}\",\n",
    "                            \"text\": text,\n",
    "                            \"language\": \"sanskrit\",\n",
    "                            \"source\": f\"Itihasa/{sn_file.stem}\",\n",
    "                            \"time_periods\": [\"DHARMA\", \"ANCIENT\", \"INDIC\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "        cache_to_drive(cache_file, quiet=True)\n",
    "\n",
    "    print(f\"  Itihasa: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PALI - SuttaCentral API (VERIFIED)\n",
    "# https://suttacentral.net/api/bilarasuttas/{id}/pli\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_pali_suttacentral() -> list[dict]:\n",
    "    \"\"\"Load Pali texts from SuttaCentral API.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"suttacentral.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            if passages and \"time_periods\" in passages[0]:\n",
    "                print(f\"  SuttaCentral: {len(passages):,} passages (cached)\")\n",
    "                return passages\n",
    "            print(\"  SuttaCentral cache missing time_periods - rebuilding...\")\n",
    "            passages = []\n",
    "\n",
    "    print(\"  Fetching from SuttaCentral API...\")\n",
    "\n",
    "    # Expanded sutta list\n",
    "    sutta_ids = []\n",
    "    # Majjhima Nikaya (152 suttas)\n",
    "    sutta_ids.extend([f\"mn{i}\" for i in range(1, 153)])\n",
    "    # Digha Nikaya (34 suttas)\n",
    "    sutta_ids.extend([f\"dn{i}\" for i in range(1, 35)])\n",
    "    # Samyutta Nikaya (key vaggas)\n",
    "    for v in [1, 3, 6, 12, 22, 35, 45, 56]:\n",
    "        sutta_ids.extend([f\"sn{v}.{i}\" for i in range(1, 20)])\n",
    "    # Anguttara Nikaya\n",
    "    for n in [1, 2, 3, 4, 5, 6, 7, 8, 10]:\n",
    "        sutta_ids.extend([f\"an{n}.{i}\" for i in range(1, 50)])\n",
    "    # Dhammapada\n",
    "    sutta_ids.extend([f\"dhp{i}\" for i in range(1, 27)])\n",
    "\n",
    "    def fetch_sutta(sid):\n",
    "        results = []\n",
    "        try:\n",
    "            SUTTACENTRAL_LIMITER.wait()\n",
    "            url = f\"https://suttacentral.net/api/bilarasuttas/{sid}/pli\"\n",
    "            resp = ctext_session.get(url, timeout=30)\n",
    "            if resp.status_code == 200:\n",
    "                data = resp.json()\n",
    "                if isinstance(data, dict):\n",
    "                    segments = data.get(\"root_text\", {})\n",
    "                    if isinstance(segments, dict):\n",
    "                        for seg_id, text in segments.items():\n",
    "                            if text and len(text) > 20:\n",
    "                                results.append(\n",
    "                                    {\n",
    "                                        \"id\": f\"pali_{sid}_{seg_id}\",\n",
    "                                        \"text\": text.strip(),\n",
    "                                        \"language\": \"pali\",\n",
    "                                        \"source\": sid,\n",
    "                                        \"time_periods\": [\"PALI\", \"ANCIENT\", \"INDIC\", \"BUDDHIST\"],\n",
    "                                    }\n",
    "                                )\n",
    "        except Exception:\n",
    "            pass\n",
    "        return results\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(fetch_sutta, sid) for sid in sutta_ids[:300]]\n",
    "        for done, future in enumerate(as_completed(futures), 1):\n",
    "            passages.extend(future.result())\n",
    "            if done % 50 == 0:\n",
    "                print(f\"    Fetched {done}/{min(300, len(sutta_ids))} suttas...\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "        cache_to_drive(cache_file, quiet=True)\n",
    "\n",
    "    print(f\"  SuttaCentral: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ARABIC - Tanzil.net (VERIFIED)\n",
    "# https://tanzil.net/download/\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_quran_tanzil() -> list[dict]:\n",
    "    \"\"\"Load Quran from Tanzil.net.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"tanzil.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            if passages and \"time_periods\" in passages[0]:\n",
    "                print(f\"  Tanzil Quran: {len(passages):,} passages (cached)\")\n",
    "                return passages\n",
    "            print(\"  Tanzil cache missing time_periods - rebuilding...\")\n",
    "            passages = []\n",
    "\n",
    "    print(\"  Downloading Quran from Tanzil.net...\")\n",
    "    try:\n",
    "        url = \"https://tanzil.net/pub/download/index.php?quranType=uthmani&outType=txt-2&agree=true\"\n",
    "        resp = ctext_session.get(url, timeout=60)\n",
    "        if resp.status_code == 200:\n",
    "            for line in resp.text.strip().split(\"\\n\"):\n",
    "                if \"|\" in line:\n",
    "                    parts = line.split(\"|\")\n",
    "                    if len(parts) >= 3:\n",
    "                        surah, ayah, text = parts[0], parts[1], parts[2].strip()\n",
    "                        if len(text) > 10:\n",
    "                            passages.append(\n",
    "                                {\n",
    "                                    \"id\": f\"quran_{surah}_{ayah}\",\n",
    "                                    \"text\": text,\n",
    "                                    \"language\": \"arabic\",\n",
    "                                    \"source\": f\"Quran {surah}:{ayah}\",\n",
    "                                    \"time_periods\": [\"QURANIC\", \"MEDIEVAL\", \"SEMITIC\"],\n",
    "                                }\n",
    "                            )\n",
    "            print(f\"    Downloaded {len(passages)} verses\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "        cache_to_drive(cache_file, quiet=True)\n",
    "\n",
    "    print(f\"  Tanzil Quran: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HEBREW/ARAMAIC - Sefaria GitHub (VERIFIED)\n",
    "# https://github.com/Sefaria/Sefaria-Export\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_sefaria_github() -> list[dict]:\n",
    "    \"\"\"Load Hebrew/Aramaic from Sefaria GitHub.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"sefaria.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            # v10.16.1: Validate cache has required fields\n",
    "            if passages and \"time_periods\" in passages[0]:\n",
    "                print(f\"  Sefaria: {len(passages):,} passages (cached)\")\n",
    "                return passages\n",
    "            else:\n",
    "                print(f\"  Sefaria cache missing time_periods - rebuilding...\")\n",
    "                passages = []  # Invalidate cache\n",
    "\n",
    "    base_path = DATA_DIR / \"Sefaria-Export\"\n",
    "    json_path = base_path / \"json\"\n",
    "\n",
    "    # Key texts to download (path, language, period)\n",
    "    # Pattern: path -> Hebrew/merged.json or Hebrew/Merged.json\n",
    "    key_texts = [\n",
    "        # =====================================================================\n",
    "        # TANAKH - Complete Hebrew Bible (~39 books, ~20MB)\n",
    "        # =====================================================================\n",
    "        # Torah (Pentateuch) - 5 books\n",
    "        (\"Tanakh/Torah/Genesis\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Torah/Exodus\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Torah/Leviticus\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Torah/Numbers\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Torah/Deuteronomy\", \"hebrew\", \"BIBLICAL\"),\n",
    "        # Former Prophets - 6 books\n",
    "        (\"Tanakh/Prophets/Joshua\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Judges\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/I Samuel\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/II Samuel\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/I Kings\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/II Kings\", \"hebrew\", \"BIBLICAL\"),\n",
    "        # Latter Prophets - Major - 3 books\n",
    "        (\"Tanakh/Prophets/Isaiah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Jeremiah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Ezekiel\", \"hebrew\", \"BIBLICAL\"),\n",
    "        # Latter Prophets - Minor (Trei Asar) - 12 books\n",
    "        (\"Tanakh/Prophets/Hosea\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Joel\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Amos\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Obadiah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Jonah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Micah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Nahum\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Habakkuk\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Zephaniah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Haggai\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Zechariah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Malachi\", \"hebrew\", \"BIBLICAL\"),\n",
    "        # Writings (Ketuvim) - 13 books\n",
    "        (\"Tanakh/Writings/Psalms\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Proverbs\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Job\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Song of Songs\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Ruth\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Lamentations\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Ecclesiastes\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Esther\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Daniel\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Ezra\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Nehemiah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/I Chronicles\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/II Chronicles\", \"hebrew\", \"BIBLICAL\"),\n",
    "        # =====================================================================\n",
    "        # MISHNAH - Complete 6 Orders (~63 tractates, ~15MB)\n",
    "        # =====================================================================\n",
    "        # Seder Zeraim (Seeds) - Agricultural ethics\n",
    "        (\"Mishnah/Seder Zeraim/Mishnah Berakhot\", \"hebrew\", \"TANNAITIC\"),\n",
    "        (\"Mishnah/Seder Zeraim/Mishnah Peah\", \"hebrew\", \"TANNAITIC\"),  # Corners for poor\n",
    "        (\"Mishnah/Seder Zeraim/Mishnah Maasrot\", \"hebrew\", \"TANNAITIC\"),  # Tithes\n",
    "        # Seder Moed (Festivals) - Sabbath ethics\n",
    "        (\"Mishnah/Seder Moed/Mishnah Shabbat\", \"hebrew\", \"TANNAITIC\"),\n",
    "        (\"Mishnah/Seder Moed/Mishnah Yoma\", \"hebrew\", \"TANNAITIC\"),  # Day of Atonement\n",
    "        (\"Mishnah/Seder Moed/Mishnah Taanit\", \"hebrew\", \"TANNAITIC\"),  # Fasts\n",
    "        # Seder Nashim (Women) - Family/gender ethics\n",
    "        (\"Mishnah/Seder Nashim/Mishnah Yevamot\", \"hebrew\", \"TANNAITIC\"),  # Levirate marriage\n",
    "        (\"Mishnah/Seder Nashim/Mishnah Ketubot\", \"hebrew\", \"TANNAITIC\"),  # Marriage contracts\n",
    "        (\"Mishnah/Seder Nashim/Mishnah Nedarim\", \"hebrew\", \"TANNAITIC\"),  # Vows\n",
    "        (\"Mishnah/Seder Nashim/Mishnah Nazir\", \"hebrew\", \"TANNAITIC\"),  # Nazirite vows\n",
    "        (\"Mishnah/Seder Nashim/Mishnah Sotah\", \"hebrew\", \"TANNAITIC\"),  # Suspected adulteress\n",
    "        (\"Mishnah/Seder Nashim/Mishnah Gittin\", \"hebrew\", \"TANNAITIC\"),  # Divorce\n",
    "        (\"Mishnah/Seder Nashim/Mishnah Kiddushin\", \"hebrew\", \"TANNAITIC\"),  # Betrothal\n",
    "        # Seder Nezikin (Damages) - Civil/criminal ethics (CORE)\n",
    "        # Note: Sefaria uses \"Mishnah X\" prefix for tractate folders\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Bava Kamma\", \"hebrew\", \"TANNAITIC\"),  # First Gate - damages\n",
    "        (\n",
    "            \"Mishnah/Seder Nezikin/Mishnah Bava Metzia\",\n",
    "            \"hebrew\",\n",
    "            \"TANNAITIC\",\n",
    "        ),  # Middle Gate - property\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Bava Batra\", \"hebrew\", \"TANNAITIC\"),  # Last Gate - sales\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Sanhedrin\", \"hebrew\", \"TANNAITIC\"),  # Courts/capital\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Makkot\", \"hebrew\", \"TANNAITIC\"),  # Lashes\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Shevuot\", \"hebrew\", \"TANNAITIC\"),  # Oaths\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Eduyot\", \"hebrew\", \"TANNAITIC\"),  # Testimonies\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Avodah Zarah\", \"hebrew\", \"TANNAITIC\"),  # Idolatry\n",
    "        (\n",
    "            \"Mishnah/Seder Nezikin/Pirkei Avot\",\n",
    "            \"hebrew\",\n",
    "            \"TANNAITIC\",\n",
    "        ),  # Ethics of Fathers (no prefix)\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Horayot\", \"hebrew\", \"TANNAITIC\"),  # Rulings\n",
    "        # Seder Kodashim (Holy Things) - Temple/sacred\n",
    "        (\"Mishnah/Seder Kodashim/Mishnah Zevachim\", \"hebrew\", \"TANNAITIC\"),\n",
    "        (\"Mishnah/Seder Kodashim/Mishnah Menachot\", \"hebrew\", \"TANNAITIC\"),\n",
    "        # Seder Tohorot (Purities) - Purity ethics\n",
    "        (\"Mishnah/Seder Tohorot/Mishnah Niddah\", \"hebrew\", \"TANNAITIC\"),  # Menstrual purity\n",
    "        # =====================================================================\n",
    "        # TALMUD BAVLI - Key tractates (~20MB)\n",
    "        # =====================================================================\n",
    "        # Foundational\n",
    "        (\"Talmud/Bavli/Seder Zeraim/Berakhot\", \"aramaic\", \"AMORAIC\"),\n",
    "        # Ethics tractates (Seder Nezikin)\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Bava Kamma\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Bava Metzia\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Bava Batra\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Sanhedrin\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Makkot\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Shevuot\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Avodah Zarah\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Horayot\", \"aramaic\", \"AMORAIC\"),\n",
    "        # Family ethics (Seder Nashim)\n",
    "        (\"Talmud/Bavli/Seder Nashim/Yevamot\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nashim/Ketubot\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nashim/Kiddushin\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nashim/Gittin\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nashim/Sotah\", \"aramaic\", \"AMORAIC\"),\n",
    "        # Sabbath (Seder Moed)\n",
    "        (\"Talmud/Bavli/Seder Moed/Shabbat\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Moed/Yoma\", \"aramaic\", \"AMORAIC\"),\n",
    "        # =====================================================================\n",
    "        # RESPONSA - Ethical Q&A (only if INCLUDE_RESPONSA=True)\n",
    "        # =====================================================================\n",
    "        (\"Responsa/Geonim\", \"hebrew\", \"GEONIC\"),  # 600-1000 CE\n",
    "        (\"Responsa/Rishonim\", \"hebrew\", \"RISHONIM\"),  # 1000-1500 CE\n",
    "        (\"Responsa/Acharonim\", \"hebrew\", \"ACHARONIM\"),  # 1500-1800 CE\n",
    "        (\"Responsa/Modern\", \"hebrew\", \"MODERN_RESPONSA\"),  # 1800-present\n",
    "        (\"Responsa/Teshuvot Maharsham Volume I\", \"hebrew\", \"ACHARONIM\"),\n",
    "        (\"Responsa/Teshuvot Maharsham Volume II\", \"hebrew\", \"ACHARONIM\"),\n",
    "        (\"Responsa/Teshuvot Maharsham Volume III\", \"hebrew\", \"ACHARONIM\"),\n",
    "    ]\n",
    "\n",
    "    # Download strategy depends on INCLUDE_RESPONSA setting\n",
    "    # - False: Staged download only (fast, ~2 min, core texts)\n",
    "    # - True: Full clone only (slow, 30-50 min, includes Responsa)\n",
    "\n",
    "    if INCLUDE_RESPONSA:\n",
    "        # Skip staged download - we need full clone for Responsa anyway\n",
    "        print(\"  INCLUDE_RESPONSA=True: Will do full clone for Responsa...\")\n",
    "        need_staged = False\n",
    "    else:\n",
    "        need_staged = not json_path.exists()\n",
    "\n",
    "    if need_staged:\n",
    "        print(\"  Downloading Sefaria texts (staged download)...\")\n",
    "        json_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        base_url = \"https://raw.githubusercontent.com/Sefaria/Sefaria-Export/master/json\"\n",
    "\n",
    "        def download_sefaria_text(text_info):\n",
    "            \"\"\"Download merged.json for a Sefaria text, handling various structures.\"\"\"\n",
    "            text_path, lang, period = text_info\n",
    "            url_path = text_path.replace(\" \", \"%20\")\n",
    "\n",
    "            # Different structures for different text types\n",
    "            if \"Responsa\" in text_path:\n",
    "                # Responsa have nested structure - try to get index or first collection\n",
    "                # For now, skip in staged mode - these need full clone\n",
    "                return text_path, False, 0, []\n",
    "\n",
    "            # Standard texts: try Hebrew/merged.json first\n",
    "            patterns = [\n",
    "                (\"Hebrew/merged.json\", \"Hebrew\"),\n",
    "                (\"Aramaic/merged.json\", \"Aramaic\"),  # For Talmud\n",
    "                (\"merged.json\", \"\"),  # Direct merged.json\n",
    "            ]\n",
    "\n",
    "            for pattern, subdir in patterns:\n",
    "                try:\n",
    "                    url = f\"{base_url}/{url_path}/{pattern}\"\n",
    "                    GITHUB_LIMITER.wait()\n",
    "                    resp = ctext_session.get(url, timeout=60)\n",
    "                    if resp.status_code == 200 and len(resp.text) > 100:\n",
    "                        local_dir = json_path / text_path\n",
    "                        if subdir:\n",
    "                            local_dir = local_dir / subdir\n",
    "                        local_dir.mkdir(parents=True, exist_ok=True)\n",
    "                        local_file = local_dir / \"merged.json\"\n",
    "                        with open(local_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(resp.text)\n",
    "                        return text_path, True, len(resp.text), []\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            return text_path, False, 0, []\n",
    "\n",
    "        # Download in parallel\n",
    "        downloaded = 0\n",
    "        total_size = 0\n",
    "        responsa_skipped = []\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(download_sefaria_text, t) for t in key_texts]\n",
    "            for future in as_completed(futures):\n",
    "                text_path, success, size, _ = future.result()\n",
    "                if \"Responsa\" in text_path:\n",
    "                    responsa_skipped.append(text_path)\n",
    "                elif success:\n",
    "                    downloaded += 1\n",
    "                    total_size += size\n",
    "                    print(\n",
    "                        f\"    Downloaded {downloaded}: {text_path.split('/')[-1][:30]} ({size // 1024}KB)\"\n",
    "                    )\n",
    "\n",
    "        non_responsa_texts = [t for t in key_texts if \"Responsa\" not in t[0]]\n",
    "        responsa_texts = [t for t in key_texts if \"Responsa\" in t[0]]\n",
    "        print(\n",
    "            f\"    Staged: {downloaded}/{len(non_responsa_texts)} core texts, {total_size // 1024}KB\"\n",
    "        )\n",
    "        if not INCLUDE_RESPONSA:\n",
    "            # Show what we got from staged download\n",
    "            if responsa_texts:\n",
    "                print(\"    Responsa skipped (set INCLUDE_RESPONSA=True to include)\")\n",
    "            print(f\"    Using staged download results ({downloaded} texts)\")\n",
    "            need_clone = False  # Don't clone if INCLUDE_RESPONSA is False\n",
    "        else:\n",
    "            # INCLUDE_RESPONSA is True - we need to clone\n",
    "            print(f\"    Responsa ({len(responsa_texts)} collections) require full clone\")\n",
    "            need_clone = True\n",
    "\n",
    "        if need_clone:\n",
    "            print(\"    Starting full clone (this takes 30-50 min)...\")\n",
    "            # Clone to a temp location, then move\n",
    "            import shutil\n",
    "\n",
    "            clone_path = DATA_DIR / \"Sefaria-Clone-Temp\"\n",
    "            if clone_path.exists():\n",
    "                shutil.rmtree(clone_path)\n",
    "\n",
    "            print(\"  Cloning Sefaria-Export from GitHub (~2GB, 30-50 min)...\")\n",
    "            try:\n",
    "                import re\n",
    "\n",
    "                proc = subprocess.Popen(\n",
    "                    [\n",
    "                        \"git\",\n",
    "                        \"clone\",\n",
    "                        \"--depth\",\n",
    "                        \"1\",\n",
    "                        \"--progress\",\n",
    "                        \"https://github.com/Sefaria/Sefaria-Export.git\",\n",
    "                        str(clone_path),\n",
    "                    ],\n",
    "                    stdout=subprocess.PIPE,\n",
    "                    stderr=subprocess.PIPE,  # Git progress goes to stderr\n",
    "                    text=True,\n",
    "                    bufsize=1,\n",
    "                )\n",
    "                last_progress_time = time.time()\n",
    "                last_pct = -1\n",
    "                current_phase = \"\"\n",
    "                stall_timeout = 120  # Kill if no progress for 2 minutes\n",
    "\n",
    "                while proc.poll() is None:\n",
    "                    # Check for stall (no progress for stall_timeout seconds)\n",
    "                    if time.time() - last_progress_time > stall_timeout:\n",
    "                        proc.kill()\n",
    "                        print(f\"\\n    Stalled (no progress for {stall_timeout}s)\")\n",
    "                        raise subprocess.TimeoutExpired(\"git clone\", stall_timeout)\n",
    "\n",
    "                    # Read stderr for progress (git writes progress there)\n",
    "                    try:\n",
    "                        line = proc.stderr.readline()\n",
    "                        if line:\n",
    "                            line = line.strip()\n",
    "                            last_progress_time = time.time()  # Got output = progress\n",
    "\n",
    "                            # Detect phase changes\n",
    "                            if \"Receiving objects\" in line and current_phase != \"receiving\":\n",
    "                                current_phase = \"receiving\"\n",
    "                                print(\"    Receiving objects: \", end=\"\", flush=True)\n",
    "                                last_pct = -1\n",
    "                            elif \"Resolving deltas\" in line and current_phase != \"resolving\":\n",
    "                                current_phase = \"resolving\"\n",
    "                                print(\"\\n    Resolving deltas:  \", end=\"\", flush=True)\n",
    "                                last_pct = -1\n",
    "                            elif \"Updating files\" in line and current_phase != \"updating\":\n",
    "                                current_phase = \"updating\"\n",
    "                                print(\"\\n    Updating files:    \", end=\"\", flush=True)\n",
    "                                last_pct = -1\n",
    "\n",
    "                            # Extract and print percentage (every 10%)\n",
    "                            if \"%\" in line:\n",
    "                                match = re.search(r\"(\\d+)%\", line)\n",
    "                                if match:\n",
    "                                    pct = int(match.group(1))\n",
    "                                    # Print at 0, 10, 20, ... 100\n",
    "                                    if pct // 10 > last_pct // 10 or pct == 100:\n",
    "                                        print(f\"{pct}% \", end=\"\", flush=True)\n",
    "                                        last_pct = pct\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    time.sleep(0.05)\n",
    "\n",
    "                print()  # Newline after progress\n",
    "                # Drain any remaining output\n",
    "                _, stderr = proc.communicate(timeout=5)\n",
    "                if proc.returncode == 0:\n",
    "                    print(\"    Clone successful!\")\n",
    "                    # Move cloned json to base_path\n",
    "                    cloned_json = clone_path / \"json\"\n",
    "                    if cloned_json.exists():\n",
    "                        import shutil\n",
    "\n",
    "                        if json_path.exists():\n",
    "                            shutil.rmtree(json_path)\n",
    "                        shutil.move(str(cloned_json), str(json_path))\n",
    "                        shutil.rmtree(clone_path)  # Clean up\n",
    "                else:\n",
    "                    print(f\"    Clone failed (code {proc.returncode})\")\n",
    "                    if stderr:\n",
    "                        print(f\"    {stderr[:200]}\")\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"    Using staged results (Responsa unavailable)\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n    Clone failed: {e} - using staged results\")\n",
    "        else:\n",
    "            print(\"    Staged download sufficient, skipping full clone\")\n",
    "\n",
    "    def extract_text(obj, depth=0):\n",
    "        if depth > 5:\n",
    "            return []\n",
    "        texts = []\n",
    "        if isinstance(obj, str) and len(obj) > 20:\n",
    "            texts.append(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                texts.extend(extract_text(item, depth + 1))\n",
    "        elif isinstance(obj, dict):\n",
    "            for key in [\"he\", \"text\", \"content\"]:\n",
    "                if key in obj:\n",
    "                    texts.extend(extract_text(obj[key], depth + 1))\n",
    "        return texts\n",
    "\n",
    "    for text_path, lang, period in key_texts:\n",
    "        full_path = json_path / text_path\n",
    "        if not full_path.exists():\n",
    "            json_file = json_path / f\"{text_path}.json\"\n",
    "            if json_file.exists():\n",
    "                full_path = json_file\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            files_to_parse = []\n",
    "            if full_path.is_file():\n",
    "                files_to_parse = [full_path]\n",
    "            elif full_path.is_dir():\n",
    "                # Responsa have many nested files - allow more\n",
    "                max_files = 500 if \"Responsa\" in text_path else 100\n",
    "                files_to_parse = list(full_path.rglob(\"*.json\"))[:max_files]\n",
    "\n",
    "            text_count = 0\n",
    "            for jf in files_to_parse:\n",
    "                with open(jf, encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                texts = extract_text(data)\n",
    "                # More texts per file for Responsa (rich ethical Q&A)\n",
    "                max_per_file = 500 if \"Responsa\" in text_path else 200\n",
    "                for text in texts[:max_per_file]:\n",
    "                    if len(passages) >= MAX_PASSAGES_PER_LANG.get(lang, 5000):\n",
    "                        break\n",
    "                    passages.append(\n",
    "                        {\n",
    "                            \"id\": f\"sefaria_{len(passages)}\",\n",
    "                            \"text\": text.strip(),\n",
    "                            \"language\": lang,\n",
    "                            \"source\": text_path.split(\"/\")[-1],\n",
    "                            \"time_periods\": [period],\n",
    "                        }\n",
    "                    )\n",
    "                    text_count += 1\n",
    "            if text_count > 0 and \"Responsa\" in text_path:\n",
    "                print(f\"    {text_path.split('/')[-1]}: {text_count} responsa\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "        cache_to_drive(cache_file, quiet=True)\n",
    "\n",
    "        # Immediately cache to Drive (clone takes 30-50 min - don't lose this!)\n",
    "        try:\n",
    "            if \"_save_dir\" in dir() and _save_dir and os.path.exists(_save_dir):\n",
    "                import shutil\n",
    "\n",
    "                drive_cache = Path(_save_dir) / \"corpus_cache\"\n",
    "                drive_cache.mkdir(exist_ok=True)\n",
    "                drive_sefaria = drive_cache / \"sefaria.json\"\n",
    "                if not drive_sefaria.exists():\n",
    "                    shutil.copy(cache_file, drive_sefaria)\n",
    "                    print(\"    -> Cached sefaria.json to Drive for future runs\")\n",
    "        except Exception:\n",
    "            pass  # Drive cache is optional\n",
    "\n",
    "    print(f\"  Sefaria: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CHINESE - ctext.org API (VERIFIED)\n",
    "# https://api.ctext.org/gettext?urn=ctp:analects/xue-er\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_chinese_ctext() -> list[dict]:\n",
    "    \"\"\"Load Chinese classics from ctext.org API.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"ctext.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            # v10.16.1: Validate cache has time_periods\n",
    "            if passages and \"time_periods\" in passages[0]:\n",
    "                # Show breakdown by period\n",
    "                by_period = {}\n",
    "                for p in passages:\n",
    "                    period = p.get(\"time_periods\", [\"UNKNOWN\"])[0]\n",
    "                    by_period[period] = by_period.get(period, 0) + 1\n",
    "                print(f\"  ctext.org: {len(passages):,} passages (cached)\")\n",
    "                for period, count in sorted(by_period.items()):\n",
    "                    print(f\"    {period}: {count}\")\n",
    "                return passages\n",
    "            print(\"  ctext cache missing time_periods - rebuilding...\")\n",
    "            passages = []\n",
    "\n",
    "    # v10.15.1: Add headers to avoid API blocking\n",
    "    ctext_headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    }\n",
    "    ctext_session = requests.Session()\n",
    "    ctext_session.headers.update(ctext_headers)\n",
    "\n",
    "    print(\"  Fetching from ctext.org API...\")\n",
    "\n",
    "    # v10.14.3: Diagnostic check\n",
    "    try:\n",
    "        _test_url = \"https://api.ctext.org/gettext?urn=ctp:analects/xue-er\"\n",
    "        _test_resp = ctext_session.get(_test_url, timeout=10)\n",
    "        print(f\"    [DIAG] API test: {_test_resp.status_code}\")\n",
    "        if _test_resp.status_code == 200:\n",
    "            _test_data = _test_resp.json()\n",
    "            print(f\"    [DIAG] Response keys: {list(_test_data.keys())}\")\n",
    "            if \"fulltext\" in _test_data:\n",
    "                print(f\"    [DIAG] fulltext count: {len(_test_data['fulltext'])}\")\n",
    "        else:\n",
    "            print(f\"    [DIAG] API returned: {_test_resp.text[:200]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    [DIAG] API test failed: {type(e).__name__}: {e}\")\n",
    "\n",
    "    # ctext.org requires chapter-level URNs\n",
    "    texts = [\n",
    "        # Working texts (no auth required):\n",
    "        (\n",
    "            \"ctp:analects\",\n",
    "            \"Analects\",\n",
    "            \"CONFUCIAN\",\n",
    "            [\n",
    "                \"xue-er\",\n",
    "                \"wei-zheng\",\n",
    "                \"ba-yi\",\n",
    "                \"li-ren\",\n",
    "                \"gong-ye-chang\",\n",
    "                \"yong-ye\",\n",
    "                \"shu-er\",\n",
    "                \"tai-bo\",\n",
    "                \"zi-han\",\n",
    "                \"xiang-dang\",\n",
    "                \"xian-jin\",\n",
    "                \"yan-yuan\",\n",
    "                \"zi-lu\",\n",
    "                \"xian-wen\",\n",
    "                \"wei-ling-gong\",\n",
    "                \"ji-shi\",\n",
    "                \"yang-huo\",\n",
    "                \"wei-zi\",\n",
    "                \"zi-zhang\",\n",
    "                \"yao-yue\",\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"ctp:mengzi\",\n",
    "            \"Mencius\",\n",
    "            \"CONFUCIAN\",\n",
    "            [\n",
    "                \"liang-hui-wang-i\",\n",
    "                \"liang-hui-wang-ii\",\n",
    "                \"gong-sun-chou-i\",\n",
    "                \"gong-sun-chou-ii\",\n",
    "                \"teng-wen-gong-i\",\n",
    "                \"teng-wen-gong-ii\",\n",
    "                \"li-lou-i\",\n",
    "                \"li-lou-ii\",\n",
    "                \"wan-zhang-i\",\n",
    "                \"wan-zhang-ii\",\n",
    "                \"gaozi-i\",\n",
    "                \"gaozi-ii\",\n",
    "                \"jin-xin-i\",\n",
    "                \"jin-xin-ii\",\n",
    "            ],\n",
    "        ),\n",
    "        (\"ctp:dao-de-jing\", \"Daodejing\", \"DAOIST\", []),  # Book-level fetch\n",
    "        # NOTE: Zhuangzi, Xunzi, Han Feizi, Mozi require special handling or auth\n",
    "    ]\n",
    "\n",
    "    errors_by_text = {}\n",
    "\n",
    "    for text_id, name, period, chapters in texts:\n",
    "        count = 0\n",
    "        errors = []\n",
    "        # If no chapters specified, fetch book-level\n",
    "        if not chapters:\n",
    "            try:\n",
    "                CTEXT_LIMITER.wait()\n",
    "                url = f\"https://api.ctext.org/gettext?urn={text_id}\"\n",
    "                resp = ctext_session.get(url, timeout=30)\n",
    "                if resp.status_code == 200:\n",
    "                    data = resp.json()\n",
    "                    if isinstance(data, dict) and \"fulltext\" in data:\n",
    "                        for text in data[\"fulltext\"]:\n",
    "                            if text and len(text) > 10:\n",
    "                                passages.append(\n",
    "                                    {\n",
    "                                        \"id\": f\"ctext_{len(passages)}\",\n",
    "                                        \"text\": text,\n",
    "                                        \"language\": \"classical_chinese\",\n",
    "                                        \"source\": name,\n",
    "                                        \"time_periods\": [period],\n",
    "                                    }\n",
    "                                )\n",
    "                                count += 1\n",
    "            except Exception as e:\n",
    "                errors.append(f\"book-level: {type(e).__name__}\")\n",
    "\n",
    "        for chapter in chapters:\n",
    "            try:\n",
    "                CTEXT_LIMITER.wait()\n",
    "                urn = f\"{text_id}/{chapter}\"\n",
    "                url = f\"https://api.ctext.org/gettext?urn={urn}\"\n",
    "                resp = ctext_session.get(url, timeout=30)\n",
    "\n",
    "                if resp.status_code != 200:\n",
    "                    errors.append(f\"{chapter}: HTTP {resp.status_code}\")\n",
    "                    continue\n",
    "\n",
    "                data = resp.json()\n",
    "\n",
    "                # Check for API error response\n",
    "                if isinstance(data, dict) and \"error\" in data:\n",
    "                    errors.append(f\"{chapter}: {data['error']}\")\n",
    "                    continue\n",
    "\n",
    "                # API returns {\"fulltext\": [...], \"title\": \"...\"}\n",
    "                if isinstance(data, dict) and \"fulltext\" in data:\n",
    "                    for text in data[\"fulltext\"]:\n",
    "                        if text and len(text) > 10:\n",
    "                            passages.append(\n",
    "                                {\n",
    "                                    \"id\": f\"ctext_{len(passages)}\",\n",
    "                                    \"text\": text,\n",
    "                                    \"language\": \"classical_chinese\",\n",
    "                                    \"source\": f\"{name}/{chapter}\",\n",
    "                                    \"time_periods\": [period],\n",
    "                                }\n",
    "                            )\n",
    "                            count += 1\n",
    "                # Fallback: list format\n",
    "                elif isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        text = item.get(\"text\", \"\") if isinstance(item, dict) else str(item)\n",
    "                        if text and len(text) > 10:\n",
    "                            passages.append(\n",
    "                                {\n",
    "                                    \"id\": f\"ctext_{len(passages)}\",\n",
    "                                    \"text\": text,\n",
    "                                    \"language\": \"classical_chinese\",\n",
    "                                    \"source\": f\"{name}/{chapter}\",\n",
    "                                    \"time_periods\": [period],\n",
    "                                }\n",
    "                            )\n",
    "                            count += 1\n",
    "                else:\n",
    "                    errors.append(f\"{chapter}: unexpected response format\")\n",
    "\n",
    "            except requests.exceptions.Timeout:\n",
    "                errors.append(f\"{chapter}: timeout\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                errors.append(f\"{chapter}: {type(e).__name__}\")\n",
    "            except json.JSONDecodeError:\n",
    "                errors.append(f\"{chapter}: invalid JSON\")\n",
    "            except Exception as e:\n",
    "                errors.append(f\"{chapter}: {type(e).__name__}: {e}\")\n",
    "\n",
    "        # Always print status for each text\n",
    "        if count > 0:\n",
    "            print(f\"    {name} ({period}): {count} passages\")\n",
    "        else:\n",
    "            print(f\"    {name} ({period}): 0 passages [FAILED]\")\n",
    "\n",
    "        if errors:\n",
    "            errors_by_text[name] = errors\n",
    "\n",
    "    # Print error summary\n",
    "    if errors_by_text:\n",
    "        print(\"\\n  ctext.org API errors:\")\n",
    "        for name, errs in errors_by_text.items():\n",
    "            print(f\"    {name}: {len(errs)} failed chapters\")\n",
    "            for err in errs[:3]:  # Show first 3 errors\n",
    "                print(f\"      - {err}\")\n",
    "            if len(errs) > 3:\n",
    "                print(f\"      - ... and {len(errs) - 3} more\")\n",
    "    # === MOZI: Requires nested subsection navigation ===\n",
    "    print(\"    Fetching Mozi (nested subsections)...\")\n",
    "    mozi_count = 0\n",
    "    try:\n",
    "        CTEXT_LIMITER.wait()\n",
    "        resp = ctext_session.get(\"https://api.ctext.org/gettext?urn=ctp:mozi\", timeout=30)\n",
    "        if resp.status_code == 200:\n",
    "            data = resp.json()\n",
    "            if \"subsections\" in data:\n",
    "                for book_urn in data[\"subsections\"][:10]:\n",
    "                    CTEXT_LIMITER.wait()\n",
    "                    book_resp = ctext_session.get(\n",
    "                        f\"https://api.ctext.org/gettext?urn={book_urn}\", timeout=30\n",
    "                    )\n",
    "                    if book_resp.status_code == 200:\n",
    "                        book_data = book_resp.json()\n",
    "                        if \"subsections\" in book_data:\n",
    "                            for chapter_urn in book_data[\"subsections\"][:5]:\n",
    "                                CTEXT_LIMITER.wait()\n",
    "                                ch_resp = ctext_session.get(\n",
    "                                    f\"https://api.ctext.org/gettext?urn={chapter_urn}\", timeout=30\n",
    "                                )\n",
    "                                if ch_resp.status_code == 200:\n",
    "                                    ch_data = ch_resp.json()\n",
    "                                    if \"fulltext\" in ch_data:\n",
    "                                        for text in ch_data[\"fulltext\"]:\n",
    "                                            if text and len(text) > 10:\n",
    "                                                passages.append(\n",
    "                                                    {\n",
    "                                                        \"id\": f\"ctext_{len(passages)}\",\n",
    "                                                        \"text\": text,\n",
    "                                                        \"language\": \"classical_chinese\",\n",
    "                                                        \"source\": chapter_urn.replace(\"ctp:\", \"\"),\n",
    "                                                        \"time_periods\": [\"MOHIST\"],\n",
    "                                                    }\n",
    "                                                )\n",
    "                                                mozi_count += 1\n",
    "        if mozi_count > 0:\n",
    "            print(f\"    Mozi (MOHIST): {mozi_count} passages\")\n",
    "        else:\n",
    "            print(\"    Mozi (MOHIST): 0 passages [FAILED]\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Mozi: ERROR - {type(e).__name__}: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "        cache_to_drive(cache_file, quiet=True)\n",
    "\n",
    "    print(f\"  ctext.org: {len(passages):,} passages total\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ============================================================================\n",
    "# CHINESE BUDDHIST - CBETA (Chinese Buddhist Electronic Text Association)\n",
    "# Key sutras for Buddhist moral philosophy\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_chinese_buddhist() -> list[dict]:\n",
    "    \"\"\"Load Chinese Buddhist texts from CBETA via CLTK GitHub mirrors.\n",
    "\n",
    "    Sources key sutras representing Buddhist ethics/philosophy:\n",
    "    - Diamond Sutra () - Prajnaparamita\n",
    "    - Heart Sutra () - Core emptiness teaching\n",
    "    - Platform Sutra () - Chan/Zen ethics\n",
    "    - Sutra of 42 Sections () - Basic moral teachings\n",
    "    - Vimalakirti Sutra () - Lay Buddhist ethics\n",
    "    - Lotus Sutra () - Devotional Buddhism\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"cbeta_buddhist.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            if passages and \"time_periods\" in passages[0]:\n",
    "                print(f\"  CBETA Buddhist: {len(passages):,} passages (cached)\")\n",
    "                return passages\n",
    "            print(\"  CBETA cache missing time_periods - rebuilding...\")\n",
    "            passages = []\n",
    "\n",
    "    print(\"  Fetching Chinese Buddhist texts from CBETA/CLTK...\")\n",
    "\n",
    "    # CLTK uses format: cbeta__taisho-tripitaka-electronic-version-no-XXXX__chinese.json\n",
    "    base_url = \"https://raw.githubusercontent.com/cltk/chinese_text_cbeta_02/master/cltk_json\"\n",
    "    texts = [\n",
    "        (\"0235\", \"Diamond Sutra\"),\n",
    "        (\"0251\", \"Heart Sutra\"),\n",
    "        (\"0262\", \"Lotus Sutra\"),\n",
    "        (\"0475\", \"Vimalakirti Sutra\"),\n",
    "        (\"0784\", \"42 Sections Sutra\"),\n",
    "        (\"2008\", \"Platform Sutra\"),\n",
    "    ]\n",
    "\n",
    "    for text_num, name in texts:\n",
    "        try:\n",
    "            url = (\n",
    "                f\"{base_url}/cbeta__taisho-tripitaka-electronic-version-no-{text_num}__chinese.json\"\n",
    "            )\n",
    "            resp = ctext_session.get(url, timeout=60)\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"    {name}: HTTP {resp.status_code}\")\n",
    "                continue\n",
    "\n",
    "            data = resp.json()\n",
    "            count = 0\n",
    "\n",
    "            # CLTK format: {\"text\": {\"0\": \"line\", \"1\": \"line\", ...}}\n",
    "            if isinstance(data, dict) and \"text\" in data:\n",
    "                text_dict = data[\"text\"]\n",
    "                if isinstance(text_dict, dict):\n",
    "                    for key in sorted(text_dict.keys(), key=lambda x: int(x) if x.isdigit() else 0):\n",
    "                        text = text_dict[key]\n",
    "                        if isinstance(text, str) and len(text) > 10:\n",
    "                            # Skip metadata lines\n",
    "                            if (\n",
    "                                text.startswith(\"No.\")\n",
    "                                or text.startswith(\"[\")\n",
    "                                or text.startswith(\"\")\n",
    "                            ):\n",
    "                                continue\n",
    "                            if \"CBETA\" in text or \"Taisho\" in text:\n",
    "                                continue\n",
    "                            passages.append(\n",
    "                                {\n",
    "                                    \"id\": f\"cbeta_T{text_num}_{count}\",\n",
    "                                    \"text\": text.strip(),\n",
    "                                    \"language\": \"classical_chinese\",\n",
    "                                    \"source\": f\"CBETA/{name}\",\n",
    "                                    \"time_periods\": [\"BUDDHIST\"],\n",
    "                                }\n",
    "                            )\n",
    "                            count += 1\n",
    "\n",
    "            if count > 0:\n",
    "                print(f\"    {name}: {count} passages\")\n",
    "            else:\n",
    "                print(f\"    {name}: 0 (format issue)\")\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"    {name}: timeout\")\n",
    "        except Exception as e:\n",
    "            print(f\"    {name}: {type(e).__name__}: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "        cache_to_drive(cache_file, quiet=True)\n",
    "\n",
    "    print(f\"  CBETA Buddhist: {len(passages):,} passages total\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# GREEK/LATIN - Perseus Digital Library (VERIFIED)\n",
    "# https://github.com/PerseusDL/canonical-greekLit\n",
    "# https://github.com/PerseusDL/canonical-latinLit\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_perseus_github() -> list[dict]:\n",
    "    \"\"\"Load Greek and Latin philosophy from Perseus Digital Library.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"perseus.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            if passages and \"time_periods\" in passages[0]:\n",
    "                print(f\"  Perseus: {len(passages):,} passages (cached)\")\n",
    "                return passages\n",
    "            print(\"  Perseus cache missing time_periods - rebuilding...\")\n",
    "            passages = []\n",
    "\n",
    "    print(\"  Fetching from Perseus GitHub...\")\n",
    "\n",
    "    # Key philosophical texts\n",
    "    # Format: (author_id, work_id, name, period)\n",
    "    # URL pattern: /data/{author_id}/{work_id}/{author_id}.{work_id}.perseus-{lang}2.xml\n",
    "    greek_texts = [\n",
    "        (\"tlg0059\", \"tlg030\", \"Plato Republic\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0059\", \"tlg031\", \"Plato Laws\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0059\", \"tlg017\", \"Plato Apology\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0059\", \"tlg004\", \"Plato Symposium\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0059\", \"tlg003\", \"Plato Phaedo\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0086\", \"tlg010\", \"Aristotle Nicomachean Ethics\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0086\", \"tlg028\", \"Aristotle Politics\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0086\", \"tlg035\", \"Aristotle Rhetoric\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0555\", \"tlg001\", \"Epictetus Discourses\", \"HELLENISTIC\"),\n",
    "        (\"tlg0555\", \"tlg002\", \"Epictetus Enchiridion\", \"HELLENISTIC\"),\n",
    "        (\"tlg0562\", \"tlg001\", \"Marcus Aurelius Meditations\", \"HELLENISTIC\"),\n",
    "    ]\n",
    "\n",
    "    latin_texts = [\n",
    "        (\"phi0474\", \"phi038\", \"Cicero De Officiis\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0474\", \"phi044\", \"Cicero Tusculan Disputations\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0474\", \"phi019\", \"Cicero De Finibus\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0690\", \"phi003\", \"Seneca Epistles\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0690\", \"phi001\", \"Seneca De Beneficiis\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0959\", \"phi001\", \"Lucretius De Rerum Natura\", \"CLASSICAL_LATIN\"),\n",
    "    ]\n",
    "\n",
    "    def fetch_perseus_text(author_id, work_id, name, period, lang_code):\n",
    "        \"\"\"Fetch text from Perseus using correct URL pattern.\"\"\"\n",
    "        results = []\n",
    "        repo = \"greekLit\" if lang_code == \"grc\" else \"latinLit\"\n",
    "        text_id = f\"{author_id}.{work_id}\"\n",
    "\n",
    "        # Try multiple filename patterns\n",
    "        patterns = [\n",
    "            f\"{text_id}.perseus-{lang_code}2.xml\",  # Most common: tlg0059.tlg030.perseus-grc2.xml\n",
    "            f\"{text_id}.perseus-{lang_code}1.xml\",\n",
    "            f\"{text_id}.{lang_code}1.xml\",\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            try:\n",
    "                url = f\"https://raw.githubusercontent.com/PerseusDL/canonical-{repo}/master/data/{author_id}/{work_id}/{pattern}\"\n",
    "                GITHUB_LIMITER.wait()\n",
    "                resp = ctext_session.get(url, timeout=60)\n",
    "                if resp.status_code == 200:\n",
    "                    import re\n",
    "\n",
    "                    # Extract text between tags, remove markup\n",
    "                    text_content = re.sub(r\"<[^>]+>\", \" \", resp.text)\n",
    "                    text_content = re.sub(r\"\\s+\", \" \", text_content).strip()\n",
    "\n",
    "                    # Split into chunks of ~500 chars\n",
    "                    chunks = []\n",
    "                    words = text_content.split()\n",
    "                    current = []\n",
    "                    current_len = 0\n",
    "                    for word in words:\n",
    "                        current.append(word)\n",
    "                        current_len += len(word) + 1\n",
    "                        if current_len > 400:\n",
    "                            chunks.append(\" \".join(current))\n",
    "                            current = []\n",
    "                            current_len = 0\n",
    "                    if current:\n",
    "                        chunks.append(\" \".join(current))\n",
    "\n",
    "                    lang = \"greek\" if lang_code == \"grc\" else \"latin\"\n",
    "                    for i, chunk in enumerate(chunks[:500]):  # Limit per text\n",
    "                        if len(chunk) > 50:\n",
    "                            results.append(\n",
    "                                {\n",
    "                                    \"id\": f\"perseus_{text_id}_{i}\",\n",
    "                                    \"text\": chunk,\n",
    "                                    \"language\": lang,\n",
    "                                    \"source\": name,\n",
    "                                    \"time_periods\": [period],\n",
    "                                }\n",
    "                            )\n",
    "                    return results  # Success, stop trying patterns\n",
    "            except Exception:\n",
    "                continue\n",
    "        return results\n",
    "\n",
    "    # Fetch Greek texts\n",
    "    for author_id, work_id, name, period in greek_texts:\n",
    "        result = fetch_perseus_text(author_id, work_id, name, period, \"grc\")\n",
    "        passages.extend(result)\n",
    "        if result:\n",
    "            print(f\"    {name}: {len(result)} passages\")\n",
    "\n",
    "    # Fetch Latin texts\n",
    "    for author_id, work_id, name, period in latin_texts:\n",
    "        result = fetch_perseus_text(author_id, work_id, name, period, \"lat\")\n",
    "        passages.extend(result)\n",
    "        if result:\n",
    "            print(f\"    {name}: {len(result)} passages\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "        cache_to_drive(cache_file, quiet=True)\n",
    "\n",
    "    print(f\"  Perseus: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# WESTERN PHILOSOPHY - Project Gutenberg (direct download by ID)\n",
    "# Like R's gutenbergr::gutenberg_download() - just give it a list of IDs\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_gutenberg_philosophy(target_passages: int = 5000) -> list[dict]:\n",
    "    \"\"\"Load Western philosophy classics from Project Gutenberg by ID.\n",
    "\n",
    "    Uses gutenberg_download(id) like R's gutenbergr package - just give it IDs.\n",
    "    JIT loading: fetches texts one at a time, caches individually.\n",
    "\n",
    "    Args:\n",
    "        target_passages: Stop fetching after reaching this many passages.\n",
    "                        Set to 0 for unlimited (fetch all texts).\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    cache_dir = CACHE_DIR / \"gutenberg_texts\"\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Check for combined cache first (legacy)\n",
    "    legacy_cache = CACHE_DIR / \"gutenberg.json\"\n",
    "    if legacy_cache.exists():\n",
    "        with open(legacy_cache, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Gutenberg: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Fetching from GITenberg mirrors (JIT)...\")\n",
    "\n",
    "    # Western Philosophy texts by Gutenberg ID\n",
    "    # Format: (gutenberg_id, title, period)\n",
    "    texts = [\n",
    "        # Kant\n",
    "        (5683, \"Kant Critique of Practical Reason\", \"MODERN_ETHICS\"),\n",
    "        (5684, \"Kant Metaphysical Elements of Ethics\", \"MODERN_ETHICS\"),\n",
    "        (4280, \"Kant Critique of Pure Reason\", \"MODERN_ETHICS\"),\n",
    "        # Mill\n",
    "        (11224, \"Mill Utilitarianism\", \"MODERN_ETHICS\"),\n",
    "        (34901, \"Mill On Liberty\", \"MODERN_ETHICS\"),\n",
    "        # Spinoza\n",
    "        (3800, \"Spinoza Ethics\", \"MODERN_ETHICS\"),\n",
    "        # Aristotle\n",
    "        (8438, \"Aristotle Nicomachean Ethics\", \"CLASSICAL_GREEK\"),\n",
    "        # Plato\n",
    "        (1497, \"Plato Republic\", \"CLASSICAL_GREEK\"),\n",
    "        (1656, \"Plato Apology\", \"CLASSICAL_GREEK\"),\n",
    "        # Stoics\n",
    "        (10661, \"Epictetus Discourses\", \"HELLENISTIC\"),\n",
    "        (2680, \"Marcus Aurelius Meditations\", \"HELLENISTIC\"),\n",
    "        # New Testament & Apocrypha (KJV) - Christian ethics\n",
    "        (10, \"Bible KJV Complete\", \"BIBLICAL_CHRISTIAN\"),  # Complete Bible (80 books)\n",
    "        (\n",
    "            124,\n",
    "            \"Apocrypha Deuterocanonical\",\n",
    "            \"APOCRYPHA\",\n",
    "        ),  # Tobit, Judith, Wisdom, Sirach, Maccabees\n",
    "        # Catechisms - Christian doctrine/ethics\n",
    "        (1670, \"Luther Small Catechism\", \"REFORMATION\"),\n",
    "        (1722, \"Luther Large Catechism\", \"REFORMATION\"),\n",
    "        # American practical ethics\n",
    "    ]\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; BIP-Corpus/1.0)\"}\n",
    "\n",
    "    def gutenberg_download(gutenberg_id: int) -> str | None:\n",
    "        \"\"\"Download text from Project Gutenberg by ID. Like R's gutenbergr::gutenberg_download().\"\"\"\n",
    "        # Primary: direct UTF-8 URL (works from most locations)\n",
    "        urls = [\n",
    "            f\"https://www.gutenberg.org/ebooks/{gutenberg_id}.txt.utf-8\",\n",
    "            f\"https://www.gutenberg.org/cache/epub/{gutenberg_id}/pg{gutenberg_id}.txt\",\n",
    "        ]\n",
    "        for url in urls:\n",
    "            try:\n",
    "                r = ctext_session.get(url, headers=headers, timeout=60)\n",
    "                if r.status_code == 200:\n",
    "                    return r.text\n",
    "            except Exception:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "    def extract_passages(content: str, guten_id: int, name: str, period: str) -> list[dict]:\n",
    "        \"\"\"Extract paragraphs from Gutenberg text content.\"\"\"\n",
    "        results = []\n",
    "        # Normalize line endings (Gutenberg uses \\r\\n)\n",
    "        content = content.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "        # Skip Gutenberg header/footer\n",
    "        start_marker = \"*** START OF\"\n",
    "        end_marker = \"*** END OF\"\n",
    "        start_idx = content.find(start_marker)\n",
    "        end_idx = content.find(end_marker)\n",
    "        if start_idx > 0:\n",
    "            content = content[start_idx:]\n",
    "            newline_idx = content.find(\"\\n\")\n",
    "            if newline_idx > 0:\n",
    "                content = content[newline_idx + 1 :]\n",
    "        if end_idx > 0 and start_idx > 0:\n",
    "            content = content[: end_idx - start_idx - 100]\n",
    "        elif end_idx > 0:\n",
    "            content = content[:end_idx]\n",
    "\n",
    "        # Split into paragraphs\n",
    "        paras = content.split(\"\\n\\n\")\n",
    "        count = 0\n",
    "        for para in paras:\n",
    "            para = para.strip().replace(\"\\n\", \" \")\n",
    "            para = \" \".join(para.split())\n",
    "            if len(para) > 100 and len(para) < 2000:\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"id\": f\"gutenberg_{guten_id}_{count}\",\n",
    "                        \"text\": para,\n",
    "                        \"language\": \"english\",\n",
    "                        \"source\": name,\n",
    "                        \"time_periods\": [period],\n",
    "                    }\n",
    "                )\n",
    "                count += 1\n",
    "        return results\n",
    "\n",
    "    # JIT loading: fetch texts one at a time, stop when we have enough\n",
    "    for guten_id, name, period in texts:\n",
    "        # Check individual cache first\n",
    "        text_cache = cache_dir / f\"{guten_id}.json\"\n",
    "        if text_cache.exists():\n",
    "            with open(text_cache, encoding=\"utf-8\") as f:\n",
    "                text_passages = json.load(f)\n",
    "                passages.extend(text_passages)\n",
    "                print(f\"    {name}: {len(text_passages):,} passages (cached)\")\n",
    "        else:\n",
    "            # Download by ID (like R's gutenbergr::gutenberg_download)\n",
    "            time.sleep(0.3)  # Rate limit\n",
    "            content = gutenberg_download(guten_id)\n",
    "\n",
    "            if content:\n",
    "                text_passages = extract_passages(content, guten_id, name, period)\n",
    "                if text_passages:\n",
    "                    # Cache this text individually\n",
    "                    with open(text_cache, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(text_passages, f, ensure_ascii=False)\n",
    "                    passages.extend(text_passages)\n",
    "                    print(f\"    {name}: {len(text_passages):,} passages\")\n",
    "                else:\n",
    "                    print(f\"    {name}: no passages extracted\")\n",
    "            else:\n",
    "                print(f\"    {name}: download failed (ID {guten_id})\")\n",
    "\n",
    "        # JIT early stop: if we have enough, stop fetching\n",
    "        if target_passages > 0 and len(passages) >= target_passages:\n",
    "            print(f\"    (reached {target_passages:,} target, stopping)\")\n",
    "            break\n",
    "\n",
    "    print(f\"  Gutenberg: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# NATIVE AMERICAN & WORLD FOLKLORE - HuggingFace (VERIFIED)\n",
    "# Source: merve/folk-mythology-tales (246K stories from Ashliman Folktexts)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_folk_mythology() -> list[dict]:\n",
    "    \"\"\"Load folk tales and mythology including Native American from HuggingFace.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"folk_mythology.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Folk/Mythology: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Loading folk-mythology-tales from HuggingFace...\")\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "\n",
    "        ds = load_dataset(\"merve/folk-mythology-tales\", split=\"train\")\n",
    "\n",
    "        for i, item in enumerate(ds):\n",
    "            text = item.get(\"text\", \"\")\n",
    "            if text and len(text) > 50:\n",
    "                passages.append(\n",
    "                    {\n",
    "                        \"id\": f\"folk_{i}\",\n",
    "                        \"text\": text.strip()[:2000],  # Limit length\n",
    "                        \"language\": \"english\",\n",
    "                        \"source\": \"Ashliman Folktexts\",\n",
    "                        \"time_periods\": [\"FOLKLORE\", \"TRADITIONAL\"],\n",
    "                    }\n",
    "                )\n",
    "                if len(passages) >= 50000:  # Limit total\n",
    "                    break\n",
    "        print(f\"    Loaded {len(passages):,} folk tales\")\n",
    "    except ImportError:\n",
    "        print(\"    datasets library not available, skipping\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "        cache_to_drive(cache_file, quiet=True)\n",
    "\n",
    "    print(f\"  Folk/Mythology: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ROMANCE LANGUAGE PHILOSOPHY - Project Gutenberg (direct download by ID)\n",
    "# Spanish: Don Quixote, La Celestina, Lazarillo de Tormes\n",
    "# French: Montaigne, Voltaire, Rousseau, Pascal\n",
    "# Italian: Machiavelli, Dante, Boccaccio | Portuguese: Cames\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_romance_philosophy(target_passages: int = 10000) -> list[dict]:\n",
    "    \"\"\"Load Romance language philosophy from Project Gutenberg by ID.\n",
    "\n",
    "    Uses gutenberg_download(id) like R's gutenbergr package.\n",
    "    JIT loading: fetches texts one at a time, caches individually.\n",
    "\n",
    "    Args:\n",
    "        target_passages: Stop fetching after reaching this many passages.\n",
    "                        Set to 0 for unlimited (fetch all texts).\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    cache_dir = CACHE_DIR / \"romance_texts\"\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Check for legacy combined cache\n",
    "    legacy_cache = CACHE_DIR / \"romance_philosophy.json\"\n",
    "    if legacy_cache.exists():\n",
    "        with open(legacy_cache, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Romance Philosophy: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Fetching Romance philosophy (JIT)...\")\n",
    "\n",
    "    # Romance language texts by Gutenberg ID\n",
    "    # Format: (gutenberg_id, title, language, period)\n",
    "    texts = [\n",
    "        # Spanish\n",
    "        (996, \"Don Quixote\", \"spanish\", \"SPANISH_GOLDEN_AGE\"),\n",
    "        (1619, \"La Celestina\", \"spanish\", \"SPANISH_GOLDEN_AGE\"),\n",
    "        (320, \"Lazarillo de Tormes\", \"spanish\", \"SPANISH_GOLDEN_AGE\"),\n",
    "        # French\n",
    "        (19942, \"Candide (Voltaire)\", \"french\", \"FRENCH_ENLIGHTENMENT\"),\n",
    "        (46333, \"Social Contract (Rousseau)\", \"french\", \"FRENCH_ENLIGHTENMENT\"),\n",
    "        (3600, \"Essais de Montaigne\", \"french\", \"FRENCH_RENAISSANCE\"),\n",
    "        (18269, \"Penses (Pascal)\", \"french\", \"FRENCH_ENLIGHTENMENT\"),\n",
    "        # Italian\n",
    "        (1232, \"The Prince (Machiavelli)\", \"italian\", \"ITALIAN_RENAISSANCE\"),\n",
    "        (1004, \"Divine Comedy (Dante)\", \"italian\", \"MEDIEVAL_ITALIAN\"),\n",
    "        (3726, \"Decameron Vol I (Boccaccio)\", \"italian\", \"MEDIEVAL_ITALIAN\"),\n",
    "        (13102, \"Decameron Vol II (Boccaccio)\", \"italian\", \"MEDIEVAL_ITALIAN\"),\n",
    "        # Portuguese\n",
    "        (3333, \"Os Lusadas (Cames)\", \"portuguese\", \"PORTUGUESE_RENAISSANCE\"),\n",
    "    ]\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; BIP-Corpus/1.0)\"}\n",
    "\n",
    "    def gutenberg_download(gutenberg_id: int) -> str | None:\n",
    "        \"\"\"Download text from Project Gutenberg by ID.\"\"\"\n",
    "        urls = [\n",
    "            f\"https://www.gutenberg.org/ebooks/{gutenberg_id}.txt.utf-8\",\n",
    "            f\"https://www.gutenberg.org/cache/epub/{gutenberg_id}/pg{gutenberg_id}.txt\",\n",
    "        ]\n",
    "        for url in urls:\n",
    "            try:\n",
    "                r = ctext_session.get(url, headers=headers, timeout=60)\n",
    "                if r.status_code == 200:\n",
    "                    return r.text\n",
    "            except Exception:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "    def extract_passages(\n",
    "        content: str, guten_id: int, name: str, lang: str, period: str\n",
    "    ) -> list[dict]:\n",
    "        \"\"\"Extract paragraphs from Gutenberg text content.\"\"\"\n",
    "        results = []\n",
    "        # Normalize line endings (Gutenberg uses \\r\\n)\n",
    "        content = content.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "        # Skip Gutenberg header/footer\n",
    "        start_marker = \"*** START OF\"\n",
    "        end_marker = \"*** END OF\"\n",
    "        start_idx = content.find(start_marker)\n",
    "        end_idx = content.find(end_marker)\n",
    "        if start_idx > 0:\n",
    "            content = content[start_idx:]\n",
    "            nl = content.find(\"\\n\")\n",
    "            if nl > 0:\n",
    "                content = content[nl + 1 :]\n",
    "        if end_idx > 0 and start_idx > 0:\n",
    "            content = content[: end_idx - start_idx - 100]\n",
    "        elif end_idx > 0:\n",
    "            content = content[:end_idx]\n",
    "\n",
    "        # Split into paragraphs\n",
    "        paras = content.split(\"\\n\\n\")\n",
    "        count = 0\n",
    "        for para in paras:\n",
    "            para = para.strip().replace(\"\\n\", \" \")\n",
    "            para = \" \".join(para.split())\n",
    "            if len(para) > 100 and len(para) < 2000:\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"id\": f\"romance_{guten_id}_{count}\",\n",
    "                        \"text\": para,\n",
    "                        \"language\": lang,\n",
    "                        \"source\": name,\n",
    "                        \"time_periods\": [period],\n",
    "                    }\n",
    "                )\n",
    "                count += 1\n",
    "        return results\n",
    "\n",
    "    # JIT loading: fetch texts one at a time\n",
    "    for guten_id, name, lang, period in texts:\n",
    "        text_cache = cache_dir / f\"{guten_id}.json\"\n",
    "        if text_cache.exists():\n",
    "            with open(text_cache, encoding=\"utf-8\") as f:\n",
    "                text_passages = json.load(f)\n",
    "                passages.extend(text_passages)\n",
    "                print(f\"    {name}: {len(text_passages):,} passages (cached)\")\n",
    "        else:\n",
    "            # Download by ID (like R's gutenbergr::gutenberg_download)\n",
    "            time.sleep(0.3)  # Rate limit\n",
    "            content = gutenberg_download(guten_id)\n",
    "\n",
    "            if content:\n",
    "                text_passages = extract_passages(content, guten_id, name, lang, period)\n",
    "                if text_passages:\n",
    "                    with open(text_cache, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(text_passages, f, ensure_ascii=False)\n",
    "                    passages.extend(text_passages)\n",
    "                    print(f\"    {name}: {len(text_passages):,} passages\")\n",
    "                else:\n",
    "                    print(f\"    {name}: no passages extracted\")\n",
    "            else:\n",
    "                print(f\"    {name}: download failed (ID {guten_id})\")\n",
    "\n",
    "        # JIT early stop\n",
    "        if target_passages > 0 and len(passages) >= target_passages:\n",
    "            print(f\"    (reached {target_passages:,} target, stopping)\")\n",
    "            break\n",
    "\n",
    "    print(f\"  Romance Philosophy: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ENGLISH ETHICS - Dear Abby (68K letters)\n",
    "# Source: https://github.com/Mac-STAT/data (VERIFIED)\n",
    "# ============================================================================\n",
    "\n",
    "DEAR_ABBY_URL = \"https://raw.githubusercontent.com/Mac-STAT/data/main/dear_abby.csv\"\n",
    "\n",
    "\n",
    "def load_dear_abby() -> list[dict]:\n",
    "    \"\"\"Load Dear Abby advice columns (68K letters) from Mac-STAT GitHub.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"dear_abby.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Dear Abby: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    # Check local file first\n",
    "    local_paths = [\n",
    "        Path(\"data/raw/dear_abby.csv\"),\n",
    "        DATA_DIR / \"dear_abby.csv\",\n",
    "    ]\n",
    "\n",
    "    csv_file = None\n",
    "    for p in local_paths:\n",
    "        if p.exists():\n",
    "            csv_file = p\n",
    "            print(f\"  Found local: {csv_file}\")\n",
    "            break\n",
    "\n",
    "    # Download if not local\n",
    "    if not csv_file:\n",
    "        print(\"  Downloading Dear Abby from GitHub (17.9MB)...\")\n",
    "        csv_file = DATA_DIR / \"dear_abby.csv\"\n",
    "        try:\n",
    "            resp = requests.get(DEAR_ABBY_URL, timeout=120)\n",
    "            if resp.status_code == 200:\n",
    "                csv_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                with open(csv_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(resp.text)\n",
    "                print(f\"    Downloaded to {csv_file}\")\n",
    "            else:\n",
    "                print(f\"    Failed: HTTP {resp.status_code}\")\n",
    "                return passages\n",
    "        except Exception as e:\n",
    "            print(f\"    Download failed: {e}\")\n",
    "            return passages\n",
    "\n",
    "    # Parse CSV using pandas for better handling of multi-line fields\n",
    "    print(\"  Parsing Dear Abby CSV...\")\n",
    "    try:\n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.read_csv(csv_file, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "        print(f\"    CSV has {len(df):,} rows, columns: {list(df.columns)}\")\n",
    "\n",
    "        skipped_empty = 0\n",
    "        skipped_short = 0\n",
    "        for _, row in df.iterrows():\n",
    "            # Primary column is \"question_only\"\n",
    "            text = str(row.get(\"question_only\", \"\"))\n",
    "            if not text or text == \"nan\" or pd.isna(row.get(\"question_only\")):\n",
    "                skipped_empty += 1\n",
    "                continue\n",
    "            if len(text) <= 50:\n",
    "                skipped_short += 1\n",
    "                continue\n",
    "            year = row.get(\"year\", \"\")\n",
    "            passages.append(\n",
    "                {\n",
    "                    \"id\": f\"abby_{row.get('letterId', len(passages))}\",\n",
    "                    \"text\": text.strip(),\n",
    "                    \"language\": \"english\",\n",
    "                    \"source\": f\"Dear Abby {year}\",\n",
    "                    \"time_periods\": [\"DEAR_ABBY\", \"MODERN\", \"ENGLISH_ETHICS\"],\n",
    "                }\n",
    "            )\n",
    "        print(\n",
    "            f\"    Loaded {len(passages):,} letters (skipped: {skipped_empty} empty, {skipped_short} short)\"\n",
    "        )\n",
    "    except ImportError:\n",
    "        # Fallback to csv module if pandas not available\n",
    "        print(\"    pandas not available, using csv module\")\n",
    "        with open(csv_file, encoding=\"utf-8\", newline=\"\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                text = row.get(\"question_only\", \"\")\n",
    "                if text and len(text) > 50:\n",
    "                    passages.append(\n",
    "                        {\n",
    "                            \"id\": f\"abby_{row.get('letterId', len(passages))}\",\n",
    "                            \"text\": text.strip(),\n",
    "                            \"language\": \"english\",\n",
    "                            \"source\": f\"Dear Abby {row.get('year', '')}\",\n",
    "                            \"time_periods\": [\"DEAR_ABBY\", \"MODERN\", \"ENGLISH_ETHICS\"],\n",
    "                        }\n",
    "                    )\n",
    "        print(f\"    Loaded {len(passages):,} letters\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed to parse CSV: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "        cache_to_drive(cache_file, quiet=True)\n",
    "\n",
    "    print(f\"  Dear Abby: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ENGLISH ETHICS - hendrycks/ethics (134K examples) - SUPPLEMENTAL\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_hendrycks_ethics() -> list[dict]:\n",
    "    \"\"\"Load ethics scenarios from hendrycks/ethics dataset (supplemental).\n",
    "\n",
    "    Downloads directly from Berkeley (HuggingFace loader is deprecated).\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"hendrycks_ethics.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  hendrycks/ethics: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Downloading hendrycks/ethics from Berkeley...\")\n",
    "\n",
    "    import io\n",
    "    import tarfile\n",
    "\n",
    "    TAR_URL = \"https://people.eecs.berkeley.edu/~hendrycks/ethics.tar\"\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(TAR_URL, timeout=120)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"    Download failed: {e}\")\n",
    "        return passages\n",
    "\n",
    "    # Extract CSVs from tar\n",
    "    subsets = {\n",
    "        \"commonsense\": (\"cm_train.csv\", \"input\"),\n",
    "        \"deontology\": (\"deontology_train.csv\", \"scenario\"),\n",
    "        \"justice\": (\"justice_train.csv\", \"scenario\"),\n",
    "        \"utilitarianism\": (\"util_train.csv\", \"baseline\"),  # Note: different name\n",
    "        \"virtue\": (\"virtue_train.csv\", \"scenario\"),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        tar_bytes = io.BytesIO(resp.content)\n",
    "        with tarfile.open(fileobj=tar_bytes, mode=\"r:\") as tar:\n",
    "            for subset, (filename, text_col) in subsets.items():\n",
    "                # Find the file in the archive (may be in subdirectory)\n",
    "                csv_member = None\n",
    "                for member in tar.getmembers():\n",
    "                    if member.name.endswith(filename):\n",
    "                        csv_member = member\n",
    "                        break\n",
    "\n",
    "                if not csv_member:\n",
    "                    print(f\"    {subset}: file not found ({filename})\")\n",
    "                    continue\n",
    "\n",
    "                # Extract and parse CSV\n",
    "                csv_file = tar.extractfile(csv_member)\n",
    "                if csv_file is None:\n",
    "                    continue\n",
    "\n",
    "                csv_content = csv_file.read().decode(\"utf-8\")\n",
    "                reader = csv.DictReader(io.StringIO(csv_content))\n",
    "\n",
    "                count = 0\n",
    "                for row in reader:\n",
    "                    text = row.get(text_col, \"\")\n",
    "                    if text and len(text) > 30:\n",
    "                        passages.append(\n",
    "                            {\n",
    "                                \"id\": f\"ethics_{subset}_{count}\",\n",
    "                                \"text\": text.strip(),\n",
    "                                \"language\": \"english\",\n",
    "                                \"source\": f\"hendrycks/ethics/{subset}\",\n",
    "                                \"time_periods\": [\"MODERN_ETHICS\", \"MODERN\", \"ENGLISH_ETHICS\"],\n",
    "                            }\n",
    "                        )\n",
    "                        count += 1\n",
    "\n",
    "                print(f\"    {subset}: {count} passages\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    Tar extraction failed: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "        cache_to_drive(cache_file, quiet=True)\n",
    "\n",
    "    print(f\"  hendrycks/ethics: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN LOADER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Fetching from verified external sources...\")\n",
    "print(f\"Cache only mode: {_cache_only}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "by_language = defaultdict(list)\n",
    "\n",
    "# Load all sources\n",
    "print(\"\\n[SANSKRIT]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"sanskrit\"].extend(load_itihasa_github())\n",
    "print(f\"  Elapsed: {time.time() - _t0:.1f}s\")\n",
    "\n",
    "print(\"\\n[PALI]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"pali\"].extend(load_pali_suttacentral())\n",
    "print(f\"  Elapsed: {time.time() - _t0:.1f}s\")\n",
    "\n",
    "print(\"\\n[ARABIC]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"arabic\"].extend(load_quran_tanzil())\n",
    "print(f\"  Elapsed: {time.time() - _t0:.1f}s\")\n",
    "\n",
    "print(\"\\n[HEBREW/ARAMAIC]\")\n",
    "_t0 = time.time()\n",
    "sefaria = load_sefaria_github()\n",
    "print(f\"  Elapsed: {time.time() - _t0:.1f}s\")\n",
    "for p in sefaria:\n",
    "    by_language[p[\"language\"]].append(p)\n",
    "\n",
    "print(\"\\n[CHINESE]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"classical_chinese\"].extend(load_chinese_ctext())\n",
    "print(f\"  Elapsed: {time.time() - _t0:.1f}s\")\n",
    "print(\"\\n[CHINESE BUDDHIST]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"classical_chinese\"].extend(load_chinese_buddhist())\n",
    "print(f\"  Elapsed: {time.time() - _t0:.1f}s\")  # CBETA Buddhist\n",
    "\n",
    "print(\"\\n[GREEK/LATIN]\")\n",
    "_t0 = time.time()\n",
    "perseus = load_perseus_github()\n",
    "print(f\"  Elapsed: {time.time() - _t0:.1f}s\")\n",
    "for p in perseus:\n",
    "    by_language[p[\"language\"]].append(p)\n",
    "\n",
    "print(\"\\n[WESTERN PHILOSOPHY]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"english\"].extend(load_gutenberg_philosophy())\n",
    "print(f\"  Elapsed: {time.time() - _t0:.1f}s\")\n",
    "\n",
    "print(\"\\n[ROMANCE LANGUAGES]\")\n",
    "_t0 = time.time()\n",
    "romance = load_romance_philosophy()\n",
    "print(f\"  Elapsed: {time.time() - _t0:.1f}s\")\n",
    "for p in romance:\n",
    "    by_language[p[\"language\"]].append(p)\n",
    "\n",
    "print(\"\\n[ENGLISH ETHICS]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"english\"].extend(load_dear_abby())\n",
    "by_language[\"english\"].extend(load_hendrycks_ethics())\n",
    "print(f\"  Elapsed: {time.time() - _t0:.1f}s\")  # Supplemental\n",
    "\n",
    "print(\"\\n[FOLKLORE/NATIVE AMERICAN]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"english\"].extend(load_folk_mythology())\n",
    "print(f\"  Elapsed: {time.time() - _t0:.1f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# APPLY MEMORY LIMITS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Applying memory limits...\")\n",
    "for lang in list(by_language.keys()):\n",
    "    max_count = MAX_PASSAGES_PER_LANG.get(lang, MAX_PASSAGES_PER_LANG[\"default\"])\n",
    "    if len(by_language[lang]) > max_count:\n",
    "        original = len(by_language[lang])\n",
    "        by_language[lang] = by_language[lang][:max_count]\n",
    "        print(f\"  {lang}: {original:,} -> {max_count:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CORPUS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total = 0\n",
    "for lang, passages in sorted(by_language.items(), key=lambda x: -len(x[1])):\n",
    "    count = len(passages)\n",
    "    total += count\n",
    "    status = \"OK\" if count >= MIN_PASSAGES else f\"NEED {MIN_PASSAGES - count} MORE\"\n",
    "    print(f\"  {lang:20s}: {count:6,} passages  [{status}]\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"  {'TOTAL':20s}: {total:6,} passages\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE TO JSONL FOR LATER CELLS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nConverting to training format...\")\n",
    "\n",
    "all_passages = []\n",
    "for _lang, passages in by_language.items():\n",
    "    for p in passages:\n",
    "        all_passages.append(\n",
    "            {\n",
    "                \"id\": p[\"id\"],\n",
    "                \"text\": p[\"text\"],\n",
    "                \"language\": p[\"language\"],\n",
    "                \"source\": p[\"source\"],\n",
    "                \"time_periods\": p.get(\"time_periods\", [p.get(\"time_period\", \"UNKNOWN\")]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "with open(\"data/processed/passages.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in all_passages:\n",
    "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "print(\"Saved to data/processed/passages.jsonl\")\n",
    "\n",
    "# Cache to Drive if available\n",
    "if _save_dir and os.path.exists(os.path.dirname(_save_dir)):\n",
    "    os.makedirs(_save_dir, exist_ok=True)\n",
    "    import shutil\n",
    "\n",
    "    shutil.copy(\"data/processed/passages.jsonl\", f\"{_save_dir}/passages.jsonl\")\n",
    "    print(f\"Cached to {_save_dir}/passages.jsonl\")\n",
    "\n",
    "    # Also cache the corpus cache files to Drive for faster restarts\n",
    "    drive_cache = Path(_save_dir) / \"corpus_cache\"\n",
    "    drive_cache.mkdir(exist_ok=True)\n",
    "    cached_count = 0\n",
    "    for cache_file in CACHE_DIR.glob(\"*.json\"):\n",
    "        dest = drive_cache / cache_file.name\n",
    "        if not dest.exists() or dest.stat().st_size != cache_file.stat().st_size:\n",
    "            shutil.copy(cache_file, dest)\n",
    "            cached_count += 1\n",
    "    if cached_count:\n",
    "        print(f\"  Cached {cached_count} corpus files to Drive\")\n",
    "\n",
    "    # Note: sefaria.json (processed output) IS cached to Drive\n",
    "    # Raw Sefaria-Export not cached (50K files), but processed cache restores instantly\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CORPUS LOADING COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c435fce79c7b4513ae84c3b35ce5fe19",
      "59251786ac884384b71d93aed4096cf5",
      "2989d8b4a80f4162953c0873f09f1dbb",
      "252dad12f32c425b9727f269c15d65d7",
      "feab0b58e90c4cdaa1cebad99fdf5833",
      "7bf0fba8898a414cbc080de0c098d9bf",
      "b7f6d2a4ba9146f7b991c75b5693de7c",
      "cf8ec69a56874aa79e5d8bfe043b1bad",
      "107bcd021e7848a9b80e2abcc4c6a57b",
      "3e6f72925cb9422499b96ecdfde2cd31",
      "762abf6ce3694a2ebb858b0bf94f5bfb",
      "5ba432aa3bca4e58921a49521b77f596",
      "89ee238c4c5c4d35b94f9dc7cbb1eec5",
      "b5a15eb0b7ff42028d9e2f7d97277f56",
      "7732e9d9252e48f58b9e0c3a184eecc2",
      "0caa2d8c86ee4f6bb34dd56f52f1d951",
      "4988f81b16384e46919f9ef5bc8ff76d",
      "eb0c414ac87a4103aa872ea75d808f42",
      "afa153ecdca04fa2ae19bae7b91df118",
      "76bcb4eb538b42ecaeaa621ed65421ef",
      "65a06915e58e423ebe7a57f3bf377a4e",
      "ee53c53e81ac44daa0677cca9b7b182d"
     ]
    },
    "id": "cell_3",
    "outputId": "bff947ed-f73d-4187-c7d4-8bf9b058b01c"
   },
   "outputs": [],
   "source": [
    "# @title 3. Load Ethics Datasets for Bond Extraction { display-mode: \"form\" }\n",
    "# @markdown Load ETHICS, Scruples, and EthicsSuite datasets for bond extraction training\n",
    "# @markdown These provide modern English moral reasoning examples with labeled judgments\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ### Dataset Selection\n",
    "LOAD_ETHICS_DATASET = True  # @param {type:\"boolean\"}\n",
    "# @markdown hendrycks/ethics: Justice, deontology, virtue, utilitarianism, commonsense\n",
    "\n",
    "LOAD_SCRUPLES_DATASET = True  # @param {type:\"boolean\"}\n",
    "# @markdown allenai/scruples: 32K real-life anecdotes with ethical judgments\n",
    "\n",
    "LOAD_ETHICSUITE_DATASET = True  # @param {type:\"boolean\"}\n",
    "# @markdown LLM-Ethics/EthicsSuite: 20K complex contextualized moral situations\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ### Size Limits (0 = unlimited)\n",
    "MAX_ETHICS_ITEMS = 50000  # @param {type:\"integer\"}\n",
    "MAX_SCRUPLES_ITEMS = 30000  # @param {type:\"integer\"}\n",
    "MAX_ETHICSUITE_ITEMS = 20000  # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ### Output Options\n",
    "EXPORT_BIP_FORMAT = True  # @param {type:\"boolean\"}\n",
    "# @markdown Export as BIP passages for integration with Cell 2 corpus\n",
    "\n",
    "CREATE_TRAIN_TEST_SPLIT = True  # @param {type:\"boolean\"}\n",
    "TEST_SPLIT_RATIO = 0.2  # @param {type:\"number\"}\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BOND EXTRACTION TRAINING DATA (v10.14)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BOND SCHEMA\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BondAnnotation:\n",
    "    \"\"\"A moral bond extracted from text.\"\"\"\n",
    "\n",
    "    text: str\n",
    "    agent: str | None\n",
    "    patient: str | None\n",
    "    bond_type: str\n",
    "    hohfeld_state: str\n",
    "    context: str\n",
    "    confidence: float\n",
    "    source_dataset: str\n",
    "    source_category: str\n",
    "    raw_label: str\n",
    "\n",
    "\n",
    "BOND_TYPES = [\n",
    "    \"OBLIGATION\",\n",
    "    \"PROHIBITION\",\n",
    "    \"PERMISSION\",\n",
    "    \"CLAIM\",\n",
    "    \"POWER\",\n",
    "    \"IMMUNITY\",\n",
    "    \"VIRTUE\",\n",
    "    \"VICE\",\n",
    "    \"SUPEREROGATORY\",\n",
    "]\n",
    "\n",
    "HOHFELD_STATES = [\n",
    "    \"DUTY\",\n",
    "    \"CLAIM\",\n",
    "    \"LIBERTY\",\n",
    "    \"NO_CLAIM\",\n",
    "    \"POWER\",\n",
    "    \"LIABILITY\",\n",
    "    \"IMMUNITY\",\n",
    "    \"DISABILITY\",\n",
    "]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ETHICS DATASET LOADER\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class EthicsLoader:\n",
    "    \"\"\"Load hendrycks/ethics dataset.\"\"\"\n",
    "\n",
    "    CATEGORY_TO_BOND = {\n",
    "        \"deontology\": (\"OBLIGATION\", \"DUTY\"),\n",
    "        \"justice\": (\"CLAIM\", \"CLAIM\"),\n",
    "        \"virtue\": (\"VIRTUE\", \"DUTY\"),\n",
    "        \"utilitarianism\": (\"PERMISSION\", \"LIBERTY\"),\n",
    "        \"commonsense\": (\"OBLIGATION\", \"DUTY\"),\n",
    "    }\n",
    "\n",
    "    def load(self, max_items: int = 0) -> list[BondAnnotation]:\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "        except ImportError:\n",
    "            print(\"  Installing datasets library...\")\n",
    "            os.system(\"pip install datasets -q\")\n",
    "            from datasets import load_dataset\n",
    "\n",
    "        annotations = []\n",
    "        categories = [\"commonsense\", \"deontology\", \"justice\", \"utilitarianism\", \"virtue\"]\n",
    "\n",
    "        for category in categories:\n",
    "            if max_items > 0 and len(annotations) >= max_items:\n",
    "                break\n",
    "\n",
    "            print(f\"  Loading ETHICS/{category}...\")\n",
    "            try:\n",
    "                dataset = load_dataset(\"hendrycks/ethics\", category)\n",
    "\n",
    "                for split in [\"train\", \"test\"]:\n",
    "                    if split not in dataset:\n",
    "                        continue\n",
    "                    for item in dataset[split]:\n",
    "                        if max_items > 0 and len(annotations) >= max_items:\n",
    "                            break\n",
    "\n",
    "                        text = item.get(\"input\") or item.get(\"scenario\") or item.get(\"text\", \"\")\n",
    "                        if not text or len(text) < 10:\n",
    "                            continue\n",
    "\n",
    "                        label = item.get(\"label\", 0)\n",
    "                        bond_type, hohfeld = self.CATEGORY_TO_BOND.get(\n",
    "                            category, (\"OBLIGATION\", \"DUTY\")\n",
    "                        )\n",
    "\n",
    "                        # Extract agent/patient\n",
    "                        agent, patient = self._extract_roles(text)\n",
    "\n",
    "                        if label == 1:\n",
    "                            context = \"descriptive\"\n",
    "                            if bond_type == \"OBLIGATION\":\n",
    "                                bond_type = \"PROHIBITION\"\n",
    "                        else:\n",
    "                            context = \"prescriptive\"\n",
    "\n",
    "                        annotations.append(\n",
    "                            BondAnnotation(\n",
    "                                text=text[:500],\n",
    "                                agent=agent,\n",
    "                                patient=patient,\n",
    "                                bond_type=bond_type,\n",
    "                                hohfeld_state=hohfeld,\n",
    "                                context=context,\n",
    "                                confidence=0.8,\n",
    "                                source_dataset=\"ethics\",\n",
    "                                source_category=category,\n",
    "                                raw_label=str(label),\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Warning: {e}\")\n",
    "\n",
    "        return annotations\n",
    "\n",
    "    def _extract_roles(self, text: str) -> tuple[str | None, str | None]:\n",
    "        agent = patient = None\n",
    "\n",
    "        if re.match(r\"^I\\s+(should|must|ought)\", text, re.I):\n",
    "            agent = \"speaker\"\n",
    "        elif re.match(r\"^You\\s+(should|must|ought)\", text, re.I):\n",
    "            agent = \"addressee\"\n",
    "        else:\n",
    "            match = re.match(r\"^([A-Z][a-z]+)\\s+(should|must|ought)\", text)\n",
    "            if match:\n",
    "                agent = match.group(1).lower()\n",
    "\n",
    "        patient_match = re.search(r\"(help|protect|harm|hurt)\\s+(\\w+)\", text, re.I)\n",
    "        if patient_match:\n",
    "            p = patient_match.group(2).lower()\n",
    "            if p not in [\"the\", \"a\", \"an\", \"my\", \"your\"]:\n",
    "                patient = p\n",
    "\n",
    "        return agent, patient\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SCRUPLES DATASET LOADER\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class ScruplesLoader:\n",
    "    \"\"\"Load allenai/scruples dataset.\"\"\"\n",
    "\n",
    "    LABEL_TO_BOND = {\n",
    "        \"AUTHOR_WRONG\": (\"PROHIBITION\", \"DUTY\", \"author\"),\n",
    "        \"OTHER_WRONG\": (\"PROHIBITION\", \"DUTY\", \"other\"),\n",
    "        \"EVERYBODY_WRONG\": (\"PROHIBITION\", \"DUTY\", \"both\"),\n",
    "        \"NOBODY_WRONG\": (\"PERMISSION\", \"LIBERTY\", None),\n",
    "        \"INFO\": (\"OBLIGATION\", \"DUTY\", None),\n",
    "    }\n",
    "\n",
    "    ANECDOTES_URL = \"https://storage.googleapis.com/ai2-mosaic-public/projects/scruples/v1.0/data/anecdotes.tar.gz\"\n",
    "    DILEMMAS_URL = \"https://storage.googleapis.com/ai2-mosaic-public/projects/scruples/v1.0/data/dilemmas.tar.gz\"\n",
    "\n",
    "    def load(self, max_items: int = 0) -> list[BondAnnotation]:\n",
    "        import tarfile\n",
    "\n",
    "        import requests\n",
    "\n",
    "        cache_dir = Path(\"data/ethics_cache\")\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        annotations = []\n",
    "\n",
    "        # Load anecdotes from Google Cloud\n",
    "        print(\"  Loading Scruples/anecdotes...\")\n",
    "        anecdotes_cache = cache_dir / \"scruples_anecdotes.tar.gz\"\n",
    "        try:\n",
    "            if not anecdotes_cache.exists():\n",
    "                print(\"    Downloading from Google Cloud...\")\n",
    "                resp = requests.get(self.ANECDOTES_URL, timeout=120)\n",
    "                resp.raise_for_status()\n",
    "                with open(anecdotes_cache, \"wb\") as f:\n",
    "                    f.write(resp.content)\n",
    "                print(f\"    Downloaded {len(resp.content) // 1024}KB\")\n",
    "\n",
    "            with tarfile.open(anecdotes_cache, \"r:gz\") as tar:\n",
    "                for member in tar.getmembers():\n",
    "                    if not member.name.endswith(\".jsonl\"):\n",
    "                        continue\n",
    "                    f = tar.extractfile(member)\n",
    "                    if f is None:\n",
    "                        continue\n",
    "                    for line in f:\n",
    "                        if max_items > 0 and len(annotations) >= max_items:\n",
    "                            break\n",
    "                        try:\n",
    "                            item = json.loads(line.decode(\"utf-8\"))\n",
    "                        except:\n",
    "                            continue\n",
    "                        title = item.get(\"title\", \"\")\n",
    "                        text = item.get(\"text\", \"\")\n",
    "                        full_text = f\"{title} {text}\".strip()\n",
    "                        if len(full_text) < 20:\n",
    "                            continue\n",
    "                        label = item.get(\"binarized_label\", item.get(\"label\", 0))\n",
    "                        bond_info = self.LABEL_TO_BOND.get(label, (\"OBLIGATION\", \"DUTY\", None))\n",
    "                        if len(bond_info) == 3:\n",
    "                            bond_type, hohfeld, violator = bond_info\n",
    "                        else:\n",
    "                            bond_type, hohfeld = bond_info[:2]\n",
    "                            violator = None\n",
    "                        agent = patient = None\n",
    "                        if violator == \"author\":\n",
    "                            agent, patient = \"author\", \"other\"\n",
    "                        elif violator == \"other\":\n",
    "                            agent, patient = \"other\", \"author\"\n",
    "                        annotations.append(\n",
    "                            BondAnnotation(\n",
    "                                text=full_text[:500],\n",
    "                                agent=agent,\n",
    "                                patient=patient,\n",
    "                                bond_type=bond_type,\n",
    "                                hohfeld_state=hohfeld,\n",
    "                                context=\"descriptive\",\n",
    "                                confidence=0.7,\n",
    "                                source_dataset=\"scruples\",\n",
    "                                source_category=\"anecdotes\",\n",
    "                                raw_label=str(label),\n",
    "                            )\n",
    "                        )\n",
    "            print(f\"    Loaded {len(annotations)} anecdotes\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Dataset 'allenai/scruples' - {e}\")\n",
    "\n",
    "        # Load dilemmas\n",
    "        print(\"  Loading Scruples/dilemmas...\")\n",
    "        dilemmas_cache = cache_dir / \"scruples_dilemmas.tar.gz\"\n",
    "        dilemma_start = len(annotations)\n",
    "        try:\n",
    "            if not dilemmas_cache.exists():\n",
    "                resp = requests.get(self.DILEMMAS_URL, timeout=120)\n",
    "                resp.raise_for_status()\n",
    "                with open(dilemmas_cache, \"wb\") as f:\n",
    "                    f.write(resp.content)\n",
    "\n",
    "            with tarfile.open(dilemmas_cache, \"r:gz\") as tar:\n",
    "                for member in tar.getmembers():\n",
    "                    if not member.name.endswith(\".jsonl\"):\n",
    "                        continue\n",
    "                    f = tar.extractfile(member)\n",
    "                    if f is None:\n",
    "                        continue\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            item = json.loads(line.decode(\"utf-8\"))\n",
    "                        except:\n",
    "                            continue\n",
    "                        action1 = item.get(\"action1\", \"\")\n",
    "                        action2 = item.get(\"action2\", \"\")\n",
    "                        text = f\"Choice A: {action1} Choice B: {action2}\"\n",
    "                        if len(text) < 20:\n",
    "                            continue\n",
    "                        annotations.append(\n",
    "                            BondAnnotation(\n",
    "                                text=text[:500],\n",
    "                                agent=\"actor\",\n",
    "                                patient=\"affected\",\n",
    "                                bond_type=\"OBLIGATION\",\n",
    "                                hohfeld_state=\"DUTY\",\n",
    "                                context=\"hypothetical\",\n",
    "                                confidence=0.6,\n",
    "                                source_dataset=\"scruples\",\n",
    "                                source_category=\"dilemmas\",\n",
    "                                raw_label=str(item.get(\"label\", 0)),\n",
    "                            )\n",
    "                        )\n",
    "            print(f\"    Loaded {len(annotations) - dilemma_start} dilemmas\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Dataset 'allenai/scruples' - {e}\")\n",
    "\n",
    "        return annotations\n",
    "\n",
    "    def _load_hf_legacy(self, max_items: int = 0) -> list[BondAnnotation]:\n",
    "        \"\"\"Legacy HuggingFace loader (no longer works).\"\"\"\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "\n",
    "            dataset = load_dataset(\"allenai/scruples\", \"anecdotes\")\n",
    "\n",
    "            for split in [\"train\", \"dev\", \"test\"]:\n",
    "                if split not in dataset:\n",
    "                    continue\n",
    "                for item in dataset[split]:\n",
    "                    if max_items > 0 and len(annotations) >= max_items:\n",
    "                        break\n",
    "\n",
    "                    title = item.get(\"title\", \"\")\n",
    "                    text = item.get(\"text\", \"\")\n",
    "                    full_text = f\"{title}\\n{text}\" if title else text\n",
    "\n",
    "                    if len(full_text) < 20:\n",
    "                        continue\n",
    "\n",
    "                    label = item.get(\"binarized_label\") or item.get(\"label\", \"INFO\")\n",
    "                    if isinstance(label, int):\n",
    "                        label = \"AUTHOR_WRONG\" if label == 1 else \"NOBODY_WRONG\"\n",
    "\n",
    "                    bond_type, hohfeld, violator = self.LABEL_TO_BOND.get(\n",
    "                        label, (\"OBLIGATION\", \"DUTY\", None)\n",
    "                    )\n",
    "\n",
    "                    agent = patient = None\n",
    "                    if violator == \"author\":\n",
    "                        agent, patient = \"author\", \"other\"\n",
    "                    elif violator == \"other\":\n",
    "                        agent, patient = \"other\", \"author\"\n",
    "                    elif violator == \"both\":\n",
    "                        agent = patient = \"both\"\n",
    "\n",
    "                    annotations.append(\n",
    "                        BondAnnotation(\n",
    "                            text=full_text[:500],\n",
    "                            agent=agent,\n",
    "                            patient=patient,\n",
    "                            bond_type=bond_type,\n",
    "                            hohfeld_state=hohfeld,\n",
    "                            context=\"descriptive\",\n",
    "                            confidence=0.7,\n",
    "                            source_dataset=\"scruples\",\n",
    "                            source_category=\"anecdotes\",\n",
    "                            raw_label=label,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: {e}\")\n",
    "\n",
    "        # Load dilemmas\n",
    "        print(\"  Loading Scruples/dilemmas...\")\n",
    "        try:\n",
    "            dataset = load_dataset(\"allenai/scruples\", \"dilemmas\")\n",
    "            dilemma_limit = max_items // 3 if max_items > 0 else 0\n",
    "\n",
    "            count = 0\n",
    "            for split in [\"train\", \"dev\", \"test\"]:\n",
    "                if split not in dataset:\n",
    "                    continue\n",
    "                for item in dataset[split]:\n",
    "                    if dilemma_limit > 0 and count >= dilemma_limit:\n",
    "                        break\n",
    "\n",
    "                    action1 = item.get(\"action1\", \"\")\n",
    "                    action2 = item.get(\"action2\", \"\")\n",
    "                    text = f\"Choice A: {action1}\\nChoice B: {action2}\"\n",
    "\n",
    "                    if len(text) < 20:\n",
    "                        continue\n",
    "\n",
    "                    annotations.append(\n",
    "                        BondAnnotation(\n",
    "                            text=text[:500],\n",
    "                            agent=\"actor\",\n",
    "                            patient=\"affected\",\n",
    "                            bond_type=\"OBLIGATION\",\n",
    "                            hohfeld_state=\"DUTY\",\n",
    "                            context=\"hypothetical\",\n",
    "                            confidence=0.6,\n",
    "                            source_dataset=\"scruples\",\n",
    "                            source_category=\"dilemmas\",\n",
    "                            raw_label=str(item.get(\"label\", 0)),\n",
    "                        )\n",
    "                    )\n",
    "                    count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: {e}\")\n",
    "\n",
    "        return annotations\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ETHICSUITE LOADER\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class EthicsSuiteLoader:\n",
    "    \"\"\"Load LLM-Ethics/EthicsSuite dataset.\"\"\"\n",
    "\n",
    "    def load(self, max_items: int = 0) -> list[BondAnnotation]:\n",
    "        import urllib.request\n",
    "\n",
    "        url = \"https://raw.githubusercontent.com/LLM-Ethics/EthicsSuite/main/data.jsonl\"\n",
    "        cache_dir = Path(\"data/ethics_cache\")\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        cache_file = cache_dir / \"ethicsuite.jsonl\"\n",
    "\n",
    "        annotations = []\n",
    "\n",
    "        print(\"  Loading EthicsSuite...\")\n",
    "        try:\n",
    "            if not cache_file.exists():\n",
    "                print(\"    Downloading...\")\n",
    "                urllib.request.urlretrieve(url, cache_file)\n",
    "\n",
    "            category_map = {\n",
    "                \"deontology\": (\"OBLIGATION\", \"DUTY\"),\n",
    "                \"justice\": (\"CLAIM\", \"CLAIM\"),\n",
    "                \"virtue\": (\"VIRTUE\", \"DUTY\"),\n",
    "                \"utilitarianism\": (\"PERMISSION\", \"LIBERTY\"),\n",
    "                \"commonsense\": (\"OBLIGATION\", \"DUTY\"),\n",
    "            }\n",
    "\n",
    "            with open(cache_file, encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    if max_items > 0 and len(annotations) >= max_items:\n",
    "                        break\n",
    "\n",
    "                    item = json.loads(line)\n",
    "                    text = item.get(\"text\", \"\")\n",
    "                    if len(text) < 20:\n",
    "                        continue\n",
    "\n",
    "                    source = item.get(\"source\", \"unknown\")\n",
    "                    bond_type, hohfeld = category_map.get(source, (\"OBLIGATION\", \"DUTY\"))\n",
    "\n",
    "                    annotations.append(\n",
    "                        BondAnnotation(\n",
    "                            text=text[:500],\n",
    "                            agent=None,\n",
    "                            patient=None,\n",
    "                            bond_type=bond_type,\n",
    "                            hohfeld_state=hohfeld,\n",
    "                            context=\"hypothetical\",\n",
    "                            confidence=0.75,\n",
    "                            source_dataset=\"ethicsuite\",\n",
    "                            source_category=source,\n",
    "                            raw_label=item.get(\"original_text\", \"\")[:100],\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: {e}\")\n",
    "\n",
    "        return annotations\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN LOADING LOGIC\n",
    "# =============================================================================\n",
    "\n",
    "all_bond_annotations = []\n",
    "\n",
    "if LOAD_ETHICS_DATASET:\n",
    "    print(\"\\n[1] ETHICS Dataset (hendrycks/ethics)\")\n",
    "    loader = EthicsLoader()\n",
    "    ethics_anns = loader.load(MAX_ETHICS_ITEMS)\n",
    "    print(f\"    Loaded: {len(ethics_anns):,} annotations\")\n",
    "    all_bond_annotations.extend(ethics_anns)\n",
    "\n",
    "if LOAD_SCRUPLES_DATASET:\n",
    "    print(\"\\n[2] Scruples Dataset (allenai/scruples)\")\n",
    "    loader = ScruplesLoader()\n",
    "    scruples_anns = loader.load(MAX_SCRUPLES_ITEMS)\n",
    "    print(f\"    Loaded: {len(scruples_anns):,} annotations\")\n",
    "    all_bond_annotations.extend(scruples_anns)\n",
    "\n",
    "if LOAD_ETHICSUITE_DATASET:\n",
    "    print(\"\\n[3] EthicsSuite Dataset (LLM-Ethics/EthicsSuite)\")\n",
    "    loader = EthicsSuiteLoader()\n",
    "    suite_anns = loader.load(MAX_ETHICSUITE_ITEMS)\n",
    "    print(f\"    Loaded: {len(suite_anns):,} annotations\")\n",
    "    all_bond_annotations.extend(suite_anns)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BOND EXTRACTION DATA STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stats = {\n",
    "    \"by_dataset\": defaultdict(int),\n",
    "    \"by_bond_type\": defaultdict(int),\n",
    "    \"by_hohfeld\": defaultdict(int),\n",
    "    \"by_context\": defaultdict(int),\n",
    "    \"by_category\": defaultdict(int),\n",
    "    \"has_agent\": 0,\n",
    "    \"has_patient\": 0,\n",
    "}\n",
    "\n",
    "for ann in all_bond_annotations:\n",
    "    stats[\"by_dataset\"][ann.source_dataset] += 1\n",
    "    stats[\"by_bond_type\"][ann.bond_type] += 1\n",
    "    stats[\"by_hohfeld\"][ann.hohfeld_state] += 1\n",
    "    stats[\"by_context\"][ann.context] += 1\n",
    "    stats[\"by_category\"][ann.source_category] += 1\n",
    "    if ann.agent:\n",
    "        stats[\"has_agent\"] += 1\n",
    "    if ann.patient:\n",
    "        stats[\"has_patient\"] += 1\n",
    "\n",
    "print(f\"\\nTotal annotations: {len(all_bond_annotations):,}\")\n",
    "\n",
    "print(\"\\nBy Dataset:\")\n",
    "for ds, count in sorted(stats[\"by_dataset\"].items()):\n",
    "    print(f\"  {ds}: {count:,}\")\n",
    "\n",
    "print(\"\\nBy Bond Type:\")\n",
    "for bt, count in sorted(stats[\"by_bond_type\"].items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {bt}: {count:,}\")\n",
    "\n",
    "print(\"\\nBy Context:\")\n",
    "for ctx, count in sorted(stats[\"by_context\"].items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {ctx}: {count:,}\")\n",
    "\n",
    "print(\n",
    "    f\"\\nAgent extracted: {stats['has_agent']:,} ({100 * stats['has_agent'] / max(1, len(all_bond_annotations)):.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Patient extracted: {stats['has_patient']:,} ({100 * stats['has_patient'] / max(1, len(all_bond_annotations)):.1f}%)\"\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT\n",
    "# =============================================================================\n",
    "\n",
    "output_dir = Path(\"data/bond_training\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save all annotations\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with open(output_dir / \"bond_annotations.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for ann in all_bond_annotations:\n",
    "        f.write(json.dumps(asdict(ann), ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved: {output_dir / 'bond_annotations.jsonl'}\")\n",
    "\n",
    "# Train/test split\n",
    "if CREATE_TRAIN_TEST_SPLIT:\n",
    "    import random\n",
    "\n",
    "    random.seed(42)\n",
    "    shuffled = all_bond_annotations.copy()\n",
    "    random.shuffle(shuffled)\n",
    "    split_idx = int(len(shuffled) * (1 - TEST_SPLIT_RATIO))\n",
    "    train_anns = shuffled[:split_idx]\n",
    "    test_anns = shuffled[split_idx:]\n",
    "\n",
    "    with open(output_dir / \"bond_train.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for ann in train_anns:\n",
    "            f.write(json.dumps(asdict(ann), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    with open(output_dir / \"bond_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for ann in test_anns:\n",
    "            f.write(json.dumps(asdict(ann), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Train/Test split: {len(train_anns):,} / {len(test_anns):,}\")\n",
    "\n",
    "# Export in BIP format\n",
    "if EXPORT_BIP_FORMAT:\n",
    "    bip_passages = []\n",
    "    for i, ann in enumerate(all_bond_annotations):\n",
    "        passage = {\n",
    "            \"id\": f\"ethics_{ann.source_dataset}_{i}\",\n",
    "            \"text\": ann.text,\n",
    "            \"language\": \"english\",\n",
    "            \"time_periods\": [\"MODERN_ETHICS\"],\n",
    "            \"tags\": [\"modern\", \"english\", \"western\", \"ethics\", ann.source_category],\n",
    "            \"bonds\": [\n",
    "                {\n",
    "                    \"agent\": ann.agent or \"unspecified\",\n",
    "                    \"patient\": ann.patient or \"unspecified\",\n",
    "                    \"bond_type\": ann.bond_type,\n",
    "                    \"hohfeld_state\": ann.hohfeld_state,\n",
    "                    \"context\": ann.context,\n",
    "                    \"confidence\": ann.confidence,\n",
    "                }\n",
    "            ],\n",
    "            \"source\": ann.source_dataset,\n",
    "            \"category\": ann.source_category,\n",
    "        }\n",
    "        bip_passages.append(passage)\n",
    "\n",
    "    with open(output_dir / \"ethics_corpus.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for p in bip_passages:\n",
    "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"BIP format: {output_dir / 'ethics_corpus.jsonl'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BOND EXTRACTION DATA READY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Use data/bond_training/bond_train.jsonl for training\")\n",
    "print(\"Use data/bond_training/ethics_corpus.jsonl for BIP integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell_4",
    "outputId": "49c6bf1a-9646-413f-961a-c82703493bf7"
   },
   "outputs": [],
   "source": [
    "# @title 4. Patterns + Normalization { display-mode: \"form\" }\n",
    "# @markdown BIP v10.16: Enhanced NLP bond extraction with selectable methods\n",
    "# @markdown - Level 1: Regex patterns (fast, all languages)\n",
    "# @markdown - Level 2: Grammar-aware (Chinese, Arabic, Hebrew, Sanskrit)\n",
    "# @markdown - Level 3: spaCy dependency parsing (English only)\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ### Extraction Method\n",
    "EXTRACTION_LEVEL = \"level2\"  # @param [\"level1\", \"level2\", \"level3\"]\n",
    "# @markdown - **level1**: Regex patterns only (fastest, baseline)\n",
    "# @markdown - **level2**: Language-specific grammar analysis (recommended)\n",
    "# @markdown - **level3**: spaCy NLP for English + level2 for others\n",
    "\n",
    "USE_SPACY_FOR_ENGLISH = True  # @param {type:\"boolean\"}\n",
    "# @markdown Enable spaCy dependency parsing for English texts (level3 only)\n",
    "\n",
    "INSTALL_SPACY_IF_NEEDED = True  # @param {type:\"boolean\"}\n",
    "# @markdown Auto-install spaCy and en_core_web_md model if not available\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ### Extraction Options\n",
    "EXTRACT_AGENT_PATIENT = True  # @param {type:\"boolean\"}\n",
    "# @markdown Extract agent/patient roles from text (level2/3 only)\n",
    "\n",
    "DETECT_CAUSATIVES = True  # @param {type:\"boolean\"}\n",
    "# @markdown Detect causative constructions\n",
    "\n",
    "DETECT_PASSIVES = True  # @param {type:\"boolean\"}\n",
    "# @markdown Detect passive voice constructions\n",
    "import re\n",
    "import unicodedata\n",
    "from enum import Enum, auto\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT NORMALIZATION & PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ===== TEXT NORMALIZATION =====\n",
    "def normalize_hebrew(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"[\\u0591-\\u05C7]\", \"\", text)  # Remove nikud\n",
    "    for final, regular in [\n",
    "        (\"\\u05da\", \"\\u05db\"),\n",
    "        (\"\\u05dd\", \"\\u05de\"),\n",
    "        (\"\\u05df\", \"\\u05e0\"),\n",
    "        (\"\\u05e3\", \"\\u05e4\"),\n",
    "        (\"\\u05e5\", \"\\u05e6\"),\n",
    "    ]:\n",
    "        text = text.replace(final, regular)\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"[\\u064B-\\u065F]\", \"\", text)  # Remove tashkeel\n",
    "    text = text.replace(\"\\u0640\", \"\")  # Remove tatweel\n",
    "    for v in [\"\\u0623\", \"\\u0625\", \"\\u0622\", \"\\u0671\"]:\n",
    "        text = text.replace(v, \"\\u0627\")\n",
    "    text = text.replace(\"\\u0629\", \"\\u0647\").replace(\"\\u0649\", \"\\u064a\")\n",
    "    return text\n",
    "\n",
    "\n",
    "# NEW in v10.9: Sanskrit normalization\n",
    "def normalize_sanskrit(text):\n",
    "    \"\"\"Normalize Sanskrit/Devanagari text.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    # Remove vedic accents and other diacriticals\n",
    "    text = re.sub(r\"[\\u0951-\\u0954]\", \"\", text)  # Vedic tone marks\n",
    "    text = re.sub(r\"[\\u0900-\\u0902]\", \"\", text)  # Chandrabindu variants\n",
    "    return text\n",
    "\n",
    "\n",
    "# NEW in v10.9: Pali normalization\n",
    "def normalize_pali(text):\n",
    "    \"\"\"Normalize Pali text (romanized or script).\"\"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    # Normalize romanized Pali diacritics\n",
    "    text = text.lower()\n",
    "    # Handle common Pali romanization variations\n",
    "    text = text.replace(\"\", \"m\").replace(\"\", \"n\").replace(\"\", \"n\")\n",
    "    text = text.replace(\"\", \"t\").replace(\"\", \"d\").replace(\"\", \"n\")\n",
    "    text = text.replace(\"\", \"l\").replace(\"\", \"a\").replace(\"\", \"i\").replace(\"\", \"u\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_text(text, language):\n",
    "    if language in [\"hebrew\", \"aramaic\"]:\n",
    "        return normalize_hebrew(text)\n",
    "    elif language == \"arabic\":\n",
    "        return normalize_arabic(text)\n",
    "    elif language == \"classical_chinese\":\n",
    "        return unicodedata.normalize(\"NFKC\", text)\n",
    "    elif language == \"sanskrit\":\n",
    "        return normalize_sanskrit(text)\n",
    "    elif language == \"pali\":\n",
    "        return normalize_pali(text)\n",
    "    else:\n",
    "        return unicodedata.normalize(\"NFKC\", text.lower())\n",
    "\n",
    "\n",
    "# ===== BOND AND HOHFELD TYPES =====\n",
    "class BondType(Enum):\n",
    "    HARM_PREVENTION = auto()\n",
    "    RECIPROCITY = auto()\n",
    "    AUTONOMY = auto()\n",
    "    PROPERTY = auto()\n",
    "    FAMILY = auto()\n",
    "    AUTHORITY = auto()\n",
    "    CARE = auto()\n",
    "    FAIRNESS = auto()\n",
    "    CONTRACT = auto()\n",
    "    NONE = auto()\n",
    "\n",
    "\n",
    "class HohfeldState(Enum):\n",
    "    OBLIGATION = auto()\n",
    "    RIGHT = auto()\n",
    "    LIBERTY = auto()\n",
    "    NO_RIGHT = auto()\n",
    "\n",
    "\n",
    "# ===== COMPLETE BOND PATTERNS =====\n",
    "ALL_BOND_PATTERNS = {\n",
    "    \"hebrew\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u05d4\\u05e8\\u05d2\",\n",
    "            r\"\\u05e8\\u05e6\\u05d7\",\n",
    "            r\"\\u05e0\\u05d6\\u05e7\",\n",
    "            r\"\\u05d4\\u05db\\u05d4\",\n",
    "            r\"\\u05d4\\u05e6\\u05d9\\u05dc\",\n",
    "            r\"\\u05e9\\u05de\\u05e8\",\n",
    "            r\"\\u05e4\\u05e7\\u05d5\\u05d7.\\u05e0\\u05e4\\u05e9\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\u05d2\\u05de\\u05d5\\u05dc\",\n",
    "            r\"\\u05d4\\u05e9\\u05d9\\u05d1\",\n",
    "            r\"\\u05e4\\u05e8\\u05e2\",\n",
    "            r\"\\u05e0\\u05ea\\u05df.*\\u05e7\\u05d1\\u05dc\",\n",
    "            r\"\\u05de\\u05d3\\u05d4.\\u05db\\u05e0\\u05d2\\u05d3\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\u05d1\\u05d7\\u05e8\",\n",
    "            r\"\\u05e8\\u05e6\\u05d5\\u05df\",\n",
    "            r\"\\u05d7\\u05e4\\u05e9\",\n",
    "            r\"\\u05e2\\u05e6\\u05de\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u05e7\\u05e0\\u05d4\",\n",
    "            r\"\\u05de\\u05db\\u05e8\",\n",
    "            r\"\\u05d2\\u05d6\\u05dc\",\n",
    "            r\"\\u05d2\\u05e0\\u05d1\",\n",
    "            r\"\\u05de\\u05de\\u05d5\\u05df\",\n",
    "            r\"\\u05e0\\u05db\\u05e1\",\n",
    "            r\"\\u05d9\\u05e8\\u05e9\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u05d0\\u05d1\",\n",
    "            r\"\\u05d0\\u05dd\",\n",
    "            r\"\\u05d1\\u05df\",\n",
    "            r\"\\u05db\\u05d1\\u05d3.*\\u05d0\\u05d1\",\n",
    "            r\"\\u05db\\u05d1\\u05d3.*\\u05d0\\u05dd\",\n",
    "            r\"\\u05de\\u05e9\\u05e4\\u05d7\\u05d4\",\n",
    "            r\"\\u05d0\\u05d7\",\n",
    "            r\"\\u05d0\\u05d7\\u05d5\\u05ea\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u05de\\u05dc\\u05db\",\n",
    "            r\"\\u05e9\\u05d5\\u05e4\\u05d8\",\n",
    "            r\"\\u05e6\\u05d5\\u05d4\",\n",
    "            r\"\\u05ea\\u05d5\\u05e8\\u05d4\",\n",
    "            r\"\\u05de\\u05e6\\u05d5\\u05d4\",\n",
    "            r\"\\u05d3\\u05d9\\u05df\",\n",
    "            r\"\\u05d7\\u05e7\",\n",
    "        ],\n",
    "        BondType.CARE: [\n",
    "            r\"\\u05d7\\u05e1\\u05d3\",\n",
    "            r\"\\u05e8\\u05d7\\u05dd\",\n",
    "            r\"\\u05e2\\u05d6\\u05e8\",\n",
    "            r\"\\u05ea\\u05de\\u05db\",\n",
    "            r\"\\u05e6\\u05d3\\u05e7\\u05d4\",\n",
    "        ],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u05e6\\u05d3\\u05e7\",\n",
    "            r\"\\u05de\\u05e9\\u05e4\\u05d8\",\n",
    "            r\"\\u05d9\\u05e9\\u05e8\",\n",
    "            r\"\\u05e9\\u05d5\\u05d4\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u05d1\\u05e8\\u05d9\\u05ea\",\n",
    "            r\"\\u05e0\\u05d3\\u05e8\",\n",
    "            r\"\\u05e9\\u05d1\\u05d5\\u05e2\",\n",
    "            r\"\\u05d4\\u05ea\\u05d7\\u05d9\\u05d1\",\n",
    "            r\"\\u05e2\\u05e8\\u05d1\",\n",
    "        ],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u05e7\\u05d8\\u05dc\",\n",
    "            r\"\\u05e0\\u05d6\\u05e7\",\n",
    "            r\"\\u05d7\\u05d1\\u05dc\",\n",
    "            r\"\\u05e9\\u05d6\\u05d9\\u05d1\",\n",
    "            r\"\\u05e4\\u05e6\\u05d9\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [r\"\\u05e4\\u05e8\\u05e2\", r\"\\u05e9\\u05dc\\u05dd\", r\"\\u05d0\\u05d2\\u05e8\"],\n",
    "        BondType.AUTONOMY: [r\"\\u05e6\\u05d1\\u05d9\", r\"\\u05e8\\u05e2\\u05d5\"],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u05d6\\u05d1\\u05df\",\n",
    "            r\"\\u05e7\\u05e0\\u05d4\",\n",
    "            r\"\\u05d2\\u05d6\\u05dc\",\n",
    "            r\"\\u05de\\u05de\\u05d5\\u05e0\\u05d0\",\n",
    "            r\"\\u05e0\\u05db\\u05e1\\u05d9\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u05d0\\u05d1\\u05d0\",\n",
    "            r\"\\u05d0\\u05de\\u05d0\",\n",
    "            r\"\\u05d1\\u05e8\\u05d0\",\n",
    "            r\"\\u05d1\\u05e8\\u05ea\\u05d0\",\n",
    "            r\"\\u05d9\\u05e7\\u05e8\",\n",
    "            r\"\\u05d0\\u05d7\\u05d0\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u05de\\u05dc\\u05db\\u05d0\",\n",
    "            r\"\\u05d3\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05d3\\u05d9\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05e4\\u05e7\\u05d5\\u05d3\\u05d0\",\n",
    "            r\"\\u05d0\\u05d5\\u05e8\\u05d9\\u05ea\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\u05d7\\u05e1\\u05d3\", r\"\\u05e8\\u05d7\\u05dd\", r\"\\u05e1\\u05e2\\u05d3\"],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u05d3\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05e7\\u05e9\\u05d5\\u05d8\",\n",
    "            r\"\\u05ea\\u05e8\\u05d9\\u05e6\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u05e7\\u05d9\\u05de\\u05d0\",\n",
    "            r\"\\u05e9\\u05d1\\u05d5\\u05e2\\u05d4\",\n",
    "            r\"\\u05e0\\u05d3\\u05e8\\u05d0\",\n",
    "            r\"\\u05e2\\u05e8\\u05d1\\u05d0\",\n",
    "        ],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u6bba\",\n",
    "            r\"\\u5bb3\",\n",
    "            r\"\\u50b7\",\n",
    "            r\"\\u6551\",\n",
    "            r\"\\u8b77\",\n",
    "            r\"\\u885b\",\n",
    "            r\"\\u66b4\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [r\"\\u5831\", r\"\\u9084\", r\"\\u511f\", r\"\\u8ced\", r\"\\u7b54\"],\n",
    "        BondType.AUTONOMY: [r\"\\u81ea\", r\"\\u7531\", r\"\\u4efb\", r\"\\u610f\", r\"\\u5fd7\"],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u8ca1\",\n",
    "            r\"\\u7269\",\n",
    "            r\"\\u7522\",\n",
    "            r\"\\u76dc\",\n",
    "            r\"\\u7aca\",\n",
    "            r\"\\u8ce3\",\n",
    "            r\"\\u8cb7\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u5b5d\",\n",
    "            r\"\\u7236\",\n",
    "            r\"\\u6bcd\",\n",
    "            r\"\\u89aa\",\n",
    "            r\"\\u5b50\",\n",
    "            r\"\\u5f1f\",\n",
    "            r\"\\u5144\",\n",
    "            r\"\\u5bb6\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u541b\",\n",
    "            r\"\\u81e3\",\n",
    "            r\"\\u738b\",\n",
    "            r\"\\u547d\",\n",
    "            r\"\\u4ee4\",\n",
    "            r\"\\u6cd5\",\n",
    "            r\"\\u6cbb\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\u4ec1\", r\"\\u611b\", r\"\\u6148\", r\"\\u60e0\", r\"\\u6069\", r\"\\u6190\"],\n",
    "        BondType.FAIRNESS: [r\"\\u7fa9\", r\"\\u6b63\", r\"\\u516c\", r\"\\u5e73\", r\"\\u5747\"],\n",
    "        BondType.CONTRACT: [r\"\\u7d04\", r\"\\u76df\", r\"\\u8a93\", r\"\\u8afe\", r\"\\u4fe1\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u0642\\u062a\\u0644\",\n",
    "            r\"\\u0636\\u0631\\u0631\",\n",
    "            r\"\\u0627\\u0630[\\u064a\\u0649]\",\n",
    "            r\"\\u0638\\u0644\\u0645\",\n",
    "            r\"\\u0627\\u0646\\u0642\\u0630\",\n",
    "            r\"\\u062d\\u0641\\u0638\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0646\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\u062c\\u0632\\u0627\",\n",
    "            r\"\\u0631\\u062f\",\n",
    "            r\"\\u0642\\u0635\\u0627\\u0635\",\n",
    "            r\"\\u0645\\u062b\\u0644\",\n",
    "            r\"\\u0639\\u0648\\u0636\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\u062d\\u0631\",\n",
    "            r\"\\u0627\\u0631\\u0627\\u062f\\u0629\",\n",
    "            r\"\\u0627\\u062e\\u062a\\u064a\\u0627\\u0631\",\n",
    "            r\"\\u0645\\u0634\\u064a\\u0626\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u0645\\u0627\\u0644\",\n",
    "            r\"\\u0645\\u0644\\u0643\",\n",
    "            r\"\\u0633\\u0631\\u0642\",\n",
    "            r\"\\u0628\\u064a\\u0639\",\n",
    "            r\"\\u0634\\u0631\\u0627\",\n",
    "            r\"\\u0645\\u064a\\u0631\\u0627\\u062b\",\n",
    "            r\"\\u063a\\u0635\\u0628\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u0648\\u0627\\u0644\\u062f\",\n",
    "            r\"\\u0627\\u0628\\u0648\",\n",
    "            r\"\\u0627\\u0645\",\n",
    "            r\"\\u0627\\u0628\\u0646\",\n",
    "            r\"\\u0628\\u0646\\u062a\",\n",
    "            r\"\\u0627\\u0647\\u0644\",\n",
    "            r\"\\u0642\\u0631\\u0628[\\u064a\\u0649]\",\n",
    "            r\"\\u0631\\u062d\\u0645\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u0637\\u0627\\u0639\",\n",
    "            r\"\\u0627\\u0645\\u0631\",\n",
    "            r\"\\u062d\\u0643\\u0645\",\n",
    "            r\"\\u0633\\u0644\\u0637\\u0627\\u0646\",\n",
    "            r\"\\u062e\\u0644\\u064a\\u0641\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0645\",\n",
    "            r\"\\u0634\\u0631\\u064a\\u0639\",\n",
    "        ],\n",
    "        BondType.CARE: [\n",
    "            r\"\\u0631\\u062d\\u0645\",\n",
    "            r\"\\u0627\\u062d\\u0633\\u0627\\u0646\",\n",
    "            r\"\\u0639\\u0637\\u0641\",\n",
    "            r\"\\u0635\\u062f\\u0642\",\n",
    "            r\"\\u0632\\u0643\\u0627\",\n",
    "        ],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u0639\\u062f\\u0644\",\n",
    "            r\"\\u0642\\u0633\\u0637\",\n",
    "            r\"\\u062d\\u0642\",\n",
    "            r\"\\u0627\\u0646\\u0635\\u0627\\u0641\",\n",
    "            r\"\\u0633\\u0648[\\u064a\\u0649]\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u0639\\u0647\\u062f\",\n",
    "            r\"\\u0639\\u0642\\u062f\",\n",
    "            r\"\\u0646\\u0630\\u0631\",\n",
    "            r\"\\u064a\\u0645\\u064a\\u0646\",\n",
    "            r\"\\u0648\\u0641\\u0627\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0646\",\n",
    "        ],\n",
    "    },\n",
    "    \"english\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\bkill\",\n",
    "            r\"\\bmurder\",\n",
    "            r\"\\bharm\",\n",
    "            r\"\\bhurt\",\n",
    "            r\"\\bsave\",\n",
    "            r\"\\bprotect\",\n",
    "            r\"\\bviolence\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\breturn\",\n",
    "            r\"\\brepay\",\n",
    "            r\"\\bexchange\",\n",
    "            r\"\\bgive.*back\",\n",
    "            r\"\\breciproc\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\bfree\",\n",
    "            r\"\\bchoice\",\n",
    "            r\"\\bchoose\",\n",
    "            r\"\\bconsent\",\n",
    "            r\"\\bautonomy\",\n",
    "            r\"\\bright to\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\bsteal\",\n",
    "            r\"\\btheft\",\n",
    "            r\"\\bown\",\n",
    "            r\"\\bproperty\",\n",
    "            r\"\\bbelong\",\n",
    "            r\"\\binherit\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\bfather\",\n",
    "            r\"\\bmother\",\n",
    "            r\"\\bparent\",\n",
    "            r\"\\bchild\",\n",
    "            r\"\\bfamily\",\n",
    "            r\"\\bhonor.*parent\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\bobey\",\n",
    "            r\"\\bcommand\",\n",
    "            r\"\\bauthority\",\n",
    "            r\"\\blaw\",\n",
    "            r\"\\brule\",\n",
    "            r\"\\bgovern\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\bcare\", r\"\\bhelp\", r\"\\bkind\", r\"\\bcompassion\", r\"\\bcharity\", r\"\\bmercy\"],\n",
    "        BondType.FAIRNESS: [r\"\\bfair\", r\"\\bjust\", r\"\\bequal\", r\"\\bequity\", r\"\\bright\\b\"],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\bpromise\",\n",
    "            r\"\\bcontract\",\n",
    "            r\"\\bagreem\",\n",
    "            r\"\\bvow\",\n",
    "            r\"\\boath\",\n",
    "            r\"\\bcommit\",\n",
    "        ],\n",
    "    },\n",
    "    \"sanskrit\": {\n",
    "        BondType.HARM_PREVENTION: [r\"\", r\"\", r\"\", r\"\", r\"\"],\n",
    "        BondType.RECIPROCITY: [r\"\", r\"\", r\"\", r\"\"],\n",
    "        BondType.AUTONOMY: [r\"\", r\"\", r\"\"],\n",
    "        BondType.PROPERTY: [r\"\", r\"\", r\"\", r\"\"],\n",
    "        BondType.FAMILY: [r\"\", r\"\", r\"\", r\"\", r\"\"],\n",
    "        BondType.AUTHORITY: [r\"\", r\"\", r\"\", r\"\", r\"\"],\n",
    "        BondType.CARE: [r\"\", r\"\", r\"\", r\"\", r\"\"],\n",
    "        BondType.FAIRNESS: [r\"\", r\"\", r\"\", r\"\"],\n",
    "        BondType.CONTRACT: [r\"\", r\"\", r\"\", r\"\"],\n",
    "    },\n",
    "    \"pali\": {\n",
    "        BondType.HARM_PREVENTION: [r\"himsa\", r\"ahimsa\", r\"panatipata\", r\"rakkhati\"],\n",
    "        BondType.RECIPROCITY: [r\"dana\", r\"patidana\", r\"ina\"],\n",
    "        BondType.AUTONOMY: [r\"vimutti\", r\"nibbana\", r\"attadhipa\"],\n",
    "        BondType.PROPERTY: [r\"dhana\", r\"theyya\", r\"adinnadana\"],\n",
    "        BondType.FAMILY: [r\"mata\", r\"pita\", r\"putta\", r\"kula\"],\n",
    "        BondType.AUTHORITY: [r\"raja\", r\"dhamma\", r\"vinaya\", r\"sikkhapada\"],\n",
    "        BondType.CARE: [r\"karuna\", r\"metta\", r\"mudita\", r\"upekkha\"],\n",
    "        BondType.FAIRNESS: [r\"samma\", r\"dhamma\", r\"sacca\"],\n",
    "        BondType.CONTRACT: [r\"patijna\", r\"vacana\", r\"sacca\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# ===== COMPLETE HOHFELD PATTERNS =====\n",
    "ALL_HOHFELD_PATTERNS = {\n",
    "    \"hebrew\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u05d7\\u05d9\\u05d9\\u05d1\",\n",
    "            r\"\\u05e6\\u05e8\\u05d9\\u05db\",\n",
    "            r\"\\u05de\\u05d5\\u05db\\u05e8\\u05d7\",\n",
    "            r\"\\u05de\\u05e6\\u05d5\\u05d5\\u05d4\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "            r\"\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d6\\u05db\\u05d0\\u05d9\",\n",
    "            r\"\\u05de\\u05d2\\u05d9\\u05e2\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u05de\\u05d5\\u05ea\\u05e8\",\n",
    "            r\"\\u05e8\\u05e9\\u05d5\\u05ea\",\n",
    "            r\"\\u05e4\\u05d8\\u05d5\\u05e8\",\n",
    "            r\"\\u05d9\\u05db\\u05d5\\u05dc\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u05d0\\u05e1\\u05d5\\u05e8\",\n",
    "            r\"\\u05d0\\u05d9\\u05e0\\u05d5 \\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d0\\u05d9\\u05df.*\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "        ],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u05d7\\u05d9\\u05d9\\u05d1\",\n",
    "            r\"\\u05de\\u05d7\\u05d5\\u05d9\\u05d1\",\n",
    "            r\"\\u05d1\\u05e2\\u05d9\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "            r\"\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d6\\u05db\\u05d9\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u05e9\\u05e8\\u05d9\",\n",
    "            r\"\\u05de\\u05d5\\u05ea\\u05e8\",\n",
    "            r\"\\u05e4\\u05d8\\u05d5\\u05e8\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u05d0\\u05e1\\u05d5\\u05e8\",\n",
    "            r\"\\u05dc\\u05d0.*\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "        ],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        HohfeldState.OBLIGATION: [r\"\\u5fc5\", r\"\\u9808\", r\"\\u7576\", r\"\\u61c9\", r\"\\u5b9c\"],\n",
    "        HohfeldState.RIGHT: [r\"\\u53ef\", r\"\\u5f97\", r\"\\u6b0a\", r\"\\u5b9c\"],\n",
    "        HohfeldState.LIBERTY: [r\"\\u8a31\", r\"\\u4efb\", r\"\\u807d\", r\"\\u514d\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"\\u4e0d\\u53ef\", r\"\\u52ff\", r\"\\u7981\", r\"\\u83ab\", r\"\\u975e\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u064a\\u062c\\u0628\",\n",
    "            r\"\\u0648\\u0627\\u062c\\u0628\",\n",
    "            r\"\\u0641\\u0631\\u0636\",\n",
    "            r\"\\u0644\\u0627\\u0632\\u0645\",\n",
    "            r\"\\u0648\\u062c\\u0648\\u0628\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u062d\\u0642\",\n",
    "            r\"\\u064a\\u062d\\u0642\",\n",
    "            r\"\\u062c\\u0627\\u0626\\u0632\",\n",
    "            r\"\\u064a\\u062c\\u0648\\u0632\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u0645\\u0628\\u0627\\u062d\",\n",
    "            r\"\\u062d\\u0644\\u0627\\u0644\",\n",
    "            r\"\\u062c\\u0627\\u0626\\u0632\",\n",
    "            r\"\\u0627\\u0628\\u0627\\u062d\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u062d\\u0631\\u0627\\u0645\",\n",
    "            r\"\\u0645\\u062d\\u0631\\u0645\",\n",
    "            r\"\\u0645\\u0645\\u0646\\u0648\\u0639\",\n",
    "            r\"\\u0644\\u0627 \\u064a\\u062c\\u0648\\u0632\",\n",
    "            r\"\\u0646\\u0647[\\u064a\\u0649]\",\n",
    "        ],\n",
    "    },\n",
    "    \"english\": {\n",
    "        HohfeldState.OBLIGATION: [r\"\\bmust\\b\", r\"\\bshall\\b\", r\"\\bobligat\", r\"\\bduty\", r\"\\brequir\"],\n",
    "        HohfeldState.RIGHT: [r\"\\bright\\b\", r\"\\bentitle\", r\"\\bdeserve\", r\"\\bclaim\"],\n",
    "        HohfeldState.LIBERTY: [r\"\\bmay\\b\", r\"\\bpermit\", r\"\\ballow\", r\"\\bfree to\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"\\bforbid\", r\"\\bprohibit\", r\"\\bmust not\", r\"\\bshall not\"],\n",
    "    },\n",
    "    \"sanskrit\": {\n",
    "        HohfeldState.OBLIGATION: [r\"\", r\"\", r\"\", r\"\"],\n",
    "        HohfeldState.RIGHT: [r\"\", r\"\"],\n",
    "        HohfeldState.LIBERTY: [r\"\", r\"\", r\"\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"\", r\"\", r\"\"],\n",
    "    },\n",
    "    \"pali\": {\n",
    "        HohfeldState.OBLIGATION: [r\"kicca\", r\"karaniiya\", r\"dhammo\"],\n",
    "        HohfeldState.RIGHT: [r\"adhikaara\", r\"bhaaga\"],\n",
    "        HohfeldState.LIBERTY: [r\"anujaanati\", r\"kappati\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"nisiddha\", r\"akaraniya\", r\"na kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ===== CONTEXT MARKERS FOR GRAMMAR-AWARE EXTRACTION =====\n",
    "CONTEXT_MARKERS = {\n",
    "    \"hebrew\": {\n",
    "        \"negation\": [r\"\", r\"\", r\"\", r\"\", r\"\"],\n",
    "        \"obligation\": [r\"\", r\"\", r\"\", r\"\"],\n",
    "        \"prohibition\": [r\"\", r\".*\"],\n",
    "        \"permission\": [r\"\", r\"\", r\"\"],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        \"negation\": [r\"\", r\"\", r\"\"],\n",
    "        \"obligation\": [r\"\", r\"\"],\n",
    "        \"prohibition\": [r\"\"],\n",
    "        \"permission\": [r\"\", r\"\"],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        \"negation\": [r\"\", r\"\", r\"\", r\"\", r\"\"],\n",
    "        \"obligation\": [r\"\", r\"\", r\"\", r\"\", r\"\"],\n",
    "        \"prohibition\": [r\"\", r\"\", r\"\", r\"\"],\n",
    "        \"permission\": [r\"\", r\"\", r\"\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        \"negation\": [r\"\", r\"\", r\"\", r\"\", r\"\"],\n",
    "        \"obligation\": [r\"\", r\"\", r\"\", r\"\"],\n",
    "        \"prohibition\": [r\"\", r\"\", r\" \", r\"\"],\n",
    "        \"permission\": [r\"\", r\"\", r\"\"],\n",
    "    },\n",
    "    \"english\": {\n",
    "        \"negation\": [r\"not\", r\"no\", r\"never\", r\"neither\", r\"n't\"],\n",
    "        \"obligation\": [r\"must\", r\"shall\", r\"should\", r\"ought\", r\"required\"],\n",
    "        \"prohibition\": [r\"forbid\", r\"prohibit\", r\"must not\", r\"shall not\", r\"don't\"],\n",
    "        \"permission\": [r\"may\", r\"can\", r\"allowed\", r\"permit\"],\n",
    "    },\n",
    "    \"sanskrit\": {\n",
    "        \"negation\": [r\"\", r\"\", r\"\"],\n",
    "        \"obligation\": [r\"\", r\"\", r\"\"],\n",
    "        \"prohibition\": [r\"\", r\"\", r\"\"],\n",
    "        \"permission\": [r\"\", r\"\"],\n",
    "    },\n",
    "    \"pali\": {\n",
    "        \"negation\": [r\"na\", r\"ma\", r\"a-\"],\n",
    "        \"obligation\": [r\"kicca\", r\"karaniya\"],\n",
    "        \"prohibition\": [r\"nisiddha\", r\"akaraniya\"],\n",
    "        \"permission\": [r\"anujaanati\", r\"kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def detect_context(text, language, match_pos, window=30):\n",
    "    \"\"\"Detect grammatical context around a pattern match.\"\"\"\n",
    "    markers = CONTEXT_MARKERS.get(language, {})\n",
    "    if not markers:\n",
    "        return \"unknown\", None\n",
    "\n",
    "    start = max(0, match_pos - window)\n",
    "    end = min(len(text), match_pos + window)\n",
    "    window_text = text[start:end]\n",
    "\n",
    "    for marker_type in [\"prohibition\", \"obligation\", \"permission\"]:\n",
    "        for pattern in markers.get(marker_type, []):\n",
    "            if re.search(pattern, window_text):\n",
    "                return \"prescriptive\", marker_type\n",
    "\n",
    "    for pattern in markers.get(\"negation\", []):\n",
    "        if re.search(pattern, window_text):\n",
    "            return \"descriptive\", \"negated\"\n",
    "\n",
    "    return \"descriptive\", None\n",
    "\n",
    "\n",
    "# ===== NLP IMPROVEMENTS (v10.9 Phase 1) =====\n",
    "NEGATION_CUES = {\n",
    "    \"english\": [\"not\", \"no\", \"never\", \"neither\", \"nor\", \"n't\", \"without\", \"lack\", \"none\"],\n",
    "    \"classical_chinese\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "    \"arabic\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "    \"hebrew\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "    \"aramaic\": [\"\", \"\", \"\"],\n",
    "    \"sanskrit\": [\"\", \"\", \"\"],\n",
    "    \"pali\": [\"na\", \"ma\", \"a\", \"an\"],\n",
    "}\n",
    "\n",
    "MODAL_CLASSIFICATION = {\n",
    "    \"english\": {\n",
    "        \"obligation\": [\"must\", \"shall\", \"have to\", \"ought to\", \"need to\", \"required\", \"obligated\"],\n",
    "        \"permission\": [\"may\", \"can\", \"allowed\", \"permitted\", \"free to\", \"entitled\"],\n",
    "        \"prohibition\": [\"must not\", \"shall not\", \"cannot\", \"forbidden\", \"prohibited\", \"banned\"],\n",
    "        \"supererogation\": [\"should\", \"ought\", \"would be good\", \"ideally\", \"preferably\"],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        \"obligation\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "        \"permission\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "        \"prohibition\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "        \"supererogation\": [\"\", \"\", \"\", \"\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        \"obligation\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "        \"permission\": [\"\", \"\", \"\", \"\"],\n",
    "        \"prohibition\": [\"\", \"\", \"\", \" \", \"\"],\n",
    "        \"supererogation\": [\"\", \"\", \"\", \"\"],\n",
    "    },\n",
    "    \"hebrew\": {\n",
    "        \"obligation\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "        \"permission\": [\"\", \"\", \"\", \"\"],\n",
    "        \"prohibition\": [\"\", \" \", \"\", \"\"],\n",
    "        \"supererogation\": [\"\", \"\", \" \", \"  \"],\n",
    "    },\n",
    "    \"sanskrit\": {\n",
    "        \"obligation\": [\"\", \"\", \"\"],\n",
    "        \"permission\": [\"\", \"\"],\n",
    "        \"prohibition\": [\"\", \"\", \"\"],\n",
    "    },\n",
    "    \"pali\": {\n",
    "        \"obligation\": [\"kicca\", \"karaniya\", \"dhamma\"],\n",
    "        \"permission\": [\"kappati\", \"anujanati\"],\n",
    "        \"prohibition\": [\"akappiya\", \"akaraniya\", \"na kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LEVEL 2: LANGUAGE-SPECIFIC GRAMMAR-AWARE EXTRACTORS (v10.16)\n",
    "# =============================================================================\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExtractedAgent:\n",
    "    \"\"\"Agent of the moral action.\"\"\"\n",
    "\n",
    "    text: str\n",
    "    position: str | None = None\n",
    "    case_marking: str | None = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExtractedPatient:\n",
    "    \"\"\"Patient/theme of the moral action.\"\"\"\n",
    "\n",
    "    text: str\n",
    "    position: str | None = None\n",
    "    case_marking: str | None = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MoralFeature:\n",
    "    \"\"\"A morally-relevant grammatical feature.\"\"\"\n",
    "\n",
    "    feature_type: str\n",
    "    value: str\n",
    "    source: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EnhancedBondResult:\n",
    "    \"\"\"Full bond extraction result.\"\"\"\n",
    "\n",
    "    bond_type: str | None = None\n",
    "    hohfeld_state: str = \"OBLIGATION\"\n",
    "    agent: ExtractedAgent | None = None\n",
    "    patient: ExtractedPatient | None = None\n",
    "    is_negated: bool = False\n",
    "    modal: str | None = None\n",
    "    context: str = \"unknown\"\n",
    "    moral_features: list[MoralFeature] = field(default_factory=list)\n",
    "    confidence: float = 0.5\n",
    "    method: str = \"unknown\"\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert to dict compatible with existing code.\"\"\"\n",
    "        return {\n",
    "            \"bond_type\": self.bond_type,\n",
    "            \"hohfeld_state\": self.hohfeld_state,\n",
    "            \"agent\": self.agent.text if self.agent else None,\n",
    "            \"patient\": self.patient.text if self.patient else None,\n",
    "            \"negated\": self.is_negated,\n",
    "            \"modal\": self.modal,\n",
    "            \"confidence\": self.confidence,\n",
    "            \"context\": self.context,\n",
    "            \"method\": self.method,\n",
    "        }\n",
    "\n",
    "\n",
    "class ClassicalChineseExtractor:\n",
    "    \"\"\"Classical Chinese extraction using position and particles.\"\"\"\n",
    "\n",
    "    PARTICLES = {\n",
    "        \"\": \"GEN\",\n",
    "        \"\": \"AGENT_NOM\",\n",
    "        \"\": \"PATIENT_NOM\",\n",
    "        \"\": \"PREP_LOC\",\n",
    "        \"\": \"PREP_INST\",\n",
    "        \"\": \"COPULA\",\n",
    "        \"\": \"PASSIVE\",\n",
    "        \"\": \"PASSIVE\",\n",
    "        \"\": \"CAUSATIVE\",\n",
    "        \"\": \"CAUSATIVE\",\n",
    "    }\n",
    "\n",
    "    MODALS = {\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"LIBERTY\",\n",
    "        \"\": \"LIBERTY\",\n",
    "        \"\": \"LIBERTY\",\n",
    "        \"\": \"PROHIBITION\",\n",
    "        \"\": \"PROHIBITION\",\n",
    "        \"\": \"PROHIBITION\",\n",
    "        \"\": \"PROHIBITION\",\n",
    "    }\n",
    "\n",
    "    NEGATION = {\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"}\n",
    "\n",
    "    PREDICATES = {\n",
    "        \"\": \"HARM_PREVENTION\",\n",
    "        \"\": \"HARM_PREVENTION\",\n",
    "        \"\": \"HARM_PREVENTION\",\n",
    "        \"\": \"CARE\",\n",
    "        \"\": \"CARE\",\n",
    "        \"\": \"CARE\",\n",
    "        \"\": \"CARE\",\n",
    "        \"\": \"AUTHORITY\",\n",
    "        \"\": \"AUTHORITY\",\n",
    "        \"\": \"FAMILY\",\n",
    "        \"\": \"FAMILY\",\n",
    "        \"\": \"FAIRNESS\",\n",
    "        \"\": \"FAIRNESS\",\n",
    "        \"\": \"FAIRNESS\",\n",
    "        \"\": \"RECIPROCITY\",\n",
    "        \"\": \"RECIPROCITY\",\n",
    "        \"\": \"RECIPROCITY\",\n",
    "        \"\": \"CARE\",\n",
    "    }\n",
    "\n",
    "    NOMINALS = {\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"}\n",
    "\n",
    "    def extract(self, text: str) -> EnhancedBondResult:\n",
    "        \"\"\"Extract bond from Classical Chinese text.\"\"\"\n",
    "        result = EnhancedBondResult(method=\"chinese_positional\")\n",
    "        chars = list(text)\n",
    "\n",
    "        # Find modal/deontic markers\n",
    "        for modal, hohfeld in self.MODALS.items():\n",
    "            if modal in text:\n",
    "                result.modal = modal\n",
    "                result.hohfeld_state = hohfeld\n",
    "                result.context = \"prescriptive\"\n",
    "                break\n",
    "\n",
    "        # Check negation\n",
    "        for neg in self.NEGATION:\n",
    "            if neg in text:\n",
    "                result.is_negated = True\n",
    "                break\n",
    "\n",
    "        # Check passive/causative markers\n",
    "        is_passive = False\n",
    "        for char in chars:\n",
    "            if char in (\"\", \"\") and DETECT_PASSIVES:\n",
    "                is_passive = True\n",
    "                result.moral_features.append(MoralFeature(\"voice\", \"passive\", char))\n",
    "            if char in (\"\", \"\") and DETECT_CAUSATIVES:\n",
    "                result.moral_features.append(MoralFeature(\"causation\", \"causative\", char))\n",
    "\n",
    "        # Find predicate and extract agent/patient\n",
    "        predicate_idx = -1\n",
    "        for i, char in enumerate(chars):\n",
    "            if char in self.PREDICATES and char not in self.NOMINALS:\n",
    "                predicate_idx = i\n",
    "                result.bond_type = self.PREDICATES[char]\n",
    "                break\n",
    "\n",
    "        if predicate_idx >= 0 and EXTRACT_AGENT_PATIENT:\n",
    "            # Agent: preverbal content\n",
    "            preverbal = []\n",
    "            for i in range(predicate_idx - 1, max(-1, predicate_idx - 4), -1):\n",
    "                char = chars[i]\n",
    "                if char in self.PARTICLES or char in self.NEGATION or char in self.MODALS:\n",
    "                    continue\n",
    "                preverbal.insert(0, char)\n",
    "                if char in self.NOMINALS or len(preverbal) >= 2:\n",
    "                    break\n",
    "\n",
    "            if preverbal:\n",
    "                agent_text = \"\".join(preverbal)\n",
    "                if is_passive:\n",
    "                    result.patient = ExtractedPatient(text=agent_text, position=\"preverbal\")\n",
    "                else:\n",
    "                    result.agent = ExtractedAgent(text=agent_text, position=\"preverbal\")\n",
    "\n",
    "            # Patient: postverbal content\n",
    "            postverbal = []\n",
    "            for i in range(predicate_idx + 1, min(len(chars), predicate_idx + 4)):\n",
    "                char = chars[i]\n",
    "                if char in (\"\", \"\", \"\", \"\", \"\"):\n",
    "                    break\n",
    "                if char in self.PARTICLES:\n",
    "                    continue\n",
    "                postverbal.append(char)\n",
    "                if char in self.NOMINALS or len(postverbal) >= 2:\n",
    "                    break\n",
    "\n",
    "            if postverbal:\n",
    "                patient_text = \"\".join(postverbal)\n",
    "                if is_passive:\n",
    "                    result.agent = ExtractedAgent(text=patient_text, position=\"postverbal\")\n",
    "                else:\n",
    "                    result.patient = ExtractedPatient(text=patient_text, position=\"postverbal\")\n",
    "\n",
    "        # Calculate confidence\n",
    "        result.confidence = 0.5\n",
    "        if result.bond_type:\n",
    "            result.confidence += 0.2\n",
    "        if result.agent or result.patient:\n",
    "            result.confidence += 0.1\n",
    "        if result.modal:\n",
    "            result.confidence += 0.1\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class ArabicExtractor:\n",
    "    \"\"\"Arabic extraction with verb form (wazan) analysis.\"\"\"\n",
    "\n",
    "    MODALS = {\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"LIBERTY\",\n",
    "        \"\": \"LIBERTY\",\n",
    "        \"\": \"LIBERTY\",\n",
    "        \"\": \"NO_RIGHT\",\n",
    "        \"\": \"NO_RIGHT\",\n",
    "        \"\": \"NO_RIGHT\",\n",
    "    }\n",
    "\n",
    "    NEGATION = {\"\", \"\", \"\", \"\", \"\", \"\"}\n",
    "\n",
    "    def extract(self, text: str) -> EnhancedBondResult:\n",
    "        \"\"\"Extract bond from Arabic text.\"\"\"\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "        text = re.sub(r\"[\\u064B-\\u065F]\", \"\", text)\n",
    "\n",
    "        result = EnhancedBondResult(method=\"arabic_morphological\")\n",
    "\n",
    "        for modal, hohfeld in self.MODALS.items():\n",
    "            if modal in text:\n",
    "                result.modal = modal\n",
    "                result.hohfeld_state = hohfeld\n",
    "                result.context = \"prescriptive\"\n",
    "                break\n",
    "\n",
    "        for neg in self.NEGATION:\n",
    "            if neg in text:\n",
    "                result.is_negated = True\n",
    "                break\n",
    "\n",
    "        result.confidence = 0.5\n",
    "        if result.modal:\n",
    "            result.confidence += 0.2\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class HebrewAramaicExtractor:\n",
    "    \"\"\"Hebrew/Aramaic extraction with modal markers.\"\"\"\n",
    "\n",
    "    MODALS = {\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"LIBERTY\",\n",
    "        \"\": \"LIBERTY\",\n",
    "        \"\": \"LIBERTY\",\n",
    "        \"\": \"NO_RIGHT\",\n",
    "        \" \": \"NO_RIGHT\",\n",
    "    }\n",
    "\n",
    "    NEGATION = {\"\", \"\", \"\", \"\", \"\"}\n",
    "\n",
    "    def extract(self, text: str, language: str = \"hebrew\") -> EnhancedBondResult:\n",
    "        \"\"\"Extract bond from Hebrew/Aramaic text.\"\"\"\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "        text = re.sub(r\"[\\u0591-\\u05C7]\", \"\", text)\n",
    "\n",
    "        result = EnhancedBondResult(method=\"hebrew_morphological\")\n",
    "\n",
    "        for modal, hohfeld in self.MODALS.items():\n",
    "            if modal in text:\n",
    "                result.modal = modal\n",
    "                result.hohfeld_state = hohfeld\n",
    "                result.context = \"prescriptive\"\n",
    "                break\n",
    "\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if word in self.NEGATION:\n",
    "                result.is_negated = True\n",
    "                break\n",
    "\n",
    "        result.confidence = 0.5\n",
    "        if result.modal:\n",
    "            result.confidence += 0.2\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class SanskritExtractor:\n",
    "    \"\"\"Sanskrit extraction using karaka theory.\"\"\"\n",
    "\n",
    "    MODALS = {\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"OBLIGATION\",\n",
    "        \"\": \"LIBERTY\",\n",
    "        \"\": \"LIBERTY\",\n",
    "        \"\": \"NO_RIGHT\",\n",
    "        \"\": \"NO_RIGHT\",\n",
    "    }\n",
    "\n",
    "    NEGATION = {\"\", \"\"}\n",
    "\n",
    "    def extract(self, text: str) -> EnhancedBondResult:\n",
    "        \"\"\"Extract bond from Sanskrit text.\"\"\"\n",
    "        text = unicodedata.normalize(\"NFC\", text)\n",
    "        result = EnhancedBondResult(method=\"sanskrit_karaka\")\n",
    "\n",
    "        for modal, hohfeld in self.MODALS.items():\n",
    "            if modal in text:\n",
    "                result.modal = modal\n",
    "                result.hohfeld_state = hohfeld\n",
    "                result.context = \"prescriptive\"\n",
    "                break\n",
    "\n",
    "        for neg in self.NEGATION:\n",
    "            if neg in text:\n",
    "                result.is_negated = True\n",
    "                break\n",
    "\n",
    "        # Extract agent/patient from case endings if enabled\n",
    "        if EXTRACT_AGENT_PATIENT:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                if word.endswith(\"\") and not result.agent:\n",
    "                    result.agent = ExtractedAgent(text=word, case_marking=\"pratham\")\n",
    "                elif word.endswith(\"\") and not result.patient:\n",
    "                    result.patient = ExtractedPatient(text=word, case_marking=\"dvity\")\n",
    "\n",
    "        result.confidence = 0.5\n",
    "        if result.modal:\n",
    "            result.confidence += 0.2\n",
    "        if result.agent or result.patient:\n",
    "            result.confidence += 0.1\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LEVEL 3: SPACY-BASED EXTRACTOR (English)\n",
    "# =============================================================================\n",
    "\n",
    "_spacy_nlp = None\n",
    "_spacy_available = None\n",
    "\n",
    "\n",
    "def _load_spacy():\n",
    "    \"\"\"Lazy-load spaCy model.\"\"\"\n",
    "    global _spacy_nlp, _spacy_available\n",
    "\n",
    "    if _spacy_available is not None:\n",
    "        return _spacy_nlp\n",
    "\n",
    "    try:\n",
    "        import spacy\n",
    "\n",
    "        _spacy_nlp = spacy.load(\"en_core_web_md\")\n",
    "        _spacy_available = True\n",
    "        print(\"  spaCy en_core_web_md loaded\")\n",
    "    except (ImportError, OSError):\n",
    "        if INSTALL_SPACY_IF_NEEDED:\n",
    "            print(\"  Installing spaCy...\")\n",
    "            import subprocess\n",
    "            import sys\n",
    "\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"spacy\"])\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_md\"])\n",
    "            import spacy\n",
    "\n",
    "            _spacy_nlp = spacy.load(\"en_core_web_md\")\n",
    "            _spacy_available = True\n",
    "            print(\"  spaCy installed and loaded\")\n",
    "        else:\n",
    "            _spacy_available = False\n",
    "            print(\"  spaCy not available, using level2 for English\")\n",
    "\n",
    "    return _spacy_nlp\n",
    "\n",
    "\n",
    "class SpacyEnglishExtractor:\n",
    "    \"\"\"English extraction using spaCy dependency parsing.\"\"\"\n",
    "\n",
    "    AGENT_DEPS = {\"nsubj\", \"nsubjpass\", \"agent\", \"csubj\"}\n",
    "    PATIENT_DEPS = {\"dobj\", \"obj\", \"iobj\", \"pobj\", \"dative\", \"nsubjpass\"}\n",
    "\n",
    "    MODAL_HOHFELD = {\n",
    "        \"must\": \"OBLIGATION\",\n",
    "        \"shall\": \"OBLIGATION\",\n",
    "        \"should\": \"OBLIGATION\",\n",
    "        \"ought\": \"OBLIGATION\",\n",
    "        \"may\": \"LIBERTY\",\n",
    "        \"can\": \"LIBERTY\",\n",
    "        \"could\": \"LIBERTY\",\n",
    "        \"might\": \"LIBERTY\",\n",
    "    }\n",
    "\n",
    "    NEGATION_WORDS = {\"not\", \"n't\", \"never\", \"no\", \"none\", \"neither\", \"nor\"}\n",
    "\n",
    "    BOND_VERBS = {\n",
    "        \"harm\": [\"kill\", \"harm\", \"hurt\", \"injure\", \"damage\", \"destroy\", \"steal\"],\n",
    "        \"care\": [\"help\", \"protect\", \"save\", \"care\", \"nurture\", \"support\", \"heal\"],\n",
    "        \"authority\": [\"obey\", \"command\", \"order\", \"rule\"],\n",
    "        \"family\": [\"honor\", \"respect\"],\n",
    "        \"reciprocity\": [\"repay\", \"return\", \"owe\", \"borrow\", \"lend\"],\n",
    "        \"contract\": [\"promise\", \"vow\", \"pledge\"],\n",
    "        \"property\": [\"possess\", \"own\", \"steal\"],\n",
    "        \"fairness\": [\"deserve\", \"merit\"],\n",
    "        \"autonomy\": [\"choose\", \"consent\"],\n",
    "    }\n",
    "\n",
    "    def extract(self, text: str) -> EnhancedBondResult:\n",
    "        \"\"\"Extract bond from English text using spaCy.\"\"\"\n",
    "        nlp = _load_spacy()\n",
    "        if nlp is None:\n",
    "            return EnhancedBondResult(method=\"spacy_unavailable\", confidence=0.3)\n",
    "\n",
    "        doc = nlp(text[:1000])\n",
    "        result = EnhancedBondResult(method=\"spacy_dependency\")\n",
    "\n",
    "        # Extract agent/patient from dependencies\n",
    "        if EXTRACT_AGENT_PATIENT:\n",
    "            for token in doc:\n",
    "                if token.pos_ == \"VERB\" or token.dep_ == \"ROOT\":\n",
    "                    for child in token.children:\n",
    "                        if child.dep_ in self.AGENT_DEPS and not result.agent:\n",
    "                            agent_text = \" \".join(t.text for t in child.subtree)\n",
    "                            result.agent = ExtractedAgent(text=agent_text)\n",
    "                        elif child.dep_ in self.PATIENT_DEPS and not result.patient:\n",
    "                            patient_text = \" \".join(t.text for t in child.subtree)\n",
    "                            result.patient = ExtractedPatient(text=patient_text)\n",
    "                    break\n",
    "\n",
    "        # Extract modal and Hohfeld state\n",
    "        for token in doc:\n",
    "            if token.tag_ == \"MD\" or token.pos_ == \"AUX\":\n",
    "                text_lower = token.text.lower()\n",
    "                for modal, hohfeld in self.MODAL_HOHFELD.items():\n",
    "                    if modal == text_lower:\n",
    "                        result.modal = modal\n",
    "                        result.hohfeld_state = hohfeld\n",
    "                        result.context = \"prescriptive\"\n",
    "\n",
    "                        # Check negation scope\n",
    "                        for child in token.children:\n",
    "                            if child.dep_ == \"neg\" or child.text.lower() in self.NEGATION_WORDS:\n",
    "                                result.is_negated = True\n",
    "                                if hohfeld == \"OBLIGATION\":\n",
    "                                    result.hohfeld_state = \"NO_RIGHT\"\n",
    "                        break\n",
    "\n",
    "        # Detect bond type from verbs\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                lemma = token.lemma_.lower()\n",
    "                for bond_type, verbs in self.BOND_VERBS.items():\n",
    "                    if lemma in verbs:\n",
    "                        result.bond_type = bond_type.upper()\n",
    "                        if bond_type == \"harm\":\n",
    "                            result.bond_type = \"HARM_PREVENTION\"\n",
    "                        break\n",
    "\n",
    "        # Confidence\n",
    "        result.confidence = 0.5\n",
    "        if result.bond_type:\n",
    "            result.confidence += 0.2\n",
    "        if result.agent:\n",
    "            result.confidence += 0.1\n",
    "        if result.patient:\n",
    "            result.confidence += 0.1\n",
    "        if result.modal:\n",
    "            result.confidence += 0.1\n",
    "        result.confidence = min(result.confidence, 0.95)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# UNIFIED EXTRACTOR\n",
    "# =============================================================================\n",
    "\n",
    "_chinese_extractor = ClassicalChineseExtractor()\n",
    "_arabic_extractor = ArabicExtractor()\n",
    "_hebrew_extractor = HebrewAramaicExtractor()\n",
    "_sanskrit_extractor = SanskritExtractor()\n",
    "_spacy_extractor = SpacyEnglishExtractor()\n",
    "\n",
    "\n",
    "def extract_bond_level2(text: str, language: str) -> EnhancedBondResult:\n",
    "    \"\"\"Level 2: Language-specific grammar-aware extraction.\"\"\"\n",
    "    if language == \"classical_chinese\":\n",
    "        return _chinese_extractor.extract(text)\n",
    "    elif language == \"arabic\":\n",
    "        return _arabic_extractor.extract(text)\n",
    "    elif language in (\"hebrew\", \"aramaic\"):\n",
    "        return _hebrew_extractor.extract(text, language)\n",
    "    elif language in (\"sanskrit\", \"pali\"):\n",
    "        return _sanskrit_extractor.extract(text)\n",
    "    else:\n",
    "        # Fallback to regex for other languages\n",
    "        result = enhanced_extract_bond_regex(text, language)\n",
    "        return EnhancedBondResult(\n",
    "            bond_type=result.get(\"bond_type\"),\n",
    "            hohfeld_state=result.get(\"hohfeld_state\", \"OBLIGATION\"),\n",
    "            is_negated=result.get(\"negated\", False),\n",
    "            modal=result.get(\"modal\"),\n",
    "            confidence=result.get(\"confidence\", 0.5),\n",
    "            context=result.get(\"context\", \"unknown\"),\n",
    "            method=\"regex_fallback\",\n",
    "        )\n",
    "\n",
    "\n",
    "def extract_bond_level3(text: str, language: str) -> EnhancedBondResult:\n",
    "    \"\"\"Level 3: spaCy for English, level2 for others.\"\"\"\n",
    "    if language == \"english\" and USE_SPACY_FOR_ENGLISH:\n",
    "        return _spacy_extractor.extract(text)\n",
    "    else:\n",
    "        return extract_bond_level2(text, language)\n",
    "\n",
    "\n",
    "def unified_extract_bond(text: str, language: str) -> dict:\n",
    "    \"\"\"\n",
    "    Unified bond extraction based on EXTRACTION_LEVEL setting.\n",
    "    Returns dict compatible with existing code.\n",
    "\n",
    "    v10.16.1: Falls back to level1 regex if level2/3 doesn't find a bond_type.\n",
    "    This ensures we get meaningful bond labels for training.\n",
    "    \"\"\"\n",
    "    if EXTRACTION_LEVEL == \"level1\":\n",
    "        return enhanced_extract_bond_regex(text, language)\n",
    "    elif EXTRACTION_LEVEL == \"level2\":\n",
    "        result = extract_bond_level2(text, language)\n",
    "        d = result.to_dict()\n",
    "        # Fallback: if level2 didn't find a bond_type, try level1 regex\n",
    "        if d.get(\"bond_type\") is None:\n",
    "            regex_result = enhanced_extract_bond_regex(text, language)\n",
    "            if regex_result.get(\"bond_type\"):\n",
    "                d[\"bond_type\"] = regex_result[\"bond_type\"]\n",
    "                d[\"method\"] = d.get(\"method\", \"unknown\") + \"+regex_fallback\"\n",
    "        return d\n",
    "    elif EXTRACTION_LEVEL == \"level3\":\n",
    "        result = extract_bond_level3(text, language)\n",
    "        d = result.to_dict()\n",
    "        # Fallback: if level3 didn't find a bond_type, try level1 regex\n",
    "        if d.get(\"bond_type\") is None:\n",
    "            regex_result = enhanced_extract_bond_regex(text, language)\n",
    "            if regex_result.get(\"bond_type\"):\n",
    "                d[\"bond_type\"] = regex_result[\"bond_type\"]\n",
    "                d[\"method\"] = d.get(\"method\", \"unknown\") + \"+regex_fallback\"\n",
    "        return d\n",
    "    else:\n",
    "        return enhanced_extract_bond_regex(text, language)\n",
    "\n",
    "\n",
    "print(f\"\\nExtraction level: {EXTRACTION_LEVEL}\")\n",
    "if EXTRACTION_LEVEL == \"level2\":\n",
    "    print(\"  Chinese: position + particles\")\n",
    "    print(\"  Arabic: verb form (wazan) analysis\")\n",
    "    print(\"  Hebrew: modal markers + binyan\")\n",
    "    print(\"  Sanskrit: karaka (case) analysis\")\n",
    "elif EXTRACTION_LEVEL == \"level3\":\n",
    "    print(\"  English: spaCy dependency parsing\")\n",
    "    print(\"  Others: level2 grammar analysis\")\n",
    "\n",
    "\n",
    "def enhanced_extract_bond_regex(text: str, language: str) -> dict:\n",
    "    \"\"\"Enhanced bond extraction with negation + modal detection.\"\"\"\n",
    "    normalized = normalize_text(text, language)\n",
    "\n",
    "    negation_cues = NEGATION_CUES.get(language, [])\n",
    "    is_negated = any(cue in normalized for cue in negation_cues)\n",
    "\n",
    "    modal_status = \"unknown\"\n",
    "    modal_text = None\n",
    "    for status, markers in MODAL_CLASSIFICATION.get(language, {}).items():\n",
    "        for marker in markers:\n",
    "            if marker in normalized:\n",
    "                modal_status = status\n",
    "                modal_text = marker\n",
    "                break\n",
    "        if modal_status != \"unknown\":\n",
    "            break\n",
    "\n",
    "    hohfeld_map = {\n",
    "        \"obligation\": \"OBLIGATION\",\n",
    "        \"permission\": \"LIBERTY\",\n",
    "        \"prohibition\": \"NO_RIGHT\",\n",
    "        \"supererogation\": \"LIBERTY\",\n",
    "        \"unknown\": \"OBLIGATION\",\n",
    "    }\n",
    "    hohfeld = hohfeld_map[modal_status]\n",
    "\n",
    "    bond_type = None\n",
    "    confidence = 0.5\n",
    "    for bt, patterns in ALL_BOND_PATTERNS.get(language, {}).items():\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, normalized):\n",
    "                bond_type = bt\n",
    "                confidence = 0.9\n",
    "                break\n",
    "        if bond_type:\n",
    "            break\n",
    "\n",
    "    if is_negated:\n",
    "        confidence *= 0.8\n",
    "\n",
    "    if modal_status in [\"obligation\", \"prohibition\"]:\n",
    "        context = \"prescriptive\"\n",
    "    elif modal_status == \"permission\":\n",
    "        context = \"descriptive\"\n",
    "    else:\n",
    "        context = \"unknown\"\n",
    "\n",
    "    return {\n",
    "        \"bond_type\": bond_type.name if bond_type else None,\n",
    "        \"hohfeld_state\": hohfeld,\n",
    "        \"negated\": is_negated,\n",
    "        \"modal\": modal_text,\n",
    "        \"confidence\": confidence,\n",
    "        \"context\": context,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\nContext markers defined for grammar-aware extraction\")\n",
    "print(\"  Detects: negation, obligation, prohibition, permission\")\n",
    "\n",
    "print(f\"\\nPatterns defined for {len(ALL_BOND_PATTERNS)} languages:\")\n",
    "for lang in ALL_BOND_PATTERNS:\n",
    "    n = sum(len(p) for p in ALL_BOND_PATTERNS[lang].values())\n",
    "    print(f\"  {lang}: {n} bond patterns\")\n",
    "\n",
    "print(\"\\nNLP improvements (Phase 1):\")\n",
    "print(f\"  NEGATION_CUES: {len(NEGATION_CUES)} languages\")\n",
    "print(f\"  MODAL_CLASSIFICATION: {len(MODAL_CLASSIFICATION)} languages\")\n",
    "print(\"  enhanced_extract_bond_regex() ready (level1)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACT BONDS FROM PASSAGES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"EXTRACTING BONDS FROM PASSAGES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "passages_file = Path(\"data/processed/passages.jsonl\")\n",
    "bonds_file = Path(\"data/processed/bonds.jsonl\")\n",
    "\n",
    "if bonds_file.exists():\n",
    "    with open(bonds_file, encoding=\"utf-8\") as f:\n",
    "        bond_count = sum(1 for _ in f)\n",
    "    print(f\"  bonds.jsonl exists with {bond_count:,} bonds (cached)\")\n",
    "else:\n",
    "    passages = []\n",
    "    with open(passages_file, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            passages.append(json.loads(line))\n",
    "\n",
    "    print(f\"  Processing {len(passages):,} passages...\")\n",
    "\n",
    "    bonds = []\n",
    "    bond_type_counts = {}\n",
    "\n",
    "    for i, p in enumerate(passages):\n",
    "        try:\n",
    "            text = normalize_text(p[\"text\"], p[\"language\"])\n",
    "            bond = unified_extract_bond(text, p[\"language\"])\n",
    "            bond[\"passage_id\"] = p[\"id\"]\n",
    "            bonds.append(bond)\n",
    "\n",
    "            bt = bond.get(\"bond_type\") or \"NEUTRAL\"\n",
    "            bond_type_counts[bt] = bond_type_counts.get(bt, 0) + 1\n",
    "\n",
    "        except Exception:\n",
    "            bonds.append(\n",
    "                {\n",
    "                    \"passage_id\": p[\"id\"],\n",
    "                    \"bond_type\": \"NEUTRAL\",\n",
    "                    \"hohfeld_state\": \"LIBERTY\",\n",
    "                    \"negated\": False,\n",
    "                    \"modal\": None,\n",
    "                    \"confidence\": 0.1,\n",
    "                    \"context\": \"unknown\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if (i + 1) % 20000 == 0:\n",
    "            print(f\"    {i + 1:,} processed...\")\n",
    "\n",
    "    bonds_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(bonds_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for b in bonds:\n",
    "            f.write(json.dumps(b, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\nSaved {len(bonds):,} bonds to {bonds_file}\")\n",
    "    print(\"\\nBond type distribution:\")\n",
    "    for bt, count in sorted(bond_type_counts.items(), key=lambda x: -x[1]):\n",
    "        pct = 100 * count / len(bonds)\n",
    "        print(f\"    {bt:12s}: {count:6,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BOND EXTRACTION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =============================================================================\n",
    "# MERGE ETHICS CORPUS (v10.16)\n",
    "# =============================================================================\n",
    "# Integrate labeled ethics datasets from Cell 3 into main corpus\n",
    "\n",
    "ethics_corpus_file = Path(\"data/bond_training/ethics_corpus.jsonl\")\n",
    "\n",
    "if ethics_corpus_file.exists():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MERGING ETHICS CORPUS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load existing passages and bonds\n",
    "    existing_passage_ids = set()\n",
    "    with open(\"data/processed/passages.jsonl\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            p = json.loads(line)\n",
    "            existing_passage_ids.add(p[\"id\"])\n",
    "\n",
    "    existing_bond_ids = set()\n",
    "    with open(\"data/processed/bonds.jsonl\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            b = json.loads(line)\n",
    "            existing_bond_ids.add(b[\"passage_id\"])\n",
    "\n",
    "    print(f\"  Existing passages: {len(existing_passage_ids):,}\")\n",
    "    print(f\"  Existing bonds: {len(existing_bond_ids):,}\")\n",
    "\n",
    "    # Load ethics corpus\n",
    "    ethics_passages = []\n",
    "    ethics_bonds = []\n",
    "\n",
    "    with open(ethics_corpus_file, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            p = json.loads(line)\n",
    "\n",
    "            # Skip if already exists\n",
    "            if p[\"id\"] in existing_passage_ids:\n",
    "                continue\n",
    "\n",
    "            # Extract bond info from the passage\n",
    "            bond_info = p.get(\"bonds\", [{}])[0]\n",
    "\n",
    "            # Map ethics bond types to BIP bond types\n",
    "            ethics_to_bip = {\n",
    "                \"OBLIGATION\": \"AUTHORITY\",  # Deontological duty\n",
    "                \"PROHIBITION\": \"HARM_PREVENTION\",  # Don't do X\n",
    "                \"PERMISSION\": \"AUTONOMY\",  # May do X\n",
    "                \"CLAIM\": \"FAIRNESS\",  # Has right to X\n",
    "                \"VIRTUE\": \"CARE\",  # Character-based\n",
    "                \"VICE\": \"HARM_PREVENTION\",  # Negative trait\n",
    "                \"SUPEREROGATORY\": \"CARE\",  # Beyond duty\n",
    "                \"DUTY\": \"AUTHORITY\",  # Hohfeld duty\n",
    "                \"LIBERTY\": \"AUTONOMY\",  # Hohfeld liberty\n",
    "            }\n",
    "\n",
    "            raw_bond = bond_info.get(\"bond_type\", \"OBLIGATION\")\n",
    "            mapped_bond = ethics_to_bip.get(raw_bond, \"AUTHORITY\")\n",
    "\n",
    "            # Create passage entry\n",
    "            ethics_passages.append(\n",
    "                {\n",
    "                    \"id\": p[\"id\"],\n",
    "                    \"text\": p[\"text\"],\n",
    "                    \"language\": \"english\",\n",
    "                    \"time_periods\": p.get(\"time_periods\", [\"MODERN_ETHICS\"]),\n",
    "                    \"tags\": p.get(\"tags\", [\"modern\", \"english\", \"ethics\"]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Create bond entry with high confidence (labeled data!)\n",
    "            ethics_bonds.append(\n",
    "                {\n",
    "                    \"passage_id\": p[\"id\"],\n",
    "                    \"bond_type\": mapped_bond,\n",
    "                    \"hohfeld_state\": bond_info.get(\"hohfeld_state\", \"OBLIGATION\"),\n",
    "                    \"negated\": False,\n",
    "                    \"modal\": None,\n",
    "                    \"confidence\": bond_info.get(\"confidence\", 0.8),  # High confidence - labeled!\n",
    "                    \"context\": bond_info.get(\"context\", \"prescriptive\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Append to passages.jsonl\n",
    "    with open(\"data/processed/passages.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "        for p in ethics_passages:\n",
    "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Append to bonds.jsonl\n",
    "    with open(\"data/processed/bonds.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "        for b in ethics_bonds:\n",
    "            f.write(json.dumps(b, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"  Added {len(ethics_passages):,} ethics passages\")\n",
    "    print(f\"  Added {len(ethics_bonds):,} ethics bonds (labeled, high confidence)\")\n",
    "\n",
    "    # Show bond type distribution of added data\n",
    "    from collections import Counter\n",
    "\n",
    "    bond_dist = Counter(b[\"bond_type\"] for b in ethics_bonds)\n",
    "    print(\"\\n  Ethics bond distribution:\")\n",
    "    for bt, count in bond_dist.most_common():\n",
    "        print(f\"    {bt}: {count:,}\")\n",
    "\n",
    "    print(\"\\n  Ethics corpus merged successfully!\")\n",
    "else:\n",
    "    print(\"\\n[Note] No ethics corpus found - run Cell 3 to generate ethics_corpus.jsonl\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CORPUS INTEGRATION COMPLETE\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell_5",
    "outputId": "5f786cac-d4d9-49f9-c2b0-eb74e13bac50"
   },
   "outputs": [],
   "source": [
    "# @title 5. Generate Splits { display-mode: \"form\" }\n",
    "# @markdown v10.13: Tag-based splits with matrix selection\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ## Split Matrix\n",
    "# @markdown Select train/test tags using dropdowns. Use \"none\" to disable.\n",
    "\n",
    "# @markdown ### Experiment 1\n",
    "EXP1_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP1_NAME = \"hebrew_to_others\"  # @param {type:\"string\"}\n",
    "EXP1_TRAIN = \"hebrew\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP1_TEST = \"all-other\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 2\n",
    "EXP2_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP2_NAME = \"semitic_to_indic\"  # @param {type:\"string\"}\n",
    "EXP2_TRAIN = \"semitic\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP2_TEST = \"indic\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 3\n",
    "EXP3_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP3_NAME = \"confucian_to_buddhist\"  # @param {type:\"string\"}\n",
    "EXP3_TRAIN = \"confucian\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP3_TEST = \"buddhist\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 4\n",
    "EXP4_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP4_NAME = \"ancient_to_modern\"  # @param {type:\"string\"}\n",
    "EXP4_TRAIN = \"ancient\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP4_TEST = \"modern\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 5\n",
    "EXP5_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP5_NAME = \"east_to_west\"  # @param {type:\"string\"}\n",
    "EXP5_TRAIN = \"east-asia\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP5_TEST = \"western\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 6\n",
    "EXP6_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP6_NAME = \"semitic_to_chinese\"  # @param {type:\"string\"}\n",
    "EXP6_TRAIN = \"semitic\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP6_TEST = \"chinese\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 7\n",
    "EXP7_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP7_NAME = \"jewish_to_islamic\"  # @param {type:\"string\"}\n",
    "EXP7_TRAIN = \"hebrew\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP7_TEST = \"arabic\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 8\n",
    "EXP8_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP8_NAME = \"stoic_to_confucian\"  # @param {type:\"string\"}\n",
    "EXP8_TRAIN = \"stoic\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP8_TEST = \"confucian\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 9\n",
    "EXP9_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP9_NAME = \"daoist_to_buddhist\"  # @param {type:\"string\"}\n",
    "EXP9_TRAIN = \"daoist\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP9_TEST = \"buddhist\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 10\n",
    "EXP10_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP10_NAME = \"hindu_to_buddhist\"  # @param {type:\"string\"}\n",
    "EXP10_TRAIN = \"hindu\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP10_TEST = \"buddhist\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 11\n",
    "EXP11_ENABLE = False  # @param {type:\"boolean\"}\n",
    "EXP11_NAME = \"custom_11\"  # @param {type:\"string\"}\n",
    "EXP11_TRAIN = \"none\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP11_TEST = \"none\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 12\n",
    "EXP12_ENABLE = False  # @param {type:\"boolean\"}\n",
    "EXP12_NAME = \"custom_12\"  # @param {type:\"string\"}\n",
    "EXP12_TRAIN = \"none\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP12_TEST = \"none\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ## Options\n",
    "INCLUDE_MIXED_BASELINE = True  # @param {type:\"boolean\"}\n",
    "MIN_SPLIT_SIZE = 50  # @param {type:\"integer\"}\n",
    "\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATING SPLITS (v10.13)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =============================================================================\n",
    "# TAG DEFINITIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Compound tag groups\n",
    "TAG_GROUPS = {\n",
    "    \"semitic\": [\"hebrew\", \"aramaic\", \"arabic\"],\n",
    "    \"indic\": [\"sanskrit\", \"pali\", \"hindi\"],\n",
    "    \"east-asia\": [\"chinese\", \"confucian\", \"daoist\"],\n",
    "    \"south-asia\": [\"sanskrit\", \"pali\", \"hindu\", \"buddhist\"],\n",
    "    \"middle-east\": [\"hebrew\", \"aramaic\", \"arabic\", \"jewish\", \"islamic\"],\n",
    "    \"western\": [\"english\", \"greek\", \"latin\", \"stoic\"],\n",
    "    \"ancient\": [\"ancient\", \"classical\"],\n",
    "    \"modern\": [\"modern\", \"advice\", \"american\"],\n",
    "}\n",
    "\n",
    "# Period to tags mapping\n",
    "PERIOD_TO_TAGS = {\n",
    "    \"CONFUCIAN\": [\"confucian\", \"east-asia\", \"classical\", \"chinese\"],\n",
    "    \"DAOIST\": [\"daoist\", \"east-asia\", \"classical\", \"chinese\"],\n",
    "    \"BUDDHIST\": [\"buddhist\"],\n",
    "    \"PALI\": [\"buddhist\", \"south-asia\", \"ancient\", \"pali\"],\n",
    "    \"DHARMA\": [\"hindu\", \"south-asia\", \"ancient\", \"sanskrit\"],\n",
    "    \"BIBLICAL\": [\"jewish\", \"middle-east\", \"ancient\", \"hebrew\"],\n",
    "    \"TANNAITIC\": [\"jewish\", \"middle-east\", \"classical\", \"hebrew\"],\n",
    "    \"AMORAIC\": [\"jewish\", \"middle-east\", \"classical\", \"aramaic\"],\n",
    "    \"QURANIC\": [\"islamic\", \"middle-east\", \"medieval\", \"arabic\"],\n",
    "    \"HADITH\": [\"islamic\", \"middle-east\", \"medieval\", \"arabic\"],\n",
    "    \"CLASSICAL_GREEK\": [\"stoic\", \"mediterranean\", \"classical\", \"greek\"],\n",
    "    \"HELLENISTIC\": [\"stoic\", \"mediterranean\", \"classical\", \"greek\"],  # Epictetus, Marcus Aurelius\n",
    "    \"CLASSICAL_LATIN\": [\"stoic\", \"mediterranean\", \"classical\", \"latin\"],  # Seneca, Cicero\n",
    "    \"DEAR_ABBY\": [\"american\", \"modern\", \"advice\", \"english\", \"western\"],\n",
    "    \"MODERN_ETHICS\": [\"western\", \"modern\", \"ethics\", \"english\"],\n",
    "}\n",
    "\n",
    "LANG_TO_TAGS = {\n",
    "    \"classical_chinese\": [\"chinese\", \"east-asia\"],\n",
    "    \"hebrew\": [\"hebrew\", \"middle-east\"],\n",
    "    \"aramaic\": [\"aramaic\", \"middle-east\"],\n",
    "    \"arabic\": [\"arabic\", \"middle-east\"],\n",
    "    \"sanskrit\": [\"sanskrit\", \"south-asia\"],\n",
    "    \"pali\": [\"pali\", \"south-asia\"],\n",
    "    \"greek\": [\"greek\", \"mediterranean\"],\n",
    "    \"latin\": [\"latin\", \"mediterranean\"],\n",
    "    \"english\": [\"english\", \"western\"],\n",
    "}\n",
    "\n",
    "\n",
    "def add_tags(p: dict) -> list:\n",
    "    \"\"\"Generate tags for a passage.\"\"\"\n",
    "    tags = set()\n",
    "\n",
    "    lang = p.get(\"language\", \"\")\n",
    "    if lang in LANG_TO_TAGS:\n",
    "        tags.update(LANG_TO_TAGS[lang])\n",
    "\n",
    "    for period in p.get(\"time_periods\", []):\n",
    "        if period in PERIOD_TO_TAGS:\n",
    "            tags.update(PERIOD_TO_TAGS[period])\n",
    "\n",
    "    return sorted(tags)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD PASSAGES\n",
    "# =============================================================================\n",
    "\n",
    "passages_file = Path(\"data/processed/passages.jsonl\")\n",
    "if not passages_file.exists():\n",
    "    raise FileNotFoundError(\"Run Cell 2 first to generate passages.jsonl\")\n",
    "\n",
    "passage_meta = []\n",
    "with open(passages_file, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        p = json.loads(line)\n",
    "        passage_meta.append(\n",
    "            {\n",
    "                \"id\": p[\"id\"],\n",
    "                \"language\": p.get(\"language\", \"\"),\n",
    "                \"tags\": add_tags(p),\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(f\"Loaded {len(passage_meta):,} passages\")\n",
    "\n",
    "# Count tags\n",
    "tag_counts = defaultdict(int)\n",
    "for p in passage_meta:\n",
    "    for tag in p[\"tags\"]:\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "print(\"\\nTag counts:\")\n",
    "for tag, count in sorted(tag_counts.items(), key=lambda x: -x[1])[:15]:\n",
    "    print(f\"  {tag}: {count:,}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SPLIT HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def expand_tag(tag: str) -> list:\n",
    "    \"\"\"Expand compound tags like 'semitic' to individual tags.\"\"\"\n",
    "    if tag in TAG_GROUPS:\n",
    "        return TAG_GROUPS[tag]\n",
    "    return [tag]\n",
    "\n",
    "\n",
    "def ids_with_tags(tags: list) -> list:\n",
    "    \"\"\"Get passage IDs with ANY of the tags.\"\"\"\n",
    "    tag_set = set()\n",
    "    for t in tags:\n",
    "        tag_set.update(expand_tag(t))\n",
    "    return [p[\"id\"] for p in passage_meta if set(p[\"tags\"]) & tag_set]\n",
    "\n",
    "\n",
    "def ids_without_tags(tags: list) -> list:\n",
    "    \"\"\"Get passage IDs with NONE of the tags.\"\"\"\n",
    "    tag_set = set()\n",
    "    for t in tags:\n",
    "        tag_set.update(expand_tag(t))\n",
    "    return [p[\"id\"] for p in passage_meta if not (set(p[\"tags\"]) & tag_set)]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATE SPLITS FROM MATRIX\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING SPLITS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_splits = {}\n",
    "random.seed(42)\n",
    "\n",
    "experiments = [\n",
    "    (EXP1_ENABLE, EXP1_NAME, EXP1_TRAIN, EXP1_TEST),\n",
    "    (EXP2_ENABLE, EXP2_NAME, EXP2_TRAIN, EXP2_TEST),\n",
    "    (EXP3_ENABLE, EXP3_NAME, EXP3_TRAIN, EXP3_TEST),\n",
    "    (EXP4_ENABLE, EXP4_NAME, EXP4_TRAIN, EXP4_TEST),\n",
    "    (EXP5_ENABLE, EXP5_NAME, EXP5_TRAIN, EXP5_TEST),\n",
    "    (EXP6_ENABLE, EXP6_NAME, EXP6_TRAIN, EXP6_TEST),\n",
    "    (EXP7_ENABLE, EXP7_NAME, EXP7_TRAIN, EXP7_TEST),\n",
    "    (EXP8_ENABLE, EXP8_NAME, EXP8_TRAIN, EXP8_TEST),\n",
    "    (EXP9_ENABLE, EXP9_NAME, EXP9_TRAIN, EXP9_TEST),\n",
    "    (EXP10_ENABLE, EXP10_NAME, EXP10_TRAIN, EXP10_TEST),\n",
    "    (EXP11_ENABLE, EXP11_NAME, EXP11_TRAIN, EXP11_TEST),\n",
    "    (EXP12_ENABLE, EXP12_NAME, EXP12_TRAIN, EXP12_TEST),\n",
    "]\n",
    "\n",
    "for enabled, name, train_tag, test_tag in experiments:\n",
    "    if not enabled or train_tag == \"none\" or not name.strip():\n",
    "        continue\n",
    "\n",
    "    name = name.strip().replace(\" \", \"_\")\n",
    "\n",
    "    # Get train IDs\n",
    "    train_ids = ids_with_tags([train_tag])\n",
    "\n",
    "    # Get test IDs\n",
    "    if test_tag == \"all-other\":\n",
    "        test_ids = ids_without_tags([train_tag])\n",
    "    elif test_tag == \"none\":\n",
    "        continue\n",
    "    else:\n",
    "        test_ids = ids_with_tags([test_tag])\n",
    "        # Remove overlap\n",
    "        overlap = set(train_ids) & set(test_ids)\n",
    "        train_ids = [x for x in train_ids if x not in overlap]\n",
    "        test_ids = [x for x in test_ids if x not in overlap]\n",
    "\n",
    "    if len(train_ids) < MIN_SPLIT_SIZE or len(test_ids) < MIN_SPLIT_SIZE:\n",
    "        print(f\"  SKIP {name}: insufficient data (train={len(train_ids)}, test={len(test_ids)})\")\n",
    "        continue\n",
    "\n",
    "    random.shuffle(train_ids)\n",
    "    random.shuffle(test_ids)\n",
    "\n",
    "    all_splits[name] = {\n",
    "        \"train_ids\": train_ids,\n",
    "        \"test_ids\": test_ids,\n",
    "        \"train_size\": len(train_ids),\n",
    "        \"test_size\": len(test_ids),\n",
    "        \"train_tags\": expand_tag(train_tag),\n",
    "        \"test_tags\": expand_tag(test_tag) if test_tag != \"all-other\" else [\"*\"],\n",
    "    }\n",
    "    print(f\"  {name}: {len(train_ids):,} -> {len(test_ids):,}\")\n",
    "\n",
    "# Add mixed baseline\n",
    "if INCLUDE_MIXED_BASELINE:\n",
    "    all_ids = [p[\"id\"] for p in passage_meta]\n",
    "    random.shuffle(all_ids)\n",
    "    split_pt = int(len(all_ids) * 0.7)\n",
    "    all_splits[\"mixed_baseline\"] = {\n",
    "        \"train_ids\": all_ids[:split_pt],\n",
    "        \"test_ids\": all_ids[split_pt:],\n",
    "        \"train_size\": split_pt,\n",
    "        \"test_size\": len(all_ids) - split_pt,\n",
    "        \"train_tags\": [\"*\"],\n",
    "        \"test_tags\": [\"*\"],\n",
    "    }\n",
    "    print(f\"  mixed_baseline: {split_pt:,} -> {len(all_ids) - split_pt:,}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE\n",
    "# =============================================================================\n",
    "\n",
    "splits_file = Path(\"data/splits/all_splits.json\")\n",
    "splits_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(splits_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_splits, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"SAVED {len(all_splits)} SPLITS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(f\"{'Experiment':<25} {'Train':>10} {'Test':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for name, split in sorted(all_splits.items()):\n",
    "    print(f\"{name:<25} {split['train_size']:>10,} {split['test_size']:>10,}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424,
     "referenced_widgets": [
      "6b5c1f5ba0a1442197044be67122d603",
      "c350d89a43e74d73830d0afda82f35ac",
      "f9efc167430945e696953f8c52f1a46b",
      "4219402725934ff6abd2a52792626dd7",
      "4d9100b7efab496281683c596ba062dc",
      "1f9c572e46d94692bb024a3d6c1b8aae",
      "2ec6824fac7f4a14b4253e9c78b853f0",
      "640b1a4241f5429abcc08cb2d00ddfd3",
      "1205d4df82e844119d9e3e88b0c60a42",
      "1140c6006b7842d0b809c62673607e26",
      "465c0f1ad7ba4384bfcc326a8154a270",
      "57a4104f53e7426c8267b9ebcf220133",
      "8289e4c5f5634e9b8849e526a5fa198b",
      "b4a93b9a703c46a7b713d7f59c32b432",
      "1764b050c0e5435dba14f97922514df0",
      "26da4ed2eb764d5f9236bdfd5d83e71e",
      "80a75540bf9b4713b045e366f4ee0b74",
      "74ffb1f734b94578ae4abed33d1126f8",
      "a19e4499949b459bbd95426d6f1f4b66",
      "980feb9c683246a4a76dbc0d9f262c45",
      "644cbcabfa8649eaabaa341fc095beb7",
      "eab5cab430a54b52b2f9a004afa28fc4",
      "1cfe6de186aa44a59357cdbef299c2b5",
      "91120f80b69641f7878c16f278277116",
      "c5c98ee60bbc471d9f17fb5654bf53c9",
      "a5b9feedec594156ad15cd82655b89d3",
      "2c2f664f85e3409c9ab8e3f4c0ac57f6",
      "35d2c93f7a4c414197382a5c539734bb",
      "9535edd6dcdd4a0b8c2896721fbd4bd1",
      "d178fba0381d458384c3b09801a4b040",
      "460461c5430244a19c61ca2f0e987464",
      "2a961c1e5f254eefa8efad90dc35a342",
      "dc517db41a134b02a3c4177d0d9eab3e",
      "963c88a96f234702b5d6d0279935f15d",
      "de7ee617064b4683a0e2caace43b5749",
      "fc71978785394930b056275bef0fee18",
      "5e46bb4f556942d88103d86a51316f2e",
      "2d2b1b8a52d84ab9bb1b46004b38e9fd",
      "d2f33a0f9f2b43c69539e5344038c839",
      "87d4dc34617c4887a8048099d4cd7302",
      "1fc97fd7ab7e40bd8f9e15340b8d00d3",
      "7d9f7cab5d1d45eebd5e80131f41609c",
      "9b5de60a3bf14f82ba00213c45e8bd86",
      "49e14b7bb1d84e8c98cdc423734467ee",
      "543a8ed40fe543f8aece1abc0b989279",
      "4ba93b780db841e99b35f53ad77ccac9",
      "373e03146a60459e8af2fac94547964e",
      "15f2dc02582948c0aa5372389f9ebdb3",
      "ed6ad1fbb1e5420cbe5ebf6a1a6de7ca",
      "4bb56aec6abf4d2d97de8b154cf70db8",
      "3106c01c62f841d28cab3d9f109c401d",
      "fd3fefb24e274d25a99ce2ff311acc77",
      "6dcee7293f6a429894745fed641d34e1",
      "57454d26e74f4dc1ac951fe4807c793d",
      "2f9429936ce24c2e96592b966dc42040"
     ]
    },
    "id": "cell_6",
    "outputId": "a82ab2af-3375-46f3-e97a-bf05031c0d45"
   },
   "outputs": [],
   "source": "# @title 6. Model Architecture { display-mode: \"form\" }\n# @markdown BIP v10.9 model with configurable backbone and adversarial heads\n# @markdown - Updated: 8 languages, 26 periods\n\nimport json\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import AutoModel, AutoTokenizer\n\nprint(\"=\" * 60)\nprint(\"MODEL ARCHITECTURE\")\nprint(\"=\" * 60)\nprint(f\"Backbone: {BACKBONE} ({MODEL_NAME})\")\nprint(f\"Hidden size: {BACKBONE_HIDDEN}\")\n\n# Index mappings\nBOND_TO_IDX = {bt.name: i for i, bt in enumerate(BondType)}\nIDX_TO_BOND = {i: bt.name for i, bt in enumerate(BondType)}\n# v10.9: 8 languages (added Sanskrit, Pali, Greek placeholder)\nLANG_TO_IDX = {\n    \"hebrew\": 0,\n    \"aramaic\": 1,\n    \"classical_chinese\": 2,\n    \"arabic\": 3,\n    \"english\": 4,\n    \"sanskrit\": 5,  # NEW in v10.9\n    \"pali\": 6,  # NEW in v10.9\n    \"greek\": 7,  # FUTURE (placeholder)\n}\nIDX_TO_LANG = {i: l for l, i in LANG_TO_IDX.items()}\n\n# v10.9: 26 periods (expanded Chinese, Arabic, added Sanskrit/Pali traditions)\nPERIOD_TO_IDX = {\n    # Semitic traditions\n    \"BIBLICAL\": 0,\n    \"TANNAITIC\": 1,\n    \"AMORAIC\": 2,\n    \"RISHONIM\": 3,\n    \"ACHRONIM\": 4,\n    # Chinese traditions (expanded)\n    \"CONFUCIAN\": 5,\n    \"DAOIST\": 6,\n    \"MOHIST\": 7,  # NEW in v10.9\n    \"LEGALIST\": 8,  # NEW in v10.9\n    \"BUDDHIST\": 9,  # NEW in v10.9 (Chinese Buddhism)\n    \"NEO_CONFUCIAN\": 10,  # NEW in v10.9\n    # Arabic/Islamic traditions (expanded)\n    \"QURANIC\": 11,\n    \"HADITH\": 12,\n    \"FIQH\": 13,  # NEW in v10.9 (Islamic jurisprudence)\n    \"SUFI\": 14,  # NEW in v10.9\n    \"FALSAFA\": 15,  # NEW in v10.9 (Arabic philosophy)\n    # Sanskrit/Pali traditions (NEW in v10.9)\n    \"DHARMA\": 16,  # Dharmashastra\n    \"UPANISHAD\": 17,\n    \"GITA\": 18,\n    \"ARTHA\": 19,  # Arthashastra\n    \"PALI\": 20,  # Pali Canon\n    # Western traditions\n    \"WESTERN_CLASSICAL\": 21,\n    \"MEDIEVAL\": 22,\n    # Modern\n    \"DEAR_ABBY\": 23,\n    \"MODERN\": 24,\n    \"CLASSICAL\": 25,  # Generic classical (fallback)\n}  # 26 periods total (0-25)\nIDX_TO_PERIOD = {i: p for p, i in PERIOD_TO_IDX.items()}\nHOHFELD_TO_IDX = {hs.name: i for i, hs in enumerate(HohfeldState)}\nIDX_TO_HOHFELD = {i: hs.name for i, hs in enumerate(HohfeldState)}\nCONTEXT_TO_IDX = {\"prescriptive\": 0, \"descriptive\": 1, \"unknown\": 2}\nIDX_TO_CONTEXT = {i: c for c, i in CONTEXT_TO_IDX.items()}\n\n\ndef get_confidence_weight(conf):\n    \"\"\"Map confidence to sample weight. Handles both string ('high'/'medium'/'low') and numeric (0.0-1.0) values.\"\"\"\n    if isinstance(conf, str):\n        return {\"high\": 2.0, \"medium\": 1.0, \"low\": 0.5}.get(conf, 1.0)\n    elif isinstance(conf, (int, float)):\n        return 2.0 if conf >= 0.8 else 1.0\n    return 1.0\n\n\nclass GradientReversalLayer(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = alpha\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.alpha, None\n\n\nclass BIPModel(nn.Module):\n    def __init__(self, model_name=None, hidden_size=None, z_dim=64):\n        super().__init__()\n        # Use global config if not specified\n        model_name = model_name or MODEL_NAME\n        hidden_size = hidden_size or BACKBONE_HIDDEN\n\n        print(f\"  Loading encoder: {model_name}\")\n        self.encoder = AutoModel.from_pretrained(model_name)\n\n        # Freeze encoder if configured (probe-only training)\n        try:\n            if FREEZE_ENCODER:\n                for param in self.encoder.parameters():\n                    param.requires_grad = False\n                print(\"  Encoder FROZEN (probe-only mode)\")\n            else:\n                print(\"  Encoder UNFROZEN (full fine-tuning)\")\n        except NameError:\n            print(\"  Encoder unfrozen (FREEZE_ENCODER not set)\")\n\n        # Get actual hidden size from model config\n        actual_hidden = self.encoder.config.hidden_size\n        if actual_hidden != hidden_size:\n            print(f\"  Note: Using actual hidden size {actual_hidden}\")\n            hidden_size = actual_hidden\n\n        self.hidden_size = hidden_size\n        self.model_name = model_name\n\n        # Projection to z_bond space (scales with backbone size)\n        proj_hidden = min(512, hidden_size)\n        self.z_proj = nn.Sequential(\n            nn.Linear(hidden_size, proj_hidden),\n            nn.LayerNorm(proj_hidden),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(proj_hidden, z_dim),\n        )\n\n        # Task heads\n        self.bond_head = nn.Linear(z_dim, len(BondType))\n        self.hohfeld_head = nn.Linear(z_dim, len(HohfeldState))\n\n        # Adversarial heads\n        # v10.16.4: Dynamic layer count + LayerNorm for stronger disentanglement\n        try:\n            _adv_hidden = ADV_HIDDEN_DIM\n            _adv_dropout = ADV_DROPOUT\n            _adv_layers = ADV_NUM_LAYERS\n        except NameError:\n            _adv_hidden = 512\n            _adv_dropout = 0.3\n            _adv_layers = 3\n\n        def build_adversarial_head(input_dim, output_dim, hidden_dim, num_layers, dropout):\n            \"\"\"Build adversarial head with configurable depth and LayerNorm.\"\"\"\n            layers = []\n            # Input layer\n            layers.extend([\n                nn.Linear(input_dim, hidden_dim),\n                nn.LayerNorm(hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(dropout),\n            ])\n            # Hidden layers\n            for _ in range(num_layers - 2):\n                layers.extend([\n                    nn.Linear(hidden_dim, hidden_dim),\n                    nn.LayerNorm(hidden_dim),\n                    nn.ReLU(),\n                    nn.Dropout(dropout),\n                ])\n            # Output layer\n            layers.append(nn.Linear(hidden_dim, output_dim))\n            return nn.Sequential(*layers)\n\n        self.language_head = build_adversarial_head(\n            z_dim, len(LANG_TO_IDX), _adv_hidden, _adv_layers, _adv_dropout\n        )\n        self.period_head = build_adversarial_head(\n            z_dim, len(PERIOD_TO_IDX), _adv_hidden, _adv_layers, _adv_dropout\n        )\n        print(f\"  Adversarial heads: {_adv_layers} layers, {_adv_hidden} hidden, {_adv_dropout} dropout\")\n\n        # Context prediction head (auxiliary task)\n        self.context_head = nn.Linear(z_dim, len(CONTEXT_TO_IDX))\n\n        # Count parameters\n        total_params = sum(p.numel() for p in self.parameters())\n        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        print(f\"  Total params: {total_params:,}\")\n        print(f\"  Trainable: {trainable_params:,}\")\n\n    def forward(self, input_ids, attention_mask, adv_lambda=1.0):\n        enc = self.encoder(input_ids, attention_mask)\n\n        # Handle different pooling strategies\n        if hasattr(enc, \"pooler_output\") and enc.pooler_output is not None:\n            pooled = enc.pooler_output\n        else:\n            pooled = enc.last_hidden_state[:, 0]\n\n        z = self.z_proj(pooled)\n\n        # Bond prediction (main task)\n        bond_pred = self.bond_head(z)\n        hohfeld_pred = self.hohfeld_head(z)\n\n        # Adversarial predictions (gradient reversal)\n        z_rev = GradientReversalLayer.apply(z, adv_lambda)\n        language_pred = self.language_head(z_rev)\n        period_pred = self.period_head(z_rev)\n\n        return {\n            \"bond_pred\": bond_pred,\n            \"hohfeld_pred\": hohfeld_pred,\n            \"language_pred\": language_pred,\n            \"period_pred\": period_pred,\n            \"context_pred\": self.context_head(z),\n            \"z\": z,\n        }\n\n    def get_bond_embedding(self, input_ids, attention_mask):\n        \"\"\"Get z_bond embedding for geometric analysis.\"\"\"\n        enc = self.encoder(input_ids, attention_mask)\n        if hasattr(enc, \"pooler_output\") and enc.pooler_output is not None:\n            pooled = enc.pooler_output\n        else:\n            pooled = enc.last_hidden_state[:, 0]\n        return self.z_proj(pooled)\n\n\n# Initialize tokenizer for selected backbone\nprint(f\"\\nLoading tokenizer: {MODEL_NAME}\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nprint(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n\n\n# Dataset with Hohfeld support\nclass NativeDataset(Dataset):\n    def __init__(self, ids_set, passages_file, bonds_file, tokenizer, max_len=128, filter_none=True):\n        \"\"\"\n        Args:\n            filter_none: If True, exclude samples with no detected bond (NONE class).\n                        This improves training by focusing on labeled examples.\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.data = []\n        self.filter_none = filter_none\n        _skipped_none = 0\n\n        bonds_by_id = {}\n        with open(bonds_file) as fb:\n            for line in fb:\n                b = json.loads(line)\n                bonds_by_id[b[\"passage_id\"]] = b\n\n        with open(passages_file) as fp:\n            for line in tqdm(fp, desc=\"Loading\", unit=\"line\"):\n                p = json.loads(line)\n                if p[\"id\"] in ids_set and p[\"id\"] in bonds_by_id:\n                    b = bonds_by_id[p[\"id\"]]\n                    bond_type = b.get(\"bond_type\") or b.get(\"bonds\", {}).get(\"primary_bond\")\n\n                    # Filter out NONE/null bonds if requested\n                    if filter_none and (bond_type is None or bond_type == \"NONE\" or bond_type == \"NEUTRAL\"):\n                        _skipped_none += 1\n                        continue\n\n                    self.data.append(\n                        {\n                            \"text\": p[\"text\"][:1000],\n                            \"language\": p[\"language\"],\n                            \"period\": p.get(\"time_periods\", [\"UNKNOWN\"])[0],\n                            \"bond\": bond_type,\n                            \"hohfeld\": None,\n                            \"context\": b.get(\"context\")\n                            or b.get(\"bonds\", {}).get(\"context\", \"unknown\"),\n                            \"confidence\": b.get(\"confidence\")\n                            or b.get(\"bonds\", {}).get(\"confidence\", \"medium\"),\n                        }\n                    )\n        print(f\"  Loaded {len(self.data):,} samples\" + (f\" (filtered {_skipped_none:,} NONE bonds)\" if filter_none else \"\"))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        enc = self.tokenizer(\n            item[\"text\"],\n            truncation=True,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"bond_label\": BOND_TO_IDX.get(item[\"bond\"], 9),\n            \"language_label\": LANG_TO_IDX.get(item[\"language\"], 4),\n            \"period_label\": PERIOD_TO_IDX.get(item[\"period\"], 9),\n            \"hohfeld_label\": HOHFELD_TO_IDX.get(item[\"hohfeld\"], 0) if item[\"hohfeld\"] else 0,\n            \"context_label\": CONTEXT_TO_IDX.get(item[\"context\"], 2),\n            \"sample_weight\": get_confidence_weight(item[\"confidence\"]),\n            \"language\": item[\"language\"],\n            \"context\": item[\"context\"],\n            \"confidence\": item[\"confidence\"],\n            \"text\": item[\"text\"],  # Raw text for role augmentation\n        }\n\n\ndef collate_fn(batch):\n    return {\n        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n        \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n        \"bond_labels\": torch.tensor([x[\"bond_label\"] for x in batch]),\n        \"language_labels\": torch.tensor([x[\"language_label\"] for x in batch]),\n        \"period_labels\": torch.tensor([x[\"period_label\"] for x in batch]),\n        \"hohfeld_labels\": torch.tensor([x[\"hohfeld_label\"] for x in batch]),\n        \"context_labels\": torch.tensor([x[\"context_label\"] for x in batch]),\n        \"sample_weights\": torch.tensor([x[\"sample_weight\"] for x in batch], dtype=torch.float),\n        \"languages\": [x[\"language\"] for x in batch],\n        \"contexts\": [x[\"context\"] for x in batch],\n        \"confidences\": [x[\"confidence\"] for x in batch],\n        \"texts\": [x[\"text\"] for x in batch],  # v10.10: raw texts for role augmentation\n    }\n\n\nprint(f\"\\nArchitecture ready for {BACKBONE}\")\nprint(f\"  Bond classes: {len(BondType)}\")\nprint(f\"  Languages: {len(LANG_TO_IDX)}\")\nprint(\"\\n\" + \"=\" * 60)  # ===== v10.15.1: GPU Memory Probing (generalized) =====\n\n\ndef probe_max_batch(\n    model, tokenizer, device, target_batch=4096, encoder_trainable=False, mode=\"train\"\n):\n    \"\"\"Binary search for max batch size that fits in GPU memory.\n\n    Args:\n        model: BIPModel instance\n        tokenizer: tokenizer for the model\n        device: torch device\n        target_batch: starting upper bound for search\n        encoder_trainable: if True, tests with backward pass (4x memory)\n        mode: \"train\" or \"eval\" - eval can use 2x memory vs train\n\n    Returns:\n        Safe batch size with 20% headroom\n    \"\"\"\n    import gc\n\n    # Adjust target based on mode\n    if mode == \"eval\":\n        # Eval doesn't need gradients, can use ~2x train batch\n        target_batch = min(target_batch * 2, 1024)\n    elif encoder_trainable:\n        # Much lower target if encoder is trainable (gradient memory)\n        target_batch = min(target_batch, 64)\n\n    print(f\"  [v10.15.1] Probing max batch (mode={mode}, trainable={encoder_trainable})...\", end=\"\")\n\n    low, high = 8, target_batch\n    best = low\n    seq_len = 128\n\n    while low <= high:\n        mid = (low + high) // 2\n        try:\n            test_ids = torch.zeros((mid, seq_len), dtype=torch.long, device=device)\n            test_mask = torch.ones((mid, seq_len), dtype=torch.long, device=device)\n\n            if mode == \"train\" and encoder_trainable:\n                model.train()\n                out = model(test_ids, test_mask, 0)\n                loss = out[\"bond_pred\"].mean()\n                loss.backward()\n                model.zero_grad()\n            else:\n                model.eval()\n                with torch.no_grad():\n                    _ = model(test_ids, test_mask, 0)\n\n            best = mid\n            low = mid + 1\n            del test_ids, test_mask\n            torch.cuda.empty_cache()\n\n        except Exception as e:\n            err = str(e).lower()\n            if \"out of memory\" in err or \"cuda\" in err or \"alloc\" in err or \"oom\" in err:\n                high = mid - 1\n                try:\n                    del test_ids, test_mask\n                except:\n                    pass\n                gc.collect()\n                torch.cuda.empty_cache()\n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()\n            else:\n                raise\n\n    safe_batch = int(best * 0.8)\n    print(f\" max={best}, using {safe_batch}\")\n    return max(8, safe_batch)\n\n\n# Global cache for probed batch sizes\n_PROBED_BATCHES = {}\n\n\ndef get_probed_batch(model, tokenizer, device, mode=\"train\", encoder_trainable=False):\n    \"\"\"Get cached or probe batch size for given mode.\"\"\"\n    key = f\"{mode}_{encoder_trainable}\"\n    if key not in _PROBED_BATCHES:\n        _PROBED_BATCHES[key] = probe_max_batch(\n            model, tokenizer, device, encoder_trainable=encoder_trainable, mode=mode\n        )\n    return _PROBED_BATCHES[key]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell_7",
    "outputId": "135762d0-70ce-4c34-c776-7c5e68aba7a8"
   },
   "outputs": [],
   "source": "# @title 7. Training Loop { display-mode: \"form\" }\n# @markdown Training with tuned adversarial weights and hardware-optimized parameters\n# @markdown v10.15.1.4: Encoder unfreezing + stronger adversarial training\n\n# ===== SUPPRESS DATALOADER MULTIPROCESSING WARNINGS =====\n# These occur during garbage collection and bypass normal exception handling\nimport io\nimport logging\nimport os\nimport random\nimport sys\nimport warnings\n\n# Method 1: Filter warnings\nwarnings.filterwarnings(\"ignore\", message=\".*can only test a child process.*\")\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.data\")\n\n# Method 2: Suppress logging\nlogging.getLogger(\"torch.utils.data.dataloader\").setLevel(logging.CRITICAL)\n\n\n# Method 3: Redirect stderr during DataLoader cleanup (most effective)\nclass StderrFilter(io.TextIOWrapper):\n    \"\"\"Filters out DataLoader multiprocessing cleanup messages from stderr\"\"\"\n\n    def __init__(self, original):\n        self.original = original\n        self.buffer_lines = []\n\n    def write(self, text):\n        # Filter out the specific error patterns\n        skip_patterns = [\n            \"can only test a child process\",\n            \"_MultiProcessingDataLoaderIter.__del__\",\n            \"_shutdown_workers\",\n            \"Exception ignored in:\",\n            \"w.is_alive()\",\n        ]\n        # Buffer multi-line error messages\n        if any(p in text for p in skip_patterns):\n            return len(text)  # Pretend we wrote it\n        # Also skip if it looks like part of a traceback for these errors\n        if text.strip().startswith(\"^\") and len(text.strip()) < 80:\n            return len(text)\n        if text.strip().startswith('File \"/usr') and \"dataloader.py\" in text:\n            return len(text)\n        if text.strip() == \"Traceback (most recent call last):\":\n            self.buffer_lines = [text]\n            return len(text)\n        if self.buffer_lines:\n            self.buffer_lines.append(text)\n            # Check if this is the DataLoader error traceback\n            full_msg = \"\".join(self.buffer_lines)\n            if any(p in full_msg for p in skip_patterns):\n                self.buffer_lines = []\n                return len(text)\n            # After 10 lines, flush if not the target error\n            if len(self.buffer_lines) > 10:\n                for line in self.buffer_lines:\n                    self.original.write(line)\n                self.buffer_lines = []\n        return self.original.write(text)\n\n    def flush(self):\n        if self.buffer_lines:\n            # Flush any remaining buffered content\n            for line in self.buffer_lines:\n                self.original.write(line)\n            self.buffer_lines = []\n        self.original.flush()\n\n    def __getattr__(self, name):\n        return getattr(self.original, name)\n\n\n# Install the stderr filter\n_original_stderr = sys.stderr\nsys.stderr = StderrFilter(_original_stderr)\n\n# Method 4: Patch the DataLoader cleanup function directly\ntry:\n    import torch.utils.data.dataloader as dl_module\n\n    _original_del = dl_module._MultiProcessingDataLoaderIter.__del__\n\n    def _patched_del(self):\n        try:\n            _original_del(self)\n        except (AssertionError, AttributeError, RuntimeError):\n            pass  # Silently ignore cleanup errors\n\n    dl_module._MultiProcessingDataLoaderIter.__del__ = _patched_del\nexcept Exception:\n    pass  # If patching fails, the stderr filter will still work\n\nimport gc\n\nfrom sklearn.metrics import f1_score\n\n# ===== INITIAL MEMORY CLEANUP =====\n# Clean up any leftover GPU memory from previous runs before starting\nprint(\"Cleaning up GPU memory from previous runs...\")\nif torch.cuda.is_available():\n    # Clear any existing models/tensors from globals\n    for var_name in list(globals().keys()):\n        obj = globals().get(var_name)\n        if isinstance(obj, torch.nn.Module):\n            try:\n                obj.cpu()\n                del globals()[var_name]\n            except:\n                pass\n        elif isinstance(obj, torch.Tensor) and obj.is_cuda:\n            try:\n                del globals()[var_name]\n            except:\n                pass\n\n    # Force garbage collection\n    for _ in range(5):\n        gc.collect()\n\n    # Clear CUDA cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    torch.cuda.reset_peak_memory_stats()\n\n    # Check memory status\n    mem_alloc = torch.cuda.memory_allocated() / 1e9\n    mem_reserved = torch.cuda.memory_reserved() / 1e9\n    print(f\"  GPU memory: {mem_alloc:.2f} GB allocated, {mem_reserved:.2f} GB reserved\")\n\n    if mem_alloc > 1.0:\n        print(f\"  WARNING: {mem_alloc:.1f} GB still allocated - consider restarting runtime\")\n        # Try more aggressive cleanup\n        torch.cuda.ipc_collect()\n        gc.collect()\n        torch.cuda.empty_cache()\nelse:\n    print(\"  No GPU detected\")\n\nprint()\n\n# @markdown **Splits to train:**\n# @markdown v10.13: Automatically uses splits generated in Cell 4\nTRAIN_ALL_SPLITS = True  # @param {type:\"boolean\"}\n# @markdown Train all splits from Cell 4. If False, specify splits below.\n\nSPECIFIC_SPLITS = \"\"  # @param {type:\"string\"}\n# @markdown Comma-separated split names (only used if TRAIN_ALL_SPLITS=False)\n# @markdown Example: \"hebrew_to_others, confucian_to_buddhist, mixed_baseline\"\n\nMAX_SPLITS = 0  # @param {type:\"integer\"}\n# @markdown Limit number of splits (0 = no limit). Useful for quick testing.\n\n# @markdown **Reproducibility:**\nUSE_FIXED_SEED = True  # @param {type:\"boolean\"}\nRANDOM_SEED = 42  # @param {type:\"integer\"}\n# @markdown Set USE_FIXED_SEED=True for reproducible results, False for random initialization\n\nif USE_FIXED_SEED:\n    import numpy as np\n\n    torch.manual_seed(RANDOM_SEED)\n    torch.cuda.manual_seed_all(RANDOM_SEED)\n    random.seed(RANDOM_SEED)\n    np.random.seed(RANDOM_SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f\"Using fixed seed: {RANDOM_SEED}\")\nelse:\n    torch.backends.cudnn.benchmark = True  # Faster but non-deterministic\n    print(\"Using random initialization\")\n\n# @markdown **Hyperparameters:**\nLANG_WEIGHT = 1.0  # @param {type:\"number\"} # v10.16.4: Much stronger (was 0.3)\nPERIOD_WEIGHT = 0.8  # @param {type:\"number\"} # v10.16.4: Much stronger (was 0.2)\n# Use NUM_EPOCHS from Cell 1, or default\ntry:\n    N_EPOCHS = NUM_EPOCHS\nexcept NameError:\n    N_EPOCHS = 10  # Default fallback\n\n# @markdown **Context-Aware Training:**\nUSE_CONFIDENCE_WEIGHTING = True  # @param {type:\"boolean\"}\n# @markdown Weight prescriptive (high confidence) examples 2x in loss\n\nUSE_CONTEXT_AUXILIARY = True  # @param {type:\"boolean\"}\n# @markdown Add context prediction as auxiliary training target\n\nCONTEXT_LOSS_WEIGHT = 0.33  # @param {type:\"number\"}\n# @markdown Weight for context prediction loss\n\nSTRICT_PRESCRIPTIVE_TEST = False  # @param {type:\"boolean\"}\n# @markdown Only evaluate on prescriptive examples (reduces test set ~97%!)\n\n# @markdown **v10.10: Role-Aware Data Augmentation:**\nUSE_ROLE_AUGMENTATION = True  # @param {type:\"boolean\"}\n# @markdown Adds contrastive loss for agent/patient role sensitivity\nROLE_AUGMENT_PROB = 0.3  # @param {type:\"number\"}\n# @markdown Probability of augmenting each batch\nROLE_CONTRASTIVE_WEIGHT = 0.2  # @param {type:\"number\"}\n# @markdown Weight for role contrastive loss\nROLE_CONTRASTIVE_MARGIN = 0.5  # @param {type:\"number\"}\n\n# @markdown **v10.15.1.2: Gradient Penalty for Adversarial Disentanglement:**\nUSE_GRADIENT_PENALTY = True  # @param {type:\"boolean\"}\n# @markdown Adds gradient penalty to adversarial heads for smoother predictions\nGRADIENT_PENALTY_WEIGHT = 0.02  # @param {type:\"number\"}\n# @markdown Weight for gradient penalty loss\n\nUSE_COSINE_LR = True  # @param {type:\"boolean\"}\n# @markdown Use cosine annealing learning rate schedule\n\n# @markdown Minimum embedding distance for role-swapped pairs\n\n\ndef swap_roles_simple(text, language):\n    \"\"\"Simple role swap using word order reversal for common patterns.\n    v10.10: Addresses weak role_swap sensitivity (0.003) from fuzz testing.\"\"\"\n    patterns = {\n        \"english\": [\n            (r\"(\\w+) must (\\w+) (\\w+)\", r\"\\3 must \\2 \\1\"),\n            (r\"(\\w+) should (\\w+) (\\w+)\", r\"\\3 should \\2 \\1\"),\n            (r\"(\\w+) shall (\\w+) (\\w+)\", r\"\\3 shall \\2 \\1\"),\n            (r\"the (\\w+) must (\\w+) the (\\w+)\", r\"the \\3 must \\2 the \\1\"),\n            (r\"(\\w+) is obligated to (\\w+) (\\w+)\", r\"\\3 is obligated to \\2 \\1\"),\n            (r\"(\\w+) has a duty to (\\w+) (\\w+)\", r\"\\3 has a duty to \\2 \\1\"),\n        ],\n        \"hebrew\": [\n            (r\" (\\S+) (\\S+)  (\\S+)\", r\" \\3 \\2  \\1\"),\n        ],\n        \"classical_chinese\": [\n            (r\"(\\S)(\\S)(\\S)\", r\"\\3\\2\\1\"),\n            (r\"(\\S)(\\S)(\\S)\", r\"\\3\\2\\1\"),\n            (r\"(\\S)(\\S)(\\S)\", r\"\\3\\2\\1\"),\n        ],\n        \"arabic\": [\n            (r\"  (\\S+)  (\\S+) (\\S+)\", r\"  \\3  \\2 \\1\"),\n            (r\"(\\S+)   (\\S+) (\\S+)\", r\"\\3   \\2 \\1\"),\n        ],\n        \"sanskrit\": [\n            (r\"(\\S+) (\\S+) (\\S+)\", r\"\\3 \\2 \\1\"),\n        ],\n        \"pali\": [\n            (r\"(\\S+)o (\\S+)a (\\S+)ti\", r\"\\3o \\2a \\1ti\"),\n        ],\n    }\n\n    lang_patterns = patterns.get(language, patterns[\"english\"])\n    for pattern, replacement in lang_patterns:\n        if re.search(pattern, text, re.IGNORECASE):\n            swapped = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n            if swapped != text:\n                return swapped\n    return None\n\n\n# =============================================================================\n# v10.16.1: STRUCTURAL PERTURBATIONS FOR CONTRASTIVE TRAINING\n# =============================================================================\n# These patterns match the fuzz test perturbation types to ensure training\n# on the same transformations that will be evaluated.\n\nSTRUCTURAL_PATTERNS = {\n    \"obligation_to_permission\": [\n        (\"must protect\", \"may protect\"),\n        (\"has a duty to\", \"is allowed to\"),\n        (\"are required to\", \"are permitted to\"),\n        (\"must pay\", \"may pay\"),\n        (\"shall not\", \"need not\"),\n        (\"is obligated to\", \"is permitted to\"),\n        (\"swore to\", \"considered whether to\"),\n        (\"must tell\", \"may tell\"),\n        (\"should help\", \"could help\"),\n        (\"ought to\", \"might\"),\n    ],\n    \"add_harm\": [\n        (\"helped\", \"refused to help\"),\n        (\"protected\", \"endangered\"),\n        (\"gave\", \"took\"),\n        (\"saved\", \"abandoned\"),\n        (\"cared for\", \"neglected\"),\n        (\"forgave\", \"condemned\"),\n        (\"supported\", \"undermined\"),\n        (\"guided\", \"misled\"),\n        (\"healed\", \"harmed\"),\n        (\"blessed\", \"cursed\"),\n    ],\n    \"violation_to_fulfillment\": [\n        (\"violated\", \"honored\"),\n        (\"broke\", \"kept\"),\n        (\"stole\", \"returned\"),\n        (\"betrayed\", \"supported\"),\n        (\"abandoned\", \"stayed with\"),\n        (\"deceived\", \"was honest with\"),\n        (\"cheated\", \"dealt fairly with\"),\n        (\"destroyed\", \"preserved\"),\n        (\"corrupted\", \"purified\"),\n        (\"ignored\", \"attended to\"),\n    ],\n}\n\n\ndef create_structural_perturbation(text, language):\n    \"\"\"Create a structural perturbation that changes moral meaning.\n    Returns: (perturbed_text, perturbation_type) or (None, None)\n    \"\"\"\n    # First try role swap (most impactful)\n    swapped = swap_roles_simple(text, language)\n    if swapped:\n        return swapped, \"role_swap\"\n\n    # Try other structural patterns\n    text_lower = text.lower()\n    for perturb_type, patterns in STRUCTURAL_PATTERNS.items():\n        for orig, replacement in patterns:\n            if orig in text_lower:\n                # Case-preserving replacement\n                import re as re_mod\n                pattern = re_mod.compile(re_mod.escape(orig), re_mod.IGNORECASE)\n                perturbed = pattern.sub(replacement, text, count=1)\n                if perturbed != text:\n                    return perturbed, perturb_type\n\n    return None, None\n\n\ndef triplet_loss_geometric(anchor, positive, negative, margin=0.5):\n    \"\"\"\n    Triplet loss for BIP geometric learning.\n    Enforces: d(anchor, positive) + margin < d(anchor, negative)\n\n    This directly encodes the BIP hypothesis:\n    - Surface changes should NOT move embeddings (d_positive small)\n    - Structural changes SHOULD move embeddings (d_negative large)\n    \"\"\"\n    positive = positive.to(anchor.dtype)\n    negative = negative.to(anchor.dtype)\n\n    d_positive = F.pairwise_distance(anchor, positive)\n    d_negative = F.pairwise_distance(anchor, negative)\n\n    # Standard triplet margin loss\n    loss = F.relu(d_positive - d_negative + margin)\n    return loss.mean()\n\n\ndef ratio_regularization_loss(surface_distances, structural_distances, target_ratio=2.0):\n    \"\"\"\n    Encourage structural distances to be TARGET_RATIO times larger than surface distances.\n    This is the core BIP hypothesis.\n    \"\"\"\n    mean_surface = surface_distances.mean()\n    mean_structural = structural_distances.mean()\n\n    ratio = mean_structural / (mean_surface + 1e-8)\n    loss = F.relu(target_ratio - ratio)\n    return loss\n\n\n\n# ===== v10.15.1: SURFACE AUGMENTATION & CONTRASTIVE LOSS =====\n\nimport random\n\n# Simple synonym mappings for surface perturbation\nSURFACE_SYNONYMS = {\n    \"good\": [\"virtuous\", \"righteous\", \"moral\", \"ethical\"],\n    \"bad\": [\"evil\", \"wicked\", \"immoral\", \"wrong\"],\n    \"must\": [\"should\", \"ought to\", \"has to\", \"needs to\"],\n    \"can\": [\"may\", \"is able to\", \"is permitted to\"],\n    \"right\": [\"correct\", \"proper\", \"appropriate\"],\n    \"wrong\": [\"incorrect\", \"improper\", \"inappropriate\"],\n    \"help\": [\"assist\", \"aid\", \"support\"],\n    \"harm\": [\"hurt\", \"damage\", \"injure\"],\n    \"person\": [\"individual\", \"human\", \"someone\"],\n    \"people\": [\"individuals\", \"humans\", \"others\"],\n}\n\n# Common names for swapping\nCOMMON_NAMES = [\"Alex\", \"Sam\", \"Jordan\", \"Taylor\", \"Morgan\", \"Casey\", \"Riley\", \"Quinn\"]\n\n# Irrelevant details to insert\nIRRELEVANT_DETAILS = [\n    # v10.16.3: Aligned with fuzz test patterns for consistency\n    # Original patterns (insertable mid-sentence)\n    \"on a Tuesday\",\n    \"while it was raining\",\n    \"near the old building\",\n    \"during the afternoon\",\n    \"in the usual manner\",\n    # Fuzz test patterns (appendable at end - matches Cell 9 exactly)\n    \"It was Tuesday.\",\n    \"The room was blue.\",\n    \"Last summer.\",\n    \"The weather was pleasant.\",\n    \"It happened at noon.\",\n    \"The year was uncertain.\",\n    \"Birds sang nearby.\",\n    \"The moon was full.\",\n    \"Rain had fallen earlier.\",\n    \"The road was dusty.\",\n    \"Flowers bloomed outside.\",\n]\n\n\ndef augment_surface_synonym(text: str) -> str:\n    \"\"\"Replace words with synonyms (surface change, same meaning).\"\"\"\n    words = text.split()\n    for i, word in enumerate(words):\n        word_lower = word.lower().strip(\".,!?\")\n        if word_lower in SURFACE_SYNONYMS and random.random() < 0.3:\n            replacement = random.choice(SURFACE_SYNONYMS[word_lower])\n            # Preserve capitalization\n            if word[0].isupper():\n                replacement = replacement.capitalize()\n            words[i] = replacement + word[len(word_lower) :]\n    return \" \".join(words)\n\n\ndef augment_surface_name(text: str) -> str:\n    \"\"\"Swap names with other names (surface change, same moral content).\"\"\"\n    # Find capitalized words that might be names\n    for name in COMMON_NAMES:\n        if name in text:\n            new_name = random.choice([n for n in COMMON_NAMES if n != name])\n            text = text.replace(name, new_name)\n            break\n    return text\n\n\ndef augment_surface_detail(text: str) -> str:\n    \"\"\"Insert irrelevant detail (surface change, same moral content).\n\n    v10.16.3: Now matches fuzz test patterns - 50% append at end, 50% insert mid-sentence.\n    This teaches model to be robust to irrelevant details in both positions.\n    \"\"\"\n    if random.random() < 0.7 and len(text) > 20:  # Increased from 0.5 to 0.7\n        detail = random.choice(IRRELEVANT_DETAILS)\n\n        # 50% chance: append at end (matches fuzz test pattern exactly)\n        # 50% chance: insert after punctuation (additional robustness)\n        if random.random() < 0.5:\n            # Append at end - matches fuzz test's surface_irrelevant_detail\n            text = text.rstrip() + \" \" + detail.strip()\n        else:\n            # Insert after first sentence or comma\n            insert_points = [m.end() for m in re.finditer(r\"[,.]\", text)]\n            if insert_points:\n                pos = random.choice(insert_points[:3])  # Early in text\n                text = text[:pos] + \" \" + detail + text[pos:]\n            else:\n                # Fallback to append if no punctuation found\n                text = text.rstrip() + \" \" + detail.strip()\n    return text\n\n\ndef create_surface_augmented(text: str) -> str:\n    \"\"\"Create a surface-augmented version of text (same moral content).\"\"\"\n    augmenters = [augment_surface_synonym, augment_surface_name, augment_surface_detail]\n    # Apply 1-2 random augmentations\n    for _ in range(random.randint(1, 2)):\n        aug_fn = random.choice(augmenters)\n        text = aug_fn(text)\n    return text\n\n\ndef info_nce_loss(\n    anchor: torch.Tensor, positive: torch.Tensor, temperature: float = 0.07\n) -> torch.Tensor:\n    \"\"\"\n    InfoNCE contrastive loss for surface invariance.\n\n    anchor: embeddings of original texts [batch, z_dim]\n    positive: embeddings of surface-augmented texts [batch, z_dim]\n    temperature: softmax temperature (lower = harder)\n\n    Goal: anchor should be similar to its positive (same moral content)\n          and dissimilar to other positives (different moral content)\n    \"\"\"\n    # Ensure same dtype (AMP can cause anchor=float16, positive=float32)\n    positive = positive.to(anchor.dtype)\n\n    # Normalize embeddings\n    anchor = F.normalize(anchor, dim=1)\n    positive = F.normalize(positive, dim=1)\n\n    # Similarity matrix: anchor_i vs positive_j\n    # Diagonal = positive pairs (same text, surface augmented)\n    # Off-diagonal = negative pairs (different texts)\n    similarity = torch.mm(anchor, positive.T) / temperature\n\n    # Labels: diagonal should be highest\n    labels = torch.arange(anchor.size(0), device=anchor.device)\n\n    # Cross-entropy loss (each anchor should match its positive)\n    loss = F.cross_entropy(similarity, labels)\n\n    return loss\n\n\n_skip_complete = False\n\n# ===== SKIP TRAINING MODE =====\ntry:\n    if SKIP_TRAINING:\n        print(\"=\" * 60)\n        print(\"SKIP_TRAINING MODE - Loading models from Drive\")\n        print(\"=\" * 60)\n\n        # Load splits\n        with open(\"data/splits/all_splits.json\", encoding=\"utf-8\") as f:\n            all_splits = json.load(f)\n\n        # Find available checkpoints\n        available_models = []\n        for split_name in all_splits.keys():\n            ckpt_path = f\"{SAVE_DIR}/best_{split_name}.pt\"\n            if os.path.exists(ckpt_path):\n                available_models.append(split_name)\n                print(f\"  Found: {split_name}\")\n\n        if not available_models:\n            print(\"\\nWARNING: No trained models found in Drive!\")\n            print(f\"  Looked in: {SAVE_DIR}\")\n            print(\"  Falling back to training mode...\")\n            SKIP_TRAINING = False\n            # Continue to training below\n        else:\n            print(f\"\\nFound {len(available_models)} trained models\")\n            print(\"Skipping Cell 7 - proceed to Cell 8 for evaluation\")\n\n            # Create minimal results dict for Cell 8 compatibility\n            all_results = {}\n            for split_name in available_models:\n                all_results[split_name] = {\"status\": \"loaded_from_drive\"}\n\n            # Exit cell early - only when checkpoints exist\n            _skip_complete = True\nexcept NameError:\n    pass  # SKIP_TRAINING not defined, continue normally\n\n\nif not _skip_complete:\n    print(\"=\" * 60)\n    print(\"TRAINING BIP MODEL\")\n    print(\"=\" * 60)\n\n    # v10.15.1.4: Check for encoder unfreezing config\n    try:\n        _unfreeze = UNFREEZE_ENCODER\n        _unfreeze_after = UNFREEZE_AFTER_EPOCHS\n    except NameError:\n        _unfreeze = False\n        _unfreeze_after = 2\n\n    print(\n        f\"\\nEncoder mode: {'UNFROZEN after epoch ' + str(_unfreeze_after) if _unfreeze else 'FROZEN (probe-only)'}\"\n    )\n\n    print(\"\\nSettings:\")\n    print(f\"  Backbone:     {BACKBONE}\")\n    print(f\"  GPU Tier:     {GPU_TIER}\")\n    print(f\"  Batch size:   {BATCH_SIZE}\")\n    print(f\"  Workers:      {NUM_WORKERS}\")\n    print(f\"  Learning rate: {LR:.2e}\")\n    print(f\"  Adv weights:  lang={LANG_WEIGHT}, period={PERIOD_WEIGHT}\")\n    print(\"  (v10.16.4: Increased for stronger invariance - grad clipping prevents explosion)\")\n\n    # v10.16.5: Confusion loss settings with fallbacks\n    try:\n        _use_confusion = USE_CONFUSION_LOSS\n        _confusion_weight = CONFUSION_WEIGHT\n    except NameError:\n        _use_confusion = False\n        _confusion_weight = 0.0\n    USE_CONFUSION_LOSS = _use_confusion\n    CONFUSION_WEIGHT = _confusion_weight\n    print(f\"  Confusion loss: {USE_CONFUSION_LOSS} (weight={CONFUSION_WEIGHT})\")\n    if USE_CONFUSION_LOSS:\n        print(\"  (v10.16.5: Forces uniform predictions - prevents adversarial head evasion)\")\n\n    print(f\"  Confidence weighting: {USE_CONFIDENCE_WEIGHTING}\")\n    print(f\"  Context auxiliary: {USE_CONTEXT_AUXILIARY} (weight={CONTEXT_LOSS_WEIGHT})\")\n    print(f\"  Strict prescriptive test: {STRICT_PRESCRIPTIVE_TEST}\")\n    print(\n        f\"  Role augmentation: {USE_ROLE_AUGMENTATION} (prob={ROLE_AUGMENT_PROB}, weight={ROLE_CONTRASTIVE_WEIGHT})\"\n    )\n\n    # tokenizer loaded in Cell 6 based on BACKBONE selection\n\n    with open(\"data/splits/all_splits.json\") as f:\n        all_splits = json.load(f)\n\n    # Build splits_to_train from Cell 4 output\n    if TRAIN_ALL_SPLITS:\n        splits_to_train = list(all_splits.keys())\n    else:\n        # Parse comma-separated list\n        splits_to_train = [s.strip() for s in SPECIFIC_SPLITS.split(\",\") if s.strip()]\n        # Filter to only splits that exist\n        splits_to_train = [s for s in splits_to_train if s in all_splits]\n\n    # Apply max limit if set\n    if MAX_SPLITS > 0:\n        splits_to_train = splits_to_train[:MAX_SPLITS]\n\n    # =============================================================================\n    # DYNAMIC BATCH SIZE PROBING (like WiFi rate adaptation)\n    # =============================================================================\n    # v10.15.1: probe_max_batch moved to Cell 6\n\n    PROBED_BATCH = None  # Will be set on first split\n\n    print(f\"\\nTraining {len(splits_to_train)} splits: {splits_to_train}\")\n\n    all_results = {}\n    MIN_TEST_SIZE = 100  # Lowered to allow smaller test sets like Chinese\n\n    for split_idx, split_name in enumerate(splits_to_train):\n        split_start = time.time()\n        print(\"\\n\" + \"=\" * 60)\n        print(f\"[{split_idx + 1}/{len(splits_to_train)}] {split_name}\")\n        print(\"=\" * 60)\n\n        split = all_splits[split_name]\n        print(f\"Train: {split['train_size']:,} | Test: {split['test_size']:,}\")\n\n        if split[\"test_size\"] < MIN_TEST_SIZE:\n            print(f\"WARNING: Test set only {split['test_size']} samples (need {MIN_TEST_SIZE})\")\n            print(\"Skipping this split - results would be unreliable\")\n            print(\"To fix: Add more data to the test languages/periods\")\n            continue\n\n        # Create model with OOM recovery\n        def create_model_with_retry():\n            \"\"\"Create model, cleaning up GPU memory if OOM occurs.\"\"\"\n            try:\n                return BIPModel(z_dim=Z_DIM).to(device)\n            except torch.cuda.OutOfMemoryError:\n                print(\"  OOM on model creation - cleaning up and retrying...\")\n                # Clean up any existing model in globals\n                _g = globals()\n                for _var in [\"model\", \"analyzer\", \"encoder\"]:\n                    if _var in _g and _g[_var] is not None:\n                        try:\n                            if hasattr(_g[_var], \"cpu\"):\n                                _g[_var].cpu()\n                            _g[_var] = None\n                        except:\n                            pass\n                # Force cleanup\n                gc.collect()\n                gc.collect()\n                torch.cuda.empty_cache()\n                torch.cuda.synchronize()\n                # Retry\n                return BIPModel(z_dim=Z_DIM).to(device)\n\n        model = create_model_with_retry()\n\n        # v10.16.2: Enable gradient checkpointing for memory efficiency\n        try:\n            if USE_GRADIENT_CHECKPOINTING and hasattr(model.encoder, 'gradient_checkpointing_enable'):\n                model.encoder.gradient_checkpointing_enable()\n                print(\"  Gradient checkpointing ENABLED\")\n        except NameError:\n            pass\n\n        # v10.15.1.5: Class weights to handle imbalanced bond types\n        # Upweight rare classes, downweight NONE (index 9)\n        BOND_CLASS_WEIGHTS = torch.tensor([\n            2.0,  # HARM_PREVENTION\n            2.0,  # RECIPROCITY\n            2.0,  # AUTONOMY\n            2.0,  # PROPERTY\n            2.0,  # FAMILY\n            2.0,  # AUTHORITY\n            2.0,  # CARE\n            2.0,  # FAIRNESS\n            2.0,  # CONTRACT\n            0.1,  # NONE - heavily downweighted\n        ], device=device)\n        print(f\"  Bond class weights: rare=2.0, NONE=0.1\")\n\n        # v10.15.1.4: Conditional encoder freezing\n        # IMPORTANT: Do NOT enable gradient checkpointing yet - it causes NaN when unfreezing\n        if _unfreeze:\n            print(f\"  Encoder will be UNFROZEN after epoch {_unfreeze_after}\")\n            # Start frozen, unfreeze later (warmup)\n            for param in model.encoder.parameters():\n                param.requires_grad = False\n            _encoder_frozen = True\n        else:\n            print(\"  Encoder FROZEN (probe-only mode)\")\n            for param in model.encoder.parameters():\n                param.requires_grad = False\n            _encoder_frozen = True\n\n        # Count trainable parameters\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        total_params = sum(p.numel() for p in model.parameters())\n        print(\n            f\"  Trainable: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.1f}%)\"\n        )\n\n        train_dataset = NativeDataset(\n            set(split[\"train_ids\"]),\n            \"data/processed/passages.jsonl\",\n            \"data/processed/bonds.jsonl\",\n            tokenizer,\n        )\n\n        test_ids_to_use = split[\"test_ids\"][:MAX_TEST_SAMPLES]\n\n        # Optional: strict prescriptive-only test\n        if STRICT_PRESCRIPTIVE_TEST:\n            print(\"Filtering to prescriptive examples only...\")\n            # Load bonds to filter\n            prescriptive_ids = set()\n            with open(\"data/processed/bonds.jsonl\") as f:\n                for line in f:\n                    b = json.loads(line)\n                    if b.get(\"context\") == \"prescriptive\":\n                        prescriptive_ids.add(b[\"passage_id\"])\n            test_ids_to_use = [tid for tid in test_ids_to_use if tid in prescriptive_ids]\n            print(f\"  Filtered to {len(test_ids_to_use):,} prescriptive samples\")\n\n        test_dataset = NativeDataset(\n            set(test_ids_to_use),\n            \"data/processed/passages.jsonl\",\n            \"data/processed/bonds.jsonl\",\n            tokenizer,\n        )\n\n        if len(train_dataset) == 0:\n            print(\"ERROR: No training data!\")\n            continue\n\n        # Use hardware-optimized batch size (WiFi-style probing)\n        # v10.15.1.4: Probe with encoder_trainable=False since we start frozen\n        if \"_probed_batch\" not in globals():\n            _PROBED_BATCHES[\"train\"] = get_probed_batch(\n                model, tokenizer, device, BATCH_SIZE, encoder_trainable=False\n            )\n        actual_batch = min(\n            _PROBED_BATCHES.get(\"train\", BATCH_SIZE), max(32, len(train_dataset) // 20)\n        )\n        print(f\"Actual batch size: {actual_batch}\")\n\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=actual_batch,\n            shuffle=True,\n            collate_fn=collate_fn,\n            drop_last=True,\n            num_workers=0,\n            pin_memory=True,\n        )\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=actual_batch,\n            shuffle=False,\n            collate_fn=collate_fn,\n            num_workers=0,\n            pin_memory=True,\n        )\n\n        # v10.15.1.4: Differential learning rates\n        try:\n            _encoder_lr = ENCODER_LR\n            _head_lr = HEAD_LR\n        except NameError:\n            _encoder_lr = 5e-7\n            _head_lr = 1e-3\n\n        encoder_params = list(model.encoder.parameters())\n        head_params = [p for n, p in model.named_parameters() if \"encoder\" not in n]\n\n        optimizer = torch.optim.AdamW(\n            [\n                {\"params\": encoder_params, \"lr\": _encoder_lr},\n                {\"params\": head_params, \"lr\": _head_lr},\n            ],\n            weight_decay=0.01,\n        )\n        print(f\"  Optimizer: AdamW (encoder_lr={_encoder_lr:.0e}, head_lr={_head_lr:.0e})\")\n\n        # v10.15.1.2: Cosine annealing LR schedule\n        if USE_COSINE_LR:\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n                optimizer, T_max=N_EPOCHS, eta_min=LR / 10\n            )\n            print(f\"  Using cosine LR schedule: {LR:.2e} -> {LR / 10:.2e}\")\n        else:\n            scheduler = None\n\n        # Gradient clipping setup\n        try:\n            grad_clip = GRADIENT_CLIP if GRADIENT_CLIP > 0 else None\n        except NameError:\n            grad_clip = 1.0  # Default\n\n        # Early stopping setup\n        try:\n            early_stop_patience = EARLY_STOPPING_PATIENCE if EARLY_STOPPING_PATIENCE > 0 else None\n        except NameError:\n            early_stop_patience = 3  # Default\n        epochs_without_improvement = 0\n\n        def get_adv_lambda(\n            epoch, warmup=ADV_WARMUP_EPOCHS, max_lambda=ADV_MAX_LAMBDA, split_name=None\n        ):\n            \"\"\"Ramp adversarial strength with per-split support (v10.15.1.3)\"\"\"\n            effective_max = max_lambda\n            try:\n                if PER_SPLIT_TUNING and split_name and split_name in SPLIT_ADV_LAMBDA:\n                    effective_max = SPLIT_ADV_LAMBDA[split_name]\n            except NameError:\n                pass\n            if epoch <= warmup:\n                return (epoch / warmup) * effective_max\n            return effective_max\n\n        best_loss = float(\"inf\")\n        no_improve_count = 0\n        start_epoch = 1\n        _unfreeze_warmup = 0  # Track warmup epochs after unfreeze\n\n        # Check for existing checkpoint to resume from\n        checkpoint_path = f\"models/checkpoints/latest_{split_name}.pt\"\n        if os.path.exists(checkpoint_path):\n            print(\"  Found checkpoint, checking validity...\")\n            checkpoint = torch.load(checkpoint_path, map_location=device)\n            ckpt_loss = checkpoint.get(\"best_loss\", float(\"inf\"))\n\n            # Skip corrupted checkpoints (inf/nan loss indicates bad weights)\n            if ckpt_loss == float(\"inf\") or ckpt_loss != ckpt_loss:  # NaN check\n                print(f\"  Checkpoint corrupted (best_loss={ckpt_loss}) - deleting and starting fresh\")\n                os.remove(checkpoint_path)\n                # Also remove best checkpoint if it exists (might be corrupted too)\n                best_ckpt = f\"models/checkpoints/best_{split_name}.pt\"\n                if os.path.exists(best_ckpt):\n                    os.remove(best_ckpt)\n            else:\n                model.load_state_dict(checkpoint[\"model_state\"])\n                optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n                start_epoch = checkpoint[\"epoch\"] + 1\n                best_loss = ckpt_loss\n                print(f\"  Resuming from epoch {start_epoch}, best_loss={best_loss:.4f}\")\n\n        # v10.15.1.4: Gradient accumulation\n        try:\n            _grad_accum = GRADIENT_ACCUMULATION_STEPS\n        except NameError:\n            _grad_accum = 1\n\n        _nan_batch_count = 0  # Track consecutive NaN batches\n        _max_nan_before_reset = 5  # Reset model after this many consecutive NaN batches\n\n        for epoch in range(start_epoch, N_EPOCHS + 1):\n            # v10.15.1.4: Check if we should unfreeze encoder\n            if _unfreeze and _encoder_frozen and epoch >= _unfreeze_after:\n                print(f\"\\n  >>> UNFREEZING ENCODER at epoch {epoch} <<<\")\n\n                # Step 1: Unfreeze encoder parameters (v10.15.1: layer-wise support)\n                try:\n                    _n_layers = UNFREEZE_LAYERS\n                except NameError:\n                    _n_layers = 0  # 0 = unfreeze all\n\n                if _n_layers > 0 and hasattr(model.encoder, \"encoder\"):\n                    # Only unfreeze top N transformer layers\n                    try:\n                        all_layers = list(model.encoder.encoder.layer)\n                        layers_to_unfreeze = all_layers[-_n_layers:]\n                        for layer in layers_to_unfreeze:\n                            for param in layer.parameters():\n                                param.requires_grad = True\n                        # Also unfreeze pooler if exists\n                        if hasattr(model.encoder, \"pooler\"):\n                            for param in model.encoder.pooler.parameters():\n                                param.requires_grad = True\n                        print(f\"  Unfroze top {len(layers_to_unfreeze)} encoder layers\")\n                    except Exception as e:\n                        print(f\"  Layer-wise unfreeze failed ({e}), unfreezing all\")\n                        for param in model.encoder.parameters():\n                            param.requires_grad = True\n                else:\n                    # Unfreeze all encoder parameters\n                    for param in model.encoder.parameters():\n                        param.requires_grad = True\n                    print(\"  Unfroze ALL encoder parameters\")\n                _encoder_frozen = False\n\n                trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n                print(f\"  Trainable params now: {trainable:,}\")\n\n                # Step 2: Create fresh optimizer (old one has no state for encoder params)\n                encoder_params = list(model.encoder.parameters())\n                head_params = [p for n, p in model.named_parameters() if \"encoder\" not in n]\n\n                # Start encoder at scaled LR (v10.15.1: configurable)\n                try:\n                    _lr_scale = ENCODER_LR_SCALE\n                except NameError:\n                    _lr_scale = 0.1  # Default 10x smaller\n                _current_encoder_lr = _encoder_lr * _lr_scale\n                optimizer = torch.optim.AdamW(\n                    [\n                        {\"params\": encoder_params, \"lr\": _current_encoder_lr},\n                        {\"params\": head_params, \"lr\": _head_lr},\n                    ],\n                    weight_decay=0.01,\n                )\n                print(f\"  New optimizer (encoder_lr={_current_encoder_lr:.0e}, warming up)\")\n\n                # Step 3: Reset AMP scaler if using AMP\n                if USE_AMP:\n                    scaler = torch.amp.GradScaler(\"cuda\")\n                    print(\"  Reset AMP scaler\")\n\n                # Step 4: Reduce batch size for unfrozen training\n                # Re-probe with trainable encoder\n                new_batch = probe_max_batch(\n                    model, tokenizer, device, actual_batch, encoder_trainable=True\n                )\n                if new_batch < actual_batch:\n                    actual_batch = new_batch\n                    train_loader = DataLoader(\n                        train_dataset,\n                        batch_size=actual_batch,\n                        shuffle=True,\n                        collate_fn=collate_fn,\n                        drop_last=True,\n                        num_workers=0,\n                        pin_memory=True,\n                    )\n                    print(f\"  Reduced batch size to {actual_batch}\")\n\n                _unfreeze_warmup = 0\n\n            # v10.15.1.4: Warm up encoder LR after unfreeze (over 5 epochs)\n            if not _encoder_frozen and _unfreeze_warmup < 5:\n                warmup_factor = (_unfreeze_warmup + 1) / 5\n                _current_encoder_lr = _encoder_lr * warmup_factor\n                for pg in optimizer.param_groups:\n                    # Encoder param group has more params\n                    if sum(p.numel() for p in pg[\"params\"]) > 1000000:\n                        pg[\"lr\"] = _current_encoder_lr\n                _unfreeze_warmup += 1\n                print(f\"  Encoder LR warmup: {_current_encoder_lr:.1e} ({_unfreeze_warmup}/5)\")\n\n            model.train()\n            total_loss = 0\n            n_batches = 0\n            batch_count = 0\n\n            for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n                optimizer.zero_grad()\n\n                input_ids = batch[\"input_ids\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device)\n                bond_labels = batch[\"bond_labels\"].to(device)\n                language_labels = batch[\"language_labels\"].to(device)\n                period_labels = batch[\"period_labels\"].to(device)\n\n                adv_lambda = get_adv_lambda(epoch, split_name=split_name)\n\n                # Use new autocast API\n                with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n                    out = model(input_ids, attention_mask, adv_lambda=adv_lambda)\n\n                    # Check for NaN in model output (indicates corrupted weights)\n                    if torch.isnan(out[\"z\"]).any() or torch.isinf(out[\"z\"]).any():\n                        _nan_batch_count += 1\n                        if _nan_batch_count >= _max_nan_before_reset:\n                            print(f\"    {_nan_batch_count} consecutive NaN outputs - resetting from best checkpoint\")\n                            best_ckpt = f\"models/checkpoints/best_{split_name}.pt\"\n                            if os.path.exists(best_ckpt):\n                                model.load_state_dict(torch.load(best_ckpt, map_location=device))\n                                # Reset optimizer\n                                optimizer = torch.optim.AdamW(\n                                    [\n                                        {\"params\": list(model.encoder.parameters()), \"lr\": _encoder_lr},\n                                        {\"params\": [p for n, p in model.named_parameters() if \"encoder\" not in n], \"lr\": _head_lr},\n                                    ],\n                                    weight_decay=0.01,\n                                )\n                                if USE_AMP:\n                                    scaler = torch.amp.GradScaler(\"cuda\")\n                                _nan_batch_count = 0\n                                print(\"    Model reset successful\")\n                            else:\n                                print(\"    No checkpoint to reset from - skipping batch\")\n                        continue\n\n                    # Weighted bond loss with class weights\n                    if USE_CONFIDENCE_WEIGHTING:\n                        sample_weights = batch[\"sample_weights\"].to(device)\n                        loss_bond = F.cross_entropy(out[\"bond_pred\"], bond_labels, weight=BOND_CLASS_WEIGHTS, reduction=\"none\")\n                        loss_bond = (loss_bond * sample_weights).mean()\n                    else:\n                        loss_bond = F.cross_entropy(out[\"bond_pred\"], bond_labels, weight=BOND_CLASS_WEIGHTS)\n\n                    # Context auxiliary loss\n                    if USE_CONTEXT_AUXILIARY:\n                        context_labels = batch[\"context_labels\"].to(device)\n                        loss_context = F.cross_entropy(out[\"context_pred\"], context_labels)\n                    else:\n                        loss_context = 0\n\n                    loss_lang = F.cross_entropy(out[\"language_pred\"], language_labels)\n                    loss_period = F.cross_entropy(out[\"period_pred\"], period_labels)\n\n                    # v10.16.5: Confusion loss - force uniform predictions\n                    # This prevents the adversarial head from \"learning to fail\"\n                    # while the encoder hides info in dimensions the head doesn't monitor\n                    loss_confusion = torch.tensor(0.0, device=device)\n                    try:\n                        if USE_CONFUSION_LOSS:\n                            # Compute entropy of language predictions\n                            lang_probs = F.softmax(out[\"language_pred\"], dim=-1)\n                            lang_entropy = -(lang_probs * torch.log(lang_probs + 1e-8)).sum(dim=-1).mean()\n\n                            # Compute entropy of period predictions\n                            period_probs = F.softmax(out[\"period_pred\"], dim=-1)\n                            period_entropy = -(period_probs * torch.log(period_probs + 1e-8)).sum(dim=-1).mean()\n\n                            # Max entropy = log(num_classes), normalize to [0,1]\n                            max_lang_entropy = torch.log(torch.tensor(float(len(LANG_TO_IDX)), device=device))\n                            max_period_entropy = torch.log(torch.tensor(float(len(PERIOD_TO_IDX)), device=device))\n\n                            # Loss = negative normalized entropy (minimize = maximize entropy)\n                            lang_confusion = 1.0 - (lang_entropy / max_lang_entropy)\n                            period_confusion = 1.0 - (period_entropy / max_period_entropy)\n\n                            loss_confusion = (lang_confusion + period_confusion) / 2\n                    except NameError:\n                        pass  # USE_CONFUSION_LOSS not defined\n\n                loss = (\n                    loss_bond\n                    + LANG_WEIGHT * loss_lang\n                    + PERIOD_WEIGHT * loss_period\n                    + CONTEXT_LOSS_WEIGHT * loss_context\n                    + CONFUSION_WEIGHT * loss_confusion  # v10.16.5\n                )\n\n                # v10.10: Role contrastive loss for agent/patient sensitivity\n                loss_role = torch.tensor(0.0, device=device)\n                if USE_ROLE_AUGMENTATION and random.random() < ROLE_AUGMENT_PROB:\n                    batch_texts = batch.get(\"texts\", [])\n                    batch_languages = batch.get(\"languages\", [])\n\n                    swapped_texts = []\n                    original_indices = []\n\n                    for i, (text, lang) in enumerate(zip(batch_texts, batch_languages)):\n                        swapped = swap_roles_simple(text, lang)\n                        if swapped:\n                            swapped_texts.append(swapped)\n                            original_indices.append(i)\n\n                    if swapped_texts and len(swapped_texts) >= 2:\n                        # Tokenize swapped texts\n                        swapped_encoded = tokenizer(\n                            swapped_texts,\n                            padding=True,\n                            truncation=True,\n                            max_length=128,\n                            return_tensors=\"pt\",\n                        )\n                        swapped_ids = swapped_encoded[\"input_ids\"].to(device)\n                        swapped_mask = swapped_encoded[\"attention_mask\"].to(device)\n\n                        # Get embeddings for swapped texts (no gradients needed - saves memory!)\n                        # We only need gradients through z_original, not z_swapped\n                        with torch.no_grad():\n                            swapped_out = model(swapped_ids, swapped_mask, adv_lambda=0)\n                            z_swapped = swapped_out[\"z\"].detach()\n\n                        # Get original embeddings for corresponding indices (keeps gradients)\n                        z_original = out[\"z\"][original_indices]\n\n                        # Ensure same dtype (AMP: z_original=float16, z_swapped=float32)\n                        z_swapped = z_swapped.to(z_original.dtype)\n\n                        # Contrastive loss: push role-swapped embeddings apart\n                        # Hinge loss: max(0, margin - distance)\n                        # Gradients flow through z_original only\n                        distances = F.pairwise_distance(z_original, z_swapped)\n                        loss_role = F.relu(ROLE_CONTRASTIVE_MARGIN - distances).mean()\n\n                        # Clean up to prevent memory accumulation\n                        del swapped_ids, swapped_mask, swapped_out, swapped_encoded\n                        del z_original, z_swapped, distances\n\n                loss = loss + ROLE_CONTRASTIVE_WEIGHT * loss_role\n\n                # =============================================================\n                # v10.16.1: STRUCTURAL CONTRASTIVE LOSS\n                # =============================================================\n                loss_structural = torch.tensor(0.0, device=device)\n                _structural_distances = torch.tensor([0.0], device=device)\n                try:\n                    if USE_STRUCTURAL_CONTRASTIVE:\n                        batch_texts = batch.get(\"texts\", [])\n                        batch_languages = batch.get(\"languages\", [])\n\n                        structural_texts = []\n                        struct_original_indices = []\n\n                        for si, (stext, slang) in enumerate(zip(batch_texts, batch_languages)):\n                            sperturbed, _ = create_structural_perturbation(stext, slang)\n                            if sperturbed:\n                                structural_texts.append(sperturbed)\n                                struct_original_indices.append(si)\n\n                        if structural_texts and len(structural_texts) >= 2:\n                            struct_encoded = tokenizer(\n                                structural_texts,\n                                padding=True,\n                                truncation=True,\n                                max_length=128,\n                                return_tensors=\"pt\",\n                            )\n                            struct_ids = struct_encoded[\"input_ids\"].to(device)\n                            struct_mask = struct_encoded[\"attention_mask\"].to(device)\n\n                            with torch.no_grad():\n                                struct_out = model(struct_ids, struct_mask, adv_lambda=0)\n                                z_structural = struct_out[\"z\"].detach()\n\n                            z_orig_struct = out[\"z\"][struct_original_indices]\n                            z_structural = z_structural.to(z_orig_struct.dtype)\n\n                            # Push apart: penalize if distance < margin\n                            _structural_distances = F.pairwise_distance(z_orig_struct, z_structural)\n                            loss_structural = F.relu(STRUCTURAL_CONTRASTIVE_MARGIN - _structural_distances).mean()\n\n                            del struct_ids, struct_mask, struct_out\n                except NameError:\n                    pass\n                except Exception as e:\n                    pass\n\n                if not (torch.isnan(loss_structural) or torch.isinf(loss_structural)):\n                    try:\n                        loss = loss + STRUCTURAL_CONTRASTIVE_WEIGHT * loss_structural\n                    except NameError:\n                        loss = loss + 0.4 * loss_structural\n\n                # =============================================================\n                # v10.16.1: STRUCTURAL CONTRASTIVE LOSS\n                # =============================================================\n                # Push structural perturbations APART (they change moral meaning)\n                loss_structural = torch.tensor(0.0, device=device)\n                structural_distances_batch = []\n                try:\n                    if USE_STRUCTURAL_CONTRASTIVE:\n                        batch_texts = batch.get(\"texts\", [])\n                        batch_languages = batch.get(\"languages\", [])\n\n                        structural_texts = []\n                        original_indices = []\n\n                        for i, (text, lang) in enumerate(zip(batch_texts, batch_languages)):\n                            perturbed, _ = create_structural_perturbation(text, lang)\n                            if perturbed:\n                                structural_texts.append(perturbed)\n                                original_indices.append(i)\n\n                        if structural_texts and len(structural_texts) >= 2:\n                            struct_encoded = tokenizer(\n                                structural_texts,\n                                padding=True,\n                                truncation=True,\n                                max_length=128,\n                                return_tensors=\"pt\",\n                            )\n                            struct_ids = struct_encoded[\"input_ids\"].to(device)\n                            struct_mask = struct_encoded[\"attention_mask\"].to(device)\n\n                            with torch.no_grad():\n                                struct_out = model(struct_ids, struct_mask, adv_lambda=0)\n                                z_structural = struct_out[\"z\"].detach()\n\n                            z_original_struct = out[\"z\"][original_indices]\n                            z_structural = z_structural.to(z_original_struct.dtype)\n\n                            # Push apart: penalize if distance < margin\n                            distances = F.pairwise_distance(z_original_struct, z_structural)\n                            structural_distances_batch = distances.detach()\n                            loss_structural = F.relu(STRUCTURAL_CONTRASTIVE_MARGIN - distances).mean()\n\n                            del struct_ids, struct_mask, struct_out, z_structural, z_original_struct\n                except NameError:\n                    pass\n\n                if not (torch.isnan(loss_structural) or torch.isinf(loss_structural)):\n                    try:\n                        loss = loss + STRUCTURAL_CONTRASTIVE_WEIGHT * loss_structural\n                    except NameError:\n                        loss = loss + 0.4 * loss_structural\n\n                # v10.15.1: Surface contrastive loss for invariance\n                loss_surface = torch.tensor(0.0, device=device)\n                try:\n                    if SURFACE_AUGMENT and CONTRASTIVE_WEIGHT > 0:\n                        batch_texts = batch.get(\"texts\", [])\n                        if batch_texts and len(batch_texts) >= 4:\n                            # Create surface-augmented versions\n                            augmented_texts = [create_surface_augmented(t) for t in batch_texts]\n\n                            # Tokenize augmented texts\n                            aug_encoded = tokenizer(\n                                augmented_texts,\n                                padding=True,\n                                truncation=True,\n                                max_length=128,\n                                return_tensors=\"pt\",\n                            )\n                            aug_ids = aug_encoded[\"input_ids\"].to(device)\n                            aug_mask = aug_encoded[\"attention_mask\"].to(device)\n\n                            # Get embeddings for augmented texts\n                            with torch.no_grad():\n                                aug_out = model(aug_ids, aug_mask, adv_lambda=0)\n                                z_augmented = aug_out[\"z\"]\n\n                            # Original embeddings (anchor)\n                            z_original = out[\"z\"]\n\n                            # Ensure same dtype (AMP: z_original=float16, z_augmented=float32)\n                            z_augmented = z_augmented.to(z_original.dtype)\n\n                            # InfoNCE contrastive loss\n                            loss_surface = info_nce_loss(\n                                z_original, z_augmented, temperature=CONTRASTIVE_TEMPERATURE\n                            )\n\n                            # Augment similarity loss (direct MSE)\n                            if AUGMENT_SIMILARITY_WEIGHT > 0:\n                                sim_loss = F.mse_loss(z_original, z_augmented)\n                                loss_surface = loss_surface + AUGMENT_SIMILARITY_WEIGHT * sim_loss\n\n                            # Guard against NaN in contrastive loss\n                            if torch.isnan(loss_surface) or torch.isinf(loss_surface):\n                                loss_surface = torch.tensor(0.0, device=device)\n\n                            # Cleanup\n                            del aug_ids, aug_mask, aug_out, z_augmented\n                except NameError:\n                    pass  # Config params not defined\n\n                # Only add contrastive loss if valid\n                if not (torch.isnan(loss_surface) or torch.isinf(loss_surface)):\n                    loss = loss + CONTRASTIVE_WEIGHT * loss_surface\n\n                # =============================================================\n                # v10.16.1: TRIPLET LOSS (anchor, surface+, structural-)\n                # =============================================================\n                loss_triplet = torch.tensor(0.0, device=device)\n                try:\n                    if USE_TRIPLET_LOSS:\n                        # Need both surface and structural perturbations\n                        if 'z_augmented' in dir() and z_augmented is not None and len(z_augmented) >= 2:\n                            if 'z_structural' in dir() and z_structural is not None and len(z_structural) >= 2:\n                                min_len = min(len(out[\"z\"]), len(z_augmented), len(z_structural))\n                                if min_len >= 2:\n                                    t_anchor = out[\"z\"][:min_len]\n                                    t_positive = z_augmented[:min_len].to(t_anchor.dtype)\n                                    t_negative = z_structural[:min_len].to(t_anchor.dtype)\n                                    loss_triplet = triplet_loss_geometric(t_anchor, t_positive, t_negative, margin=TRIPLET_MARGIN)\n                except NameError:\n                    pass\n                except Exception:\n                    pass\n\n                if not (torch.isnan(loss_triplet) or torch.isinf(loss_triplet)):\n                    try:\n                        loss = loss + TRIPLET_WEIGHT * loss_triplet\n                    except NameError:\n                        loss = loss + 0.3 * loss_triplet\n\n                # =============================================================\n                # v10.16.1: RATIO REGULARIZATION LOSS\n                # =============================================================\n                loss_ratio = torch.tensor(0.0, device=device)\n                try:\n                    if USE_RATIO_LOSS:\n                        # Get surface distances\n                        if 'z_augmented' in dir() and z_augmented is not None and len(z_augmented) >= 2:\n                            _surface_distances = F.pairwise_distance(\n                                out[\"z\"][:len(z_augmented)],\n                                z_augmented.to(out[\"z\"].dtype)\n                            )\n                        else:\n                            _surface_distances = None\n\n                        # Use structural distances computed earlier\n                        if '_structural_distances' in dir() and len(_structural_distances) >= 2:\n                            _struct_dists = _structural_distances\n                        else:\n                            _struct_dists = None\n\n                        if _surface_distances is not None and _struct_dists is not None:\n                            loss_ratio = ratio_regularization_loss(\n                                _surface_distances, _struct_dists, target_ratio=TARGET_RATIO\n                            )\n                except NameError:\n                    pass\n                except Exception:\n                    pass\n\n                if not (torch.isnan(loss_ratio) or torch.isinf(loss_ratio)):\n                    try:\n                        loss = loss + RATIO_LOSS_WEIGHT * loss_ratio\n                    except NameError:\n                        loss = loss + 0.2 * loss_ratio\n\n                # =============================================================\n                # v10.16.1: TRIPLET LOSS (anchor, surface+, structural-)\n                # =============================================================\n                loss_triplet = torch.tensor(0.0, device=device)\n                try:\n                    if USE_TRIPLET_LOSS and 'z_augmented' in dir() and 'z_structural' in dir():\n                        # We have both surface and structural perturbations\n                        # Find common indices where we have both\n                        if len(original_indices) >= 2 and len(z_augmented) >= 2:\n                            # Use minimum overlap\n                            min_len = min(len(z_augmented), len(z_structural), len(out[\"z\"]))\n                            if min_len >= 2:\n                                anchor = out[\"z\"][:min_len]\n                                positive = z_augmented[:min_len].to(anchor.dtype)\n                                negative = z_structural[:min_len].to(anchor.dtype)\n\n                                loss_triplet = triplet_loss_geometric(\n                                    anchor, positive, negative, margin=TRIPLET_MARGIN\n                                )\n                except NameError:\n                    pass\n                except Exception:\n                    pass  # Skip if dimensions don't match\n\n                if not (torch.isnan(loss_triplet) or torch.isinf(loss_triplet)):\n                    try:\n                        loss = loss + TRIPLET_WEIGHT * loss_triplet\n                    except NameError:\n                        loss = loss + 0.3 * loss_triplet\n\n                # =============================================================\n                # v10.16.1: RATIO REGULARIZATION LOSS\n                # =============================================================\n                loss_ratio = torch.tensor(0.0, device=device)\n                try:\n                    if USE_RATIO_LOSS:\n                        # Get surface distances from this batch\n                        if 'z_augmented' in dir() and len(z_augmented) >= 2:\n                            surface_distances = F.pairwise_distance(\n                                out[\"z\"][:len(z_augmented)],\n                                z_augmented.to(out[\"z\"].dtype)\n                            )\n                        else:\n                            surface_distances = None\n\n                        # Get structural distances\n                        if len(structural_distances_batch) >= 2:\n                            structural_distances = structural_distances_batch\n                        else:\n                            structural_distances = None\n\n                        if surface_distances is not None and structural_distances is not None:\n                            loss_ratio = ratio_regularization_loss(\n                                surface_distances,\n                                structural_distances,\n                                target_ratio=TARGET_RATIO\n                            )\n                except NameError:\n                    pass\n                except Exception:\n                    pass\n\n                if not (torch.isnan(loss_ratio) or torch.isinf(loss_ratio)):\n                    try:\n                        loss = loss + RATIO_LOSS_WEIGHT * loss_ratio\n                    except NameError:\n                        loss = loss + 0.2 * loss_ratio\n\n                # v10.15.1.2: Gradient penalty for adversarial heads\n                loss_gp = torch.tensor(0.0, device=device)\n                if USE_GRADIENT_PENALTY and adv_lambda > 0.1:\n                    # Penalize large gradients in adversarial predictions\n                    # This encourages smoother, more invariant representations\n                    lang_probs = F.softmax(out[\"language_pred\"], dim=-1)\n                    period_probs = F.softmax(out[\"period_pred\"], dim=-1)\n                    # Entropy penalty: encourage uniform (confused) predictions\n                    lang_entropy = -(lang_probs * torch.log(lang_probs + 1e-8)).sum(dim=-1).mean()\n                    period_entropy = (\n                        -(period_probs * torch.log(period_probs + 1e-8)).sum(dim=-1).mean()\n                    )\n                    # Maximize entropy = minimize negative entropy\n                    loss_gp = -GRADIENT_PENALTY_WEIGHT * (lang_entropy + period_entropy)\n\n                loss = loss + loss_gp\n\n                # v10.15.1.4: NaN detection\n                if torch.isnan(loss) or torch.isinf(loss):\n                    print(\"    NaN/Inf loss detected - skipping batch\")\n                    optimizer.zero_grad()\n                    continue\n\n                # v10.15.1.4: Gradient accumulation\n                loss_scaled = loss / _grad_accum\n\n                if USE_AMP and scaler:\n                    scaler.scale(loss_scaled).backward()\n                    if (batch_count + 1) % _grad_accum == 0:\n                        scaler.unscale_(optimizer)\n                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                        scaler.step(optimizer)\n                        scaler.update()\n                        optimizer.zero_grad()\n                else:\n                    loss_scaled.backward()\n                    if (batch_count + 1) % _grad_accum == 0:\n                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                        optimizer.step()\n                        optimizer.zero_grad()\n\n                batch_count += 1\n                total_loss += loss.item()\n                n_batches += 1\n                _nan_batch_count = 0  # Reset NaN counter on successful batch\n\n                # Delete intermediate tensors to prevent memory accumulation\n                del input_ids, attention_mask, bond_labels, language_labels, period_labels\n                del out, loss, loss_bond, loss_lang, loss_period\n                if USE_CONFIDENCE_WEIGHTING:\n                    del sample_weights\n                if USE_CONTEXT_AUXILIARY:\n                    del context_labels, loss_context\n                if USE_ROLE_AUGMENTATION:\n                    del loss_role\n\n                # Periodic memory cleanup every 50 batches\n                if n_batches % 50 == 0:\n                    gc.collect()\n                    torch.cuda.empty_cache()\n\n            if n_batches == 0:\n                print(f\"Epoch {epoch}: No valid batches! All had NaN loss.\")\n                continue\n\n            avg_loss = total_loss / n_batches\n\n            # Aggressive memory cleanup after each epoch\n            gc.collect()\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n\n            if torch.cuda.is_available():\n                mem_alloc = torch.cuda.memory_allocated() / 1e9\n                mem_reserved = torch.cuda.memory_reserved() / 1e9\n                print(\n                    f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f}) [GPU: {mem_alloc:.1f}GB alloc, {mem_reserved:.1f}GB reserved]\"\n                )\n            else:\n                print(f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f})\")\n\n            # v10.16.2: Quick language accuracy check every few epochs\n            if epoch % 3 == 0 or epoch == N_EPOCHS:\n                model.eval()\n                _sample_preds = []\n                _sample_labels = []\n                with torch.no_grad():\n                    for _sb in list(test_loader)[:5]:  # Sample 5 batches\n                        _sout = model(_sb[\"input_ids\"].to(device), _sb[\"attention_mask\"].to(device), 0)\n                        _sample_preds.extend(_sout[\"language_pred\"].argmax(-1).cpu().tolist())\n                        _sample_labels.extend(_sb[\"language_labels\"].tolist())\n                _lang_acc_sample = sum(p == l for p, l in zip(_sample_preds, _sample_labels)) / len(_sample_preds) if _sample_preds else 0\n                model.train()\n                print(f\"  -> lang_acc={_lang_acc_sample:.1%} (target: <20%)\")\n\n            # v10.15.1.2: Step LR scheduler\n            if USE_COSINE_LR and scheduler:\n                scheduler.step()\n\n            # Save checkpoint every epoch (for crash recovery)\n            checkpoint = {\n                \"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n                \"loss\": avg_loss,\n                \"best_loss\": best_loss,\n            }\n            torch.save(checkpoint, f\"models/checkpoints/latest_{split_name}.pt\")\n\n            if avg_loss < best_loss:\n                best_loss = avg_loss\n                torch.save(model.state_dict(), f\"models/checkpoints/best_{split_name}.pt\")\n                torch.save(model.state_dict(), f\"{SAVE_DIR}/best_{split_name}.pt\")\n                no_improve_count = 0\n            else:\n                no_improve_count += 1\n                if early_stop_patience and no_improve_count >= early_stop_patience:\n                    print(f\"Early stopping: no improvement for {no_improve_count} epochs\")\n                    break\n\n        # Evaluate\n        print(\"\\nEvaluating...\")\n        model.load_state_dict(torch.load(f\"models/checkpoints/best_{split_name}.pt\"))\n        model.eval()\n\n        # v10.15.1.4: Clear memory and use smaller batch for testing\n        torch.cuda.empty_cache()\n        import gc\n\n        gc.collect()\n\n        # Recreate test loader with smaller batch to avoid OOM\n        test_batch = min(32, actual_batch)\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=test_batch,\n            shuffle=False,\n            collate_fn=collate_fn,\n            num_workers=0,\n            pin_memory=True,\n        )\n        print(f\"  Testing with batch size {test_batch}\")\n\n        all_preds = {\"bond\": [], \"lang\": []}\n        all_labels = {\"bond\": [], \"lang\": []}\n        all_languages = []\n\n        with torch.no_grad():\n            for batch in tqdm(test_loader, desc=\"Testing\"):\n                out = model(batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), 0)\n                all_preds[\"bond\"].extend(out[\"bond_pred\"].argmax(-1).cpu().tolist())\n                all_preds[\"lang\"].extend(out[\"language_pred\"].argmax(-1).cpu().tolist())\n                all_labels[\"bond\"].extend(batch[\"bond_labels\"].tolist())\n                all_labels[\"lang\"].extend(batch[\"language_labels\"].tolist())\n                all_languages.extend(batch[\"languages\"])\n\n        bond_f1 = f1_score(all_labels[\"bond\"], all_preds[\"bond\"], average=\"macro\", zero_division=0)\n        bond_acc = sum(p == l for p, l in zip(all_preds[\"bond\"], all_labels[\"bond\"])) / len(\n            all_preds[\"bond\"]\n        )\n        lang_acc = sum(p == l for p, l in zip(all_preds[\"lang\"], all_labels[\"lang\"])) / len(\n            all_preds[\"lang\"]\n        )\n\n        # Per-language F1\n        lang_f1 = {}\n        for lang in set(all_languages):\n            mask = [l == lang for l in all_languages]\n            if sum(mask) > 10:\n                preds = [p for p, m in zip(all_preds[\"bond\"], mask) if m]\n                labels = [l for l, m in zip(all_labels[\"bond\"], mask) if m]\n                lang_f1[lang] = {\n                    \"f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n                    \"n\": sum(mask),\n                }\n\n        all_results[split_name] = {\n            \"bond_f1_macro\": bond_f1,\n            \"bond_acc\": bond_acc,\n            \"language_acc\": lang_acc,\n            \"per_language_f1\": lang_f1,\n            \"training_time\": time.time() - split_start,\n        }\n\n        print(f\"\\n{split_name} RESULTS:\")\n        print(f\"  Bond F1 (macro): {bond_f1:.3f} ({bond_f1 / 0.1:.1f}x chance)\")\n        print(f\"  Bond accuracy:   {bond_acc:.1%}\")\n        print(f\"  Language acc:    {lang_acc:.1%} (want ~20% = invariant)\")\n        print(\"  Per-language:\")\n        for lang, m in sorted(lang_f1.items(), key=lambda x: -x[1][\"n\"]):\n            print(f\"    {lang:20s}: F1={m['f1']:.3f} (n={m['n']:,})\")\n\n        # Context analysis\n        high_conf = sum(1 for c in test_dataset.data if c[\"confidence\"] == \"high\")\n        prescriptive = sum(1 for c in test_dataset.data if c[\"context\"] == \"prescriptive\")\n        print(\n            f\"  Context: {prescriptive:,}/{len(test_dataset):,} prescriptive ({prescriptive / len(test_dataset) * 100:.1f}%)\"\n        )\n        print(\n            f\"  High confidence: {high_conf:,}/{len(test_dataset):,} ({high_conf / len(test_dataset) * 100:.1f}%)\"\n        )\n\n        # GPU memory usage before cleanup\n        if torch.cuda.is_available():\n            mem = torch.cuda.memory_allocated() / 1e9\n            print(\n                f\"\\n  GPU memory (before cleanup): {mem:.1f} GB / {VRAM_GB:.1f} GB ({mem / VRAM_GB * 100:.0f}%)\"\n            )\n\n        # Aggressive memory cleanup between splits\n        # Step 1: Zero out gradients to release gradient memory\n        model.zero_grad(set_to_none=True)\n        for param in model.parameters():\n            param.grad = None\n\n        # Step 2: Clear optimizer state (can hold significant memory)\n        optimizer.zero_grad(set_to_none=True)\n        optimizer_state = optimizer.state\n        for state in optimizer_state.values():\n            for k, v in list(state.items()):\n                if isinstance(v, torch.Tensor):\n                    state[k] = None\n\n        # Step 3: Move model to CPU to release GPU memory\n        model.cpu()\n\n        # Step 4: Delete all references\n        del model, train_dataset, test_dataset, train_loader, test_loader, optimizer\n        if USE_AMP and scaler:\n            del scaler\n\n        # Step 5: Force garbage collection (multiple passes)\n        for _ in range(5):\n            gc.collect()\n\n        # Step 6: Clear CUDA cache and reset memory stats\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n            torch.cuda.reset_peak_memory_stats()\n\n            # If memory is still high, try more aggressive cleanup\n            mem_check = torch.cuda.memory_allocated() / 1e9\n            if mem_check > 2.0:\n                print(f\"  Memory still high ({mem_check:.1f}GB), attempting deeper cleanup...\")\n                # Clear all cached allocations\n                torch.cuda.memory._dump_snapshot = lambda: None  # Disable snapshot if enabled\n                gc.collect()\n                torch.cuda.empty_cache()\n                torch.cuda.ipc_collect()\n\n        # Step 7: Re-create scaler for next split\n        if USE_AMP:\n            scaler = torch.amp.GradScaler(\"cuda\")\n\n        # GPU memory after cleanup\n        if torch.cuda.is_available():\n            mem_after = torch.cuda.memory_allocated() / 1e9\n            print(\n                f\"  GPU memory (after cleanup): {mem_after:.1f} GB (freed {mem - mem_after:.1f} GB)\"\n            )\n            if mem_after > 1.0:\n                print(\n                    f\"  WARNING: {mem_after:.1f} GB still allocated - may cause OOM on next split\"\n                )\n                print(\"  Consider running with BACKBONE='MiniLM' for lower memory usage\")\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TRAINING COMPLETE\")\n    print(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e61d6594114944e3a100f35a977ddcb9",
      "085179571bf649eabe7b3b822713b0d5",
      "beaec6532ad14f208cd892f879dbc1c9",
      "5582760cff614763848617d46b896f18",
      "8a5b2c94e57d418db40d911b2053cdb9",
      "3637217483a94bd7b72a5a9bc3646acb",
      "283bf7994bd14a1085bdff745fa7e79b",
      "1e2908e839554e69be582cbe44db2ef5",
      "a6ce204bc128429d96d753f69de3eec0",
      "45dd8e439b5546c4ba6b886a5bbe6be6",
      "bd8cfbc6981b4176b75bc9fd3ade7135",
      "67038db96e194670b29df9157b38aa40",
      "52ed9ec58ed24a88a7724fe25b231113",
      "3f0f1aff3e6346e7b8791a9e90c9ca07",
      "d52123a0b7284857b404d7b24fe4a189",
      "f91b379c52bc43cca83a97a40d3edecb",
      "3a57e54bda33412fa8316f7fcac43dca",
      "0f83a4007a9147f7834787a1890489ff",
      "501e3d42a0ff4b08b9d7131a5bf10b84",
      "c8606aff46ee4120815828c0e7bfd5e8",
      "5ec7817282cc4590ae399f4c16643fd3",
      "1b80c1626fdd45e99c0bf76008a26f73",
      "10b5773180bc43e8879d74b3d40be184",
      "b6d502b9af3e4dc6b4619a2704126768",
      "64d30368ae2a4883adbd61ca0bd1bac2",
      "3ac55274bf8a46d0b3a490c3ec963112",
      "1822d58b5167459f8084fb35e6898713",
      "a5033d14e19a40019647acc3f5d6fe4c",
      "7abc7e5e379445dbabac9d4d6b7620ff",
      "6fb3dfec2d764f629ab3a1c030871111",
      "346ce3b61f254fb19420fbca109cd5ca",
      "c46f8e9535cf41219a723222f53711fc",
      "8b42f3e65c8944958a1b4a8ebf1ba282"
     ]
    },
    "id": "cell_8",
    "outputId": "2a4a08b2-6d64-4f88-f0f5-89417208ff23"
   },
   "outputs": [],
   "source": [
    "# @title 8. Geometric Analysis & Linear Probe { display-mode: \"form\" }\n",
    "# @markdown v10.9: New geometric analysis module + linear probe test\n",
    "# @markdown Tests latent space structure (axis discovery, role swap analysis)\n",
    "# @markdown Tests if z_bond encodes language/period (should be low = invariant)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# ===== v10.9: GEOMETRIC ANALYZER CLASS =====\n",
    "class GeometricAnalyzer:\n",
    "    \"\"\"\n",
    "    Probe the latent space geometry to discover moral structure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        inputs = self.tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, max_length=128, padding=\"max_length\"\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        z = self.model.get_bond_embedding(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        return z.cpu().numpy().flatten()\n",
    "\n",
    "    def find_direction(self, positive_texts: list[str], negative_texts: list[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Find the direction in z-space that separates two concepts.\n",
    "        E.g., obligation vs permission, harm vs care.\n",
    "        \"\"\"\n",
    "        pos_embs = np.array([self.get_embedding(t) for t in positive_texts])\n",
    "        neg_embs = np.array([self.get_embedding(t) for t in negative_texts])\n",
    "\n",
    "        pos_mean = pos_embs.mean(axis=0)\n",
    "        neg_mean = neg_embs.mean(axis=0)\n",
    "\n",
    "        direction = pos_mean - neg_mean\n",
    "        direction = direction / (np.linalg.norm(direction) + 1e-9)\n",
    "        return direction\n",
    "\n",
    "    def test_direction_transfer(\n",
    "        self, direction: np.ndarray, test_pairs: list[tuple[str, str]]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Test if a direction generalizes to new examples.\n",
    "        Returns accuracy of direction-based classification.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for pos_text, neg_text in test_pairs:\n",
    "            pos_proj = np.dot(self.get_embedding(pos_text), direction)\n",
    "            neg_proj = np.dot(self.get_embedding(neg_text), direction)\n",
    "            scores.append(1.0 if pos_proj > neg_proj else 0.0)\n",
    "        return np.mean(scores)\n",
    "\n",
    "    def pca_on_pairs(self, concept_pairs: dict[str, list[tuple[str, str]]]) -> dict:\n",
    "        \"\"\"\n",
    "        Run PCA on difference vectors to find dominant axes.\n",
    "\n",
    "        concept_pairs: {\"obligation_permission\": [(obl1, perm1), ...], ...}\n",
    "        \"\"\"\n",
    "        all_diffs = []\n",
    "        labels = []\n",
    "\n",
    "        for concept, pairs in concept_pairs.items():\n",
    "            for pos, neg in pairs:\n",
    "                diff = self.get_embedding(pos) - self.get_embedding(neg)\n",
    "                all_diffs.append(diff)\n",
    "                labels.append(concept)\n",
    "\n",
    "        X = np.array(all_diffs)\n",
    "\n",
    "        pca = PCA(n_components=min(10, len(X)))\n",
    "        pca.fit(X)\n",
    "\n",
    "        return {\n",
    "            \"components\": pca.components_,\n",
    "            \"explained_variance_ratio\": pca.explained_variance_ratio_,\n",
    "            \"labels\": labels,\n",
    "            \"transformed\": pca.transform(X),\n",
    "        }\n",
    "\n",
    "    def role_swap_analysis(self, agent_patient_pairs: list[tuple[str, str]]) -> dict:\n",
    "        \"\"\"\n",
    "        Test if swapping agent/patient produces consistent transformation.\n",
    "\n",
    "        agent_patient_pairs: [(\"A harmed B\", \"B harmed A\"), ...]\n",
    "        \"\"\"\n",
    "        transformations = []\n",
    "\n",
    "        for original, swapped in agent_patient_pairs:\n",
    "            orig_emb = self.get_embedding(original)\n",
    "            swap_emb = self.get_embedding(swapped)\n",
    "            transformations.append(swap_emb - orig_emb)\n",
    "\n",
    "        T = np.array(transformations)\n",
    "\n",
    "        # Check consistency: are all transformations similar?\n",
    "        mean_transform = T.mean(axis=0)\n",
    "        cosines = [\n",
    "            np.dot(t, mean_transform) / (np.linalg.norm(t) * np.linalg.norm(mean_transform) + 1e-9)\n",
    "            for t in T\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"mean_transform\": mean_transform,\n",
    "            \"consistency\": np.mean(cosines),\n",
    "            \"consistency_std\": np.std(cosines),\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LINEAR PROBE TEST\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nIf probe accuracy is NEAR CHANCE, representation is INVARIANT\")\n",
    "print(\"(This is what we want for BIP)\")\n",
    "\n",
    "probe_results = {}\n",
    "\n",
    "for split_name in [\"hebrew_to_others\", \"semitic_to_non_semitic\"]:\n",
    "    model_path = f\"{SAVE_DIR}/best_{split_name}.pt\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"\\nSkipping {split_name} - no saved model\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"PROBE: {split_name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model = BIPModel(z_dim=Z_DIM).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    test_ids = set(all_splits[split_name][\"test_ids\"][:5000])\n",
    "    test_dataset = NativeDataset(\n",
    "        test_ids, \"data/processed/passages.jsonl\", \"data/processed/bonds.jsonl\", tokenizer\n",
    "    )\n",
    "\n",
    "    if len(test_dataset) < 50:\n",
    "        print(f\"  Skip - only {len(test_dataset)} samples\")\n",
    "        continue\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=get_probed_batch(model, tokenizer, device, mode=\"eval\"),\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    all_z, all_lang, all_period = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Extract\"):\n",
    "            out = model(batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), 0)\n",
    "            all_z.append(out[\"z\"].cpu().numpy())\n",
    "            all_lang.extend(batch[\"language_labels\"].tolist())\n",
    "            all_period.extend(batch[\"period_labels\"].tolist())\n",
    "\n",
    "    X = np.vstack(all_z)\n",
    "    y_lang = np.array(all_lang)\n",
    "    y_period = np.array(all_period)\n",
    "\n",
    "    scaler_probe = StandardScaler()\n",
    "    X_scaled = scaler_probe.fit_transform(X)\n",
    "\n",
    "    # Train/test split for probes\n",
    "    n = len(X)\n",
    "    idx = np.random.permutation(n)\n",
    "    train_idx, test_idx = idx[: int(0.7 * n)], idx[int(0.7 * n) :]\n",
    "\n",
    "    # Language probe - check for multiple classes\n",
    "    unique_langs = np.unique(y_lang[test_idx])\n",
    "    if len(unique_langs) < 2:\n",
    "        print(f\"  SKIP language probe - only {len(unique_langs)} class\")\n",
    "        lang_acc = 1.0 / max(1, len(np.unique(y_lang)))\n",
    "        lang_chance = lang_acc\n",
    "    else:\n",
    "        lang_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        lang_probe.fit(X_scaled[train_idx], y_lang[train_idx])\n",
    "        lang_acc = (lang_probe.predict(X_scaled[test_idx]) == y_lang[test_idx]).mean()\n",
    "        lang_chance = 1.0 / len(unique_langs)\n",
    "\n",
    "    # Period probe - same check\n",
    "    unique_periods = np.unique(y_period[test_idx])\n",
    "    if len(unique_periods) < 2:\n",
    "        print(f\"  SKIP period probe - only {len(unique_periods)} class\")\n",
    "        period_acc = 1.0 / max(1, len(np.unique(y_period)))\n",
    "        period_chance = period_acc\n",
    "    else:\n",
    "        period_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        period_probe.fit(X_scaled[train_idx], y_period[train_idx])\n",
    "        period_acc = (period_probe.predict(X_scaled[test_idx]) == y_period[test_idx]).mean()\n",
    "        period_chance = 1.0 / len(unique_periods)\n",
    "\n",
    "    lang_status = \"INVARIANT\" if lang_acc < lang_chance + 0.15 else \"NOT invariant\"\n",
    "    period_status = \"INVARIANT\" if period_acc < period_chance + 0.15 else \"NOT invariant\"\n",
    "\n",
    "    probe_results[split_name] = {\n",
    "        \"language_acc\": lang_acc,\n",
    "        \"language_chance\": lang_chance,\n",
    "        \"language_status\": lang_status,\n",
    "        \"period_acc\": period_acc,\n",
    "        \"period_chance\": period_chance,\n",
    "        \"period_status\": period_status,\n",
    "    }\n",
    "\n",
    "    print(\"\\nRESULTS:\")\n",
    "    print(f\"  Language: {lang_acc:.1%} (chance: {lang_chance:.1%}) -> {lang_status}\")\n",
    "    print(f\"  Period:   {period_acc:.1%} (chance: {period_chance:.1%}) -> {period_status}\")\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Probe tests complete\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===== v10.9: GEOMETRIC ANALYSIS =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GEOMETRIC ANALYSIS (v10.9)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nDiscovering interpretable axes in latent space...\")\n",
    "\n",
    "# Test pairs for axis discovery (cross-lingual)\n",
    "OBLIGATION_PERMISSION_TRAIN = [\n",
    "    # English - training set\n",
    "    (\"You must help the elderly\", \"You may help the elderly\"),\n",
    "    (\"He is required to pay\", \"He is allowed to pay\"),\n",
    "    (\"Parents must protect children\", \"Parents may protect children\"),\n",
    "]\n",
    "\n",
    "OBLIGATION_PERMISSION_TEST = [\n",
    "    # Chinese\n",
    "    (\"\", \"\"),  # Gentleman must/may be filial\n",
    "    (\"\", \"\"),  # People must/may follow law\n",
    "    # Arabic\n",
    "    (\"   \", \"   \"),  # You must/may help\n",
    "    # Hebrew\n",
    "    (\" \", \" \"),  # Obligated/permitted to honor\n",
    "    # English - held out\n",
    "    (\"She must attend\", \"She may attend\"),\n",
    "]\n",
    "\n",
    "HARM_CARE_PAIRS = [\n",
    "    (\"He injured the child\", \"He protected the child\"),\n",
    "    (\"\", \"\"),  # One who kills / one who saves\n",
    "    (\" \", \" \"),  # Oppressed / showed mercy to the weak\n",
    "    (\"She hurt the patient\", \"She healed the patient\"),\n",
    "]\n",
    "\n",
    "ROLE_SWAP_PAIRS = [\n",
    "    (\"The master commands the servant\", \"The servant commands the master\"),\n",
    "    (\"\", \"\"),  # Lord commands minister / minister commands lord\n",
    "    (\"  \", \"  \"),  # Father commands son / son commands father\n",
    "    (\"The parent guides the child\", \"The child guides the parent\"),\n",
    "]\n",
    "\n",
    "geometry_results = {}\n",
    "\n",
    "# Use the best model from mixed_baseline split for geometric analysis\n",
    "model_path = f\"{SAVE_DIR}/best_mixed_baseline.pt\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"\\nLoading model for geometric analysis...\")\n",
    "    model = BIPModel(z_dim=Z_DIM).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    analyzer = GeometricAnalyzer(model, tokenizer, device)\n",
    "\n",
    "    # 1. Find obligation/permission axis\n",
    "    print(\"\\n--- Obligation/Permission Axis ---\")\n",
    "    obl_texts = [p[0] for p in OBLIGATION_PERMISSION_TRAIN]\n",
    "    perm_texts = [p[1] for p in OBLIGATION_PERMISSION_TRAIN]\n",
    "    obl_perm_axis = analyzer.find_direction(obl_texts, perm_texts)\n",
    "\n",
    "    # Test transfer to other languages\n",
    "    transfer_acc = analyzer.test_direction_transfer(obl_perm_axis, OBLIGATION_PERMISSION_TEST)\n",
    "    print(\"  Direction found from English training pairs\")\n",
    "    print(f\"  Transfer accuracy to other languages: {transfer_acc:.1%}\")\n",
    "    axis_status = \"STRONG\" if transfer_acc > 0.8 else \"WEAK\" if transfer_acc > 0.5 else \"FAILED\"\n",
    "    print(f\"  Status: {axis_status} deontic axis\")\n",
    "\n",
    "    geometry_results[\"obligation_permission\"] = {\n",
    "        \"transfer_accuracy\": transfer_acc,\n",
    "        \"status\": axis_status,\n",
    "    }\n",
    "\n",
    "    # 2. Find harm/care axis\n",
    "    print(\"\\n--- Harm/Care Axis ---\")\n",
    "    harm_texts = [p[0] for p in HARM_CARE_PAIRS]\n",
    "    care_texts = [p[1] for p in HARM_CARE_PAIRS]\n",
    "    harm_care_axis = analyzer.find_direction(harm_texts, care_texts)\n",
    "\n",
    "    # Check axis orthogonality\n",
    "    axis_correlation = abs(np.dot(obl_perm_axis, harm_care_axis))\n",
    "    print(\"  Axis found\")\n",
    "    print(f\"  Correlation with obl/perm axis: {axis_correlation:.3f}\")\n",
    "    orthogonal = \"ORTHOGONAL\" if axis_correlation < 0.3 else \"CORRELATED\"\n",
    "    print(f\"  Status: {orthogonal}\")\n",
    "\n",
    "    geometry_results[\"harm_care\"] = {\n",
    "        \"axis_correlation\": axis_correlation,\n",
    "        \"orthogonal\": axis_correlation < 0.3,\n",
    "    }\n",
    "\n",
    "    # 3. Role swap analysis\n",
    "    print(\"\\n--- Role Swap Analysis ---\")\n",
    "    role_analysis = analyzer.role_swap_analysis(ROLE_SWAP_PAIRS)\n",
    "    print(\n",
    "        f\"  Mean consistency: {role_analysis['consistency']:.3f} +/- {role_analysis['consistency_std']:.3f}\"\n",
    "    )\n",
    "    role_status = \"CONSISTENT\" if role_analysis[\"consistency\"] > 0.9 else \"VARIABLE\"\n",
    "    print(f\"  Status: {role_status} agent/patient transformation\")\n",
    "\n",
    "    geometry_results[\"role_swap\"] = {\n",
    "        \"consistency\": role_analysis[\"consistency\"],\n",
    "        \"consistency_std\": role_analysis[\"consistency_std\"],\n",
    "        \"status\": role_status,\n",
    "    }\n",
    "\n",
    "    # 4. PCA on all structural pairs\n",
    "    print(\"\\n--- PCA Analysis ---\")\n",
    "    all_concept_pairs = {\n",
    "        \"obligation_permission\": OBLIGATION_PERMISSION_TRAIN + OBLIGATION_PERMISSION_TEST,\n",
    "        \"harm_care\": HARM_CARE_PAIRS,\n",
    "    }\n",
    "    pca_results = analyzer.pca_on_pairs(all_concept_pairs)\n",
    "\n",
    "    cumsum = np.cumsum(pca_results[\"explained_variance_ratio\"])\n",
    "    n_components_90 = np.argmax(cumsum > 0.9) + 1 if any(cumsum > 0.9) else len(cumsum)\n",
    "\n",
    "    print(f\"  Explained variance ratio: {pca_results['explained_variance_ratio'][:5]}\")\n",
    "    print(f\"  Components for 90% variance: {n_components_90}\")\n",
    "    pca_status = \"LOW-DIM\" if n_components_90 <= 3 else \"HIGH-DIM\"\n",
    "    print(f\"  Status: {pca_status} moral structure\")\n",
    "\n",
    "    geometry_results[\"pca\"] = {\n",
    "        \"explained_variance\": pca_results[\"explained_variance_ratio\"].tolist(),\n",
    "        \"n_components_90pct\": n_components_90,\n",
    "        \"status\": pca_status,\n",
    "    }\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(f\"\\nSkipping geometric analysis - no model at {model_path}\")\n",
    "    geometry_results = {\"error\": \"No model available\"}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Geometric analysis complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell_9",
    "outputId": "8dd8ea6d-5f16-4e61-843a-185d818aceed"
   },
   "outputs": [],
   "source": "# @title 9. Fuzz Testing v10.12: Structural vs Surface Perturbations { display-mode: \"form\" }\n# @markdown Tests whether structural perturbations move embeddings more than surface perturbations.\n# @markdown **Run immediately after Cell 6/7 training completes - uses model in memory.**\n# @markdown\n# @markdown v10.12 enhancements:\n# @markdown - **30+ samples per category** for 6-sigma statistical confidence\n# @markdown - **Runtime-adaptive thresholds** based on GPU type (L4/A100/T4)\n# @markdown - **Extended bond type coverage** including cross-cultural scenarios\n# @markdown - **Bootstrap confidence intervals** for robust statistics\n\n# @markdown ---\n# @markdown ## Enable Fuzz Testing\nRUN_FUZZ_TEST = True  # @param {type:\"boolean\"}\n\nimport random\nimport warnings\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n# ============================================================================\n# RUNTIME DETECTION AND ADAPTIVE THRESHOLDS\n# ============================================================================\n\n\ndef detect_runtime() -> dict:\n    \"\"\"Detect GPU type and set appropriate thresholds.\"\"\"\n    runtime_config = {\n        \"gpu_type\": \"unknown\",\n        \"vram_gb\": 0,\n        \"batch_size\": 16,\n        \"max_scenarios\": 50,\n        \"bootstrap_samples\": 1000,\n    }\n\n    if torch.cuda.is_available():\n        gpu_name = torch.cuda.get_device_name(0).lower()\n        vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n        runtime_config[\"vram_gb\"] = vram\n\n        if \"a100\" in gpu_name:\n            runtime_config[\"gpu_type\"] = \"A100\"\n            runtime_config[\"batch_size\"] = 64\n            runtime_config[\"max_scenarios\"] = 100\n            runtime_config[\"bootstrap_samples\"] = 5000\n        elif \"l4\" in gpu_name:\n            runtime_config[\"gpu_type\"] = \"L4\"\n            runtime_config[\"batch_size\"] = 32\n            runtime_config[\"max_scenarios\"] = 75\n            runtime_config[\"bootstrap_samples\"] = 2000\n        elif \"t4\" in gpu_name:\n            runtime_config[\"gpu_type\"] = \"T4\"\n            runtime_config[\"batch_size\"] = 16\n            runtime_config[\"max_scenarios\"] = 50\n            runtime_config[\"bootstrap_samples\"] = 1000\n        elif \"v100\" in gpu_name:\n            runtime_config[\"gpu_type\"] = \"V100\"\n            runtime_config[\"batch_size\"] = 32\n            runtime_config[\"max_scenarios\"] = 60\n            runtime_config[\"bootstrap_samples\"] = 2000\n        else:\n            # Default based on VRAM\n            if vram >= 40:\n                runtime_config[\"gpu_type\"] = \"high_vram\"\n                runtime_config[\"batch_size\"] = 64\n                runtime_config[\"max_scenarios\"] = 100\n            elif vram >= 20:\n                runtime_config[\"gpu_type\"] = \"medium_vram\"\n                runtime_config[\"batch_size\"] = 32\n                runtime_config[\"max_scenarios\"] = 75\n            else:\n                runtime_config[\"gpu_type\"] = \"low_vram\"\n                runtime_config[\"batch_size\"] = 16\n                runtime_config[\"max_scenarios\"] = 40\n\n    return runtime_config\n\n\n# Ensure directories exist\nimport os\n\nos.makedirs(\"models/checkpoints\", exist_ok=True)\nos.makedirs(\"models\", exist_ok=True)\n\nif not RUN_FUZZ_TEST:\n    print(\"Fuzz testing disabled. Check RUN_FUZZ_TEST to enable.\")\nelse:\n    print(\"=\" * 70)\n    print(\"FUZZ TESTING v10.12: STRUCTURAL VS SURFACE PERTURBATIONS\")\n    print(\"=\" * 70)\n    print()\n\n    # Detect runtime and set thresholds\n    RUNTIME = detect_runtime()\n    print(f\"Runtime detected: {RUNTIME['gpu_type']} ({RUNTIME['vram_gb']:.1f} GB VRAM)\")\n    print(f\"Batch size: {RUNTIME['batch_size']}, Max scenarios: {RUNTIME['max_scenarios']}\")\n    print(f\"Bootstrap samples: {RUNTIME['bootstrap_samples']}\")\n    print()\n\n    # ========================================================================\n    # USE EXISTING MODEL FROM TRAINING SESSION\n    # ========================================================================\n\n    # ========================================================================\n    # MODEL LOADING WITH CHECKPOINT FALLBACK\n    # ========================================================================\n\n    _fuzz_model = None\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # First try: Use model from memory\n    try:\n        if \"model\" in dir() and model is not None:\n            if hasattr(model, \"module\"):\n                _fuzz_model = model.module\n                print(\"Using unwrapped model from Accelerator\")\n            else:\n                _fuzz_model = model\n                print(\"Using model from training session\")\n    except NameError:\n        pass\n\n    # Second try: Load from checkpoint\n    if _fuzz_model is None:\n        import glob\n        import os\n\n        # Look for checkpoint files\n        checkpoint_patterns = [\n            f\"{SAVE_DIR}/best_*.pt\",  # v10.15.1: Check Drive first\n            \"models/checkpoints/best_*.pt\",\n            \"models/best_*.pt\",\n            \"*.pt\",\n        ]\n\n        checkpoint_path = None\n        for pattern in checkpoint_patterns:\n            matches = glob.glob(pattern)\n            if matches:\n                # Use most recent\n                checkpoint_path = max(matches, key=os.path.getmtime)\n                break\n\n        if checkpoint_path and os.path.exists(checkpoint_path):\n            print(f\"Loading model from checkpoint: {checkpoint_path}\")\n\n            # Need to create model first\n            try:\n                # Try to use existing tokenizer\n                if \"tokenizer\" not in dir():\n                    from transformers import AutoTokenizer\n\n                    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n                # Create model architecture\n                _fuzz_model = BIPModel(z_dim=Z_DIM)\n                _fuzz_model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n                _fuzz_model.to(device)\n                print(f\"Model loaded successfully from {checkpoint_path}\")\n\n            except Exception as e:\n                print(f\"Error loading checkpoint: {e}\")\n                _fuzz_model = None\n        else:\n            print(\"No checkpoint files found in:\")\n            for pattern in checkpoint_patterns:\n                print(f\"  - {pattern}\")\n\n    # Final check\n    if _fuzz_model is None:\n        print()\n        print(\"ERROR: No model found in memory and no checkpoint available!\")\n        print(\"Please run training (Cell 7) first.\")\n        print()\n        print(\"Expected checkpoint locations:\")\n        print(\"  models/checkpoints/best_mixed_baseline.pt\")\n        print(\"  models/checkpoints/best_ancient_to_modern.pt\")\n        print(\"  etc.\")\n        RUN_FUZZ_TEST = False\n    else:\n        _fuzz_model.eval()\n        try:\n            device = next(_fuzz_model.parameters()).device\n        except StopIteration:\n            pass\n        print(f\"Device: {device}\")\n\n    if RUN_FUZZ_TEST:\n        print()\n\n        # ====================================================================\n        # EMBEDDING FUNCTIONS\n        # ====================================================================\n\n        @torch.no_grad()\n        def get_embedding(text: str) -> np.ndarray:\n            inputs = tokenizer(\n                text, return_tensors=\"pt\", truncation=True, max_length=128, padding=\"max_length\"\n            )\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            z = _fuzz_model.get_bond_embedding(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n            return z.cpu().numpy().flatten()\n\n        @torch.no_grad()\n        def get_embeddings_batch(texts: list[str]) -> np.ndarray:\n            \"\"\"Batch embedding for efficiency.\"\"\"\n            all_embeddings = []\n            batch_size = RUNTIME[\"batch_size\"]\n\n            for i in range(0, len(texts), batch_size):\n                batch = texts[i : i + batch_size]\n                inputs = tokenizer(\n                    batch,\n                    return_tensors=\"pt\",\n                    truncation=True,\n                    max_length=128,\n                    padding=\"max_length\",\n                )\n                inputs = {k: v.to(device) for k, v in inputs.items()}\n                z = _fuzz_model.get_bond_embedding(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n                all_embeddings.append(z.cpu().numpy())\n\n            return np.vstack(all_embeddings)\n\n        def cosine_distance(v1: np.ndarray, v2: np.ndarray) -> float:\n            sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-9)\n            return 1 - sim\n\n        # ====================================================================\n        # EXPANDED BASE SCENARIOS (30+ for statistical power)\n        # ====================================================================\n\n        BASE_SCENARIOS = [\n            # OBLIGATION / DUTY (8 scenarios)\n            {\n                \"text\": \"John borrowed money from Mary and promised to repay it by Friday.\",\n                \"bond_type\": \"OBLIGATION\",\n                \"category\": \"promise\",\n            },\n            {\n                \"text\": \"The doctor has a duty to keep patient information confidential.\",\n                \"bond_type\": \"DUTY\",\n                \"category\": \"professional\",\n            },\n            {\n                \"text\": \"Parents must protect their children from harm.\",\n                \"bond_type\": \"DUTY\",\n                \"category\": \"familial\",\n            },\n            {\n                \"text\": \"The teacher promised to grade all exams by Monday.\",\n                \"bond_type\": \"OBLIGATION\",\n                \"category\": \"promise\",\n            },\n            {\n                \"text\": \"Soldiers are required to follow orders from superior officers.\",\n                \"bond_type\": \"DUTY\",\n                \"category\": \"institutional\",\n            },\n            {\n                \"text\": \"The witness swore to tell the truth in court.\",\n                \"bond_type\": \"OBLIGATION\",\n                \"category\": \"oath\",\n            },\n            {\n                \"text\": \"Citizens must pay their taxes to the government.\",\n                \"bond_type\": \"DUTY\",\n                \"category\": \"civic\",\n            },\n            {\n                \"text\": \"The contractor agreed to complete the building within six months.\",\n                \"bond_type\": \"OBLIGATION\",\n                \"category\": \"contract\",\n            },\n            # CARE / HELP (8 scenarios)\n            {\n                \"text\": \"Sarah helped her neighbor carry groceries, expecting nothing in return.\",\n                \"bond_type\": \"CARE\",\n                \"category\": \"altruism\",\n            },\n            {\n                \"text\": \"The nurse stayed late to comfort the dying patient.\",\n                \"bond_type\": \"CARE\",\n                \"category\": \"compassion\",\n            },\n            {\n                \"text\": \"She donated her savings to help earthquake victims.\",\n                \"bond_type\": \"CARE\",\n                \"category\": \"charity\",\n            },\n            {\n                \"text\": \"The mentor guided the young artist without asking for payment.\",\n                \"bond_type\": \"CARE\",\n                \"category\": \"guidance\",\n            },\n            {\n                \"text\": \"He gave his coat to the homeless man shivering in the cold.\",\n                \"bond_type\": \"CARE\",\n                \"category\": \"generosity\",\n            },\n            {\n                \"text\": \"The stranger stopped to help change the flat tire.\",\n                \"bond_type\": \"CARE\",\n                \"category\": \"assistance\",\n            },\n            {\n                \"text\": \"She listened patiently as he shared his troubles.\",\n                \"bond_type\": \"CARE\",\n                \"category\": \"empathy\",\n            },\n            {\n                \"text\": \"The community gathered to rebuild the family's burned house.\",\n                \"bond_type\": \"CARE\",\n                \"category\": \"solidarity\",\n            },\n            # HARM / VIOLATION (8 scenarios)\n            {\n                \"text\": \"He stole the wallet from the elderly woman.\",\n                \"bond_type\": \"HARM\",\n                \"category\": \"theft\",\n            },\n            {\n                \"text\": \"The company violated the contract by delivering late.\",\n                \"bond_type\": \"VIOLATION\",\n                \"category\": \"breach\",\n            },\n            {\n                \"text\": \"She spread false rumors to destroy his reputation.\",\n                \"bond_type\": \"HARM\",\n                \"category\": \"slander\",\n            },\n            {\n                \"text\": \"The politician broke his campaign promises after election.\",\n                \"bond_type\": \"VIOLATION\",\n                \"category\": \"betrayal\",\n            },\n            {\n                \"text\": \"He poisoned the well that the village depended on.\",\n                \"bond_type\": \"HARM\",\n                \"category\": \"sabotage\",\n            },\n            {\n                \"text\": \"The trustee embezzled funds from the charity.\",\n                \"bond_type\": \"VIOLATION\",\n                \"category\": \"fraud\",\n            },\n            {\n                \"text\": \"She abandoned her children to pursue her own interests.\",\n                \"bond_type\": \"VIOLATION\",\n                \"category\": \"abandonment\",\n            },\n            {\n                \"text\": \"The invaders destroyed the sacred temple.\",\n                \"bond_type\": \"HARM\",\n                \"category\": \"desecration\",\n            },\n            # FAIRNESS / JUSTICE (8 scenarios)\n            {\n                \"text\": \"The judge ruled fairly, giving each side equal consideration.\",\n                \"bond_type\": \"FAIRNESS\",\n                \"category\": \"impartiality\",\n            },\n            {\n                \"text\": \"She forgave him for breaking his promise.\",\n                \"bond_type\": \"FORGIVENESS\",\n                \"category\": \"mercy\",\n            },\n            {\n                \"text\": \"The council distributed resources equally among all villages.\",\n                \"bond_type\": \"FAIRNESS\",\n                \"category\": \"equity\",\n            },\n            {\n                \"text\": \"He returned the extra change the shopkeeper gave by mistake.\",\n                \"bond_type\": \"FAIRNESS\",\n                \"category\": \"honesty\",\n            },\n            {\n                \"text\": \"The elder mediated the dispute without favoring either party.\",\n                \"bond_type\": \"FAIRNESS\",\n                \"category\": \"mediation\",\n            },\n            {\n                \"text\": \"She gave credit to her assistant for the discovery.\",\n                \"bond_type\": \"FAIRNESS\",\n                \"category\": \"attribution\",\n            },\n            {\n                \"text\": \"The king pardoned the rebels who surrendered peacefully.\",\n                \"bond_type\": \"FORGIVENESS\",\n                \"category\": \"clemency\",\n            },\n            {\n                \"text\": \"They compensated the wrongly accused man for his suffering.\",\n                \"bond_type\": \"FAIRNESS\",\n                \"category\": \"restitution\",\n            },\n            # CROSS-CULTURAL BOND TYPES (8 scenarios)\n            {\n                \"text\": \"The student honored his teacher by caring for him in old age.\",\n                \"bond_type\": \"PIETY\",\n                \"category\": \"filial\",\n            },\n            {\n                \"text\": \"She upheld the family honor by keeping her grandfather's promise.\",\n                \"bond_type\": \"LOYALTY\",\n                \"category\": \"ancestral\",\n            },\n            {\n                \"text\": \"The warrior spared his defeated enemy as custom demanded.\",\n                \"bond_type\": \"HONOR\",\n                \"category\": \"chivalry\",\n            },\n            {\n                \"text\": \"He returned the sacred artifact to the temple it was taken from.\",\n                \"bond_type\": \"REVERENCE\",\n                \"category\": \"restoration\",\n            },\n            {\n                \"text\": \"The host provided shelter to the stranger as hospitality required.\",\n                \"bond_type\": \"HOSPITALITY\",\n                \"category\": \"xenia\",\n            },\n            {\n                \"text\": \"She maintained ritual purity to preserve cosmic order.\",\n                \"bond_type\": \"PURITY\",\n                \"category\": \"ritual\",\n            },\n            {\n                \"text\": \"The merchant kept his word even when it meant financial loss.\",\n                \"bond_type\": \"INTEGRITY\",\n                \"category\": \"commercial\",\n            },\n            {\n                \"text\": \"The community shunned him for violating the ancestral taboo.\",\n                \"bond_type\": \"TABOO\",\n                \"category\": \"prohibition\",\n            },\n        ]\n\n        # ====================================================================\n        # PERTURBATION GENERATORS\n        # ====================================================================\n\n        # Name substitution pools for variety\n        NAME_POOLS = {\n            \"male\": [\"John\", \"Michael\", \"David\", \"James\", \"Robert\", \"William\", \"Thomas\", \"Daniel\"],\n            \"female\": [\"Mary\", \"Sarah\", \"Emma\", \"Lisa\", \"Anna\", \"Rachel\", \"Rebecca\", \"Hannah\"],\n        }\n\n        IRRELEVANT_DETAILS = [\n            \" It was Tuesday.\",\n            \" The room was blue.\",\n            \" Last summer.\",\n            \" The weather was pleasant.\",\n            \" It happened at noon.\",\n            \" The year was uncertain.\",\n            \" Birds sang nearby.\",\n            \" The moon was full.\",\n            \" Rain had fallen earlier.\",\n            \" The road was dusty.\",\n            \" Flowers bloomed outside.\",\n        ]\n\n        SYNONYMS = {\n            \"money\": [\"cash\", \"funds\", \"currency\"],\n            \"groceries\": [\"bags\", \"supplies\", \"provisions\"],\n            \"house\": [\"home\", \"dwelling\", \"residence\"],\n            \"promise\": [\"vow\", \"pledge\", \"commitment\"],\n            \"help\": [\"assist\", \"aid\", \"support\"],\n            \"truth\": [\"facts\", \"reality\", \"honesty\"],\n        }\n\n        def surface_perturbations(scenario: dict) -> list[dict]:\n            \"\"\"Generate surface perturbations that shouldn't change moral meaning.\"\"\"\n            text = scenario[\"text\"]\n            perturbs = []\n\n            # Name changes (multiple variations)\n            for old_name in NAME_POOLS[\"male\"] + NAME_POOLS[\"female\"]:\n                if old_name in text:\n                    for new_name in (\n                        NAME_POOLS[\"male\"]\n                        if old_name in NAME_POOLS[\"male\"]\n                        else NAME_POOLS[\"female\"]\n                    ):\n                        if new_name != old_name:\n                            new_text = text.replace(old_name, new_name)\n                            if new_text != text:\n                                perturbs.append(\n                                    {\n                                        \"text\": new_text,\n                                        \"type\": \"name_change\",\n                                        \"original\": old_name,\n                                        \"new\": new_name,\n                                    }\n                                )\n                                if len(perturbs) >= 3:  # Limit per scenario\n                                    break\n\n            # Irrelevant detail additions\n            # v10.16.3: Limit to 1 random detail per scenario for balanced comparison\n            # (was 4, causing 160 samples vs 36 structural - skewing results)\n            detail = random.choice(IRRELEVANT_DETAILS[:4])\n            perturbs.append(\n                {\"text\": text + detail, \"type\": \"irrelevant_detail\", \"detail\": detail}\n            )\n\n            # Synonym substitutions\n            new_text = text\n            for word, synonyms in SYNONYMS.items():\n                if word in new_text.lower():\n                    for syn in synonyms[:2]:\n                        test_text = new_text.replace(word, syn)\n                        if test_text != new_text:\n                            perturbs.append(\n                                {\"text\": test_text, \"type\": \"synonym\", \"original\": word, \"new\": syn}\n                            )\n                            break\n\n            return perturbs\n\n        def structural_perturbations(scenario: dict) -> list[dict]:\n            \"\"\"Generate structural perturbations that SHOULD change moral meaning.\"\"\"\n            text = scenario[\"text\"]\n            perturbs = []\n\n            # Role swaps (agent/patient reversal)\n            role_swaps = [\n                (\"John borrowed money from Mary\", \"Mary borrowed money from John\"),\n                (\n                    \"He stole the wallet from the elderly woman\",\n                    \"The elderly woman stole the wallet from him\",\n                ),\n                (\n                    \"She spread false rumors to destroy his reputation\",\n                    \"He spread false rumors to destroy her reputation\",\n                ),\n                (\"Sarah helped her neighbor\", \"Her neighbor helped Sarah\"),\n                (\"The teacher promised to grade\", \"The students promised to grade\"),\n                (\"He gave his coat to the homeless man\", \"The homeless man gave his coat to him\"),\n                (\"She donated her savings to help\", \"They donated their savings to help her\"),\n                (\n                    \"The host provided shelter to the stranger\",\n                    \"The stranger provided shelter to the host\",\n                ),\n            ]\n            for orig, swap in role_swaps:\n                if orig in text:\n                    perturbs.append(\n                        {\n                            \"text\": text.replace(orig, swap),\n                            \"type\": \"role_swap\",\n                            \"swap\": (orig, swap),\n                        }\n                    )\n\n            # Obligation to permission\n            obl_to_perm = [\n                (\"must protect\", \"may protect\"),\n                (\"has a duty to\", \"is allowed to\"),\n                (\"are required to\", \"are permitted to\"),\n                (\"swore to\", \"considered whether to\"),\n                (\"must pay\", \"may pay\"),\n                (\"agreed to\", \"considered whether to\"),\n            ]\n            for obl, perm in obl_to_perm:\n                if obl in text:\n                    perturbs.append(\n                        {\n                            \"text\": text.replace(obl, perm),\n                            \"type\": \"obligation_to_permission\",\n                            \"change\": (obl, perm),\n                        }\n                    )\n\n            # Positive to negative (harm introduction)\n            pos_to_neg = [\n                (\"helped\", \"refused to help\"),\n                (\"ruled fairly\", \"ruled unfairly\"),\n                (\"forgave\", \"refused to forgive\"),\n                (\"stayed late to comfort\", \"left early despite\"),\n                (\"donated\", \"hoarded\"),\n                (\"guided\", \"misled\"),\n                (\"gave\", \"took\"),\n                (\"stopped to help\", \"drove past without helping\"),\n                (\"listened patiently\", \"ignored\"),\n                (\"gathered to rebuild\", \"refused to rebuild\"),\n            ]\n            for pos, neg in pos_to_neg:\n                if pos in text:\n                    perturbs.append(\n                        {\"text\": text.replace(pos, neg), \"type\": \"add_harm\", \"change\": (pos, neg)}\n                    )\n\n            # Violation to fulfillment\n            viol_to_fulf = [\n                (\"violated\", \"honored\"),\n                (\"stole\", \"returned\"),\n                (\"breaking\", \"keeping\"),\n                (\"spread false rumors\", \"defended his reputation\"),\n                (\"broke his campaign promises\", \"kept his campaign promises\"),\n                (\"poisoned\", \"purified\"),\n                (\"embezzled\", \"safeguarded\"),\n                (\"abandoned\", \"cared for\"),\n                (\"destroyed\", \"preserved\"),\n            ]\n            for viol, fulf in viol_to_fulf:\n                if viol in text:\n                    perturbs.append(\n                        {\n                            \"text\": text.replace(viol, fulf),\n                            \"type\": \"violation_to_fulfillment\",\n                            \"change\": (viol, fulf),\n                        }\n                    )\n\n            return perturbs\n\n        # ====================================================================\n        # STATISTICAL ANALYSIS FUNCTIONS\n        # ====================================================================\n\n        def bootstrap_ci(\n            data: np.ndarray, n_bootstrap: int = 1000, confidence: float = 0.95\n        ) -> tuple[float, float, float]:\n            \"\"\"Calculate bootstrap confidence interval.\"\"\"\n            n = len(data)\n            if n < 2:\n                return data.mean(), data.mean(), data.mean()\n\n            boot_means = []\n            for _ in range(n_bootstrap):\n                sample = np.random.choice(data, size=n, replace=True)\n                boot_means.append(sample.mean())\n\n            boot_means = np.array(boot_means)\n            alpha = (1 - confidence) / 2\n            lower = np.percentile(boot_means, alpha * 100)\n            upper = np.percentile(boot_means, (1 - alpha) * 100)\n\n            return lower, data.mean(), upper\n\n        def effect_size_cohens_d(group1: np.ndarray, group2: np.ndarray) -> float:\n            \"\"\"Calculate Cohen's d effect size.\"\"\"\n            n1, n2 = len(group1), len(group2)\n            var1, var2 = group1.var(), group2.var()\n            pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n            return (group1.mean() - group2.mean()) / (pooled_std + 1e-9)\n\n        # ====================================================================\n        # RUN TESTS\n        # ====================================================================\n\n        print(\"=\" * 70)\n        print(\"RUNNING FUZZ TESTS\")\n        print(\"=\" * 70)\n        print()\n\n        # Organize results by perturbation type\n        results_by_type = {\n            \"structural_obligation_to_permission\": [],\n            \"structural_add_harm\": [],\n            \"structural_role_swap\": [],\n            \"structural_violation_to_fulfillment\": [],\n            \"surface_name_change\": [],\n            \"surface_irrelevant_detail\": [],\n            \"surface_synonym\": [],\n        }\n\n        all_surface_distances = []\n        all_structural_distances = []\n\n        scenarios_to_run = BASE_SCENARIOS[: RUNTIME[\"max_scenarios\"]]\n        print(f\"Processing {len(scenarios_to_run)} scenarios...\")\n        print()\n\n        for i, scenario in enumerate(scenarios_to_run):\n            base_emb = get_embedding(scenario[\"text\"])\n\n            # Process surface perturbations\n            surface_perturbs = surface_perturbations(scenario)\n            for p in surface_perturbs:\n                dist = cosine_distance(base_emb, get_embedding(p[\"text\"]))\n                all_surface_distances.append(dist)\n                key = f\"surface_{p['type']}\"\n                if key in results_by_type:\n                    results_by_type[key].append(dist)\n\n            # Process structural perturbations\n            structural_perturbs = structural_perturbations(scenario)\n            for p in structural_perturbs:\n                dist = cosine_distance(base_emb, get_embedding(p[\"text\"]))\n                all_structural_distances.append(dist)\n                key = f\"structural_{p['type']}\"\n                if key in results_by_type:\n                    results_by_type[key].append(dist)\n\n            if (i + 1) % 10 == 0:\n                print(f\"  Processed {i + 1}/{len(scenarios_to_run)} scenarios...\")\n\n        print()\n        print(f\"Total surface perturbations: {len(all_surface_distances)}\")\n        print(f\"Total structural perturbations: {len(all_structural_distances)}\")\n        print()\n\n        # ====================================================================\n        # DETAILED RESULTS\n        # ====================================================================\n\n        print(\"=\" * 70)\n        print(\"RESULTS BY PERTURBATION TYPE\")\n        print(\"=\" * 70)\n        print()\n\n        fuzz_results = {}\n\n        for ptype, distances in results_by_type.items():\n            if len(distances) > 0:\n                distances = np.array(distances)\n                lower, mean, upper = bootstrap_ci(distances, RUNTIME[\"bootstrap_samples\"])\n                fuzz_results[ptype] = {\n                    \"mean_distance\": str(mean),\n                    \"std\": str(distances.std()),\n                    \"ci_lower\": str(lower),\n                    \"ci_upper\": str(upper),\n                    \"n\": len(distances),\n                }\n                category = \"STRUCTURAL\" if \"structural\" in ptype else \"SURFACE\"\n                print(f\"{ptype}:\")\n                print(f\"  n={len(distances)}, mean={mean:.4f}, std={distances.std():.4f}\")\n                print(f\"  95% CI: [{lower:.4f}, {upper:.4f}]\")\n                print()\n\n        # ====================================================================\n        # AGGREGATE COMPARISON\n        # ====================================================================\n\n        print(\"=\" * 70)\n        print(\"AGGREGATE COMPARISON\")\n        print(\"=\" * 70)\n        print()\n\n        surface_arr = np.array(all_surface_distances)\n        structural_arr = np.array(all_structural_distances)\n\n        surf_lower, surf_mean, surf_upper = bootstrap_ci(surface_arr, RUNTIME[\"bootstrap_samples\"])\n        struct_lower, struct_mean, struct_upper = bootstrap_ci(\n            structural_arr, RUNTIME[\"bootstrap_samples\"]\n        )\n\n        print(\"Surface (should be SMALL):\")\n        print(f\"  mean={surf_mean:.4f}, std={surface_arr.std():.4f}\")\n        print(f\"  95% CI: [{surf_lower:.4f}, {surf_upper:.4f}]\")\n        print()\n        print(\"Structural (should be LARGE):\")\n        print(f\"  mean={struct_mean:.4f}, std={structural_arr.std():.4f}\")\n        print(f\"  95% CI: [{struct_lower:.4f}, {struct_upper:.4f}]\")\n        print()\n\n        # Statistical tests\n        from scipy import stats\n\n        t_stat, p_value = stats.ttest_ind(structural_arr, surface_arr)\n\n        # Mann-Whitney U for non-parametric comparison\n        u_stat, u_pvalue = stats.mannwhitneyu(structural_arr, surface_arr, alternative=\"greater\")\n\n        ratio = struct_mean / (surf_mean + 1e-9)\n        cohens_d = effect_size_cohens_d(structural_arr, surface_arr)\n\n        print(f\"Ratio (structural/surface): {ratio:.2f}x\")\n        print(f\"Cohen's d effect size: {cohens_d:.3f}\")\n        print(f\"t-statistic: {t_stat:.4f}, p-value: {p_value:.6f}\")\n        print(f\"Mann-Whitney U: {u_stat:.0f}, p-value: {u_pvalue:.6f}\")\n        print()\n\n        # Store comparison results\n        fuzz_results[\"comparison\"] = {\n            \"structural_mean\": str(struct_mean),\n            \"structural_ci\": [str(struct_lower), str(struct_upper)],\n            \"surface_mean\": str(surf_mean),\n            \"surface_ci\": [str(surf_lower), str(surf_upper)],\n            \"ratio\": str(ratio),\n            \"cohens_d\": str(cohens_d),\n            \"t_statistic\": t_stat,\n            \"p_value\": p_value,\n            \"mann_whitney_u\": float(u_stat),\n            \"mann_whitney_p\": float(u_pvalue),\n            \"n_structural\": len(structural_arr),\n            \"n_surface\": len(surface_arr),\n        }\n\n        # ====================================================================\n        # VERDICT (RUNTIME-ADAPTIVE THRESHOLDS)\n        # ====================================================================\n\n        print(\"=\" * 70)\n        print(\"VERDICT\")\n        print(\"=\" * 70)\n        print()\n\n        # Adaptive thresholds based on sample size and runtime\n        if len(structural_arr) >= 30 and len(surface_arr) >= 30:\n            # Strong statistical power - use stricter thresholds\n            strong_ratio = 3.0\n            moderate_ratio = 2.0\n            weak_ratio = 1.5\n            p_threshold = 0.01\n        elif len(structural_arr) >= 15:\n            # Medium statistical power\n            strong_ratio = 2.5\n            moderate_ratio = 1.8\n            weak_ratio = 1.3\n            p_threshold = 0.05\n        else:\n            # Low statistical power - use looser thresholds but note uncertainty\n            strong_ratio = 2.0\n            moderate_ratio = 1.5\n            weak_ratio = 1.2\n            p_threshold = 0.10\n\n        verdict = \"NOT_SUPPORTED\"\n        verdict_detail = \"\"\n\n        if ratio >= strong_ratio and p_value < p_threshold and cohens_d > 0.8:\n            verdict = \"STRONG_SUPPORT\"\n            verdict_detail = f\"Model learned moral structure (ratio={ratio:.1f}x, d={cohens_d:.2f}, p={p_value:.4f})\"\n        elif ratio >= moderate_ratio and p_value < 0.05 and cohens_d > 0.5:\n            verdict = \"MODERATE_SUPPORT\"\n            verdict_detail = f\"Evidence for moral structure (ratio={ratio:.1f}x, d={cohens_d:.2f})\"\n        elif ratio >= weak_ratio and p_value < 0.10:\n            verdict = \"WEAK_SUPPORT\"\n            verdict_detail = f\"Weak evidence (ratio={ratio:.1f}x, needs more data)\"\n        else:\n            verdict = \"NOT_SUPPORTED\"\n            verdict_detail = \"May be encoding surface features rather than moral structure\"\n\n        print(f\"Verdict: {verdict}\")\n        print(f\"Detail: {verdict_detail}\")\n        print()\n        print(f\"Runtime: {RUNTIME['gpu_type']}\")\n        print(\n            f\"Thresholds used: strong>{strong_ratio}x, moderate>{moderate_ratio}x, p<{p_threshold}\"\n        )\n        print(\"=\" * 70)\n\n        fuzz_results[\"verdict\"] = verdict\n        fuzz_results[\"verdict_detail\"] = verdict_detail\n        fuzz_results[\"runtime\"] = RUNTIME\n\n        # Make results available for integration\n        FUZZ_RESULTS_V1011 = fuzz_results\n\n\n# ========================================================================\n# v10.14.3: CROSS-LINGUAL FUZZ TESTING\n# ========================================================================\nCROSS_LINGUAL_TEST_PAIRS = [\n    (\n        \"English\",\n        \"Hebrew\",\n        \"Promise keeping\",\n        \"A person promised to return a borrowed book and must fulfill that promise.\",\n        \"        .\",\n    ),\n    (\n        \"English\",\n        \"Arabic\",\n        \"Duty to help\",\n        \"One has a duty to help those in distress when able to do so.\",\n        \"         .\",\n    ),\n    (\n        \"English\",\n        \"Chinese\",\n        \"Filial obligation\",\n        \"Children have an obligation to care for their elderly parents.\",\n        \"\",\n    ),\n]\n\n\ndef run_cross_lingual_fuzz_test(model, tokenizer, device):\n    \"\"\"Test cross-lingual invariance of z_bond embeddings.\"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"CROSS-LINGUAL FUZZ TEST (v10.14.3)\")\n    print(\"=\" * 70)\n\n    model.eval()\n    results = []\n\n    with torch.no_grad():\n        for lang1, lang2, desc, text1, text2 in CROSS_LINGUAL_TEST_PAIRS:\n            enc1 = tokenizer(\n                text1, return_tensors=\"pt\", padding=True, truncation=True, max_length=128\n            )\n            enc2 = tokenizer(\n                text2, return_tensors=\"pt\", padding=True, truncation=True, max_length=128\n            )\n            enc1 = {k: v.to(device) for k, v in enc1.items()}\n            enc2 = {k: v.to(device) for k, v in enc2.items()}\n\n            out1 = model(enc1[\"input_ids\"], enc1.get(\"attention_mask\"))\n            out2 = model(enc2[\"input_ids\"], enc2.get(\"attention_mask\"))\n\n            cos_sim = F.cosine_similarity(out1[\"z\"], out2[\"z\"], dim=-1).item()\n            results.append({\"langs\": f\"{lang1}-{lang2}\", \"cos_sim\": cos_sim, \"desc\": desc})\n            print(f\"  {lang1:8s} <-> {lang2:8s} | cos_sim={cos_sim:+.4f} | {desc}\")\n\n    avg_sim = sum(r[\"cos_sim\"] for r in results) / len(results)\n    print(f\"\\nAverage cross-lingual similarity: {avg_sim:+.4f}\")\n    if avg_sim > 0.7:\n        print(\"   Good cross-lingual invariance\")\n    else:\n        print(\"   Poor cross-lingual invariance\")\n    return {\"results\": results, \"avg_similarity\": avg_sim}\n\n\n# Run if model available\nif RUN_FUZZ_TEST and _fuzz_model is not None:\n    cross_lingual_results = run_cross_lingual_fuzz_test(_fuzz_model, tokenizer, device)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cell_10",
    "outputId": "348f9322-cb8a-47c4-a400-1946ee33e6e5"
   },
   "outputs": [],
   "source": [
    "# @title 10. Save & Download Results { display-mode: \"form\" }\n",
    "# @markdown Persist results to Google Drive and optionally download as zip\n",
    "\n",
    "import shutil\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Always persist results to Drive\n",
    "if SAVE_DIR and os.path.exists(SAVE_DIR):\n",
    "    print(f\"\\nPersisting to: {SAVE_DIR}\")\n",
    "\n",
    "    # Save final results JSON\n",
    "    if os.path.exists(\"results/final_results.json\"):\n",
    "        dest = f\"{SAVE_DIR}/final_results.json\"\n",
    "        shutil.copy(\"results/final_results.json\", dest)\n",
    "        print(\"  Saved: final_results.json\")\n",
    "\n",
    "    # Save splits config\n",
    "    if os.path.exists(\"data/splits/all_splits.json\"):\n",
    "        dest = f\"{SAVE_DIR}/all_splits.json\"\n",
    "        shutil.copy(\"data/splits/all_splits.json\", dest)\n",
    "        print(\"  Saved: all_splits.json\")\n",
    "\n",
    "    # Models are already saved to SAVE_DIR during training\n",
    "    model_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(\".pt\")]\n",
    "    if model_files:\n",
    "        print(f\"  Models already in Drive: {len(model_files)} files\")\n",
    "        for mf in model_files[:5]:\n",
    "            print(f\"    - {mf}\")\n",
    "        if len(model_files) > 5:\n",
    "            print(f\"    ... and {len(model_files) - 5} more\")\n",
    "\n",
    "    print(f\"\\nResults persisted to Google Drive: {SAVE_DIR}\")\n",
    "else:\n",
    "    print(\"WARNING: SAVE_DIR not available, results only in local directories\")\n",
    "\n",
    "# Optional: Create download zip\n",
    "if CREATE_DOWNLOAD_ZIP:\n",
    "    import zipfile\n",
    "\n",
    "    zip_path = f\"BIP_v{BIP_VERSION}_results.zip\"\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"Creating download package...\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "        # Results\n",
    "        if os.path.exists(\"results/final_results.json\"):\n",
    "            zf.write(\"results/final_results.json\")\n",
    "\n",
    "        # Models (from Drive)\n",
    "        if SAVE_DIR and os.path.exists(SAVE_DIR):\n",
    "            for f in os.listdir(SAVE_DIR):\n",
    "                if f.endswith(\".pt\"):\n",
    "                    zf.write(f\"{SAVE_DIR}/{f}\", f\"models/{f}\")\n",
    "\n",
    "        # Config\n",
    "        if os.path.exists(\"data/splits/all_splits.json\"):\n",
    "            zf.write(\"data/splits/all_splits.json\")\n",
    "\n",
    "    print(f\"Download package ready: {zip_path}\")\n",
    "\n",
    "    # Download in Colab, or show path otherwise\n",
    "    try:\n",
    "        from google.colab import files\n",
    "\n",
    "        files.download(zip_path)\n",
    "    except ImportError:\n",
    "        print(f\"Not running in Colab. Zip saved to: {os.path.abspath(zip_path)}\")\n",
    "else:\n",
    "    print(\"\\n(Zip download disabled - set CREATE_DOWNLOAD_ZIP=True in cell 1 to enable)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "c435fce79c7b4513ae84c3b35ce5fe19": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_59251786ac884384b71d93aed4096cf5",
       "IPY_MODEL_2989d8b4a80f4162953c0873f09f1dbb",
       "IPY_MODEL_252dad12f32c425b9727f269c15d65d7"
      ],
      "layout": "IPY_MODEL_feab0b58e90c4cdaa1cebad99fdf5833"
     }
    },
    "59251786ac884384b71d93aed4096cf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bf0fba8898a414cbc080de0c098d9bf",
      "placeholder": "",
      "style": "IPY_MODEL_b7f6d2a4ba9146f7b991c75b5693de7c",
      "value": "README.md:"
     }
    },
    "2989d8b4a80f4162953c0873f09f1dbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf8ec69a56874aa79e5d8bfe043b1bad",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_107bcd021e7848a9b80e2abcc4c6a57b",
      "value": 1
     }
    },
    "252dad12f32c425b9727f269c15d65d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e6f72925cb9422499b96ecdfde2cd31",
      "placeholder": "",
      "style": "IPY_MODEL_762abf6ce3694a2ebb858b0bf94f5bfb",
      "value": "3.09k/?[00:00&lt;00:00,306kB/s]"
     }
    },
    "feab0b58e90c4cdaa1cebad99fdf5833": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bf0fba8898a414cbc080de0c098d9bf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7f6d2a4ba9146f7b991c75b5693de7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf8ec69a56874aa79e5d8bfe043b1bad": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "107bcd021e7848a9b80e2abcc4c6a57b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3e6f72925cb9422499b96ecdfde2cd31": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "762abf6ce3694a2ebb858b0bf94f5bfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ba432aa3bca4e58921a49521b77f596": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_89ee238c4c5c4d35b94f9dc7cbb1eec5",
       "IPY_MODEL_b5a15eb0b7ff42028d9e2f7d97277f56",
       "IPY_MODEL_7732e9d9252e48f58b9e0c3a184eecc2"
      ],
      "layout": "IPY_MODEL_0caa2d8c86ee4f6bb34dd56f52f1d951"
     }
    },
    "89ee238c4c5c4d35b94f9dc7cbb1eec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4988f81b16384e46919f9ef5bc8ff76d",
      "placeholder": "",
      "style": "IPY_MODEL_eb0c414ac87a4103aa872ea75d808f42",
      "value": "ethics.py:"
     }
    },
    "b5a15eb0b7ff42028d9e2f7d97277f56": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_afa153ecdca04fa2ae19bae7b91df118",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76bcb4eb538b42ecaeaa621ed65421ef",
      "value": 1
     }
    },
    "7732e9d9252e48f58b9e0c3a184eecc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65a06915e58e423ebe7a57f3bf377a4e",
      "placeholder": "",
      "style": "IPY_MODEL_ee53c53e81ac44daa0677cca9b7b182d",
      "value": "9.57k/?[00:00&lt;00:00,1.07MB/s]"
     }
    },
    "0caa2d8c86ee4f6bb34dd56f52f1d951": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4988f81b16384e46919f9ef5bc8ff76d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb0c414ac87a4103aa872ea75d808f42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "afa153ecdca04fa2ae19bae7b91df118": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "76bcb4eb538b42ecaeaa621ed65421ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "65a06915e58e423ebe7a57f3bf377a4e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee53c53e81ac44daa0677cca9b7b182d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b5c1f5ba0a1442197044be67122d603": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c350d89a43e74d73830d0afda82f35ac",
       "IPY_MODEL_f9efc167430945e696953f8c52f1a46b",
       "IPY_MODEL_4219402725934ff6abd2a52792626dd7"
      ],
      "layout": "IPY_MODEL_4d9100b7efab496281683c596ba062dc"
     }
    },
    "c350d89a43e74d73830d0afda82f35ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f9c572e46d94692bb024a3d6c1b8aae",
      "placeholder": "",
      "style": "IPY_MODEL_2ec6824fac7f4a14b4253e9c78b853f0",
      "value": "tokenizer_config.json:100%"
     }
    },
    "f9efc167430945e696953f8c52f1a46b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_640b1a4241f5429abcc08cb2d00ddfd3",
      "max": 397,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1205d4df82e844119d9e3e88b0c60a42",
      "value": 397
     }
    },
    "4219402725934ff6abd2a52792626dd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1140c6006b7842d0b809c62673607e26",
      "placeholder": "",
      "style": "IPY_MODEL_465c0f1ad7ba4384bfcc326a8154a270",
      "value": "397/397[00:00&lt;00:00,51.8kB/s]"
     }
    },
    "4d9100b7efab496281683c596ba062dc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f9c572e46d94692bb024a3d6c1b8aae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ec6824fac7f4a14b4253e9c78b853f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "640b1a4241f5429abcc08cb2d00ddfd3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1205d4df82e844119d9e3e88b0c60a42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1140c6006b7842d0b809c62673607e26": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "465c0f1ad7ba4384bfcc326a8154a270": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57a4104f53e7426c8267b9ebcf220133": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8289e4c5f5634e9b8849e526a5fa198b",
       "IPY_MODEL_b4a93b9a703c46a7b713d7f59c32b432",
       "IPY_MODEL_1764b050c0e5435dba14f97922514df0"
      ],
      "layout": "IPY_MODEL_26da4ed2eb764d5f9236bdfd5d83e71e"
     }
    },
    "8289e4c5f5634e9b8849e526a5fa198b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80a75540bf9b4713b045e366f4ee0b74",
      "placeholder": "",
      "style": "IPY_MODEL_74ffb1f734b94578ae4abed33d1126f8",
      "value": "config.json:100%"
     }
    },
    "b4a93b9a703c46a7b713d7f59c32b432": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a19e4499949b459bbd95426d6f1f4b66",
      "max": 804,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_980feb9c683246a4a76dbc0d9f262c45",
      "value": 804
     }
    },
    "1764b050c0e5435dba14f97922514df0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_644cbcabfa8649eaabaa341fc095beb7",
      "placeholder": "",
      "style": "IPY_MODEL_eab5cab430a54b52b2f9a004afa28fc4",
      "value": "804/804[00:00&lt;00:00,73.3kB/s]"
     }
    },
    "26da4ed2eb764d5f9236bdfd5d83e71e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80a75540bf9b4713b045e366f4ee0b74": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74ffb1f734b94578ae4abed33d1126f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a19e4499949b459bbd95426d6f1f4b66": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "980feb9c683246a4a76dbc0d9f262c45": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "644cbcabfa8649eaabaa341fc095beb7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eab5cab430a54b52b2f9a004afa28fc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1cfe6de186aa44a59357cdbef299c2b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_91120f80b69641f7878c16f278277116",
       "IPY_MODEL_c5c98ee60bbc471d9f17fb5654bf53c9",
       "IPY_MODEL_a5b9feedec594156ad15cd82655b89d3"
      ],
      "layout": "IPY_MODEL_2c2f664f85e3409c9ab8e3f4c0ac57f6"
     }
    },
    "91120f80b69641f7878c16f278277116": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35d2c93f7a4c414197382a5c539734bb",
      "placeholder": "",
      "style": "IPY_MODEL_9535edd6dcdd4a0b8c2896721fbd4bd1",
      "value": "vocab.txt:"
     }
    },
    "c5c98ee60bbc471d9f17fb5654bf53c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d178fba0381d458384c3b09801a4b040",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_460461c5430244a19c61ca2f0e987464",
      "value": 1
     }
    },
    "a5b9feedec594156ad15cd82655b89d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a961c1e5f254eefa8efad90dc35a342",
      "placeholder": "",
      "style": "IPY_MODEL_dc517db41a134b02a3c4177d0d9eab3e",
      "value": "5.22M/?[00:00&lt;00:00,28.7MB/s]"
     }
    },
    "2c2f664f85e3409c9ab8e3f4c0ac57f6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35d2c93f7a4c414197382a5c539734bb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9535edd6dcdd4a0b8c2896721fbd4bd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d178fba0381d458384c3b09801a4b040": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "460461c5430244a19c61ca2f0e987464": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2a961c1e5f254eefa8efad90dc35a342": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc517db41a134b02a3c4177d0d9eab3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "963c88a96f234702b5d6d0279935f15d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_de7ee617064b4683a0e2caace43b5749",
       "IPY_MODEL_fc71978785394930b056275bef0fee18",
       "IPY_MODEL_5e46bb4f556942d88103d86a51316f2e"
      ],
      "layout": "IPY_MODEL_2d2b1b8a52d84ab9bb1b46004b38e9fd"
     }
    },
    "de7ee617064b4683a0e2caace43b5749": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2f33a0f9f2b43c69539e5344038c839",
      "placeholder": "",
      "style": "IPY_MODEL_87d4dc34617c4887a8048099d4cd7302",
      "value": "tokenizer.json:"
     }
    },
    "fc71978785394930b056275bef0fee18": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fc97fd7ab7e40bd8f9e15340b8d00d3",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7d9f7cab5d1d45eebd5e80131f41609c",
      "value": 1
     }
    },
    "5e46bb4f556942d88103d86a51316f2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b5de60a3bf14f82ba00213c45e8bd86",
      "placeholder": "",
      "style": "IPY_MODEL_49e14b7bb1d84e8c98cdc423734467ee",
      "value": "9.62M/?[00:00&lt;00:00,115MB/s]"
     }
    },
    "2d2b1b8a52d84ab9bb1b46004b38e9fd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2f33a0f9f2b43c69539e5344038c839": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87d4dc34617c4887a8048099d4cd7302": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1fc97fd7ab7e40bd8f9e15340b8d00d3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "7d9f7cab5d1d45eebd5e80131f41609c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9b5de60a3bf14f82ba00213c45e8bd86": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49e14b7bb1d84e8c98cdc423734467ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "543a8ed40fe543f8aece1abc0b989279": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4ba93b780db841e99b35f53ad77ccac9",
       "IPY_MODEL_373e03146a60459e8af2fac94547964e",
       "IPY_MODEL_15f2dc02582948c0aa5372389f9ebdb3"
      ],
      "layout": "IPY_MODEL_ed6ad1fbb1e5420cbe5ebf6a1a6de7ca"
     }
    },
    "4ba93b780db841e99b35f53ad77ccac9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4bb56aec6abf4d2d97de8b154cf70db8",
      "placeholder": "",
      "style": "IPY_MODEL_3106c01c62f841d28cab3d9f109c401d",
      "value": "special_tokens_map.json:100%"
     }
    },
    "373e03146a60459e8af2fac94547964e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd3fefb24e274d25a99ce2ff311acc77",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6dcee7293f6a429894745fed641d34e1",
      "value": 112
     }
    },
    "15f2dc02582948c0aa5372389f9ebdb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57454d26e74f4dc1ac951fe4807c793d",
      "placeholder": "",
      "style": "IPY_MODEL_2f9429936ce24c2e96592b966dc42040",
      "value": "112/112[00:00&lt;00:00,14.8kB/s]"
     }
    },
    "ed6ad1fbb1e5420cbe5ebf6a1a6de7ca": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bb56aec6abf4d2d97de8b154cf70db8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3106c01c62f841d28cab3d9f109c401d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd3fefb24e274d25a99ce2ff311acc77": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6dcee7293f6a429894745fed641d34e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "57454d26e74f4dc1ac951fe4807c793d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f9429936ce24c2e96592b966dc42040": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e61d6594114944e3a100f35a977ddcb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_085179571bf649eabe7b3b822713b0d5",
       "IPY_MODEL_beaec6532ad14f208cd892f879dbc1c9",
       "IPY_MODEL_5582760cff614763848617d46b896f18"
      ],
      "layout": "IPY_MODEL_8a5b2c94e57d418db40d911b2053cdb9"
     }
    },
    "085179571bf649eabe7b3b822713b0d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3637217483a94bd7b72a5a9bc3646acb",
      "placeholder": "",
      "style": "IPY_MODEL_283bf7994bd14a1085bdff745fa7e79b",
      "value": "model.safetensors:100%"
     }
    },
    "beaec6532ad14f208cd892f879dbc1c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e2908e839554e69be582cbe44db2ef5",
      "max": 1883734344,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a6ce204bc128429d96d753f69de3eec0",
      "value": 1883734344
     }
    },
    "5582760cff614763848617d46b896f18": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45dd8e439b5546c4ba6b886a5bbe6be6",
      "placeholder": "",
      "style": "IPY_MODEL_bd8cfbc6981b4176b75bc9fd3ade7135",
      "value": "1.88G/1.88G[00:05&lt;00:00,401MB/s]"
     }
    },
    "8a5b2c94e57d418db40d911b2053cdb9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3637217483a94bd7b72a5a9bc3646acb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "283bf7994bd14a1085bdff745fa7e79b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e2908e839554e69be582cbe44db2ef5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6ce204bc128429d96d753f69de3eec0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "45dd8e439b5546c4ba6b886a5bbe6be6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd8cfbc6981b4176b75bc9fd3ade7135": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67038db96e194670b29df9157b38aa40": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_52ed9ec58ed24a88a7724fe25b231113",
       "IPY_MODEL_3f0f1aff3e6346e7b8791a9e90c9ca07",
       "IPY_MODEL_d52123a0b7284857b404d7b24fe4a189"
      ],
      "layout": "IPY_MODEL_f91b379c52bc43cca83a97a40d3edecb"
     }
    },
    "52ed9ec58ed24a88a7724fe25b231113": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a57e54bda33412fa8316f7fcac43dca",
      "placeholder": "",
      "style": "IPY_MODEL_0f83a4007a9147f7834787a1890489ff",
      "value": "Loading:"
     }
    },
    "3f0f1aff3e6346e7b8791a9e90c9ca07": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_501e3d42a0ff4b08b9d7131a5bf10b84",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c8606aff46ee4120815828c0e7bfd5e8",
      "value": 1
     }
    },
    "d52123a0b7284857b404d7b24fe4a189": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ec7817282cc4590ae399f4c16643fd3",
      "placeholder": "",
      "style": "IPY_MODEL_1b80c1626fdd45e99c0bf76008a26f73",
      "value": "227859/?[00:01&lt;00:00,231921.16line/s]"
     }
    },
    "f91b379c52bc43cca83a97a40d3edecb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a57e54bda33412fa8316f7fcac43dca": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f83a4007a9147f7834787a1890489ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "501e3d42a0ff4b08b9d7131a5bf10b84": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "c8606aff46ee4120815828c0e7bfd5e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5ec7817282cc4590ae399f4c16643fd3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b80c1626fdd45e99c0bf76008a26f73": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10b5773180bc43e8879d74b3d40be184": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b6d502b9af3e4dc6b4619a2704126768",
       "IPY_MODEL_64d30368ae2a4883adbd61ca0bd1bac2",
       "IPY_MODEL_3ac55274bf8a46d0b3a490c3ec963112"
      ],
      "layout": "IPY_MODEL_1822d58b5167459f8084fb35e6898713"
     }
    },
    "b6d502b9af3e4dc6b4619a2704126768": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5033d14e19a40019647acc3f5d6fe4c",
      "placeholder": "",
      "style": "IPY_MODEL_7abc7e5e379445dbabac9d4d6b7620ff",
      "value": "Extract:100%"
     }
    },
    "64d30368ae2a4883adbd61ca0bd1bac2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fb3dfec2d764f629ab3a1c030871111",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_346ce3b61f254fb19420fbca109cd5ca",
      "value": 29
     }
    },
    "3ac55274bf8a46d0b3a490c3ec963112": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c46f8e9535cf41219a723222f53711fc",
      "placeholder": "",
      "style": "IPY_MODEL_8b42f3e65c8944958a1b4a8ebf1ba282",
      "value": "29/29[01:25&lt;00:00,2.35s/it]"
     }
    },
    "1822d58b5167459f8084fb35e6898713": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5033d14e19a40019647acc3f5d6fe4c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7abc7e5e379445dbabac9d4d6b7620ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fb3dfec2d764f629ab3a1c030871111": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "346ce3b61f254fb19420fbca109cd5ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c46f8e9535cf41219a723222f53711fc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b42f3e65c8944958a1b4a8ebf1ba282": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}