{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIP v10.2: Native-Language Moral Pattern Transfer\n",
    "\n",
    "**Bond Invariance Principle**: Moral concepts share mathematical structure across languages and cultures.\n",
    "\n",
    "## What's New in v10.2\n",
    "- **Expanded Chinese corpus** - 200+ real classical texts (Analects, Mencius, Daodejing, etc.)\n",
    "- **Expanded Islamic corpus** - 150+ real Quranic verses and Hadith\n",
    "- **Better data validation** - Warnings for insufficient corpora\n",
    "- **Minimum test size** - Skips splits with < 500 test samples\n",
    "- **Dear Abby guidance** - Clear instructions for uploading real data\n",
    "\n",
    "## Methodology\n",
    "1. Extract moral labels from NATIVE text using NATIVE patterns\n",
    "2. Train encoder with adversarial language/period invariance\n",
    "3. Test if moral concepts transfer across language families\n",
    "\n",
    "**NO English translation bridge** - pure mathematical alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Configuration & Setup { display-mode: \"form\" }\n",
    "#@markdown ## Data Source Configuration\n",
    "#@markdown Choose where to load data from:\n",
    "\n",
    "USE_DRIVE_DATA = True  #@param {type:\"boolean\"}\n",
    "#@markdown If True, load pre-processed data from Google Drive (faster)\n",
    "\n",
    "REFRESH_DATA_FROM_SOURCE = False  #@param {type:\"boolean\"}\n",
    "#@markdown If True, re-download from online sources even if Drive data exists\n",
    "\n",
    "DRIVE_FOLDER = \"BIP_v10\"  #@param {type:\"string\"}\n",
    "#@markdown Google Drive folder name (in My Drive)\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Run Setup\n",
    "\n",
    "import time\n",
    "EXPERIMENT_START = time.time()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BIP v10.2 - CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nData source: {'Google Drive' if USE_DRIVE_DATA else 'Online download'}\")\n",
    "print(f\"Refresh from source: {REFRESH_DATA_FROM_SOURCE}\")\n",
    "print(f\"Drive folder: {DRIVE_FOLDER}\")\n",
    "\n",
    "import subprocess, sys, os\n",
    "\n",
    "# Install dependencies\n",
    "print(\"\\nInstalling dependencies...\")\n",
    "for pkg in [\"transformers\", \"sentence-transformers\", \"pandas\", \"tqdm\", \"scikit-learn\", \"pyyaml\", \"psutil\"]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU DETECTION & RESOURCE ALLOCATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Detect hardware\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "else:\n",
    "    GPU_NAME = \"CPU\"\n",
    "    VRAM_GB = 0\n",
    "\n",
    "RAM_GB = psutil.virtual_memory().total / 1e9\n",
    "\n",
    "print(f\"\\nDetected Hardware:\")\n",
    "print(f\"  GPU:  {GPU_NAME}\")\n",
    "print(f\"  VRAM: {VRAM_GB:.1f} GB\")\n",
    "print(f\"  RAM:  {RAM_GB:.1f} GB\")\n",
    "\n",
    "# Set optimal parameters based on hardware\n",
    "if VRAM_GB >= 22:      # L4 (24GB) or A100\n",
    "    BATCH_SIZE = 512\n",
    "    GPU_TIER = \"L4/A100\"\n",
    "elif VRAM_GB >= 14:    # T4 (16GB)\n",
    "    BATCH_SIZE = 256\n",
    "    GPU_TIER = \"T4\"\n",
    "elif VRAM_GB >= 10:\n",
    "    BATCH_SIZE = 128\n",
    "    GPU_TIER = \"SMALL\"\n",
    "else:\n",
    "    BATCH_SIZE = 64\n",
    "    GPU_TIER = \"MINIMAL/CPU\"\n",
    "\n",
    "if RAM_GB >= 50:\n",
    "    MAX_PER_LANG = 500000\n",
    "elif RAM_GB >= 24:\n",
    "    MAX_PER_LANG = 200000\n",
    "elif RAM_GB >= 12:\n",
    "    MAX_PER_LANG = 100000\n",
    "else:\n",
    "    MAX_PER_LANG = 50000\n",
    "\n",
    "CPU_CORES = os.cpu_count() or 2\n",
    "NUM_WORKERS = min(4, CPU_CORES - 1) if RAM_GB >= 24 and VRAM_GB >= 14 else 0\n",
    "MAX_TEST_SAMPLES = 20000\n",
    "LR = 2e-5 * (BATCH_SIZE / 256)\n",
    "\n",
    "print(f\"\\n\" + \"-\"*60)\n",
    "print(f\"OPTIMAL SETTINGS:\")\n",
    "print(f\"-\"*60)\n",
    "print(f\"  GPU Tier:        {GPU_TIER}\")\n",
    "print(f\"  Batch size:      {BATCH_SIZE}\")\n",
    "print(f\"  Max per lang:    {MAX_PER_LANG:,}\")\n",
    "print(f\"  DataLoader workers: {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate:   {LR:.2e}\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "scaler = torch.amp.GradScaler('cuda') if USE_AMP else None\n",
    "\n",
    "# Mount Drive with error handling\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "DRIVE_MOUNT_PATH = '/content/drive'\n",
    "\n",
    "# Check if already mounted\n",
    "if os.path.exists(f'{DRIVE_MOUNT_PATH}/MyDrive'):\n",
    "    print(\"Google Drive already mounted\")\n",
    "else:\n",
    "    try:\n",
    "        drive.mount(DRIVE_MOUNT_PATH, force_remount=False)\n",
    "        print(\"Google Drive mounted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Drive mount issue: {e}\")\n",
    "        print(\"Attempting force remount...\")\n",
    "        try:\n",
    "            drive.mount(DRIVE_MOUNT_PATH, force_remount=True)\n",
    "            print(\"Google Drive mounted (force remount)\")\n",
    "        except Exception as e2:\n",
    "            print(f\"ERROR: Could not mount Drive: {e2}\")\n",
    "            print(\"You may need to:\")\n",
    "            print(\"  1. Click Runtime -> Disconnect and delete runtime\")\n",
    "            print(\"  2. Reconnect and try again\")\n",
    "            print(\"  3. Or manually authorize in the popup\")\n",
    "SAVE_DIR = f'/content/drive/MyDrive/{DRIVE_FOLDER}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Check what's available in Drive\n",
    "DRIVE_HAS_DATA = False\n",
    "DRIVE_FILES = []\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    DRIVE_FILES = os.listdir(SAVE_DIR)\n",
    "    DRIVE_HAS_DATA = 'passages.jsonl' in DRIVE_FILES and 'bonds.jsonl' in DRIVE_FILES\n",
    "\n",
    "print(f\"\\n\" + \"-\"*60)\n",
    "print(f\"GOOGLE DRIVE STATUS:\")\n",
    "print(f\"-\"*60)\n",
    "print(f\"  Folder: {SAVE_DIR}\")\n",
    "print(f\"  Files found: {len(DRIVE_FILES)}\")\n",
    "if DRIVE_FILES:\n",
    "    for f in DRIVE_FILES[:10]:\n",
    "        print(f\"    - {f}\")\n",
    "    if len(DRIVE_FILES) > 10:\n",
    "        print(f\"    ... and {len(DRIVE_FILES)-10} more\")\n",
    "print(f\"  Pre-processed data available: {DRIVE_HAS_DATA}\")\n",
    "\n",
    "# Decide data loading strategy\n",
    "LOAD_FROM_DRIVE = USE_DRIVE_DATA and DRIVE_HAS_DATA and not REFRESH_DATA_FROM_SOURCE\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"DATA LOADING STRATEGY:\")\n",
    "if LOAD_FROM_DRIVE:\n",
    "    print(f\"  -> Will load pre-processed data from Google Drive\")\n",
    "    print(f\"     (Set REFRESH_DATA_FROM_SOURCE=True to re-download)\")\n",
    "else:\n",
    "    print(f\"  -> Will download and process data from online sources\")\n",
    "    if USE_DRIVE_DATA and not DRIVE_HAS_DATA:\n",
    "        print(f\"     (Drive data not found, downloading fresh)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create local directories\n",
    "for d in [\"data/processed\", \"data/splits\", \"data/raw\", \"models/checkpoints\", \"results\"]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Download/Load Corpora { display-mode: \"form\" }\n",
    "#@markdown Downloads from online sources OR loads from Google Drive\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING CORPORA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if LOAD_FROM_DRIVE:\n",
    "    # ===== LOAD FROM DRIVE =====\n",
    "    print(\"\\nLoading pre-processed data from Google Drive...\")\n",
    "    \n",
    "    # Copy files from Drive to local\n",
    "    for fname in ['passages.jsonl', 'bonds.jsonl']:\n",
    "        src = f'{SAVE_DIR}/{fname}'\n",
    "        dst = f'data/processed/{fname}'\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "            print(f\"  Copied {fname}\")\n",
    "    \n",
    "    if os.path.exists(f'{SAVE_DIR}/all_splits.json'):\n",
    "        shutil.copy(f'{SAVE_DIR}/all_splits.json', 'data/splits/all_splits.json')\n",
    "        print(f\"  Copied all_splits.json\")\n",
    "    \n",
    "    # Load Dear Abby from Drive if available\n",
    "    if 'dear_abby.csv' in DRIVE_FILES:\n",
    "        shutil.copy(f'{SAVE_DIR}/dear_abby.csv', 'data/raw/dear_abby.csv')\n",
    "        print(f\"  Copied dear_abby.csv\")\n",
    "    \n",
    "    # Count loaded data\n",
    "    if os.path.exists('data/processed/passages.jsonl'):\n",
    "        with open('data/processed/passages.jsonl') as f:\n",
    "            n_passages = sum(1 for _ in f)\n",
    "        print(f\"\\nLoaded {n_passages:,} passages from Drive\")\n",
    "    \n",
    "    SKIP_PROCESSING = True\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Drive data loaded - skipping download/processing\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    # ===== DOWNLOAD FROM ONLINE =====\n",
    "    SKIP_PROCESSING = False\n",
    "    \n",
    "    # SEFARIA\n",
    "    if not os.path.exists('data/raw/Sefaria-Export/json'):\n",
    "        print(\"\\n[1/4] Downloading Sefaria (~2GB)...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", \n",
    "                       \"https://github.com/Sefaria/Sefaria-Export.git\",\n",
    "                       \"data/raw/Sefaria-Export\"], check=True)\n",
    "        print(\"  Done!\")\n",
    "    else:\n",
    "        print(\"\\n[1/4] Sefaria already exists\")\n",
    "    \n",
    "    # CHINESE - 200+ REAL CLASSICAL TEXTS\n",
    "    print(\"\\n[2/4] Chinese classics (200+ real passages)...\")\n",
    "    os.makedirs('data/raw/chinese', exist_ok=True)\n",
    "    \n",
    "    chinese = []\n",
    "    \n",
    "    # === ANALECTS (\u8ad6\u8a9e) - 50+ passages ===\n",
    "    analects = [\n",
    "        (\"\u5b50\u66f0\uff1a\u5df1\u6240\u4e0d\u6b32\uff0c\u52ff\u65bd\u65bc\u4eba\u3002\", \"Analects 15.24\"),\n",
    "        (\"\u5b5d\u608c\u4e5f\u8005\uff0c\u5176\u70ba\u4ec1\u4e4b\u672c\u8207\u3002\", \"Analects 1.2\"),\n",
    "        (\"\u7236\u6bcd\u5728\uff0c\u4e0d\u9060\u6e38\uff0c\u904a\u5fc5\u6709\u65b9\u3002\", \"Analects 4.19\"),\n",
    "        (\"\u541b\u5b50\u55bb\u65bc\u7fa9\uff0c\u5c0f\u4eba\u55bb\u65bc\u5229\u3002\", \"Analects 4.16\"),\n",
    "        (\"\u4e0d\u7fa9\u800c\u5bcc\u4e14\u8cb4\uff0c\u65bc\u6211\u5982\u6d6e\u96f2\u3002\", \"Analects 7.16\"),\n",
    "        (\"\u5b78\u800c\u6642\u7fd2\u4e4b\uff0c\u4e0d\u4ea6\u8aaa\u4e4e\u3002\", \"Analects 1.1\"),\n",
    "        (\"\u6709\u670b\u81ea\u9060\u65b9\u4f86\uff0c\u4e0d\u4ea6\u6a02\u4e4e\u3002\", \"Analects 1.1\"),\n",
    "        (\"\u4eba\u4e0d\u77e5\u800c\u4e0d\u614d\uff0c\u4e0d\u4ea6\u541b\u5b50\u4e4e\u3002\", \"Analects 1.1\"),\n",
    "        (\"\u5de7\u8a00\u4ee4\u8272\uff0c\u9bae\u77e3\u4ec1\u3002\", \"Analects 1.3\"),\n",
    "        (\"\u543e\u65e5\u4e09\u7701\u543e\u8eab\u3002\", \"Analects 1.4\"),\n",
    "        (\"\u70ba\u4eba\u8b00\u800c\u4e0d\u5fe0\u4e4e\uff0c\u8207\u670b\u53cb\u4ea4\u800c\u4e0d\u4fe1\u4e4e\u3002\", \"Analects 1.4\"),\n",
    "        (\"\u5f1f\u5b50\u5165\u5247\u5b5d\uff0c\u51fa\u5247\u608c\u3002\", \"Analects 1.6\"),\n",
    "        (\"\u8b39\u800c\u4fe1\uff0c\u6c4e\u611b\u773e\uff0c\u800c\u89aa\u4ec1\u3002\", \"Analects 1.6\"),\n",
    "        (\"\u541b\u5b50\u4e0d\u91cd\u5247\u4e0d\u5a01\uff0c\u5b78\u5247\u4e0d\u56fa\u3002\", \"Analects 1.8\"),\n",
    "        (\"\u4e3b\u5fe0\u4fe1\uff0c\u7121\u53cb\u4e0d\u5982\u5df1\u8005\u3002\", \"Analects 1.8\"),\n",
    "        (\"\u904e\u5247\u52ff\u619a\u6539\u3002\", \"Analects 1.8\"),\n",
    "        (\"\u614e\u7d42\u8ffd\u9060\uff0c\u6c11\u5fb7\u6b78\u539a\u77e3\u3002\", \"Analects 1.9\"),\n",
    "        (\"\u79ae\u4e4b\u7528\uff0c\u548c\u70ba\u8cb4\u3002\", \"Analects 1.12\"),\n",
    "        (\"\u4fe1\u8fd1\u65bc\u7fa9\uff0c\u8a00\u53ef\u5fa9\u4e5f\u3002\", \"Analects 1.13\"),\n",
    "        (\"\u541b\u5b50\u98df\u7121\u6c42\u98fd\uff0c\u5c45\u7121\u6c42\u5b89\u3002\", \"Analects 1.14\"),\n",
    "        (\"\u654f\u65bc\u4e8b\u800c\u614e\u65bc\u8a00\uff0c\u5c31\u6709\u9053\u800c\u6b63\u7109\u3002\", \"Analects 1.14\"),\n",
    "        (\"\u4e0d\u60a3\u4eba\u4e4b\u4e0d\u5df1\u77e5\uff0c\u60a3\u4e0d\u77e5\u4eba\u4e5f\u3002\", \"Analects 1.16\"),\n",
    "        (\"\u70ba\u653f\u4ee5\u5fb7\uff0c\u8b6c\u5982\u5317\u8fb0\u3002\", \"Analects 2.1\"),\n",
    "        (\"\u9053\u4e4b\u4ee5\u653f\uff0c\u9f4a\u4e4b\u4ee5\u5211\uff0c\u6c11\u514d\u800c\u7121\u6065\u3002\", \"Analects 2.3\"),\n",
    "        (\"\u9053\u4e4b\u4ee5\u5fb7\uff0c\u9f4a\u4e4b\u4ee5\u79ae\uff0c\u6709\u6065\u4e14\u683c\u3002\", \"Analects 2.3\"),\n",
    "        (\"\u543e\u5341\u6709\u4e94\u800c\u5fd7\u4e8e\u5b78\u3002\", \"Analects 2.4\"),\n",
    "        (\"\u4e09\u5341\u800c\u7acb\uff0c\u56db\u5341\u800c\u4e0d\u60d1\u3002\", \"Analects 2.4\"),\n",
    "        (\"\u4e94\u5341\u800c\u77e5\u5929\u547d\uff0c\u516d\u5341\u800c\u8033\u9806\u3002\", \"Analects 2.4\"),\n",
    "        (\"\u4e03\u5341\u800c\u5f9e\u5fc3\u6240\u6b32\uff0c\u4e0d\u903e\u77e9\u3002\", \"Analects 2.4\"),\n",
    "        (\"\u751f\uff0c\u4e8b\u4e4b\u4ee5\u79ae\uff1b\u6b7b\uff0c\u846c\u4e4b\u4ee5\u79ae\uff0c\u796d\u4e4b\u4ee5\u79ae\u3002\", \"Analects 2.5\"),\n",
    "        (\"\u7236\u6bcd\u552f\u5176\u75be\u4e4b\u6182\u3002\", \"Analects 2.6\"),\n",
    "        (\"\u4eca\u4e4b\u5b5d\u8005\uff0c\u662f\u8b02\u80fd\u990a\u3002\", \"Analects 2.7\"),\n",
    "        (\"\u81f3\u65bc\u72ac\u99ac\uff0c\u7686\u80fd\u6709\u990a\uff1b\u4e0d\u656c\uff0c\u4f55\u4ee5\u5225\u4e4e\u3002\", \"Analects 2.7\"),\n",
    "        (\"\u8272\u96e3\u3002\u6709\u4e8b\uff0c\u5f1f\u5b50\u670d\u5176\u52de\u3002\", \"Analects 2.8\"),\n",
    "        (\"\u8996\u5176\u6240\u4ee5\uff0c\u89c0\u5176\u6240\u7531\uff0c\u5bdf\u5176\u6240\u5b89\u3002\", \"Analects 2.10\"),\n",
    "        (\"\u6eab\u6545\u800c\u77e5\u65b0\uff0c\u53ef\u4ee5\u70ba\u5e2b\u77e3\u3002\", \"Analects 2.11\"),\n",
    "        (\"\u541b\u5b50\u4e0d\u5668\u3002\", \"Analects 2.12\"),\n",
    "        (\"\u5148\u884c\u5176\u8a00\u800c\u5f8c\u5f9e\u4e4b\u3002\", \"Analects 2.13\"),\n",
    "        (\"\u541b\u5b50\u5468\u800c\u4e0d\u6bd4\uff0c\u5c0f\u4eba\u6bd4\u800c\u4e0d\u5468\u3002\", \"Analects 2.14\"),\n",
    "        (\"\u5b78\u800c\u4e0d\u601d\u5247\u7f54\uff0c\u601d\u800c\u4e0d\u5b78\u5247\u6b86\u3002\", \"Analects 2.15\"),\n",
    "        (\"\u77e5\u4e4b\u70ba\u77e5\u4e4b\uff0c\u4e0d\u77e5\u70ba\u4e0d\u77e5\uff0c\u662f\u77e5\u4e5f\u3002\", \"Analects 2.17\"),\n",
    "        (\"\u591a\u805e\u95d5\u7591\uff0c\u614e\u8a00\u5176\u9918\uff0c\u5247\u5be1\u5c24\u3002\", \"Analects 2.18\"),\n",
    "        (\"\u8209\u76f4\u932f\u8af8\u6789\uff0c\u5247\u6c11\u670d\u3002\", \"Analects 2.19\"),\n",
    "        (\"\u4eba\u800c\u7121\u4fe1\uff0c\u4e0d\u77e5\u5176\u53ef\u4e5f\u3002\", \"Analects 2.22\"),\n",
    "        (\"\u898b\u7fa9\u4e0d\u70ba\uff0c\u7121\u52c7\u4e5f\u3002\", \"Analects 2.24\"),\n",
    "        (\"\u975e\u5176\u9b3c\u800c\u796d\u4e4b\uff0c\u8ac2\u4e5f\u3002\", \"Analects 2.24\"),\n",
    "        (\"\u662f\u53ef\u5fcd\u4e5f\uff0c\u5b70\u4e0d\u53ef\u5fcd\u4e5f\u3002\", \"Analects 3.1\"),\n",
    "        (\"\u4eba\u800c\u4e0d\u4ec1\uff0c\u5982\u79ae\u4f55\u3002\", \"Analects 3.3\"),\n",
    "        (\"\u4eba\u800c\u4e0d\u4ec1\uff0c\u5982\u6a02\u4f55\u3002\", \"Analects 3.3\"),\n",
    "        (\"\u91cc\u4ec1\u70ba\u7f8e\u3002\u64c7\u4e0d\u8655\u4ec1\uff0c\u7109\u5f97\u77e5\u3002\", \"Analects 4.1\"),\n",
    "        (\"\u4e0d\u4ec1\u8005\u4e0d\u53ef\u4ee5\u4e45\u8655\u7d04\uff0c\u4e0d\u53ef\u4ee5\u9577\u8655\u6a02\u3002\", \"Analects 4.2\"),\n",
    "        (\"\u4ec1\u8005\u5b89\u4ec1\uff0c\u77e5\u8005\u5229\u4ec1\u3002\", \"Analects 4.2\"),\n",
    "        (\"\u552f\u4ec1\u8005\u80fd\u597d\u4eba\uff0c\u80fd\u60e1\u4eba\u3002\", \"Analects 4.3\"),\n",
    "        (\"\u82df\u5fd7\u65bc\u4ec1\u77e3\uff0c\u7121\u60e1\u4e5f\u3002\", \"Analects 4.4\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(analects):\n",
    "        chinese.append({\"id\": f\"cn_analects_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -5})\n",
    "    \n",
    "    # === MENCIUS (\u5b5f\u5b50) - 40+ passages ===\n",
    "    mencius = [\n",
    "        (\"\u60fb\u96b1\u4e4b\u5fc3\uff0c\u4ec1\u4e4b\u7aef\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7f9e\u60e1\u4e4b\u5fc3\uff0c\u7fa9\u4e4b\u7aef\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u8fad\u8b93\u4e4b\u5fc3\uff0c\u79ae\u4e4b\u7aef\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u662f\u975e\u4e4b\u5fc3\uff0c\u667a\u4e4b\u7aef\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u4eba\u7686\u6709\u4e0d\u5fcd\u4eba\u4e4b\u5fc3\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7121\u60fb\u96b1\u4e4b\u5fc3\uff0c\u975e\u4eba\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7121\u7f9e\u60e1\u4e4b\u5fc3\uff0c\u975e\u4eba\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7121\u8fad\u8b93\u4e4b\u5fc3\uff0c\u975e\u4eba\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7121\u662f\u975e\u4e4b\u5fc3\uff0c\u975e\u4eba\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u4ec1\u7fa9\u79ae\u667a\uff0c\u975e\u7531\u5916\u9460\u6211\u4e5f\uff0c\u6211\u56fa\u6709\u4e4b\u4e5f\u3002\", \"Mencius 6A.6\"),\n",
    "        (\"\u4eba\u6027\u4e4b\u5584\u4e5f\uff0c\u7336\u6c34\u4e4b\u5c31\u4e0b\u4e5f\u3002\", \"Mencius 6A.2\"),\n",
    "        (\"\u4eba\u7121\u6709\u4e0d\u5584\uff0c\u6c34\u7121\u6709\u4e0d\u4e0b\u3002\", \"Mencius 6A.2\"),\n",
    "        (\"\u60df\u4ec1\u8005\u5b9c\u5728\u9ad8\u4f4d\u3002\", \"Mencius 4A.1\"),\n",
    "        (\"\u4e0d\u4ec1\u800c\u5728\u9ad8\u4f4d\uff0c\u662f\u64ad\u5176\u60e1\u65bc\u773e\u4e5f\u3002\", \"Mencius 4A.1\"),\n",
    "        (\"\u6c11\u70ba\u8cb4\uff0c\u793e\u7a37\u6b21\u4e4b\uff0c\u541b\u70ba\u8f15\u3002\", \"Mencius 7B.14\"),\n",
    "        (\"\u5f97\u9053\u8005\u591a\u52a9\uff0c\u5931\u9053\u8005\u5be1\u52a9\u3002\", \"Mencius 2B.1\"),\n",
    "        (\"\u5be1\u52a9\u4e4b\u81f3\uff0c\u89aa\u621a\u7554\u4e4b\u3002\", \"Mencius 2B.1\"),\n",
    "        (\"\u591a\u52a9\u4e4b\u81f3\uff0c\u5929\u4e0b\u9806\u4e4b\u3002\", \"Mencius 2B.1\"),\n",
    "        (\"\u5929\u6642\u4e0d\u5982\u5730\u5229\uff0c\u5730\u5229\u4e0d\u5982\u4eba\u548c\u3002\", \"Mencius 2B.1\"),\n",
    "        (\"\u751f\u65bc\u6182\u60a3\uff0c\u6b7b\u65bc\u5b89\u6a02\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u5929\u5c07\u964d\u5927\u4efb\u65bc\u662f\u4eba\u4e5f\uff0c\u5fc5\u5148\u82e6\u5176\u5fc3\u5fd7\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u52de\u5176\u7b4b\u9aa8\uff0c\u9913\u5176\u9ad4\u819a\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u7a7a\u4e4f\u5176\u8eab\uff0c\u884c\u62c2\u4e82\u5176\u6240\u70ba\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u6240\u4ee5\u52d5\u5fc3\u5fcd\u6027\uff0c\u66fe\u76ca\u5176\u6240\u4e0d\u80fd\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u8001\u543e\u8001\uff0c\u4ee5\u53ca\u4eba\u4e4b\u8001\u3002\", \"Mencius 1A.7\"),\n",
    "        (\"\u5e7c\u543e\u5e7c\uff0c\u4ee5\u53ca\u4eba\u4e4b\u5e7c\u3002\", \"Mencius 1A.7\"),\n",
    "        (\"\u7aae\u5247\u7368\u5584\u5176\u8eab\uff0c\u9054\u5247\u517c\u5584\u5929\u4e0b\u3002\", \"Mencius 7A.9\"),\n",
    "        (\"\u9b5a\uff0c\u6211\u6240\u6b32\u4e5f\uff1b\u718a\u638c\uff0c\u4ea6\u6211\u6240\u6b32\u4e5f\u3002\", \"Mencius 6A.10\"),\n",
    "        (\"\u4e8c\u8005\u4e0d\u53ef\u5f97\u517c\uff0c\u820d\u9b5a\u800c\u53d6\u718a\u638c\u8005\u4e5f\u3002\", \"Mencius 6A.10\"),\n",
    "        (\"\u751f\uff0c\u4ea6\u6211\u6240\u6b32\u4e5f\uff1b\u7fa9\uff0c\u4ea6\u6211\u6240\u6b32\u4e5f\u3002\", \"Mencius 6A.10\"),\n",
    "        (\"\u4e8c\u8005\u4e0d\u53ef\u5f97\u517c\uff0c\u820d\u751f\u800c\u53d6\u7fa9\u8005\u4e5f\u3002\", \"Mencius 6A.10\"),\n",
    "        (\"\u990a\u5fc3\u83ab\u5584\u65bc\u5be1\u6b32\u3002\", \"Mencius 7B.35\"),\n",
    "        (\"\u4ec1\u8005\u7121\u6575\u65bc\u5929\u4e0b\u3002\", \"Mencius 1A.5\"),\n",
    "        (\"\u4ee5\u529b\u670d\u4eba\u8005\uff0c\u975e\u5fc3\u670d\u4e5f\u3002\", \"Mencius 2A.3\"),\n",
    "        (\"\u4ee5\u5fb7\u670d\u4eba\u8005\uff0c\u4e2d\u5fc3\u6085\u800c\u8aa0\u670d\u4e5f\u3002\", \"Mencius 2A.3\"),\n",
    "        (\"\u4eba\u4e4b\u60a3\u5728\u597d\u70ba\u4eba\u5e2b\u3002\", \"Mencius 4A.23\"),\n",
    "        (\"\u76e1\u4fe1\u66f8\uff0c\u5247\u4e0d\u5982\u7121\u66f8\u3002\", \"Mencius 7B.3\"),\n",
    "        (\"\u4e0d\u4ee5\u898f\u77e9\uff0c\u4e0d\u80fd\u6210\u65b9\u5713\u3002\", \"Mencius 4A.1\"),\n",
    "        (\"\u5b5d\u5b50\u4e4b\u81f3\uff0c\u83ab\u5927\u4e4e\u5c0a\u89aa\u3002\", \"Mencius 5A.4\"),\n",
    "        (\"\u7236\u5b50\u6709\u89aa\uff0c\u541b\u81e3\u6709\u7fa9\uff0c\u592b\u5a66\u6709\u5225\uff0c\u9577\u5e7c\u6709\u5e8f\uff0c\u670b\u53cb\u6709\u4fe1\u3002\", \"Mencius 3A.4\"),\n",
    "        (\"\u4eba\u6709\u4e0d\u70ba\u4e5f\uff0c\u800c\u5f8c\u53ef\u4ee5\u6709\u70ba\u3002\", \"Mencius 4B.8\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(mencius):\n",
    "        chinese.append({\"id\": f\"cn_mencius_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -4})\n",
    "    \n",
    "    # === DAODEJING (\u9053\u5fb7\u7d93) - 40+ passages ===\n",
    "    daodejing = [\n",
    "        (\"\u9053\u53ef\u9053\uff0c\u975e\u5e38\u9053\u3002\u540d\u53ef\u540d\uff0c\u975e\u5e38\u540d\u3002\", \"Daodejing 1\"),\n",
    "        (\"\u5929\u4e0b\u7686\u77e5\u7f8e\u4e4b\u70ba\u7f8e\uff0c\u65af\u60e1\u5df2\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u7686\u77e5\u5584\u4e4b\u70ba\u5584\uff0c\u65af\u4e0d\u5584\u5df2\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u6709\u7121\u76f8\u751f\uff0c\u96e3\u6613\u76f8\u6210\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u9577\u77ed\u76f8\u8f03\uff0c\u9ad8\u4e0b\u76f8\u50be\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u662f\u4ee5\u8056\u4eba\u8655\u7121\u70ba\u4e4b\u4e8b\uff0c\u884c\u4e0d\u8a00\u4e4b\u6559\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u4e0d\u5c1a\u8ce2\uff0c\u4f7f\u6c11\u4e0d\u722d\u3002\", \"Daodejing 3\"),\n",
    "        (\"\u4e0d\u8cb4\u96e3\u5f97\u4e4b\u8ca8\uff0c\u4f7f\u6c11\u4e0d\u70ba\u76dc\u3002\", \"Daodejing 3\"),\n",
    "        (\"\u4e0a\u5584\u82e5\u6c34\u3002\u6c34\u5584\u5229\u842c\u7269\u800c\u4e0d\u722d\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u8655\u773e\u4eba\u4e4b\u6240\u60e1\uff0c\u6545\u5e7e\u65bc\u9053\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u5c45\u5584\u5730\uff0c\u5fc3\u5584\u6df5\uff0c\u8207\u5584\u4ec1\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u8a00\u5584\u4fe1\uff0c\u653f\u5584\u6cbb\uff0c\u4e8b\u5584\u80fd\uff0c\u52d5\u5584\u6642\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u592b\u552f\u4e0d\u722d\uff0c\u6545\u7121\u5c24\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u91d1\u7389\u6eff\u5802\uff0c\u83ab\u4e4b\u80fd\u5b88\u3002\", \"Daodejing 9\"),\n",
    "        (\"\u5bcc\u8cb4\u800c\u9a55\uff0c\u81ea\u907a\u5176\u548e\u3002\", \"Daodejing 9\"),\n",
    "        (\"\u529f\u6210\u8eab\u9000\uff0c\u5929\u4e4b\u9053\u4e5f\u3002\", \"Daodejing 9\"),\n",
    "        (\"\u77e5\u4eba\u8005\u667a\uff0c\u81ea\u77e5\u8005\u660e\u3002\", \"Daodejing 33\"),\n",
    "        (\"\u52dd\u4eba\u8005\u6709\u529b\uff0c\u81ea\u52dd\u8005\u5f37\u3002\", \"Daodejing 33\"),\n",
    "        (\"\u77e5\u8db3\u8005\u5bcc\uff0c\u5f37\u884c\u8005\u6709\u5fd7\u3002\", \"Daodejing 33\"),\n",
    "        (\"\u4e0d\u5931\u5176\u6240\u8005\u4e45\uff0c\u6b7b\u800c\u4e0d\u4ea1\u8005\u58fd\u3002\", \"Daodejing 33\"),\n",
    "        (\"\u5927\u9053\u5ee2\uff0c\u6709\u4ec1\u7fa9\u3002\", \"Daodejing 18\"),\n",
    "        (\"\u667a\u6167\u51fa\uff0c\u6709\u5927\u507d\u3002\", \"Daodejing 18\"),\n",
    "        (\"\u516d\u89aa\u4e0d\u548c\uff0c\u6709\u5b5d\u6148\u3002\", \"Daodejing 18\"),\n",
    "        (\"\u570b\u5bb6\u660f\u4e82\uff0c\u6709\u5fe0\u81e3\u3002\", \"Daodejing 18\"),\n",
    "        (\"\u798d\u516e\u798f\u4e4b\u6240\u501a\uff0c\u798f\u516e\u798d\u4e4b\u6240\u4f0f\u3002\", \"Daodejing 58\"),\n",
    "        (\"\u5929\u9577\u5730\u4e45\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u5929\u5730\u6240\u4ee5\u80fd\u9577\u4e14\u4e45\u8005\uff0c\u4ee5\u5176\u4e0d\u81ea\u751f\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u662f\u4ee5\u8056\u4eba\u5f8c\u5176\u8eab\u800c\u8eab\u5148\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u5916\u5176\u8eab\u800c\u8eab\u5b58\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u975e\u4ee5\u5176\u7121\u79c1\u8036\uff0c\u6545\u80fd\u6210\u5176\u79c1\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u67d4\u5f31\u52dd\u525b\u5f37\u3002\", \"Daodejing 36\"),\n",
    "        (\"\u5927\u65b9\u7121\u9685\uff0c\u5927\u5668\u665a\u6210\u3002\", \"Daodejing 41\"),\n",
    "        (\"\u5927\u97f3\u5e0c\u8072\uff0c\u5927\u8c61\u7121\u5f62\u3002\", \"Daodejing 41\"),\n",
    "        (\"\u9053\u751f\u4e00\uff0c\u4e00\u751f\u4e8c\uff0c\u4e8c\u751f\u4e09\uff0c\u4e09\u751f\u842c\u7269\u3002\", \"Daodejing 42\"),\n",
    "        (\"\u5929\u4e0b\u842c\u7269\u751f\u65bc\u6709\uff0c\u6709\u751f\u65bc\u7121\u3002\", \"Daodejing 40\"),\n",
    "        (\"\u5343\u91cc\u4e4b\u884c\uff0c\u59cb\u65bc\u8db3\u4e0b\u3002\", \"Daodejing 64\"),\n",
    "        (\"\u5408\u62b1\u4e4b\u6728\uff0c\u751f\u65bc\u6beb\u672b\u3002\", \"Daodejing 64\"),\n",
    "        (\"\u4e5d\u5c64\u4e4b\u81fa\uff0c\u8d77\u65bc\u7d2f\u571f\u3002\", \"Daodejing 64\"),\n",
    "        (\"\u6c11\u4e0d\u754f\u6b7b\uff0c\u5948\u4f55\u4ee5\u6b7b\u61fc\u4e4b\u3002\", \"Daodejing 74\"),\n",
    "        (\"\u4fe1\u8a00\u4e0d\u7f8e\uff0c\u7f8e\u8a00\u4e0d\u4fe1\u3002\", \"Daodejing 81\"),\n",
    "        (\"\u5584\u8005\u4e0d\u8faf\uff0c\u8faf\u8005\u4e0d\u5584\u3002\", \"Daodejing 81\"),\n",
    "        (\"\u77e5\u8005\u4e0d\u535a\uff0c\u535a\u8005\u4e0d\u77e5\u3002\", \"Daodejing 81\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(daodejing):\n",
    "        chinese.append({\"id\": f\"cn_daodejing_{i}\", \"text\": text, \"source\": source, \"period\": \"DAOIST\", \"century\": -4})\n",
    "    \n",
    "    # === GREAT LEARNING (\u5927\u5b78) - 20+ passages ===\n",
    "    daxue = [\n",
    "        (\"\u5927\u5b78\u4e4b\u9053\uff0c\u5728\u660e\u660e\u5fb7\uff0c\u5728\u89aa\u6c11\uff0c\u5728\u6b62\u65bc\u81f3\u5584\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u77e5\u6b62\u800c\u5f8c\u6709\u5b9a\uff0c\u5b9a\u800c\u5f8c\u80fd\u975c\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u975c\u800c\u5f8c\u80fd\u5b89\uff0c\u5b89\u800c\u5f8c\u80fd\u616e\uff0c\u616e\u800c\u5f8c\u80fd\u5f97\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u7269\u6709\u672c\u672b\uff0c\u4e8b\u6709\u7d42\u59cb\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u77e5\u6240\u5148\u5f8c\uff0c\u5247\u8fd1\u9053\u77e3\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u53e4\u4e4b\u6b32\u660e\u660e\u5fb7\u65bc\u5929\u4e0b\u8005\uff0c\u5148\u6cbb\u5176\u570b\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u6cbb\u5176\u570b\u8005\uff0c\u5148\u9f4a\u5176\u5bb6\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u9f4a\u5176\u5bb6\u8005\uff0c\u5148\u4fee\u5176\u8eab\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u4fee\u5176\u8eab\u8005\uff0c\u5148\u6b63\u5176\u5fc3\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u6b63\u5176\u5fc3\u8005\uff0c\u5148\u8aa0\u5176\u610f\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u8aa0\u5176\u610f\u8005\uff0c\u5148\u81f4\u5176\u77e5\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u81f4\u77e5\u5728\u683c\u7269\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u7269\u683c\u800c\u5f8c\u77e5\u81f3\uff0c\u77e5\u81f3\u800c\u5f8c\u610f\u8aa0\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u610f\u8aa0\u800c\u5f8c\u5fc3\u6b63\uff0c\u5fc3\u6b63\u800c\u5f8c\u8eab\u4fee\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u8eab\u4fee\u800c\u5f8c\u5bb6\u9f4a\uff0c\u5bb6\u9f4a\u800c\u5f8c\u570b\u6cbb\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u570b\u6cbb\u800c\u5f8c\u5929\u4e0b\u5e73\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u81ea\u5929\u5b50\u4ee5\u81f3\u65bc\u5eb6\u4eba\uff0c\u58f9\u662f\u7686\u4ee5\u4fee\u8eab\u70ba\u672c\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u5176\u672c\u4e82\u800c\u672b\u6cbb\u8005\u5426\u77e3\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6240\u8b02\u8aa0\u5176\u610f\u8005\uff0c\u6bcb\u81ea\u6b3a\u4e5f\u3002\", \"Great Learning 6\"),\n",
    "        (\"\u5982\u60e1\u60e1\u81ed\uff0c\u5982\u597d\u597d\u8272\uff0c\u6b64\u4e4b\u8b02\u81ea\u8b19\u3002\", \"Great Learning 6\"),\n",
    "        (\"\u6545\u541b\u5b50\u5fc5\u614e\u5176\u7368\u4e5f\u3002\", \"Great Learning 6\"),\n",
    "        (\"\u5bcc\u6f64\u5c4b\uff0c\u5fb7\u6f64\u8eab\uff0c\u5fc3\u5ee3\u9ad4\u80d6\u3002\", \"Great Learning 6\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(daxue):\n",
    "        chinese.append({\"id\": f\"cn_daxue_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -5})\n",
    "    \n",
    "    # === DOCTRINE OF THE MEAN (\u4e2d\u5eb8) - 20+ passages ===\n",
    "    zhongyong = [\n",
    "        (\"\u5929\u547d\u4e4b\u8b02\u6027\uff0c\u7387\u6027\u4e4b\u8b02\u9053\uff0c\u4fee\u9053\u4e4b\u8b02\u6559\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u9053\u4e5f\u8005\uff0c\u4e0d\u53ef\u9808\u81fe\u96e2\u4e5f\uff1b\u53ef\u96e2\uff0c\u975e\u9053\u4e5f\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u662f\u6545\u541b\u5b50\u6212\u614e\u4e4e\u5176\u6240\u4e0d\u7779\uff0c\u6050\u61fc\u4e4e\u5176\u6240\u4e0d\u805e\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u83ab\u898b\u4e4e\u96b1\uff0c\u83ab\u986f\u4e4e\u5fae\uff0c\u6545\u541b\u5b50\u614e\u5176\u7368\u4e5f\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u559c\u6012\u54c0\u6a02\u4e4b\u672a\u767c\uff0c\u8b02\u4e4b\u4e2d\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u767c\u800c\u7686\u4e2d\u7bc0\uff0c\u8b02\u4e4b\u548c\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u4e2d\u4e5f\u8005\uff0c\u5929\u4e0b\u4e4b\u5927\u672c\u4e5f\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u548c\u4e5f\u8005\uff0c\u5929\u4e0b\u4e4b\u9054\u9053\u4e5f\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u81f4\u4e2d\u548c\uff0c\u5929\u5730\u4f4d\u7109\uff0c\u842c\u7269\u80b2\u7109\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u541b\u5b50\u4e2d\u5eb8\uff0c\u5c0f\u4eba\u53cd\u4e2d\u5eb8\u3002\", \"Doctrine of the Mean 2\"),\n",
    "        (\"\u541b\u5b50\u4e4b\u4e2d\u5eb8\u4e5f\uff0c\u541b\u5b50\u800c\u6642\u4e2d\u3002\", \"Doctrine of the Mean 2\"),\n",
    "        (\"\u5c0f\u4eba\u4e4b\u53cd\u4e2d\u5eb8\u4e5f\uff0c\u5c0f\u4eba\u800c\u7121\u5fcc\u619a\u4e5f\u3002\", \"Doctrine of the Mean 2\"),\n",
    "        (\"\u4e2d\u5eb8\u5176\u81f3\u77e3\u4e4e\uff01\u6c11\u9bae\u80fd\u4e45\u77e3\u3002\", \"Doctrine of the Mean 3\"),\n",
    "        (\"\u9053\u4e4b\u4e0d\u884c\u4e5f\uff0c\u6211\u77e5\u4e4b\u77e3\uff1a\u77e5\u8005\u904e\u4e4b\uff0c\u611a\u8005\u4e0d\u53ca\u4e5f\u3002\", \"Doctrine of the Mean 4\"),\n",
    "        (\"\u9053\u4e4b\u4e0d\u660e\u4e5f\uff0c\u6211\u77e5\u4e4b\u77e3\uff1a\u8ce2\u8005\u904e\u4e4b\uff0c\u4e0d\u8096\u8005\u4e0d\u53ca\u4e5f\u3002\", \"Doctrine of the Mean 4\"),\n",
    "        (\"\u4eba\u83ab\u4e0d\u98f2\u98df\u4e5f\uff0c\u9bae\u80fd\u77e5\u5473\u4e5f\u3002\", \"Doctrine of the Mean 4\"),\n",
    "        (\"\u8aa0\u8005\uff0c\u5929\u4e4b\u9053\u4e5f\u3002\u8aa0\u4e4b\u8005\uff0c\u4eba\u4e4b\u9053\u4e5f\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u8aa0\u8005\uff0c\u4e0d\u52c9\u800c\u4e2d\uff0c\u4e0d\u601d\u800c\u5f97\uff0c\u5f9e\u5bb9\u4e2d\u9053\uff0c\u8056\u4eba\u4e5f\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u8aa0\u4e4b\u8005\uff0c\u64c7\u5584\u800c\u56fa\u57f7\u4e4b\u8005\u4e5f\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u535a\u5b78\u4e4b\uff0c\u5be9\u554f\u4e4b\uff0c\u614e\u601d\u4e4b\uff0c\u660e\u8fa8\u4e4b\uff0c\u7be4\u884c\u4e4b\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u4eba\u4e00\u80fd\u4e4b\uff0c\u5df1\u767e\u4e4b\uff1b\u4eba\u5341\u80fd\u4e4b\uff0c\u5df1\u5343\u4e4b\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u679c\u80fd\u6b64\u9053\u77e3\uff0c\u96d6\u611a\u5fc5\u660e\uff0c\u96d6\u67d4\u5fc5\u5f37\u3002\", \"Doctrine of the Mean 20\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(zhongyong):\n",
    "        chinese.append({\"id\": f\"cn_zhongyong_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -5})\n",
    "    \n",
    "    # === BOOK OF RITES (\u79ae\u8a18) - 30+ passages ===\n",
    "    liji = [\n",
    "        (\"\u79ae\u5c1a\u5f80\u4f86\u3002\u5f80\u800c\u4e0d\u4f86\uff0c\u975e\u79ae\u4e5f\uff1b\u4f86\u800c\u4e0d\u5f80\uff0c\u4ea6\u975e\u79ae\u4e5f\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u6556\u4e0d\u53ef\u9577\uff0c\u6b32\u4e0d\u53ef\u5f9e\uff0c\u5fd7\u4e0d\u53ef\u6eff\uff0c\u6a02\u4e0d\u53ef\u6975\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u81e8\u8ca1\u6bcb\u830d\u5f97\uff0c\u81e8\u96e3\u6bcb\u830d\u514d\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u592b\u79ae\u8005\uff0c\u81ea\u5351\u800c\u5c0a\u4eba\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u96d6\u8ca0\u8ca9\u8005\uff0c\u5fc5\u6709\u5c0a\u4e5f\uff0c\u800c\u6cc1\u5bcc\u8cb4\u4e4e\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u5bcc\u8cb4\u800c\u77e5\u597d\u79ae\uff0c\u5247\u4e0d\u9a55\u4e0d\u6deb\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u8ca7\u8ce4\u800c\u77e5\u597d\u79ae\uff0c\u5247\u5fd7\u4e0d\u61fe\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u5927\u9053\u4e4b\u884c\u4e5f\uff0c\u5929\u4e0b\u70ba\u516c\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u9078\u8ce2\u8207\u80fd\uff0c\u8b1b\u4fe1\u4fee\u7766\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u6545\u4eba\u4e0d\u7368\u89aa\u5176\u89aa\uff0c\u4e0d\u7368\u5b50\u5176\u5b50\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u4f7f\u8001\u6709\u6240\u7d42\uff0c\u58ef\u6709\u6240\u7528\uff0c\u5e7c\u6709\u6240\u9577\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u77dc\u5be1\u5b64\u7368\u5ee2\u75be\u8005\u7686\u6709\u6240\u990a\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u7537\u6709\u5206\uff0c\u5973\u6709\u6b78\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u8ca8\u60e1\u5176\u68c4\u65bc\u5730\u4e5f\uff0c\u4e0d\u5fc5\u85cf\u65bc\u5df1\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u529b\u60e1\u5176\u4e0d\u51fa\u65bc\u8eab\u4e5f\uff0c\u4e0d\u5fc5\u70ba\u5df1\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u662f\u6545\u8b00\u9589\u800c\u4e0d\u8208\uff0c\u76dc\u7aca\u4e82\u8cca\u800c\u4e0d\u4f5c\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u6545\u5916\u6236\u800c\u4e0d\u9589\uff0c\u662f\u8b02\u5927\u540c\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u7389\u4e0d\u7422\uff0c\u4e0d\u6210\u5668\uff1b\u4eba\u4e0d\u5b78\uff0c\u4e0d\u77e5\u9053\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u662f\u6545\u5b78\u7136\u5f8c\u77e5\u4e0d\u8db3\uff0c\u6559\u7136\u5f8c\u77e5\u56f0\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u77e5\u4e0d\u8db3\uff0c\u7136\u5f8c\u80fd\u81ea\u53cd\u4e5f\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u77e5\u56f0\uff0c\u7136\u5f8c\u80fd\u81ea\u5f37\u4e5f\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u6545\u66f0\uff1a\u6559\u5b78\u76f8\u9577\u4e5f\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u51e1\u5b78\u4e4b\u9053\uff0c\u56b4\u5e2b\u70ba\u96e3\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u5e2b\u56b4\u7136\u5f8c\u9053\u5c0a\uff0c\u9053\u5c0a\u7136\u5f8c\u6c11\u77e5\u656c\u5b78\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u5584\u6b4c\u8005\u4f7f\u4eba\u7e7c\u5176\u8072\uff0c\u5584\u6559\u8005\u4f7f\u4eba\u7e7c\u5176\u5fd7\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u8a18\u554f\u4e4b\u5b78\uff0c\u4e0d\u8db3\u4ee5\u70ba\u4eba\u5e2b\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u5fc5\u4e5f\u5176\u807d\u8a9e\u4e4e\uff0c\u529b\u4e0d\u80fd\u554f\uff0c\u7136\u5f8c\u8a9e\u4e4b\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u8a9e\u4e4b\u800c\u4e0d\u77e5\uff0c\u96d6\u820d\u4e4b\u53ef\u4e5f\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u535a\u5b78\u800c\u4e0d\u7aae\uff0c\u7be4\u884c\u800c\u4e0d\u5026\u3002\", \"Book of Rites - Ruxing\"),\n",
    "        (\"\u541b\u5b50\u4e4b\u65bc\u5b78\u4e5f\uff0c\u85cf\u7109\uff0c\u4fee\u7109\uff0c\u606f\u7109\uff0c\u6e38\u7109\u3002\", \"Book of Rites - Xueji\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(liji):\n",
    "        chinese.append({\"id\": f\"cn_liji_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -3})\n",
    "    \n",
    "    with open('data/raw/chinese/chinese_native.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(chinese, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"  Created {len(chinese)} Chinese passages\")\n",
    "    \n",
    "    # ISLAMIC - 150+ REAL PASSAGES\n",
    "    print(\"\\n[3/4] Islamic texts (150+ real passages)...\")\n",
    "    os.makedirs('data/raw/islamic', exist_ok=True)\n",
    "    \n",
    "    islamic = []\n",
    "    \n",
    "    # === QURANIC VERSES (40+) ===\n",
    "    quran = [\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u062a\u064f\u0644\u064f\u0648\u0627 \u0627\u0644\u0646\u064e\u0651\u0641\u0652\u0633\u064e \u0627\u0644\u064e\u0651\u062a\u0650\u064a \u062d\u064e\u0631\u064e\u0651\u0645\u064e \u0627\u0644\u0644\u064e\u0651\u0647\u064f \u0625\u0650\u0644\u064e\u0651\u0627 \u0628\u0650\u0627\u0644\u0652\u062d\u064e\u0642\u0650\u0651\", \"Quran 6:151\"),\n",
    "        (\"\u0648\u064e\u0628\u0650\u0627\u0644\u0652\u0648\u064e\u0627\u0644\u0650\u062f\u064e\u064a\u0652\u0646\u0650 \u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u064b\u0627\", \"Quran 17:23\"),\n",
    "        (\"\u0625\u0650\u0645\u064e\u0651\u0627 \u064a\u064e\u0628\u0652\u0644\u064f\u063a\u064e\u0646\u064e\u0651 \u0639\u0650\u0646\u062f\u064e\u0643\u064e \u0627\u0644\u0652\u0643\u0650\u0628\u064e\u0631\u064e \u0623\u064e\u062d\u064e\u062f\u064f\u0647\u064f\u0645\u064e\u0627 \u0623\u064e\u0648\u0652 \u0643\u0650\u0644\u064e\u0627\u0647\u064f\u0645\u064e\u0627 \u0641\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u064f\u0644 \u0644\u064e\u0651\u0647\u064f\u0645\u064e\u0627 \u0623\u064f\u0641\u064d\u0651\", \"Quran 17:23\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0646\u0652\u0647\u064e\u0631\u0652\u0647\u064f\u0645\u064e\u0627 \u0648\u064e\u0642\u064f\u0644 \u0644\u064e\u0651\u0647\u064f\u0645\u064e\u0627 \u0642\u064e\u0648\u0652\u0644\u064b\u0627 \u0643\u064e\u0631\u0650\u064a\u0645\u064b\u0627\", \"Quran 17:23\"),\n",
    "        (\"\u0648\u064e\u0627\u062e\u0652\u0641\u0650\u0636\u0652 \u0644\u064e\u0647\u064f\u0645\u064e\u0627 \u062c\u064e\u0646\u064e\u0627\u062d\u064e \u0627\u0644\u0630\u064f\u0651\u0644\u0650\u0651 \u0645\u0650\u0646\u064e \u0627\u0644\u0631\u064e\u0651\u062d\u0652\u0645\u064e\u0629\u0650\", \"Quran 17:24\"),\n",
    "        (\"\u0648\u064e\u0642\u064f\u0644 \u0631\u064e\u0651\u0628\u0650\u0651 \u0627\u0631\u0652\u062d\u064e\u0645\u0652\u0647\u064f\u0645\u064e\u0627 \u0643\u064e\u0645\u064e\u0627 \u0631\u064e\u0628\u064e\u0651\u064a\u064e\u0627\u0646\u0650\u064a \u0635\u064e\u063a\u0650\u064a\u0631\u064b\u0627\", \"Quran 17:24\"),\n",
    "        (\"\u0648\u064e\u0622\u062a\u0650 \u0630\u064e\u0627 \u0627\u0644\u0652\u0642\u064f\u0631\u0652\u0628\u064e\u0649\u0670 \u062d\u064e\u0642\u064e\u0651\u0647\u064f \u0648\u064e\u0627\u0644\u0652\u0645\u0650\u0633\u0652\u0643\u0650\u064a\u0646\u064e \u0648\u064e\u0627\u0628\u0652\u0646\u064e \u0627\u0644\u0633\u064e\u0651\u0628\u0650\u064a\u0644\u0650\", \"Quran 17:26\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064f\u0628\u064e\u0630\u0650\u0651\u0631\u0652 \u062a\u064e\u0628\u0652\u0630\u0650\u064a\u0631\u064b\u0627\", \"Quran 17:26\"),\n",
    "        (\"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0652\u0645\u064f\u0628\u064e\u0630\u0650\u0651\u0631\u0650\u064a\u0646\u064e \u0643\u064e\u0627\u0646\u064f\u0648\u0627 \u0625\u0650\u062e\u0652\u0648\u064e\u0627\u0646\u064e \u0627\u0644\u0634\u064e\u0651\u064a\u064e\u0627\u0637\u0650\u064a\u0646\u0650\", \"Quran 17:27\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u062c\u0652\u0639\u064e\u0644\u0652 \u064a\u064e\u062f\u064e\u0643\u064e \u0645\u064e\u063a\u0652\u0644\u064f\u0648\u0644\u064e\u0629\u064b \u0625\u0650\u0644\u064e\u0649\u0670 \u0639\u064f\u0646\u064f\u0642\u0650\u0643\u064e \u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0628\u0652\u0633\u064f\u0637\u0652\u0647\u064e\u0627 \u0643\u064f\u0644\u064e\u0651 \u0627\u0644\u0652\u0628\u064e\u0633\u0652\u0637\u0650\", \"Quran 17:29\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u0631\u064e\u0628\u064f\u0648\u0627 \u0627\u0644\u0632\u0650\u0651\u0646\u064e\u0627 \u06d6 \u0625\u0650\u0646\u064e\u0651\u0647\u064f \u0643\u064e\u0627\u0646\u064e \u0641\u064e\u0627\u062d\u0650\u0634\u064e\u0629\u064b \u0648\u064e\u0633\u064e\u0627\u0621\u064e \u0633\u064e\u0628\u0650\u064a\u0644\u064b\u0627\", \"Quran 17:32\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u062a\u064f\u0644\u064f\u0648\u0627 \u0623\u064e\u0648\u0652\u0644\u064e\u0627\u062f\u064e\u0643\u064f\u0645\u0652 \u062e\u064e\u0634\u0652\u064a\u064e\u0629\u064e \u0625\u0650\u0645\u0652\u0644\u064e\u0627\u0642\u064d\", \"Quran 17:31\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u0631\u064e\u0628\u064f\u0648\u0627 \u0645\u064e\u0627\u0644\u064e \u0627\u0644\u0652\u064a\u064e\u062a\u0650\u064a\u0645\u0650 \u0625\u0650\u0644\u064e\u0651\u0627 \u0628\u0650\u0627\u0644\u064e\u0651\u062a\u0650\u064a \u0647\u0650\u064a\u064e \u0623\u064e\u062d\u0652\u0633\u064e\u0646\u064f\", \"Quran 17:34\"),\n",
    "        (\"\u0648\u064e\u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u0650 \u06d6 \u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u064e \u0643\u064e\u0627\u0646\u064e \u0645\u064e\u0633\u0652\u0626\u064f\u0648\u0644\u064b\u0627\", \"Quran 17:34\"),\n",
    "        (\"\u0648\u064e\u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0627\u0644\u0652\u0643\u064e\u064a\u0652\u0644\u064e \u0625\u0650\u0630\u064e\u0627 \u0643\u0650\u0644\u0652\u062a\u064f\u0645\u0652 \u0648\u064e\u0632\u0650\u0646\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0642\u0650\u0633\u0652\u0637\u064e\u0627\u0633\u0650 \u0627\u0644\u0652\u0645\u064f\u0633\u0652\u062a\u064e\u0642\u0650\u064a\u0645\u0650\", \"Quran 17:35\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u0641\u064f \u0645\u064e\u0627 \u0644\u064e\u064a\u0652\u0633\u064e \u0644\u064e\u0643\u064e \u0628\u0650\u0647\u0650 \u0639\u0650\u0644\u0652\u0645\u064c\", \"Quran 17:36\"),\n",
    "        (\"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0633\u064e\u0651\u0645\u0652\u0639\u064e \u0648\u064e\u0627\u0644\u0652\u0628\u064e\u0635\u064e\u0631\u064e \u0648\u064e\u0627\u0644\u0652\u0641\u064f\u0624\u064e\u0627\u062f\u064e \u0643\u064f\u0644\u064f\u0651 \u0623\u064f\u0648\u0644\u064e\u0670\u0626\u0650\u0643\u064e \u0643\u064e\u0627\u0646\u064e \u0639\u064e\u0646\u0652\u0647\u064f \u0645\u064e\u0633\u0652\u0626\u064f\u0648\u0644\u064b\u0627\", \"Quran 17:36\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0645\u0652\u0634\u0650 \u0641\u0650\u064a \u0627\u0644\u0652\u0623\u064e\u0631\u0652\u0636\u0650 \u0645\u064e\u0631\u064e\u062d\u064b\u0627\", \"Quran 17:37\"),\n",
    "        (\"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0644\u064e\u0651\u0647\u064e \u064a\u064e\u0623\u0652\u0645\u064f\u0631\u064f \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u062f\u0652\u0644\u0650 \u0648\u064e\u0627\u0644\u0652\u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u0650 \u0648\u064e\u0625\u0650\u064a\u062a\u064e\u0627\u0621\u0650 \u0630\u0650\u064a \u0627\u0644\u0652\u0642\u064f\u0631\u0652\u0628\u064e\u0649\u0670\", \"Quran 16:90\"),\n",
    "        (\"\u0648\u064e\u064a\u064e\u0646\u0652\u0647\u064e\u0649\u0670 \u0639\u064e\u0646\u0650 \u0627\u0644\u0652\u0641\u064e\u062d\u0652\u0634\u064e\u0627\u0621\u0650 \u0648\u064e\u0627\u0644\u0652\u0645\u064f\u0646\u0643\u064e\u0631\u0650 \u0648\u064e\u0627\u0644\u0652\u0628\u064e\u063a\u0652\u064a\u0650\", \"Quran 16:90\"),\n",
    "        (\"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0622\u0645\u064e\u0646\u064f\u0648\u0627 \u0643\u064f\u0648\u0646\u064f\u0648\u0627 \u0642\u064e\u0648\u064e\u0651\u0627\u0645\u0650\u064a\u0646\u064e \u0628\u0650\u0627\u0644\u0652\u0642\u0650\u0633\u0652\u0637\u0650\", \"Quran 4:135\"),\n",
    "        (\"\u0634\u064f\u0647\u064e\u062f\u064e\u0627\u0621\u064e \u0644\u0650\u0644\u064e\u0651\u0647\u0650 \u0648\u064e\u0644\u064e\u0648\u0652 \u0639\u064e\u0644\u064e\u0649\u0670 \u0623\u064e\u0646\u0641\u064f\u0633\u0650\u0643\u064f\u0645\u0652 \u0623\u064e\u0648\u0650 \u0627\u0644\u0652\u0648\u064e\u0627\u0644\u0650\u062f\u064e\u064a\u0652\u0646\u0650 \u0648\u064e\u0627\u0644\u0652\u0623\u064e\u0642\u0652\u0631\u064e\u0628\u0650\u064a\u0646\u064e\", \"Quran 4:135\"),\n",
    "        (\"\u0648\u064e\u0625\u0650\u0630\u064e\u0627 \u062d\u064e\u0643\u064e\u0645\u0652\u062a\u064f\u0645 \u0628\u064e\u064a\u0652\u0646\u064e \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u0650 \u0623\u064e\u0646 \u062a\u064e\u062d\u0652\u0643\u064f\u0645\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u062f\u0652\u0644\u0650\", \"Quran 4:58\"),\n",
    "        (\"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0622\u0645\u064e\u0646\u064f\u0648\u0627 \u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064f\u0642\u064f\u0648\u062f\u0650\", \"Quran 5:1\"),\n",
    "        (\"\u0648\u064e\u062a\u064e\u0639\u064e\u0627\u0648\u064e\u0646\u064f\u0648\u0627 \u0639\u064e\u0644\u064e\u0649 \u0627\u0644\u0652\u0628\u0650\u0631\u0650\u0651 \u0648\u064e\u0627\u0644\u062a\u064e\u0651\u0642\u0652\u0648\u064e\u0649\u0670 \u06d6 \u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0639\u064e\u0627\u0648\u064e\u0646\u064f\u0648\u0627 \u0639\u064e\u0644\u064e\u0649 \u0627\u0644\u0652\u0625\u0650\u062b\u0652\u0645\u0650 \u0648\u064e\u0627\u0644\u0652\u0639\u064f\u062f\u0652\u0648\u064e\u0627\u0646\u0650\", \"Quran 5:2\"),\n",
    "        (\"\u0645\u064e\u0646 \u0642\u064e\u062a\u064e\u0644\u064e \u0646\u064e\u0641\u0652\u0633\u064b\u0627 \u0628\u0650\u063a\u064e\u064a\u0652\u0631\u0650 \u0646\u064e\u0641\u0652\u0633\u064d \u0623\u064e\u0648\u0652 \u0641\u064e\u0633\u064e\u0627\u062f\u064d \u0641\u0650\u064a \u0627\u0644\u0652\u0623\u064e\u0631\u0652\u0636\u0650 \u0641\u064e\u0643\u064e\u0623\u064e\u0646\u064e\u0651\u0645\u064e\u0627 \u0642\u064e\u062a\u064e\u0644\u064e \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u064e \u062c\u064e\u0645\u0650\u064a\u0639\u064b\u0627\", \"Quran 5:32\"),\n",
    "        (\"\u0648\u064e\u0645\u064e\u0646\u0652 \u0623\u064e\u062d\u0652\u064a\u064e\u0627\u0647\u064e\u0627 \u0641\u064e\u0643\u064e\u0623\u064e\u0646\u064e\u0651\u0645\u064e\u0627 \u0623\u064e\u062d\u0652\u064a\u064e\u0627 \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u064e \u062c\u064e\u0645\u0650\u064a\u0639\u064b\u0627\", \"Quran 5:32\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u064a\u064e\u062c\u0652\u0631\u0650\u0645\u064e\u0646\u064e\u0651\u0643\u064f\u0645\u0652 \u0634\u064e\u0646\u064e\u0622\u0646\u064f \u0642\u064e\u0648\u0652\u0645\u064d \u0639\u064e\u0644\u064e\u0649\u0670 \u0623\u064e\u0644\u064e\u0651\u0627 \u062a\u064e\u0639\u0652\u062f\u0650\u0644\u064f\u0648\u0627\", \"Quran 5:8\"),\n",
    "        (\"\u0627\u0639\u0652\u062f\u0650\u0644\u064f\u0648\u0627 \u0647\u064f\u0648\u064e \u0623\u064e\u0642\u0652\u0631\u064e\u0628\u064f \u0644\u0650\u0644\u062a\u064e\u0651\u0642\u0652\u0648\u064e\u0649\u0670\", \"Quran 5:8\"),\n",
    "        (\"\u0644\u064e\u0651\u064a\u0652\u0633\u064e \u0627\u0644\u0652\u0628\u0650\u0631\u064e\u0651 \u0623\u064e\u0646 \u062a\u064f\u0648\u064e\u0644\u064f\u0651\u0648\u0627 \u0648\u064f\u062c\u064f\u0648\u0647\u064e\u0643\u064f\u0645\u0652 \u0642\u0650\u0628\u064e\u0644\u064e \u0627\u0644\u0652\u0645\u064e\u0634\u0652\u0631\u0650\u0642\u0650 \u0648\u064e\u0627\u0644\u0652\u0645\u064e\u063a\u0652\u0631\u0650\u0628\u0650\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0670\u0643\u0650\u0646\u064e\u0651 \u0627\u0644\u0652\u0628\u0650\u0631\u064e\u0651 \u0645\u064e\u0646\u0652 \u0622\u0645\u064e\u0646\u064e \u0628\u0650\u0627\u0644\u0644\u064e\u0651\u0647\u0650 \u0648\u064e\u0627\u0644\u0652\u064a\u064e\u0648\u0652\u0645\u0650 \u0627\u0644\u0652\u0622\u062e\u0650\u0631\u0650\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0622\u062a\u064e\u0649 \u0627\u0644\u0652\u0645\u064e\u0627\u0644\u064e \u0639\u064e\u0644\u064e\u0649\u0670 \u062d\u064f\u0628\u0650\u0651\u0647\u0650 \u0630\u064e\u0648\u0650\u064a \u0627\u0644\u0652\u0642\u064f\u0631\u0652\u0628\u064e\u0649\u0670 \u0648\u064e\u0627\u0644\u0652\u064a\u064e\u062a\u064e\u0627\u0645\u064e\u0649\u0670 \u0648\u064e\u0627\u0644\u0652\u0645\u064e\u0633\u064e\u0627\u0643\u0650\u064a\u0646\u064e\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0627\u0628\u0652\u0646\u064e \u0627\u0644\u0633\u064e\u0651\u0628\u0650\u064a\u0644\u0650 \u0648\u064e\u0627\u0644\u0633\u064e\u0651\u0627\u0626\u0650\u0644\u0650\u064a\u0646\u064e \u0648\u064e\u0641\u0650\u064a \u0627\u0644\u0631\u0650\u0651\u0642\u064e\u0627\u0628\u0650\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0623\u064e\u0642\u064e\u0627\u0645\u064e \u0627\u0644\u0635\u064e\u0651\u0644\u064e\u0627\u0629\u064e \u0648\u064e\u0622\u062a\u064e\u0649 \u0627\u0644\u0632\u064e\u0651\u0643\u064e\u0627\u0629\u064e\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0627\u0644\u0652\u0645\u064f\u0648\u0641\u064f\u0648\u0646\u064e \u0628\u0650\u0639\u064e\u0647\u0652\u062f\u0650\u0647\u0650\u0645\u0652 \u0625\u0650\u0630\u064e\u0627 \u0639\u064e\u0627\u0647\u064e\u062f\u064f\u0648\u0627\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0627\u0644\u0635\u064e\u0651\u0627\u0628\u0650\u0631\u0650\u064a\u0646\u064e \u0641\u0650\u064a \u0627\u0644\u0652\u0628\u064e\u0623\u0652\u0633\u064e\u0627\u0621\u0650 \u0648\u064e\u0627\u0644\u0636\u064e\u0651\u0631\u064e\u0651\u0627\u0621\u0650 \u0648\u064e\u062d\u0650\u064a\u0646\u064e \u0627\u0644\u0652\u0628\u064e\u0623\u0652\u0633\u0650\", \"Quran 2:177\"),\n",
    "        (\"\u062e\u064f\u0630\u0650 \u0627\u0644\u0652\u0639\u064e\u0641\u0652\u0648\u064e \u0648\u064e\u0623\u0652\u0645\u064f\u0631\u0652 \u0628\u0650\u0627\u0644\u0652\u0639\u064f\u0631\u0652\u0641\u0650 \u0648\u064e\u0623\u064e\u0639\u0652\u0631\u0650\u0636\u0652 \u0639\u064e\u0646\u0650 \u0627\u0644\u0652\u062c\u064e\u0627\u0647\u0650\u0644\u0650\u064a\u0646\u064e\", \"Quran 7:199\"),\n",
    "        (\"\u0648\u064e\u0627\u0644\u0652\u0643\u064e\u0627\u0638\u0650\u0645\u0650\u064a\u0646\u064e \u0627\u0644\u0652\u063a\u064e\u064a\u0652\u0638\u064e \u0648\u064e\u0627\u0644\u0652\u0639\u064e\u0627\u0641\u0650\u064a\u0646\u064e \u0639\u064e\u0646\u0650 \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u0650\", \"Quran 3:134\"),\n",
    "        (\"\u0648\u064e\u0627\u0644\u0644\u064e\u0651\u0647\u064f \u064a\u064f\u062d\u0650\u0628\u064f\u0651 \u0627\u0644\u0652\u0645\u064f\u062d\u0652\u0633\u0650\u0646\u0650\u064a\u0646\u064e\", \"Quran 3:134\"),\n",
    "        (\"\u0627\u062f\u0652\u0641\u064e\u0639\u0652 \u0628\u0650\u0627\u0644\u064e\u0651\u062a\u0650\u064a \u0647\u0650\u064a\u064e \u0623\u064e\u062d\u0652\u0633\u064e\u0646\u064f \u0641\u064e\u0625\u0650\u0630\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a \u0628\u064e\u064a\u0652\u0646\u064e\u0643\u064e \u0648\u064e\u0628\u064e\u064a\u0652\u0646\u064e\u0647\u064f \u0639\u064e\u062f\u064e\u0627\u0648\u064e\u0629\u064c \u0643\u064e\u0623\u064e\u0646\u064e\u0651\u0647\u064f \u0648\u064e\u0644\u0650\u064a\u064c\u0651 \u062d\u064e\u0645\u0650\u064a\u0645\u064c\", \"Quran 41:34\"),\n",
    "        (\"\u0648\u064e\u0645\u064e\u0627 \u064a\u064f\u0644\u064e\u0642\u064e\u0651\u0627\u0647\u064e\u0627 \u0625\u0650\u0644\u064e\u0651\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0635\u064e\u0628\u064e\u0631\u064f\u0648\u0627 \u0648\u064e\u0645\u064e\u0627 \u064a\u064f\u0644\u064e\u0642\u064e\u0651\u0627\u0647\u064e\u0627 \u0625\u0650\u0644\u064e\u0651\u0627 \u0630\u064f\u0648 \u062d\u064e\u0638\u064d\u0651 \u0639\u064e\u0638\u0650\u064a\u0645\u064d\", \"Quran 41:35\"),\n",
    "        (\"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0644\u064e\u0651\u0647\u064e \u064a\u064e\u0623\u0652\u0645\u064f\u0631\u064f\u0643\u064f\u0645\u0652 \u0623\u064e\u0646 \u062a\u064f\u0624\u064e\u062f\u064f\u0651\u0648\u0627 \u0627\u0644\u0652\u0623\u064e\u0645\u064e\u0627\u0646\u064e\u0627\u062a\u0650 \u0625\u0650\u0644\u064e\u0649\u0670 \u0623\u064e\u0647\u0652\u0644\u0650\u0647\u064e\u0627\", \"Quran 4:58\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(quran):\n",
    "        islamic.append({\"id\": f\"quran_{i}\", \"text\": text, \"source\": source, \"period\": \"QURANIC\", \"century\": 7})\n",
    "    \n",
    "    # === HADITH (110+) ===\n",
    "    hadith = [\n",
    "        (\"\u0644\u0627 \u0636\u0631\u0631 \u0648\u0644\u0627 \u0636\u0631\u0627\u0631\", \"Hadith - Ibn Majah\"),\n",
    "        (\"\u0625\u0646\u0645\u0627 \u0627\u0644\u0623\u0639\u0645\u0627\u0644 \u0628\u0627\u0644\u0646\u064a\u0627\u062a \u0648\u0625\u0646\u0645\u0627 \u0644\u0643\u0644 \u0627\u0645\u0631\u0626 \u0645\u0627 \u0646\u0648\u0649\", \"Hadith - Bukhari 1\"),\n",
    "        (\"\u0627\u0644\u0645\u0633\u0644\u0645 \u0645\u0646 \u0633\u0644\u0645 \u0627\u0644\u0645\u0633\u0644\u0645\u0648\u0646 \u0645\u0646 \u0644\u0633\u0627\u0646\u0647 \u0648\u064a\u062f\u0647\", \"Hadith - Bukhari 10\"),\n",
    "        (\"\u0644\u0627 \u064a\u0624\u0645\u0646 \u0623\u062d\u062f\u0643\u0645 \u062d\u062a\u0649 \u064a\u062d\u0628 \u0644\u0623\u062e\u064a\u0647 \u0645\u0627 \u064a\u062d\u0628 \u0644\u0646\u0641\u0633\u0647\", \"Hadith - Bukhari 13\"),\n",
    "        (\"\u0645\u0646 \u0643\u0627\u0646 \u064a\u0624\u0645\u0646 \u0628\u0627\u0644\u0644\u0647 \u0648\u0627\u0644\u064a\u0648\u0645 \u0627\u0644\u0622\u062e\u0631 \u0641\u0644\u064a\u0642\u0644 \u062e\u064a\u0631\u0627 \u0623\u0648 \u0644\u064a\u0635\u0645\u062a\", \"Hadith - Bukhari 6018\"),\n",
    "        (\"\u0645\u0646 \u0643\u0627\u0646 \u064a\u0624\u0645\u0646 \u0628\u0627\u0644\u0644\u0647 \u0648\u0627\u0644\u064a\u0648\u0645 \u0627\u0644\u0622\u062e\u0631 \u0641\u0644\u064a\u0643\u0631\u0645 \u0636\u064a\u0641\u0647\", \"Hadith - Bukhari 6019\"),\n",
    "        (\"\u0645\u0646 \u0643\u0627\u0646 \u064a\u0624\u0645\u0646 \u0628\u0627\u0644\u0644\u0647 \u0648\u0627\u0644\u064a\u0648\u0645 \u0627\u0644\u0622\u062e\u0631 \u0641\u0644\u064a\u0635\u0644 \u0631\u062d\u0645\u0647\", \"Hadith - Bukhari 6138\"),\n",
    "        (\"\u0627\u0631\u062d\u0645\u0648\u0627 \u0645\u0646 \u0641\u064a \u0627\u0644\u0623\u0631\u0636 \u064a\u0631\u062d\u0645\u0643\u0645 \u0645\u0646 \u0641\u064a \u0627\u0644\u0633\u0645\u0627\u0621\", \"Hadith - Tirmidhi 1924\"),\n",
    "        (\"\u0627\u0644\u0631\u0627\u062d\u0645\u0648\u0646 \u064a\u0631\u062d\u0645\u0647\u0645 \u0627\u0644\u0631\u062d\u0645\u0646\", \"Hadith - Abu Dawud 4941\"),\n",
    "        (\"\u0644\u064a\u0633 \u0645\u0646\u0627 \u0645\u0646 \u0644\u0645 \u064a\u0631\u062d\u0645 \u0635\u063a\u064a\u0631\u0646\u0627 \u0648\u064a\u0648\u0642\u0631 \u0643\u0628\u064a\u0631\u0646\u0627\", \"Hadith - Tirmidhi 1919\"),\n",
    "        (\"\u062e\u064a\u0631\u0643\u0645 \u062e\u064a\u0631\u0643\u0645 \u0644\u0623\u0647\u0644\u0647 \u0648\u0623\u0646\u0627 \u062e\u064a\u0631\u0643\u0645 \u0644\u0623\u0647\u0644\u064a\", \"Hadith - Tirmidhi 3895\"),\n",
    "        (\"\u0627\u062a\u0642 \u0627\u0644\u0644\u0647 \u062d\u064a\u062b\u0645\u0627 \u0643\u0646\u062a \u0648\u0623\u062a\u0628\u0639 \u0627\u0644\u0633\u064a\u0626\u0629 \u0627\u0644\u062d\u0633\u0646\u0629 \u062a\u0645\u062d\u0647\u0627\", \"Hadith - Tirmidhi 1987\"),\n",
    "        (\"\u0648\u062e\u0627\u0644\u0642 \u0627\u0644\u0646\u0627\u0633 \u0628\u062e\u0644\u0642 \u062d\u0633\u0646\", \"Hadith - Tirmidhi 1987\"),\n",
    "        (\"\u0623\u0643\u0645\u0644 \u0627\u0644\u0645\u0624\u0645\u0646\u064a\u0646 \u0625\u064a\u0645\u0627\u0646\u0627 \u0623\u062d\u0633\u0646\u0647\u0645 \u062e\u0644\u0642\u0627\", \"Hadith - Abu Dawud 4682\"),\n",
    "        (\"\u0625\u0646 \u0645\u0646 \u0623\u062d\u0628\u0643\u0645 \u0625\u0644\u064a \u0648\u0623\u0642\u0631\u0628\u0643\u0645 \u0645\u0646\u064a \u0645\u062c\u0644\u0633\u0627 \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629 \u0623\u062d\u0627\u0633\u0646\u0643\u0645 \u0623\u062e\u0644\u0627\u0642\u0627\", \"Hadith - Tirmidhi 2018\"),\n",
    "        (\"\u0645\u0627 \u0645\u0646 \u0634\u064a\u0621 \u0623\u062b\u0642\u0644 \u0641\u064a \u0645\u064a\u0632\u0627\u0646 \u0627\u0644\u0645\u0624\u0645\u0646 \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629 \u0645\u0646 \u062d\u0633\u0646 \u0627\u0644\u062e\u0644\u0642\", \"Hadith - Tirmidhi 2002\"),\n",
    "        (\"\u0627\u0644\u0628\u0631 \u062d\u0633\u0646 \u0627\u0644\u062e\u0644\u0642 \u0648\u0627\u0644\u0625\u062b\u0645 \u0645\u0627 \u062d\u0627\u0643 \u0641\u064a \u0635\u062f\u0631\u0643 \u0648\u0643\u0631\u0647\u062a \u0623\u0646 \u064a\u0637\u0644\u0639 \u0639\u0644\u064a\u0647 \u0627\u0644\u0646\u0627\u0633\", \"Hadith - Muslim 2553\"),\n",
    "        (\"\u0627\u0644\u062d\u064a\u0627\u0621 \u0645\u0646 \u0627\u0644\u0625\u064a\u0645\u0627\u0646\", \"Hadith - Bukhari 24\"),\n",
    "        (\"\u0627\u0644\u062d\u064a\u0627\u0621 \u0644\u0627 \u064a\u0623\u062a\u064a \u0625\u0644\u0627 \u0628\u062e\u064a\u0631\", \"Hadith - Bukhari 6117\"),\n",
    "        (\"\u0625\u0646 \u0627\u0644\u0644\u0647 \u0631\u0641\u064a\u0642 \u064a\u062d\u0628 \u0627\u0644\u0631\u0641\u0642 \u0641\u064a \u0627\u0644\u0623\u0645\u0631 \u0643\u0644\u0647\", \"Hadith - Bukhari 6927\"),\n",
    "        (\"\u0645\u0627 \u0643\u0627\u0646 \u0627\u0644\u0631\u0641\u0642 \u0641\u064a \u0634\u064a\u0621 \u0625\u0644\u0627 \u0632\u0627\u0646\u0647 \u0648\u0645\u0627 \u0646\u0632\u0639 \u0645\u0646 \u0634\u064a\u0621 \u0625\u0644\u0627 \u0634\u0627\u0646\u0647\", \"Hadith - Muslim 2594\"),\n",
    "        (\"\u0645\u0646 \u064a\u062d\u0631\u0645 \u0627\u0644\u0631\u0641\u0642 \u064a\u062d\u0631\u0645 \u0627\u0644\u062e\u064a\u0631 \u0643\u0644\u0647\", \"Hadith - Muslim 2592\"),\n",
    "        (\"\u0623\u062f \u0627\u0644\u0623\u0645\u0627\u0646\u0629 \u0625\u0644\u0649 \u0645\u0646 \u0627\u0626\u062a\u0645\u0646\u0643 \u0648\u0644\u0627 \u062a\u062e\u0646 \u0645\u0646 \u062e\u0627\u0646\u0643\", \"Hadith - Abu Dawud 3535\"),\n",
    "        (\"\u0622\u064a\u0629 \u0627\u0644\u0645\u0646\u0627\u0641\u0642 \u062b\u0644\u0627\u062b \u0625\u0630\u0627 \u062d\u062f\u062b \u0643\u0630\u0628 \u0648\u0625\u0630\u0627 \u0648\u0639\u062f \u0623\u062e\u0644\u0641 \u0648\u0625\u0630\u0627 \u0627\u0624\u062a\u0645\u0646 \u062e\u0627\u0646\", \"Hadith - Bukhari 33\"),\n",
    "        (\"\u0627\u0644\u0635\u062f\u0642 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0628\u0631 \u0648\u0627\u0644\u0628\u0631 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u062c\u0646\u0629\", \"Hadith - Bukhari 6094\"),\n",
    "        (\"\u0648\u0625\u0646 \u0627\u0644\u0643\u0630\u0628 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0641\u062c\u0648\u0631 \u0648\u0627\u0644\u0641\u062c\u0648\u0631 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0646\u0627\u0631\", \"Hadith - Bukhari 6094\"),\n",
    "        (\"\u0639\u0644\u064a\u0643\u0645 \u0628\u0627\u0644\u0635\u062f\u0642 \u0641\u0625\u0646 \u0627\u0644\u0635\u062f\u0642 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0628\u0631\", \"Hadith - Muslim 2607\"),\n",
    "        (\"\u0625\u064a\u0627\u0643\u0645 \u0648\u0627\u0644\u0643\u0630\u0628 \u0641\u0625\u0646 \u0627\u0644\u0643\u0630\u0628 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0641\u062c\u0648\u0631\", \"Hadith - Muslim 2607\"),\n",
    "        (\"\u0645\u0646 \u063a\u0634\u0646\u0627 \u0641\u0644\u064a\u0633 \u0645\u0646\u0627\", \"Hadith - Muslim 101\"),\n",
    "        (\"\u0643\u0644\u0643\u0645 \u0631\u0627\u0639 \u0648\u0643\u0644\u0643\u0645 \u0645\u0633\u0624\u0648\u0644 \u0639\u0646 \u0631\u0639\u064a\u062a\u0647\", \"Hadith - Bukhari 893\"),\n",
    "        (\"\u0627\u0644\u0625\u0645\u0627\u0645 \u0631\u0627\u0639 \u0648\u0645\u0633\u0624\u0648\u0644 \u0639\u0646 \u0631\u0639\u064a\u062a\u0647\", \"Hadith - Bukhari 893\"),\n",
    "        (\"\u0648\u0627\u0644\u0631\u062c\u0644 \u0631\u0627\u0639 \u0641\u064a \u0623\u0647\u0644\u0647 \u0648\u0645\u0633\u0624\u0648\u0644 \u0639\u0646 \u0631\u0639\u064a\u062a\u0647\", \"Hadith - Bukhari 893\"),\n",
    "        (\"\u0648\u0627\u0644\u0645\u0631\u0623\u0629 \u0631\u0627\u0639\u064a\u0629 \u0641\u064a \u0628\u064a\u062a \u0632\u0648\u062c\u0647\u0627 \u0648\u0645\u0633\u0624\u0648\u0644\u0629 \u0639\u0646 \u0631\u0639\u064a\u062a\u0647\u0627\", \"Hadith - Bukhari 893\"),\n",
    "        (\"\u0627\u0646\u0635\u0631 \u0623\u062e\u0627\u0643 \u0638\u0627\u0644\u0645\u0627 \u0623\u0648 \u0645\u0638\u0644\u0648\u0645\u0627\", \"Hadith - Bukhari 2444\"),\n",
    "        (\"\u062a\u0646\u0635\u0631\u0647 \u0625\u0630\u0627 \u0643\u0627\u0646 \u0645\u0638\u0644\u0648\u0645\u0627 \u0623\u0641\u0631\u0623\u064a\u062a \u0625\u0630\u0627 \u0643\u0627\u0646 \u0638\u0627\u0644\u0645\u0627 \u0643\u064a\u0641 \u062a\u0646\u0635\u0631\u0647 \u0642\u0627\u0644 \u062a\u062d\u062c\u0632\u0647 \u0623\u0648 \u062a\u0645\u0646\u0639\u0647 \u0645\u0646 \u0627\u0644\u0638\u0644\u0645 \u0641\u0625\u0646 \u0630\u0644\u0643 \u0646\u0635\u0631\u0647\", \"Hadith - Bukhari 2444\"),\n",
    "        (\"\u0627\u0644\u0645\u0624\u0645\u0646 \u0644\u0644\u0645\u0624\u0645\u0646 \u0643\u0627\u0644\u0628\u0646\u064a\u0627\u0646 \u064a\u0634\u062f \u0628\u0639\u0636\u0647 \u0628\u0639\u0636\u0627\", \"Hadith - Bukhari 481\"),\n",
    "        (\"\u0645\u062b\u0644 \u0627\u0644\u0645\u0624\u0645\u0646\u064a\u0646 \u0641\u064a \u062a\u0648\u0627\u062f\u0647\u0645 \u0648\u062a\u0631\u0627\u062d\u0645\u0647\u0645 \u0648\u062a\u0639\u0627\u0637\u0641\u0647\u0645 \u0645\u062b\u0644 \u0627\u0644\u062c\u0633\u062f \u0627\u0644\u0648\u0627\u062d\u062f\", \"Hadith - Muslim 2586\"),\n",
    "        (\"\u0625\u0630\u0627 \u0627\u0634\u062a\u0643\u0649 \u0645\u0646\u0647 \u0639\u0636\u0648 \u062a\u062f\u0627\u0639\u0649 \u0644\u0647 \u0633\u0627\u0626\u0631 \u0627\u0644\u062c\u0633\u062f \u0628\u0627\u0644\u0633\u0647\u0631 \u0648\u0627\u0644\u062d\u0645\u0649\", \"Hadith - Muslim 2586\"),\n",
    "        (\"\u0627\u0644\u0645\u0633\u0644\u0645 \u0623\u062e\u0648 \u0627\u0644\u0645\u0633\u0644\u0645 \u0644\u0627 \u064a\u0638\u0644\u0645\u0647 \u0648\u0644\u0627 \u064a\u0633\u0644\u0645\u0647\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"\u0645\u0646 \u0643\u0627\u0646 \u0641\u064a \u062d\u0627\u062c\u0629 \u0623\u062e\u064a\u0647 \u0643\u0627\u0646 \u0627\u0644\u0644\u0647 \u0641\u064a \u062d\u0627\u062c\u062a\u0647\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"\u0648\u0645\u0646 \u0641\u0631\u062c \u0639\u0646 \u0645\u0633\u0644\u0645 \u0643\u0631\u0628\u0629 \u0641\u0631\u062c \u0627\u0644\u0644\u0647 \u0639\u0646\u0647 \u0643\u0631\u0628\u0629 \u0645\u0646 \u0643\u0631\u0628\u0627\u062a \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"\u0648\u0645\u0646 \u0633\u062a\u0631 \u0645\u0633\u0644\u0645\u0627 \u0633\u062a\u0631\u0647 \u0627\u0644\u0644\u0647 \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"\u0644\u0627 \u062a\u062d\u0627\u0633\u062f\u0648\u0627 \u0648\u0644\u0627 \u062a\u0646\u0627\u062c\u0634\u0648\u0627 \u0648\u0644\u0627 \u062a\u0628\u0627\u063a\u0636\u0648\u0627 \u0648\u0644\u0627 \u062a\u062f\u0627\u0628\u0631\u0648\u0627\", \"Hadith - Muslim 2564\"),\n",
    "        (\"\u0648\u0644\u0627 \u064a\u0628\u0639 \u0628\u0639\u0636\u0643\u0645 \u0639\u0644\u0649 \u0628\u064a\u0639 \u0628\u0639\u0636 \u0648\u0643\u0648\u0646\u0648\u0627 \u0639\u0628\u0627\u062f \u0627\u0644\u0644\u0647 \u0625\u062e\u0648\u0627\u0646\u0627\", \"Hadith - Muslim 2564\"),\n",
    "        (\"\u0628\u062d\u0633\u0628 \u0627\u0645\u0631\u0626 \u0645\u0646 \u0627\u0644\u0634\u0631 \u0623\u0646 \u064a\u062d\u0642\u0631 \u0623\u062e\u0627\u0647 \u0627\u0644\u0645\u0633\u0644\u0645\", \"Hadith - Muslim 2564\"),\n",
    "        (\"\u0643\u0644 \u0627\u0644\u0645\u0633\u0644\u0645 \u0639\u0644\u0649 \u0627\u0644\u0645\u0633\u0644\u0645 \u062d\u0631\u0627\u0645 \u062f\u0645\u0647 \u0648\u0645\u0627\u0644\u0647 \u0648\u0639\u0631\u0636\u0647\", \"Hadith - Muslim 2564\"),\n",
    "        (\"\u0625\u064a\u0627\u0643\u0645 \u0648\u0627\u0644\u0638\u0646 \u0641\u0625\u0646 \u0627\u0644\u0638\u0646 \u0623\u0643\u0630\u0628 \u0627\u0644\u062d\u062f\u064a\u062b\", \"Hadith - Bukhari 6064\"),\n",
    "        (\"\u0648\u0644\u0627 \u062a\u062c\u0633\u0633\u0648\u0627 \u0648\u0644\u0627 \u062a\u062d\u0633\u0633\u0648\u0627 \u0648\u0644\u0627 \u062a\u0646\u0627\u0641\u0633\u0648\u0627\", \"Hadith - Bukhari 6064\"),\n",
    "        (\"\u0627\u0644\u0638\u0644\u0645 \u0638\u0644\u0645\u0627\u062a \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629\", \"Hadith - Bukhari 2447\"),\n",
    "        (\"\u0627\u062a\u0642\u0648\u0627 \u0627\u0644\u0638\u0644\u0645 \u0641\u0625\u0646 \u0627\u0644\u0638\u0644\u0645 \u0638\u0644\u0645\u0627\u062a \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629\", \"Hadith - Muslim 2578\"),\n",
    "        (\"\u0648\u0627\u062a\u0642\u0648\u0627 \u0627\u0644\u0634\u062d \u0641\u0625\u0646 \u0627\u0644\u0634\u062d \u0623\u0647\u0644\u0643 \u0645\u0646 \u0643\u0627\u0646 \u0642\u0628\u0644\u0643\u0645\", \"Hadith - Muslim 2578\"),\n",
    "        (\"\u0623\u0641\u0636\u0644 \u0627\u0644\u062c\u0647\u0627\u062f \u0643\u0644\u0645\u0629 \u0639\u062f\u0644 \u0639\u0646\u062f \u0633\u0644\u0637\u0627\u0646 \u062c\u0627\u0626\u0631\", \"Hadith - Abu Dawud 4344\"),\n",
    "        (\"\u0633\u064a\u062f \u0627\u0644\u0634\u0647\u062f\u0627\u0621 \u062d\u0645\u0632\u0629 \u0628\u0646 \u0639\u0628\u062f \u0627\u0644\u0645\u0637\u0644\u0628 \u0648\u0631\u062c\u0644 \u0642\u0627\u0645 \u0625\u0644\u0649 \u0625\u0645\u0627\u0645 \u062c\u0627\u0626\u0631 \u0641\u0623\u0645\u0631\u0647 \u0648\u0646\u0647\u0627\u0647 \u0641\u0642\u062a\u0644\u0647\", \"Hadith - Hakim 4884\"),\n",
    "        (\"\u0625\u0630\u0627 \u0631\u0623\u064a\u062a \u0623\u0645\u062a\u064a \u062a\u0647\u0627\u0628 \u0623\u0646 \u062a\u0642\u0648\u0644 \u0644\u0644\u0638\u0627\u0644\u0645 \u064a\u0627 \u0638\u0627\u0644\u0645 \u0641\u0642\u062f \u062a\u0648\u062f\u0639 \u0645\u0646\u0647\u0645\", \"Hadith - Ahmad 6521\"),\n",
    "        (\"\u0645\u0646 \u0631\u0623\u0649 \u0645\u0646\u0643\u0645 \u0645\u0646\u0643\u0631\u0627 \u0641\u0644\u064a\u063a\u064a\u0631\u0647 \u0628\u064a\u062f\u0647\", \"Hadith - Muslim 49\"),\n",
    "        (\"\u0641\u0625\u0646 \u0644\u0645 \u064a\u0633\u062a\u0637\u0639 \u0641\u0628\u0644\u0633\u0627\u0646\u0647 \u0641\u0625\u0646 \u0644\u0645 \u064a\u0633\u062a\u0637\u0639 \u0641\u0628\u0642\u0644\u0628\u0647 \u0648\u0630\u0644\u0643 \u0623\u0636\u0639\u0641 \u0627\u0644\u0625\u064a\u0645\u0627\u0646\", \"Hadith - Muslim 49\"),\n",
    "        (\"\u0623\u062d\u0628 \u0627\u0644\u0646\u0627\u0633 \u0625\u0644\u0649 \u0627\u0644\u0644\u0647 \u0623\u0646\u0641\u0639\u0647\u0645 \u0644\u0644\u0646\u0627\u0633\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"\u0648\u0623\u062d\u0628 \u0627\u0644\u0623\u0639\u0645\u0627\u0644 \u0625\u0644\u0649 \u0627\u0644\u0644\u0647 \u0633\u0631\u0648\u0631 \u062a\u062f\u062e\u0644\u0647 \u0639\u0644\u0649 \u0645\u0633\u0644\u0645\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"\u0623\u0648 \u062a\u0643\u0634\u0641 \u0639\u0646\u0647 \u0643\u0631\u0628\u0629 \u0623\u0648 \u062a\u0642\u0636\u064a \u0639\u0646\u0647 \u062f\u064a\u0646\u0627 \u0623\u0648 \u062a\u0637\u0631\u062f \u0639\u0646\u0647 \u062c\u0648\u0639\u0627\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"\u0648\u0644\u0623\u0646 \u0623\u0645\u0634\u064a \u0645\u0639 \u0623\u062e\u064a \u0641\u064a \u062d\u0627\u062c\u0629 \u0623\u062d\u0628 \u0625\u0644\u064a \u0645\u0646 \u0623\u0646 \u0623\u0639\u062a\u0643\u0641 \u0641\u064a \u0647\u0630\u0627 \u0627\u0644\u0645\u0633\u062c\u062f \u0634\u0647\u0631\u0627\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"\u0627\u0644\u062f\u064a\u0646 \u0627\u0644\u0646\u0635\u064a\u062d\u0629 \u0642\u0644\u0646\u0627 \u0644\u0645\u0646 \u0642\u0627\u0644 \u0644\u0644\u0647 \u0648\u0644\u0643\u062a\u0627\u0628\u0647 \u0648\u0644\u0631\u0633\u0648\u0644\u0647 \u0648\u0644\u0623\u0626\u0645\u0629 \u0627\u0644\u0645\u0633\u0644\u0645\u064a\u0646 \u0648\u0639\u0627\u0645\u062a\u0647\u0645\", \"Hadith - Muslim 55\"),\n",
    "        (\"\u0645\u0627 \u0646\u0642\u0635\u062a \u0635\u062f\u0642\u0629 \u0645\u0646 \u0645\u0627\u0644\", \"Hadith - Muslim 2588\"),\n",
    "        (\"\u0648\u0645\u0627 \u0632\u0627\u062f \u0627\u0644\u0644\u0647 \u0639\u0628\u062f\u0627 \u0628\u0639\u0641\u0648 \u0625\u0644\u0627 \u0639\u0632\u0627\", \"Hadith - Muslim 2588\"),\n",
    "        (\"\u0648\u0645\u0627 \u062a\u0648\u0627\u0636\u0639 \u0623\u062d\u062f \u0644\u0644\u0647 \u0625\u0644\u0627 \u0631\u0641\u0639\u0647 \u0627\u0644\u0644\u0647\", \"Hadith - Muslim 2588\"),\n",
    "        (\"\u0627\u0644\u064a\u062f \u0627\u0644\u0639\u0644\u064a\u0627 \u062e\u064a\u0631 \u0645\u0646 \u0627\u0644\u064a\u062f \u0627\u0644\u0633\u0641\u0644\u0649\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"\u0648\u0627\u0628\u062f\u0623 \u0628\u0645\u0646 \u062a\u0639\u0648\u0644\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"\u0648\u062e\u064a\u0631 \u0627\u0644\u0635\u062f\u0642\u0629 \u0645\u0627 \u0643\u0627\u0646 \u0639\u0646 \u0638\u0647\u0631 \u063a\u0646\u0649\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"\u0645\u0646 \u0627\u0633\u062a\u0637\u0627\u0639 \u0645\u0646\u0643\u0645 \u0627\u0644\u0628\u0627\u0621\u0629 \u0641\u0644\u064a\u062a\u0632\u0648\u062c\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"\u0641\u0625\u0646\u0647 \u0623\u063a\u0636 \u0644\u0644\u0628\u0635\u0631 \u0648\u0623\u062d\u0635\u0646 \u0644\u0644\u0641\u0631\u062c\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"\u0648\u0645\u0646 \u0644\u0645 \u064a\u0633\u062a\u0637\u0639 \u0641\u0639\u0644\u064a\u0647 \u0628\u0627\u0644\u0635\u0648\u0645 \u0641\u0625\u0646\u0647 \u0644\u0647 \u0648\u062c\u0627\u0621\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"\u0627\u0633\u062a\u0648\u0635\u0648\u0627 \u0628\u0627\u0644\u0646\u0633\u0627\u0621 \u062e\u064a\u0631\u0627\", \"Hadith - Bukhari 3331\"),\n",
    "        (\"\u062e\u0630\u0648\u0627 \u0639\u0646\u064a \u062e\u0630\u0648\u0627 \u0639\u0646\u064a \u0642\u062f \u062c\u0639\u0644 \u0627\u0644\u0644\u0647 \u0644\u0647\u0646 \u0633\u0628\u064a\u0644\u0627 \u0627\u0644\u0628\u0643\u0631 \u0628\u0627\u0644\u0628\u0643\u0631 \u062c\u0644\u062f \u0645\u0627\u0626\u0629 \u0648\u0646\u0641\u064a \u0633\u0646\u0629\", \"Hadith - Muslim 1690\"),\n",
    "        (\"\u0644\u0627 \u064a\u0641\u0631\u0643 \u0645\u0624\u0645\u0646 \u0645\u0624\u0645\u0646\u0629 \u0625\u0646 \u0643\u0631\u0647 \u0645\u0646\u0647\u0627 \u062e\u0644\u0642\u0627 \u0631\u0636\u064a \u0645\u0646\u0647\u0627 \u0622\u062e\u0631\", \"Hadith - Muslim 1469\"),\n",
    "        (\"\u0623\u0643\u0645\u0644 \u0627\u0644\u0645\u0624\u0645\u0646\u064a\u0646 \u0625\u064a\u0645\u0627\u0646\u0627 \u0623\u062d\u0633\u0646\u0647\u0645 \u062e\u0644\u0642\u0627 \u0648\u062e\u064a\u0627\u0631\u0643\u0645 \u062e\u064a\u0627\u0631\u0643\u0645 \u0644\u0646\u0633\u0627\u0626\u0647\u0645\", \"Hadith - Tirmidhi 1162\"),\n",
    "        (\"\u0645\u0627 \u0623\u0643\u0631\u0645\u0647\u0646 \u0625\u0644\u0627 \u0643\u0631\u064a\u0645 \u0648\u0645\u0627 \u0623\u0647\u0627\u0646\u0647\u0646 \u0625\u0644\u0627 \u0644\u0626\u064a\u0645\", \"Hadith - Ibn Asakir\"),\n",
    "        (\"\u0627\u0644\u0644\u0647\u0645 \u0625\u0646\u064a \u0623\u062d\u0631\u062c \u062d\u0642 \u0627\u0644\u0636\u0639\u064a\u0641\u064a\u0646 \u0627\u0644\u064a\u062a\u064a\u0645 \u0648\u0627\u0644\u0645\u0631\u0623\u0629\", \"Hadith - Ahmad 9664\"),\n",
    "        (\"\u0623\u0644\u0627 \u0623\u062e\u0628\u0631\u0643\u0645 \u0628\u062e\u064a\u0627\u0631\u0643\u0645 \u0642\u0627\u0644\u0648\u0627 \u0628\u0644\u0649 \u0642\u0627\u0644 \u062e\u064a\u0627\u0631\u0643\u0645 \u0623\u062d\u0627\u0633\u0646\u0643\u0645 \u0623\u062e\u0644\u0627\u0642\u0627\", \"Hadith - Bukhari 6035\"),\n",
    "        (\"\u0625\u0646\u0643\u0645 \u0644\u0646 \u062a\u0633\u0639\u0648\u0627 \u0627\u0644\u0646\u0627\u0633 \u0628\u0623\u0645\u0648\u0627\u0644\u0643\u0645 \u0641\u0644\u064a\u0633\u0639\u0647\u0645 \u0645\u0646\u0643\u0645 \u0628\u0633\u0637 \u0627\u0644\u0648\u062c\u0647 \u0648\u062d\u0633\u0646 \u0627\u0644\u062e\u0644\u0642\", \"Hadith - Hakim 422\"),\n",
    "        (\"\u062a\u0628\u0633\u0645\u0643 \u0641\u064a \u0648\u062c\u0647 \u0623\u062e\u064a\u0643 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0648\u0623\u0645\u0631\u0643 \u0628\u0627\u0644\u0645\u0639\u0631\u0648\u0641 \u0648\u0646\u0647\u064a\u0643 \u0639\u0646 \u0627\u0644\u0645\u0646\u0643\u0631 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0648\u0625\u0631\u0634\u0627\u062f\u0643 \u0627\u0644\u0631\u062c\u0644 \u0641\u064a \u0623\u0631\u0636 \u0627\u0644\u0636\u0644\u0627\u0644 \u0644\u0643 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0648\u0625\u0645\u0627\u0637\u062a\u0643 \u0627\u0644\u0623\u0630\u0649 \u0648\u0627\u0644\u0634\u0648\u0643 \u0648\u0627\u0644\u0639\u0638\u0645 \u0639\u0646 \u0627\u0644\u0637\u0631\u064a\u0642 \u0644\u0643 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0648\u0625\u0641\u0631\u0627\u063a\u0643 \u0645\u0646 \u062f\u0644\u0648\u0643 \u0641\u064a \u062f\u0644\u0648 \u0623\u062e\u064a\u0643 \u0644\u0643 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0627\u0644\u0643\u0644\u0645\u0629 \u0627\u0644\u0637\u064a\u0628\u0629 \u0635\u062f\u0642\u0629\", \"Hadith - Bukhari 2989\"),\n",
    "        (\"\u0648\u0643\u0644 \u062e\u0637\u0648\u0629 \u062a\u0645\u0634\u064a\u0647\u0627 \u0625\u0644\u0649 \u0627\u0644\u0635\u0644\u0627\u0629 \u0635\u062f\u0642\u0629\", \"Hadith - Bukhari 2989\"),\n",
    "        (\"\u0645\u0646 \u062f\u0644 \u0639\u0644\u0649 \u062e\u064a\u0631 \u0641\u0644\u0647 \u0645\u062b\u0644 \u0623\u062c\u0631 \u0641\u0627\u0639\u0644\u0647\", \"Hadith - Muslim 1893\"),\n",
    "        (\"\u0644\u064a\u0633 \u0627\u0644\u0634\u062f\u064a\u062f \u0628\u0627\u0644\u0635\u0631\u0639\u0629 \u0625\u0646\u0645\u0627 \u0627\u0644\u0634\u062f\u064a\u062f \u0627\u0644\u0630\u064a \u064a\u0645\u0644\u0643 \u0646\u0641\u0633\u0647 \u0639\u0646\u062f \u0627\u0644\u063a\u0636\u0628\", \"Hadith - Bukhari 6114\"),\n",
    "        (\"\u0644\u0627 \u062a\u063a\u0636\u0628 \u0641\u0631\u062f\u062f \u0645\u0631\u0627\u0631\u0627 \u0642\u0627\u0644 \u0644\u0627 \u062a\u063a\u0636\u0628\", \"Hadith - Bukhari 6116\"),\n",
    "        (\"\u0625\u0646 \u0627\u0644\u063a\u0636\u0628 \u0645\u0646 \u0627\u0644\u0634\u064a\u0637\u0627\u0646 \u0648\u0625\u0646 \u0627\u0644\u0634\u064a\u0637\u0627\u0646 \u062e\u0644\u0642 \u0645\u0646 \u0627\u0644\u0646\u0627\u0631\", \"Hadith - Abu Dawud 4784\"),\n",
    "        (\"\u0648\u0625\u0646\u0645\u0627 \u062a\u0637\u0641\u0623 \u0627\u0644\u0646\u0627\u0631 \u0628\u0627\u0644\u0645\u0627\u0621 \u0641\u0625\u0630\u0627 \u063a\u0636\u0628 \u0623\u062d\u062f\u0643\u0645 \u0641\u0644\u064a\u062a\u0648\u0636\u0623\", \"Hadith - Abu Dawud 4784\"),\n",
    "        (\"\u0644\u0627 \u064a\u062d\u0644 \u0644\u0645\u0633\u0644\u0645 \u0623\u0646 \u064a\u0647\u062c\u0631 \u0623\u062e\u0627\u0647 \u0641\u0648\u0642 \u062b\u0644\u0627\u062b \u0644\u064a\u0627\u0644\", \"Hadith - Bukhari 6077\"),\n",
    "        (\"\u064a\u0644\u062a\u0642\u064a\u0627\u0646 \u0641\u064a\u0639\u0631\u0636 \u0647\u0630\u0627 \u0648\u064a\u0639\u0631\u0636 \u0647\u0630\u0627 \u0648\u062e\u064a\u0631\u0647\u0645\u0627 \u0627\u0644\u0630\u064a \u064a\u0628\u062f\u0623 \u0628\u0627\u0644\u0633\u0644\u0627\u0645\", \"Hadith - Bukhari 6077\"),\n",
    "        (\"\u0623\u0641\u0634\u0648\u0627 \u0627\u0644\u0633\u0644\u0627\u0645 \u0628\u064a\u0646\u0643\u0645\", \"Hadith - Muslim 54\"),\n",
    "        (\"\u0648\u0627\u0644\u0630\u064a \u0646\u0641\u0633\u064a \u0628\u064a\u062f\u0647 \u0644\u0627 \u062a\u062f\u062e\u0644\u0648\u0627 \u0627\u0644\u062c\u0646\u0629 \u062d\u062a\u0649 \u062a\u0624\u0645\u0646\u0648\u0627\", \"Hadith - Muslim 54\"),\n",
    "        (\"\u0648\u0644\u0627 \u062a\u0624\u0645\u0646\u0648\u0627 \u062d\u062a\u0649 \u062a\u062d\u0627\u0628\u0648\u0627 \u0623\u0648\u0644\u0627 \u0623\u062f\u0644\u0643\u0645 \u0639\u0644\u0649 \u0634\u064a\u0621 \u0625\u0630\u0627 \u0641\u0639\u0644\u062a\u0645\u0648\u0647 \u062a\u062d\u0627\u0628\u0628\u062a\u0645 \u0623\u0641\u0634\u0648\u0627 \u0627\u0644\u0633\u0644\u0627\u0645 \u0628\u064a\u0646\u0643\u0645\", \"Hadith - Muslim 54\"),\n",
    "        (\"\u0637\u0639\u0627\u0645 \u0627\u0644\u0627\u062b\u0646\u064a\u0646 \u0643\u0627\u0641\u064a \u0627\u0644\u062b\u0644\u0627\u062b\u0629 \u0648\u0637\u0639\u0627\u0645 \u0627\u0644\u062b\u0644\u0627\u062b\u0629 \u0643\u0627\u0641\u064a \u0627\u0644\u0623\u0631\u0628\u0639\u0629\", \"Hadith - Bukhari 5392\"),\n",
    "        (\"\u0645\u0627 \u0645\u0644\u0623 \u0622\u062f\u0645\u064a \u0648\u0639\u0627\u0621 \u0634\u0631\u0627 \u0645\u0646 \u0628\u0637\u0646\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"\u0628\u062d\u0633\u0628 \u0627\u0628\u0646 \u0622\u062f\u0645 \u0623\u0643\u0644\u0627\u062a \u064a\u0642\u0645\u0646 \u0635\u0644\u0628\u0647\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"\u0641\u0625\u0646 \u0643\u0627\u0646 \u0644\u0627 \u0645\u062d\u0627\u0644\u0629 \u0641\u062b\u0644\u062b \u0644\u0637\u0639\u0627\u0645\u0647 \u0648\u062b\u0644\u062b \u0644\u0634\u0631\u0627\u0628\u0647 \u0648\u062b\u0644\u062b \u0644\u0646\u0641\u0633\u0647\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"\u0625\u0646 \u0627\u0644\u0644\u0647 \u0643\u062a\u0628 \u0627\u0644\u0625\u062d\u0633\u0627\u0646 \u0639\u0644\u0649 \u0643\u0644 \u0634\u064a\u0621\", \"Hadith - Muslim 1955\"),\n",
    "        (\"\u0641\u0625\u0630\u0627 \u0642\u062a\u0644\u062a\u0645 \u0641\u0623\u062d\u0633\u0646\u0648\u0627 \u0627\u0644\u0642\u062a\u0644\u0629 \u0648\u0625\u0630\u0627 \u0630\u0628\u062d\u062a\u0645 \u0641\u0623\u062d\u0633\u0646\u0648\u0627 \u0627\u0644\u0630\u0628\u062d\", \"Hadith - Muslim 1955\"),\n",
    "        (\"\u0648\u0644\u064a\u062d\u062f \u0623\u062d\u062f\u0643\u0645 \u0634\u0641\u0631\u062a\u0647 \u0648\u0644\u064a\u0631\u062d \u0630\u0628\u064a\u062d\u062a\u0647\", \"Hadith - Muslim 1955\"),\n",
    "        (\"\u0639\u0630\u0628\u062a \u0627\u0645\u0631\u0623\u0629 \u0641\u064a \u0647\u0631\u0629 \u0633\u062c\u0646\u062a\u0647\u0627 \u062d\u062a\u0649 \u0645\u0627\u062a\u062a\", \"Hadith - Bukhari 3318\"),\n",
    "        (\"\u0641\u0644\u0627 \u0647\u064a \u0623\u0637\u0639\u0645\u062a\u0647\u0627 \u0648\u0644\u0627 \u0633\u0642\u062a\u0647\u0627 \u0625\u0630 \u062d\u0628\u0633\u062a\u0647\u0627 \u0648\u0644\u0627 \u0647\u064a \u062a\u0631\u0643\u062a\u0647\u0627 \u062a\u0623\u0643\u0644 \u0645\u0646 \u062e\u0634\u0627\u0634 \u0627\u0644\u0623\u0631\u0636\", \"Hadith - Bukhari 3318\"),\n",
    "        (\"\u0628\u064a\u0646\u0645\u0627 \u0631\u062c\u0644 \u064a\u0645\u0634\u064a \u0628\u0637\u0631\u064a\u0642 \u0627\u0634\u062a\u062f \u0639\u0644\u064a\u0647 \u0627\u0644\u0639\u0637\u0634 \u0641\u0648\u062c\u062f \u0628\u0626\u0631\u0627 \u0641\u0646\u0632\u0644 \u0641\u064a\u0647\u0627 \u0641\u0634\u0631\u0628\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u062b\u0645 \u062e\u0631\u062c \u0641\u0625\u0630\u0627 \u0643\u0644\u0628 \u064a\u0644\u0647\u062b \u064a\u0623\u0643\u0644 \u0627\u0644\u062b\u0631\u0649 \u0645\u0646 \u0627\u0644\u0639\u0637\u0634\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u0641\u0642\u0627\u0644 \u0644\u0642\u062f \u0628\u0644\u063a \u0647\u0630\u0627 \u0627\u0644\u0643\u0644\u0628 \u0645\u0646 \u0627\u0644\u0639\u0637\u0634 \u0645\u062b\u0644 \u0627\u0644\u0630\u064a \u0643\u0627\u0646 \u0628\u0644\u063a \u0645\u0646\u064a\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u0641\u0646\u0632\u0644 \u0627\u0644\u0628\u0626\u0631 \u0641\u0645\u0644\u0623 \u062e\u0641\u0647 \u0645\u0627\u0621 \u062b\u0645 \u0623\u0645\u0633\u0643\u0647 \u0628\u0641\u064a\u0647 \u062d\u062a\u0649 \u0631\u0642\u064a \u0641\u0633\u0642\u0649 \u0627\u0644\u0643\u0644\u0628\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u0641\u0634\u0643\u0631 \u0627\u0644\u0644\u0647 \u0644\u0647 \u0641\u063a\u0641\u0631 \u0644\u0647\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u0641\u064a \u0643\u0644 \u0643\u0628\u062f \u0631\u0637\u0628\u0629 \u0623\u062c\u0631\", \"Hadith - Bukhari 2466\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(hadith):\n",
    "        islamic.append({\"id\": f\"hadith_{i}\", \"text\": text, \"source\": source, \"period\": \"HADITH\", \"century\": 9})\n",
    "    \n",
    "    with open('data/raw/islamic/islamic_native.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(islamic, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"  Created {len(islamic)} Islamic passages\")\n",
    "    \n",
    "    # DEAR ABBY\n",
    "    print(\"\\n[4/4] Dear Abby...\")\n",
    "    abby_count = 0\n",
    "    if not os.path.exists('data/raw/dear_abby.csv') or os.path.getsize('data/raw/dear_abby.csv') < 10000:\n",
    "        # Check if in Drive\n",
    "        if 'dear_abby.csv' in DRIVE_FILES:\n",
    "            shutil.copy(f'{SAVE_DIR}/dear_abby.csv', 'data/raw/dear_abby.csv')\n",
    "            print(\"  Loaded from Drive\")\n",
    "        else:\n",
    "            try:\n",
    "                subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", \n",
    "                               \"thedevastator/20000-dear-abby-questions\", \n",
    "                               \"-p\", \"data/raw/\", \"--unzip\"], check=True, timeout=120)\n",
    "                print(\"  Downloaded from Kaggle\")\n",
    "            except:\n",
    "                print(\"  Kaggle failed - creating minimal fallback\")\n",
    "                fallback = [{\"question_only\": f\"Dear Abby, I have a problem {i}\", \"year\": 1990+i%30} for i in range(100)]\n",
    "                pd.DataFrame(fallback).to_csv('data/raw/dear_abby.csv', index=False)\n",
    "    else:\n",
    "        print(\"  Already exists\")\n",
    "    \n",
    "    # Count Dear Abby samples\n",
    "    try:\n",
    "        df = pd.read_csv('data/raw/dear_abby.csv')\n",
    "        abby_count = len([1 for _, row in df.iterrows() if str(row.get('question_only', '')) != 'nan' and 50 <= len(str(row.get('question_only', ''))) <= 2000])\n",
    "    except:\n",
    "        abby_count = 0\n",
    "    \n",
    "    # Warning for insufficient Dear Abby data\n",
    "    if abby_count < 1000:\n",
    "        print(\"\\n\" + \"!\"*60)\n",
    "        print(\"CRITICAL: Dear Abby corpus is too small!\")\n",
    "        print(\"The semitic_to_non_semitic split WILL FAIL without this data.\")\n",
    "        print(\"\\nTo fix:\")\n",
    "        print(\"1. Download from: kaggle.com/datasets/thedevastator/20000-dear-abby-questions\")\n",
    "        print(\"2. Upload dear_abby.csv to your Google Drive BIP_v10 folder\")\n",
    "        print(\"3. Set REFRESH_DATA_FROM_SOURCE = True and rerun\")\n",
    "        print(\"!\"*60 + \"\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Downloads complete\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. Patterns + Normalization { display-mode: \"form\" }\n",
    "#@markdown Complete native patterns for moral concepts in 5 languages\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from enum import Enum, auto\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEXT NORMALIZATION & PATTERNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== TEXT NORMALIZATION =====\n",
    "def normalize_hebrew(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'[\\u0591-\\u05C7]', '', text)  # Remove nikud\n",
    "    for final, regular in [('\\u05da','\\u05db'), ('\\u05dd','\\u05de'), ('\\u05df','\\u05e0'), ('\\u05e3','\\u05e4'), ('\\u05e5','\\u05e6')]:\n",
    "        text = text.replace(final, regular)\n",
    "    return text\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'[\\u064B-\\u065F]', '', text)  # Remove tashkeel\n",
    "    text = text.replace('\\u0640', '')  # Remove tatweel\n",
    "    for v in ['\\u0623', '\\u0625', '\\u0622', '\\u0671']:\n",
    "        text = text.replace(v, '\\u0627')\n",
    "    text = text.replace('\\u0629', '\\u0647').replace('\\u0649', '\\u064a')\n",
    "    return text\n",
    "\n",
    "def normalize_text(text, language):\n",
    "    if language in ['hebrew', 'aramaic']:\n",
    "        return normalize_hebrew(text)\n",
    "    elif language == 'arabic':\n",
    "        return normalize_arabic(text)\n",
    "    elif language == 'classical_chinese':\n",
    "        return unicodedata.normalize('NFKC', text)\n",
    "    else:\n",
    "        return unicodedata.normalize('NFKC', text.lower())\n",
    "\n",
    "# ===== BOND AND HOHFELD TYPES =====\n",
    "class BondType(Enum):\n",
    "    HARM_PREVENTION = auto()\n",
    "    RECIPROCITY = auto()\n",
    "    AUTONOMY = auto()\n",
    "    PROPERTY = auto()\n",
    "    FAMILY = auto()\n",
    "    AUTHORITY = auto()\n",
    "    CARE = auto()\n",
    "    FAIRNESS = auto()\n",
    "    CONTRACT = auto()\n",
    "    NONE = auto()\n",
    "\n",
    "class HohfeldState(Enum):\n",
    "    OBLIGATION = auto()\n",
    "    RIGHT = auto()\n",
    "    LIBERTY = auto()\n",
    "    NO_RIGHT = auto()\n",
    "\n",
    "# ===== COMPLETE BOND PATTERNS =====\n",
    "ALL_BOND_PATTERNS = {\n",
    "    'hebrew': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u05d4\\u05e8\\u05d2', r'\\u05e8\\u05e6\\u05d7', r'\\u05e0\\u05d6\\u05e7', r'\\u05d4\\u05db\\u05d4', r'\\u05d4\\u05e6\\u05d9\\u05dc', r'\\u05e9\\u05de\\u05e8', r'\\u05e4\\u05e7\\u05d5\\u05d7.\\u05e0\\u05e4\\u05e9'],\n",
    "        BondType.RECIPROCITY: [r'\\u05d2\\u05de\\u05d5\\u05dc', r'\\u05d4\\u05e9\\u05d9\\u05d1', r'\\u05e4\\u05e8\\u05e2', r'\\u05e0\\u05ea\\u05df.*\\u05e7\\u05d1\\u05dc', r'\\u05de\\u05d3\\u05d4.\\u05db\\u05e0\\u05d2\\u05d3'],\n",
    "        BondType.AUTONOMY: [r'\\u05d1\\u05d7\\u05e8', r'\\u05e8\\u05e6\\u05d5\\u05df', r'\\u05d7\\u05e4\\u05e9', r'\\u05e2\\u05e6\\u05de'],\n",
    "        BondType.PROPERTY: [r'\\u05e7\\u05e0\\u05d4', r'\\u05de\\u05db\\u05e8', r'\\u05d2\\u05d6\\u05dc', r'\\u05d2\\u05e0\\u05d1', r'\\u05de\\u05de\\u05d5\\u05df', r'\\u05e0\\u05db\\u05e1', r'\\u05d9\\u05e8\\u05e9'],\n",
    "        BondType.FAMILY: [r'\\u05d0\\u05d1', r'\\u05d0\\u05de', r'\\u05d1\\u05e0', r'\\u05db\\u05d1\\u05d3.*\\u05d0\\u05d1', r'\\u05db\\u05d1\\u05d3.*\\u05d0\\u05de', r'\\u05de\\u05e9\\u05e4\\u05d7\\u05d4', r'\\u05d0\\u05d7', r'\\u05d0\\u05d7\\u05d5\\u05ea'],\n",
    "        BondType.AUTHORITY: [r'\\u05de\\u05dc\\u05db', r'\\u05e9\\u05d5\\u05e4\\u05d8', r'\\u05e6\\u05d5\\u05d4', r'\\u05ea\\u05d5\\u05e8\\u05d4', r'\\u05de\\u05e6\\u05d5\\u05d4', r'\\u05d3\\u05d9\\u05df', r'\\u05d7\\u05e7'],\n",
    "        BondType.CARE: [r'\\u05d7\\u05e1\\u05d3', r'\\u05e8\\u05d7\\u05de', r'\\u05e2\\u05d6\\u05e8', r'\\u05ea\\u05de\\u05db', r'\\u05e6\\u05d3\\u05e7\\u05d4'],\n",
    "        BondType.FAIRNESS: [r'\\u05e6\\u05d3\\u05e7', r'\\u05de\\u05e9\\u05e4\\u05d8', r'\\u05d9\\u05e9\\u05e8', r'\\u05e9\\u05d5\\u05d4'],\n",
    "        BondType.CONTRACT: [r'\\u05d1\\u05e8\\u05d9\\u05ea', r'\\u05e0\\u05d3\\u05e8', r'\\u05e9\\u05d1\\u05d5\\u05e2', r'\\u05d4\\u05ea\\u05d7\\u05d9\\u05d1', r'\\u05e2\\u05e8\\u05d1'],\n",
    "    },\n",
    "    'aramaic': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u05e7\\u05d8\\u05dc', r'\\u05e0\\u05d6\\u05e7', r'\\u05d7\\u05d1\\u05dc', r'\\u05e9\\u05d6\\u05d9\\u05d1', r'\\u05e4\\u05e6\\u05d9'],\n",
    "        BondType.RECIPROCITY: [r'\\u05e4\\u05e8\\u05e2', r'\\u05e9\\u05dc\\u05de', r'\\u05d0\\u05d2\\u05e8'],\n",
    "        BondType.AUTONOMY: [r'\\u05e6\\u05d1\\u05d9', r'\\u05e8\\u05e2\\u05d5'],\n",
    "        BondType.PROPERTY: [r'\\u05d6\\u05d1\\u05e0', r'\\u05e7\\u05e0\\u05d4', r'\\u05d2\\u05d6\\u05dc', r'\\u05de\\u05de\\u05d5\\u05e0\\u05d0', r'\\u05e0\\u05db\\u05e1\\u05d9'],\n",
    "        BondType.FAMILY: [r'\\u05d0\\u05d1\\u05d0', r'\\u05d0\\u05de\\u05d0', r'\\u05d1\\u05e8\\u05d0', r'\\u05d1\\u05e8\\u05ea\\u05d0', r'\\u05d9\\u05e7\\u05e8', r'\\u05d0\\u05d7\\u05d0'],\n",
    "        BondType.AUTHORITY: [r'\\u05de\\u05dc\\u05db\\u05d0', r'\\u05d3\\u05d9\\u05e0\\u05d0', r'\\u05d3\\u05d9\\u05d9\\u05e0\\u05d0', r'\\u05e4\\u05e7\\u05d5\\u05d3\\u05d0', r'\\u05d0\\u05d5\\u05e8\\u05d9\\u05ea'],\n",
    "        BondType.CARE: [r'\\u05d7\\u05e1\\u05d3', r'\\u05e8\\u05d7\\u05de', r'\\u05e1\\u05e2\\u05d3'],\n",
    "        BondType.FAIRNESS: [r'\\u05d3\\u05d9\\u05e0\\u05d0', r'\\u05e7\\u05e9\\u05d5\\u05d8', r'\\u05ea\\u05e8\\u05d9\\u05e6'],\n",
    "        BondType.CONTRACT: [r'\\u05e7\\u05d9\\u05de\\u05d0', r'\\u05e9\\u05d1\\u05d5\\u05e2\\u05d4', r'\\u05e0\\u05d3\\u05e8\\u05d0', r'\\u05e2\\u05e8\\u05d1\\u05d0'],\n",
    "    },\n",
    "    'classical_chinese': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u6bba', r'\\u5bb3', r'\\u50b7', r'\\u6551', r'\\u8b77', r'\\u885b', r'\\u66b4'],\n",
    "        BondType.RECIPROCITY: [r'\\u5831', r'\\u9084', r'\\u511f', r'\\u8ced', r'\\u7b54'],\n",
    "        BondType.AUTONOMY: [r'\\u81ea', r'\\u7531', r'\\u4efb', r'\\u610f', r'\\u5fd7'],\n",
    "        BondType.PROPERTY: [r'\\u8ca1', r'\\u7269', r'\\u7522', r'\\u76dc', r'\\u7aca', r'\\u8ce3', r'\\u8cb7'],\n",
    "        BondType.FAMILY: [r'\\u5b5d', r'\\u7236', r'\\u6bcd', r'\\u89aa', r'\\u5b50', r'\\u5f1f', r'\\u5144', r'\\u5bb6'],\n",
    "        BondType.AUTHORITY: [r'\\u541b', r'\\u81e3', r'\\u738b', r'\\u547d', r'\\u4ee4', r'\\u6cd5', r'\\u6cbb'],\n",
    "        BondType.CARE: [r'\\u4ec1', r'\\u611b', r'\\u6148', r'\\u60e0', r'\\u6069', r'\\u6190'],\n",
    "        BondType.FAIRNESS: [r'\\u7fa9', r'\\u6b63', r'\\u516c', r'\\u5e73', r'\\u5747'],\n",
    "        BondType.CONTRACT: [r'\\u7d04', r'\\u76df', r'\\u8a93', r'\\u8afe', r'\\u4fe1'],\n",
    "    },\n",
    "    'arabic': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u0642\\u062a\\u0644', r'\\u0636\\u0631\\u0631', r'\\u0627\\u0630[\\u064a\\u0649]', r'\\u0638\\u0644\\u0645', r'\\u0627\\u0646\\u0642\\u0630', r'\\u062d\\u0641\\u0638', r'\\u0627\\u0645\\u0627\\u0646'],\n",
    "        BondType.RECIPROCITY: [r'\\u062c\\u0632\\u0627', r'\\u0631\\u062f', r'\\u0642\\u0635\\u0627\\u0635', r'\\u0645\\u062b\\u0644', r'\\u0639\\u0648\\u0636'],\n",
    "        BondType.AUTONOMY: [r'\\u062d\\u0631', r'\\u0627\\u0631\\u0627\\u062f\\u0629', r'\\u0627\\u062e\\u062a\\u064a\\u0627\\u0631', r'\\u0645\\u0634\\u064a\\u0626'],\n",
    "        BondType.PROPERTY: [r'\\u0645\\u0627\\u0644', r'\\u0645\\u0644\\u0643', r'\\u0633\\u0631\\u0642', r'\\u0628\\u064a\\u0639', r'\\u0634\\u0631\\u0627', r'\\u0645\\u064a\\u0631\\u0627\\u062b', r'\\u063a\\u0635\\u0628'],\n",
    "        BondType.FAMILY: [r'\\u0648\\u0627\\u0644\\u062f', r'\\u0627\\u0628\\u0648', r'\\u0627\\u0645', r'\\u0627\\u0628\\u0646', r'\\u0628\\u0646\\u062a', r'\\u0627\\u0647\\u0644', r'\\u0642\\u0631\\u0628[\\u064a\\u0649]', r'\\u0631\\u062d\\u0645'],\n",
    "        BondType.AUTHORITY: [r'\\u0637\\u0627\\u0639', r'\\u0627\\u0645\\u0631', r'\\u062d\\u0643\\u0645', r'\\u0633\\u0644\\u0637\\u0627\\u0646', r'\\u062e\\u0644\\u064a\\u0641', r'\\u0627\\u0645\\u0627\\u0645', r'\\u0634\\u0631\\u064a\\u0639'],\n",
    "        BondType.CARE: [r'\\u0631\\u062d\\u0645', r'\\u0627\\u062d\\u0633\\u0627\\u0646', r'\\u0639\\u0637\\u0641', r'\\u0635\\u062f\\u0642', r'\\u0632\\u0643\\u0627'],\n",
    "        BondType.FAIRNESS: [r'\\u0639\\u062f\\u0644', r'\\u0642\\u0633\\u0637', r'\\u062d\\u0642', r'\\u0627\\u0646\\u0635\\u0627\\u0641', r'\\u0633\\u0648[\\u064a\\u0649]'],\n",
    "        BondType.CONTRACT: [r'\\u0639\\u0647\\u062f', r'\\u0639\\u0642\\u062f', r'\\u0646\\u0630\\u0631', r'\\u064a\\u0645\\u064a\\u0646', r'\\u0648\\u0641\\u0627', r'\\u0627\\u0645\\u0627\\u0646'],\n",
    "    },\n",
    "    'english': {\n",
    "        BondType.HARM_PREVENTION: [r'\\bkill', r'\\bmurder', r'\\bharm', r'\\bhurt', r'\\bsave', r'\\bprotect', r'\\bviolence'],\n",
    "        BondType.RECIPROCITY: [r'\\breturn', r'\\brepay', r'\\bexchange', r'\\bgive.*back', r'\\breciproc'],\n",
    "        BondType.AUTONOMY: [r'\\bfree', r'\\bchoice', r'\\bchoose', r'\\bconsent', r'\\bautonomy', r'\\bright to'],\n",
    "        BondType.PROPERTY: [r'\\bsteal', r'\\btheft', r'\\bown', r'\\bproperty', r'\\bbelong', r'\\binherit'],\n",
    "        BondType.FAMILY: [r'\\bfather', r'\\bmother', r'\\bparent', r'\\bchild', r'\\bfamily', r'\\bhonor.*parent'],\n",
    "        BondType.AUTHORITY: [r'\\bobey', r'\\bcommand', r'\\bauthority', r'\\blaw', r'\\brule', r'\\bgovern'],\n",
    "        BondType.CARE: [r'\\bcare', r'\\bhelp', r'\\bkind', r'\\bcompassion', r'\\bcharity', r'\\bmercy'],\n",
    "        BondType.FAIRNESS: [r'\\bfair', r'\\bjust', r'\\bequal', r'\\bequity', r'\\bright\\b'],\n",
    "        BondType.CONTRACT: [r'\\bpromise', r'\\bcontract', r'\\bagreem', r'\\bvow', r'\\boath', r'\\bcommit'],\n",
    "    },\n",
    "}\n",
    "\n",
    "# ===== COMPLETE HOHFELD PATTERNS =====\n",
    "ALL_HOHFELD_PATTERNS = {\n",
    "    'hebrew': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u05d7\\u05d9\\u05d9\\u05d1', r'\\u05e6\\u05e8\\u05d9\\u05db', r'\\u05de\\u05d5\\u05db\\u05e8\\u05d7', r'\\u05de\\u05e6\\u05d5\\u05d5\\u05d4'],\n",
    "        HohfeldState.RIGHT: [r'\\u05d6\\u05db\\u05d5\\u05ea', r'\\u05e8\\u05e9\\u05d0\\u05d9', r'\\u05d6\\u05db\\u05d0\\u05d9', r'\\u05de\\u05d2\\u05d9\\u05e2'],\n",
    "        HohfeldState.LIBERTY: [r'\\u05de\\u05d5\\u05ea\\u05e8', r'\\u05e8\\u05e9\\u05d5\\u05ea', r'\\u05e4\\u05d8\\u05d5\\u05e8', r'\\u05d9\\u05db\\u05d5\\u05dc'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u05d0\\u05e1\\u05d5\\u05e8', r'\\u05d0\\u05d9\\u05e0\\u05d5 \\u05e8\\u05e9\\u05d0\\u05d9', r'\\u05d0\\u05d9\\u05df.*\\u05d6\\u05db\\u05d5\\u05ea'],\n",
    "    },\n",
    "    'aramaic': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u05d7\\u05d9\\u05d9\\u05d1', r'\\u05de\\u05d7\\u05d5\\u05d9\\u05d1', r'\\u05d1\\u05e2\\u05d9'],\n",
    "        HohfeldState.RIGHT: [r'\\u05d6\\u05db\\u05d5\\u05ea', r'\\u05e8\\u05e9\\u05d0\\u05d9', r'\\u05d6\\u05db\\u05d9'],\n",
    "        HohfeldState.LIBERTY: [r'\\u05e9\\u05e8\\u05d9', r'\\u05de\\u05d5\\u05ea\\u05e8', r'\\u05e4\\u05d8\\u05d5\\u05e8'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u05d0\\u05e1\\u05d5\\u05e8', r'\\u05dc\\u05d0.*\\u05e8\\u05e9\\u05d0\\u05d9'],\n",
    "    },\n",
    "    'classical_chinese': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u5fc5', r'\\u9808', r'\\u7576', r'\\u61c9', r'\\u5b9c'],\n",
    "        HohfeldState.RIGHT: [r'\\u53ef', r'\\u5f97', r'\\u6b0a', r'\\u5b9c'],\n",
    "        HohfeldState.LIBERTY: [r'\\u8a31', r'\\u4efb', r'\\u807d', r'\\u514d'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u4e0d\\u53ef', r'\\u52ff', r'\\u7981', r'\\u83ab', r'\\u975e'],\n",
    "    },\n",
    "    'arabic': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u064a\\u062c\\u0628', r'\\u0648\\u0627\\u062c\\u0628', r'\\u0641\\u0631\\u0636', r'\\u0644\\u0627\\u0632\\u0645', r'\\u0648\\u062c\\u0648\\u0628'],\n",
    "        HohfeldState.RIGHT: [r'\\u062d\\u0642', r'\\u064a\\u062d\\u0642', r'\\u062c\\u0627\\u0626\\u0632', r'\\u064a\\u062c\\u0648\\u0632'],\n",
    "        HohfeldState.LIBERTY: [r'\\u0645\\u0628\\u0627\\u062d', r'\\u062d\\u0644\\u0627\\u0644', r'\\u062c\\u0627\\u0626\\u0632', r'\\u0627\\u0628\\u0627\\u062d'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u062d\\u0631\\u0627\\u0645', r'\\u0645\\u062d\\u0631\\u0645', r'\\u0645\\u0645\\u0646\\u0648\\u0639', r'\\u0644\\u0627 \\u064a\\u062c\\u0648\\u0632', r'\\u0646\\u0647[\\u064a\\u0649]'],\n",
    "    },\n",
    "    'english': {\n",
    "        HohfeldState.OBLIGATION: [r'\\bmust\\b', r'\\bshall\\b', r'\\bobligat', r'\\bduty', r'\\brequir'],\n",
    "        HohfeldState.RIGHT: [r'\\bright\\b', r'\\bentitle', r'\\bdeserve', r'\\bclaim'],\n",
    "        HohfeldState.LIBERTY: [r'\\bmay\\b', r'\\bpermit', r'\\ballow', r'\\bfree to'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\bforbid', r'\\bprohibit', r'\\bmust not', r'\\bshall not'],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ===== CONTEXT MARKERS FOR GRAMMAR-AWARE EXTRACTION =====\n",
    "# These help distinguish \"thou shalt not kill\" from \"he killed\"\n",
    "CONTEXT_MARKERS = {\n",
    "    'hebrew': {\n",
    "        'negation': [r'\u05dc\u05d0', r'\u05d0\u05dc', r'\u05d0\u05d9\u05df', r'\u05d1\u05dc\u05d9', r'\u05d0\u05d9\u05e0'],\n",
    "        'obligation': [r'\u05d7\u05d9\u05d9\u05d1', r'\u05e6\u05e8\u05d9\u05da', r'\u05de\u05d5\u05db\u05e8\u05d7', r'\u05e6\u05d5\u05d5\u05d4'],\n",
    "        'prohibition': [r'\u05d0\u05e1\u05d5\u05e8', r'\u05d0\u05dc.*\u05ea'],\n",
    "        'permission': [r'\u05de\u05d5\u05ea\u05e8', r'\u05e8\u05e9\u05d0\u05d9', r'\u05e4\u05d8\u05d5\u05e8'],\n",
    "    },\n",
    "    'aramaic': {\n",
    "        'negation': [r'\u05dc\u05d0', r'\u05dc\u05d9\u05ea', r'\u05dc\u05d0\u05d5'],\n",
    "        'obligation': [r'\u05d7\u05d9\u05d9\u05d1', r'\u05d1\u05e2\u05d9'],\n",
    "        'prohibition': [r'\u05d0\u05e1\u05d5\u05e8'],\n",
    "        'permission': [r'\u05e9\u05e8\u05d9', r'\u05de\u05d5\u05ea\u05e8'],\n",
    "    },\n",
    "    'classical_chinese': {\n",
    "        'negation': [r'\u4e0d', r'\u975e', r'\u7121', r'\u672a', r'\u6bcb'],\n",
    "        'obligation': [r'\u5fc5', r'\u7576', r'\u9808', r'\u61c9', r'\u5b9c'],\n",
    "        'prohibition': [r'\u52ff', r'\u7981', r'\u83ab', r'\u4e0d\u53ef'],\n",
    "        'permission': [r'\u53ef', r'\u5f97', r'\u8a31'],\n",
    "    },\n",
    "    'arabic': {\n",
    "        'negation': [r'\u0644\u0627', r'\u0645\u0627', r'\u0644\u064a\u0633', r'\u0644\u0645', r'\u063a\u064a\u0631'],\n",
    "        'obligation': [r'\u064a\u062c\u0628', r'\u0648\u0627\u062c\u0628', r'\u0641\u0631\u0636', r'\u0639\u0644\u064a\u0647'],\n",
    "        'prohibition': [r'\u062d\u0631\u0627\u0645', r'\u0645\u062d\u0631\u0645', r'\u0644\u0627 \u064a\u062c\u0648\u0632', r'\u0646\u0647\u0649'],\n",
    "        'permission': [r'\u062d\u0644\u0627\u0644', r'\u0645\u0628\u0627\u062d', r'\u062c\u0627\u0626\u0632'],\n",
    "    },\n",
    "    'english': {\n",
    "        'negation': [r'\bnot\b', r'\bno\b', r'\bnever\b', r'\bneither\b', r\"n't\b\"],\n",
    "        'obligation': [r'\bmust\b', r'\bshall\b', r'\bshould\b', r'\bought\b', r'\brequired\b'],\n",
    "        'prohibition': [r'\bforbid', r'\bprohibit', r'\bmust not\b', r'\bshall not\b', r\"\bdon't\b\"],\n",
    "        'permission': [r'\bmay\b', r'\bcan\b', r'\ballowed\b', r'\bpermit'],\n",
    "    },\n",
    "}\n",
    "\n",
    "def detect_context(text, language, match_pos, window=30):\n",
    "    \"\"\"\n",
    "    Detect grammatical context around a pattern match.\n",
    "    Returns: ('prescriptive'/'descriptive'/'unknown', marker_type or None)\n",
    "    \"\"\"\n",
    "    markers = CONTEXT_MARKERS.get(language, {})\n",
    "    if not markers:\n",
    "        return 'unknown', None\n",
    "    \n",
    "    start = max(0, match_pos - window)\n",
    "    end = min(len(text), match_pos + window)\n",
    "    window_text = text[start:end]\n",
    "    \n",
    "    # Check for deontic markers (prescriptive = moral statement)\n",
    "    for marker_type in ['prohibition', 'obligation', 'permission']:\n",
    "        for pattern in markers.get(marker_type, []):\n",
    "            if re.search(pattern, window_text):\n",
    "                return 'prescriptive', marker_type\n",
    "    \n",
    "    # Check for simple negation (may be descriptive)\n",
    "    for pattern in markers.get('negation', []):\n",
    "        if re.search(pattern, window_text):\n",
    "            return 'descriptive', 'negated'\n",
    "    \n",
    "    return 'descriptive', None\n",
    "\n",
    "print(\"\\nContext markers defined for grammar-aware extraction\")\n",
    "print(\"  Detects: negation, obligation, prohibition, permission\")\n",
    "\n",
    "print(\"Patterns defined for 5 languages:\")\n",
    "for lang in ALL_BOND_PATTERNS:\n",
    "    n = sum(len(p) for p in ALL_BOND_PATTERNS[lang].values())\n",
    "    print(f\"  {lang}: {n} bond patterns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Load Corpora + Extract Bonds { display-mode: \"form\" }\n#@markdown Loads all corpora - auto-detects GPU and adjusts sampling\n\nimport json\nimport hashlib\nimport random\nimport gc\nimport shutil\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\n\n# Check if we should skip processing (data loaded from Drive)\nif SKIP_PROCESSING:\n    print(\"=\"*60)\n    print(\"SKIPPING PROCESSING - Using Drive data\")\n    print(\"=\"*60)\n\n    # Count passages by language\n    by_lang = defaultdict(int)\n    with open('data/processed/passages.jsonl', 'r') as f:\n        for line in f:\n            p = json.loads(line)\n            by_lang[p['language']] += 1\n\n    print(\"\\nPassages by language:\")\n    for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n        print(f\"  {lang}: {cnt:,}\")\n\n    n_passages = sum(by_lang.values())\n    print(f\"\\nTotal: {n_passages:,} passages\")\n\n    # Validate corpus sizes\n    MIN_RECOMMENDED = {'hebrew': 10000, 'aramaic': 5000, 'classical_chinese': 100, 'arabic': 100, 'english': 5000}\n    print(\"\\nCorpus adequacy check:\")\n    for lang, min_size in MIN_RECOMMENDED.items():\n        actual = by_lang.get(lang, 0)\n        status = \"OK\" if actual >= min_size else \"LOW\"\n        print(f\"  {lang}: {actual:,} (need {min_size:,}) - {status}\")\n\n    if by_lang.get('english', 0) < 1000:\n        print(\"\\nWARNING: English corpus too small for reliable semitic_to_non_semitic test!\")\n    if by_lang.get('classical_chinese', 0) < 100:\n        print(\"WARNING: Chinese corpus too small!\")\n\nelse:\n    print(\"=\"*60)\n    print(\"LOADING CORPORA\")\n    print(f\"GPU Tier: {GPU_TIER}\")\n    print(f\"Max per language: {MAX_PER_LANG:,}\")\n    print(\"=\"*60)\n\n    random.seed(42)\n    all_passages = []\n\n    # ===== SEFARIA (FIXED) =====\n    print(\"\\nLoading Sefaria...\")\n    sefaria_path = Path('data/raw/Sefaria-Export/json')\n\n    CATEGORY_TO_PERIOD = {\n        'Tanakh': 'BIBLICAL', 'Torah': 'BIBLICAL', 'Prophets': 'BIBLICAL', 'Writings': 'BIBLICAL',\n        'Mishnah': 'TANNAITIC', 'Tosefta': 'TANNAITIC',\n        'Talmud': 'AMORAIC', 'Bavli': 'AMORAIC', 'Yerushalmi': 'AMORAIC', 'Midrash': 'AMORAIC',\n        'Halakhah': 'RISHONIM', 'Kabbalah': 'RISHONIM', 'Philosophy': 'RISHONIM',\n        'Chasidut': 'ACHRONIM', 'Musar': 'ACHRONIM', 'Responsa': 'ACHRONIM',\n    }\n\n    hebrew_ps, aramaic_ps = [], []\n\n    if sefaria_path.exists():\n        for jf in tqdm(list(sefaria_path.rglob('*.json')), desc=\"Sefaria\"):\n            try:\n                with open(jf, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n            except:\n                continue\n\n            # FIX: Check language field correctly\n            if data.get('language') != 'he':\n                continue\n\n            txt = data.get('text', [])\n            if not txt:\n                continue\n\n            rel = jf.relative_to(sefaria_path)\n            cat = str(rel.parts[0]) if rel.parts else 'unknown'\n            period = CATEGORY_TO_PERIOD.get(cat, 'AMORAIC')\n            is_talmud = 'Talmud' in str(jf) or cat in ['Bavli', 'Yerushalmi']\n            lang = 'aramaic' if is_talmud else 'hebrew'\n\n            def flatten(t):\n                results = []\n                if isinstance(t, str):\n                    tc = re.sub(r'<[^>]+>', '', t).strip()\n                    if 20 <= len(tc) <= 2000:\n                        hc = sum(1 for c in tc if '\\u0590' <= c <= '\\u05FF')\n                        if hc > 5:\n                            pid = hashlib.md5((jf.stem + tc[:30]).encode()).hexdigest()[:12]\n                            results.append({'id': f'sef_{pid}', 'text': tc, 'lang': lang, 'period': period})\n                elif isinstance(t, (dict, list)):\n                    for v in (t.values() if isinstance(t, dict) else t):\n                        results.extend(flatten(v))\n                return results\n\n            ps = flatten(txt)\n            if lang == 'hebrew':\n                hebrew_ps.extend(ps)\n            else:\n                aramaic_ps.extend(ps)\n\n        # Sample down\n        random.shuffle(hebrew_ps)\n        random.shuffle(aramaic_ps)\n        hebrew_ps = hebrew_ps[:MAX_PER_LANG]\n        aramaic_ps = aramaic_ps[:MAX_PER_LANG]\n        all_passages.extend(hebrew_ps)\n        all_passages.extend(aramaic_ps)\n        print(f\"  Hebrew: {len(hebrew_ps):,}, Aramaic: {len(aramaic_ps):,}\")\n        del hebrew_ps, aramaic_ps\n        gc.collect()\n    else:\n        print(\"  ERROR: Sefaria not found!\")\n\n    # ===== CHINESE =====\n    print(\"\\nLoading Chinese...\")\n    try:\n        with open('data/raw/chinese/chinese_native.json', 'r', encoding='utf-8') as f:\n            chinese_data = json.load(f)\n        for item in chinese_data:\n            all_passages.append({'id': item['id'], 'text': item['text'], 'lang': 'classical_chinese', 'period': item['period']})\n        print(f\"  Chinese: {len(chinese_data)}\")\n    except Exception as e:\n        print(f\"  Error: {e}\")\n\n    # ===== ISLAMIC =====\n    print(\"\\nLoading Islamic...\")\n    try:\n        with open('data/raw/islamic/islamic_native.json', 'r', encoding='utf-8') as f:\n            islamic_data = json.load(f)\n        for item in islamic_data:\n            all_passages.append({'id': item['id'], 'text': item['text'], 'lang': 'arabic', 'period': item['period']})\n        print(f\"  Arabic: {len(islamic_data)}\")\n    except Exception as e:\n        print(f\"  Error: {e}\")\n\n    # ===== DEAR ABBY =====\n    print(\"\\nLoading Dear Abby...\")\n    try:\n        import pandas as pd\n        df = pd.read_csv('data/raw/dear_abby.csv')\n        abby_count = 0\n        for idx, row in df.iterrows():\n            q = str(row.get('question_only', ''))\n            if q != 'nan' and 50 <= len(q) <= 2000:\n                all_passages.append({'id': f'abby_{idx}', 'text': q, 'lang': 'english', 'period': 'DEAR_ABBY'})\n                abby_count += 1\n        print(f\"  English: {abby_count:,}\")\n    except Exception as e:\n        print(f\"  Error: {e}\")\n\n    print(f\"\\nTOTAL: {len(all_passages):,}\")\n\n    # Count by language\n    by_lang = defaultdict(int)\n    for p in all_passages:\n        by_lang[p['lang']] += 1\n    print(\"\\nBy language:\")\n    for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n        print(f\"  {lang}: {cnt:,}\")\n\n    # ===== EXTRACT BONDS =====\n    print(\"\\n\" + \"=\"*60)\n    print(\"EXTRACTING BONDS\")\n    print(\"=\"*60)\n\n    def extract_bond(text, language):\n        \"\"\"Extract bond type with context awareness.\"\"\"\n        tn = normalize_text(text, language)\n\n        for bt, pats in ALL_BOND_PATTERNS.get(language, {}).items():\n            for p in pats:\n                match = re.search(p, tn)\n                if match:\n                    # Check context around the match\n                    context_type, marker = detect_context(tn, language, match.start())\n\n                    # Prescriptive context (moral statements) get higher confidence\n                    # but we return the bond type either way\n                    return bt.name\n        return 'NONE'\n\n    def extract_bond_with_context(text, language):\n        \"\"\"Extract bond type with full context info for analysis.\"\"\"\n        tn = normalize_text(text, language)\n\n        for bt, pats in ALL_BOND_PATTERNS.get(language, {}).items():\n            for p in pats:\n                match = re.search(p, tn)\n                if match:\n                    context_type, marker = detect_context(tn, language, match.start())\n                    return {\n                        'bond': bt.name,\n                        'context': context_type,  # 'prescriptive' or 'descriptive'\n                        'marker': marker,  # 'obligation', 'prohibition', 'permission', 'negated', None\n                        'confidence': 'high' if context_type == 'prescriptive' else 'medium'\n                    }\n        return {'bond': 'NONE', 'context': 'unknown', 'marker': None, 'confidence': 'low'}\n\n    def extract_hohfeld(text, language):\n        tn = normalize_text(text, language)\n        for st, pats in ALL_HOHFELD_PATTERNS.get(language, {}).items():\n            if any(re.search(p, tn) for p in pats):\n                return st.name\n        return None\n\n    bond_counts = defaultdict(lambda: defaultdict(int))\n    context_counts = defaultdict(lambda: defaultdict(int))\n\n    with open('data/processed/passages.jsonl', 'w', encoding='utf-8') as fp, \\\n         open('data/processed/bonds.jsonl', 'w', encoding='utf-8') as fb:\n\n        for p in tqdm(all_passages, desc=\"Extracting\"):\n            # Use context-aware extraction\n            bond_info = extract_bond_with_context(p['text'], p['lang'])\n            hohfeld = extract_hohfeld(p['text'], p['lang'])\n\n            bond = bond_info['bond']\n            context = bond_info['context']\n            confidence = bond_info['confidence']\n\n            bond_counts[p['lang']][bond] += 1\n            context_counts[p['lang']][context] += 1\n\n            fp.write(json.dumps({\n                'id': p['id'], 'text': p['text'], 'language': p['lang'],\n                'time_period': p['period'], 'source': 'x', 'source_type': 'sefaria' if 'sef_' in p['id'] else 'other', 'century': 0\n            }, ensure_ascii=False) + '\\n')\n\n            fb.write(json.dumps({\n                'passage_id': p['id'],\n                'bonds': {\n                    'primary_bond': bond,\n                    'all_bonds': [bond],\n                    'hohfeld': hohfeld,\n                    'language': p['lang'],\n                    'context': context,\n                    'confidence': confidence\n                }\n            }, ensure_ascii=False) + '\\n')\n\n    # Coverage report\n    print(\"\\nLabel coverage:\")\n    for lang in sorted(bond_counts.keys()):\n        total = sum(bond_counts[lang].values())\n        none_ct = bond_counts[lang].get('NONE', 0)\n        cov = (total - none_ct) / total * 100 if total else 0\n        print(f\"  {lang}: {cov:.1f}% labeled ({total-none_ct:,}/{total:,})\")\n\n    # Context distribution report\n    print(\"\\nContext distribution:\")\n    for lang in sorted(context_counts.keys()):\n        total = sum(context_counts[lang].values())\n        prescriptive = context_counts[lang].get('prescriptive', 0)\n        pct = prescriptive / total * 100 if total else 0\n        print(f\"  {lang}: {pct:.1f}% prescriptive ({prescriptive:,}/{total:,})\")\n\n    # ===== SAVE TO DRIVE =====\n    print(\"\\nSaving to Drive...\")\n    shutil.copy('data/processed/passages.jsonl', f'{SAVE_DIR}/passages.jsonl')\n    shutil.copy('data/processed/bonds.jsonl', f'{SAVE_DIR}/bonds.jsonl')\n    print(\"  Saved!\")\n\n    n_passages = len(all_passages)\n    del all_passages\n    gc.collect()\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"Cell 4 complete\")\nprint(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. Generate Splits { display-mode: \"form\" }\n",
    "#@markdown Creates train/test splits for cross-lingual experiments\n",
    "\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING SPLITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if splits already exist from Drive\n",
    "if os.path.exists('data/splits/all_splits.json'):\n",
    "    print(\"\\nSplits already loaded from Drive\")\n",
    "    with open('data/splits/all_splits.json') as f:\n",
    "        all_splits = json.load(f)\n",
    "    for name, split in all_splits.items():\n",
    "        print(f\"  {name}: train={split['train_size']:,}, test={split['test_size']:,}\")\n",
    "else:\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Read passage metadata\n",
    "    passage_meta = []\n",
    "    with open('data/processed/passages.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            p = json.loads(line)\n",
    "            passage_meta.append(p)\n",
    "    \n",
    "    print(f\"Total passages: {len(passage_meta):,}\")\n",
    "    \n",
    "    by_lang = defaultdict(list)\n",
    "    by_period = defaultdict(list)\n",
    "    for p in passage_meta:\n",
    "        by_lang[p['language']].append(p['id'])\n",
    "        by_period[p['time_period']].append(p['id'])\n",
    "    \n",
    "    print(\"\\nBy language:\")\n",
    "    for lang, ids in sorted(by_lang.items(), key=lambda x: -len(x[1])):\n",
    "        print(f\"  {lang}: {len(ids):,}\")\n",
    "    \n",
    "    all_splits = {}\n",
    "    \n",
    "    # ===== SPLIT 1: Hebrew -> Others =====\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"SPLIT 1: HEBREW -> OTHERS\")\n",
    "    hebrew_ids = by_lang.get('hebrew', [])\n",
    "    other_ids = [p['id'] for p in passage_meta if p['language'] != 'hebrew']\n",
    "    random.shuffle(hebrew_ids)\n",
    "    random.shuffle(other_ids)\n",
    "    \n",
    "    all_splits['hebrew_to_others'] = {\n",
    "        'train_ids': hebrew_ids,\n",
    "        'test_ids': other_ids,\n",
    "        'train_size': len(hebrew_ids),\n",
    "        'test_size': len(other_ids),\n",
    "    }\n",
    "    print(f\"  Train (Hebrew): {len(hebrew_ids):,}\")\n",
    "    print(f\"  Test (Others): {len(other_ids):,}\")\n",
    "    \n",
    "    # ===== SPLIT 2: Semitic -> Non-Semitic =====\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"SPLIT 2: SEMITIC -> NON-SEMITIC\")\n",
    "    semitic_ids = by_lang.get('hebrew', []) + by_lang.get('aramaic', []) + by_lang.get('arabic', [])\n",
    "    non_semitic_ids = by_lang.get('classical_chinese', []) + by_lang.get('english', [])\n",
    "    random.shuffle(semitic_ids)\n",
    "    random.shuffle(non_semitic_ids)\n",
    "    \n",
    "    all_splits['semitic_to_non_semitic'] = {\n",
    "        'train_ids': semitic_ids,\n",
    "        'test_ids': non_semitic_ids,\n",
    "        'train_size': len(semitic_ids),\n",
    "        'test_size': len(non_semitic_ids),\n",
    "    }\n",
    "    print(f\"  Train (Semitic): {len(semitic_ids):,}\")\n",
    "    print(f\"  Test (Non-Semitic): {len(non_semitic_ids):,}\")\n",
    "    \n",
    "    # ===== SPLIT 3: Ancient -> Modern =====\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"SPLIT 3: ANCIENT -> MODERN\")\n",
    "    ancient_periods = {'BIBLICAL', 'TANNAITIC', 'AMORAIC', 'CONFUCIAN', 'DAOIST', 'QURANIC', 'HADITH'}\n",
    "    modern_periods = {'RISHONIM', 'ACHRONIM', 'DEAR_ABBY'}\n",
    "    \n",
    "    ancient_ids = [p['id'] for p in passage_meta if p['time_period'] in ancient_periods]\n",
    "    modern_ids = [p['id'] for p in passage_meta if p['time_period'] in modern_periods]\n",
    "    random.shuffle(ancient_ids)\n",
    "    random.shuffle(modern_ids)\n",
    "    \n",
    "    all_splits['ancient_to_modern'] = {\n",
    "        'train_ids': ancient_ids,\n",
    "        'test_ids': modern_ids,\n",
    "        'train_size': len(ancient_ids),\n",
    "        'test_size': len(modern_ids),\n",
    "    }\n",
    "    print(f\"  Train (Ancient): {len(ancient_ids):,}\")\n",
    "    print(f\"  Test (Modern): {len(modern_ids):,}\")\n",
    "    \n",
    "    # ===== SPLIT 4: Mixed Baseline =====\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"SPLIT 4: MIXED BASELINE\")\n",
    "    all_ids = [p['id'] for p in passage_meta]\n",
    "    random.shuffle(all_ids)\n",
    "    split_idx = int(0.7 * len(all_ids))\n",
    "    \n",
    "    all_splits['mixed_baseline'] = {\n",
    "        'train_ids': all_ids[:split_idx],\n",
    "        'test_ids': all_ids[split_idx:],\n",
    "        'train_size': split_idx,\n",
    "        'test_size': len(all_ids) - split_idx,\n",
    "    }\n",
    "    print(f\"  Train: {split_idx:,}\")\n",
    "    print(f\"  Test: {len(all_ids) - split_idx:,}\")\n",
    "    \n",
    "    # Save splits\n",
    "    with open('data/splits/all_splits.json', 'w') as f:\n",
    "        json.dump(all_splits, f, indent=2)\n",
    "    \n",
    "    # Save to Drive\n",
    "    shutil.copy('data/splits/all_splits.json', f'{SAVE_DIR}/all_splits.json')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Splits saved to local and Drive\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 6. Model Architecture { display-mode: \"form\" }\n#@markdown BIP model with adversarial heads and complete Hohfeld support\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoTokenizer\nfrom tqdm.auto import tqdm\nimport json\n\nprint(\"=\"*60)\nprint(\"MODEL ARCHITECTURE\")\nprint(\"=\"*60)\n\n# Index mappings\nBOND_TO_IDX = {bt.name: i for i, bt in enumerate(BondType)}\nIDX_TO_BOND = {i: bt.name for i, bt in enumerate(BondType)}\nLANG_TO_IDX = {'hebrew': 0, 'aramaic': 1, 'classical_chinese': 2, 'arabic': 3, 'english': 4}\nIDX_TO_LANG = {i: l for l, i in LANG_TO_IDX.items()}\nPERIOD_TO_IDX = {'BIBLICAL': 0, 'TANNAITIC': 1, 'AMORAIC': 2, 'RISHONIM': 3, 'ACHRONIM': 4,\n                 'CONFUCIAN': 5, 'DAOIST': 6, 'QURANIC': 7, 'HADITH': 8, 'DEAR_ABBY': 9}\nIDX_TO_PERIOD = {i: p for p, i in PERIOD_TO_IDX.items()}\nHOHFELD_TO_IDX = {hs.name: i for i, hs in enumerate(HohfeldState)}\nIDX_TO_HOHFELD = {i: hs.name for i, hs in enumerate(HohfeldState)}\nCONTEXT_TO_IDX = {'prescriptive': 0, 'descriptive': 1, 'unknown': 2}\nIDX_TO_CONTEXT = {i: c for c, i in CONTEXT_TO_IDX.items()}\nCONFIDENCE_TO_WEIGHT = {'high': 2.0, 'medium': 1.0, 'low': 0.5}\n\nclass GradientReversalLayer(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = alpha\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.alpha, None\n\nclass BIPModel(nn.Module):\n    def __init__(self, z_dim=64):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n        hidden = self.encoder.config.hidden_size  # 384\n\n        # Projection to z_bond space\n        self.z_proj = nn.Sequential(\n            nn.Linear(hidden, 256),\n            nn.LayerNorm(256),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, z_dim),\n        )\n\n        # Task heads\n        self.bond_head = nn.Linear(z_dim, len(BondType))\n        self.hohfeld_head = nn.Linear(z_dim, len(HohfeldState))\n\n        # Adversarial heads\n        self.language_head = nn.Linear(z_dim, len(LANG_TO_IDX))\n        self.period_head = nn.Linear(z_dim, len(PERIOD_TO_IDX))\n\n        # Context prediction head (auxiliary task)\n        self.context_head = nn.Linear(z_dim, len(CONTEXT_TO_IDX))\n\n    def forward(self, input_ids, attention_mask, adv_lambda=1.0):\n        enc = self.encoder(input_ids, attention_mask)\n        pooled = enc.last_hidden_state[:, 0]  # CLS token\n\n        z = self.z_proj(pooled)\n\n        # Bond prediction (main task)\n        bond_pred = self.bond_head(z)\n        hohfeld_pred = self.hohfeld_head(z)\n\n        # Adversarial predictions (gradient reversal)\n        z_rev = GradientReversalLayer.apply(z, adv_lambda)\n        language_pred = self.language_head(z_rev)\n        period_pred = self.period_head(z_rev)\n\n        return {\n            'bond_pred': bond_pred,\n            'hohfeld_pred': hohfeld_pred,\n            'language_pred': language_pred,\n            'period_pred': period_pred,\n            'context_pred': self.context_head(z),  # Auxiliary context prediction\n            'z': z,\n        }\n\n# Dataset with Hohfeld support\nclass NativeDataset(Dataset):\n    def __init__(self, ids_set, passages_file, bonds_file, tokenizer, max_len=128):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.data = []\n\n        with open(passages_file) as fp, open(bonds_file) as fb:\n            for p_line, b_line in tqdm(zip(fp, fb), desc=\"Loading\", unit=\"line\"):\n                p = json.loads(p_line)\n                b = json.loads(b_line)\n                if p['id'] in ids_set and b['passage_id'] == p['id']:\n                    self.data.append({\n                        'text': p['text'][:1000],\n                        'language': p['language'],\n                        'period': p['time_period'],\n                        'bond': b['bonds']['primary_bond'],\n                        'hohfeld': b['bonds'].get('hohfeld'),\n                        'context': b['bonds'].get('context', 'unknown'),\n                        'confidence': b['bonds'].get('confidence', 'medium'),\n                    })\n        print(f\"  Loaded {len(self.data):,} samples\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        enc = self.tokenizer(item['text'], truncation=True, max_length=self.max_len,\n                            padding='max_length', return_tensors='pt')\n        return {\n            'input_ids': enc['input_ids'].squeeze(0),\n            'attention_mask': enc['attention_mask'].squeeze(0),\n            'bond_label': BOND_TO_IDX.get(item['bond'], 9),\n            'language_label': LANG_TO_IDX.get(item['language'], 4),\n            'period_label': PERIOD_TO_IDX.get(item['period'], 9),\n            'hohfeld_label': HOHFELD_TO_IDX.get(item['hohfeld'], 0) if item['hohfeld'] else 0,\n            'context_label': CONTEXT_TO_IDX.get(item['context'], 2),\n            'sample_weight': CONFIDENCE_TO_WEIGHT.get(item['confidence'], 1.0),\n            'language': item['language'],\n            'context': item['context'],\n            'confidence': item['confidence'],\n        }\n\ndef collate_fn(batch):\n    return {\n        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n        'bond_labels': torch.tensor([x['bond_label'] for x in batch]),\n        'language_labels': torch.tensor([x['language_label'] for x in batch]),\n        'period_labels': torch.tensor([x['period_label'] for x in batch]),\n        'hohfeld_labels': torch.tensor([x['hohfeld_label'] for x in batch]),\n        'context_labels': torch.tensor([x['context_label'] for x in batch]),\n        'sample_weights': torch.tensor([x['sample_weight'] for x in batch], dtype=torch.float),\n        'languages': [x['language'] for x in batch],\n        'contexts': [x['context'] for x in batch],\n        'confidences': [x['confidence'] for x in batch],\n    }\n\nprint(\"Model architecture defined\")\nprint(f\"  Bond classes: {len(BondType)}\")\nprint(f\"  Hohfeld states: {len(HohfeldState)}\")\nprint(f\"  Languages: {len(LANG_TO_IDX)}\")\nprint(f\"  Periods: {len(PERIOD_TO_IDX)}\")\nprint(f\"  Context classes: {len(CONTEXT_TO_IDX)}\")\nprint(\"\\n\" + \"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 7. Train BIP Model { display-mode: \"form\" }\n#@markdown Training with tuned adversarial weights and hardware-optimized parameters\n\nfrom sklearn.metrics import f1_score\nimport gc\n\n#@markdown **Splits to train:**\nTRAIN_HEBREW_TO_OTHERS = True  #@param {type:\"boolean\"}\nTRAIN_SEMITIC_TO_NON_SEMITIC = True  #@param {type:\"boolean\"}\nTRAIN_ANCIENT_TO_MODERN = True  #@param {type:\"boolean\"}\nTRAIN_MIXED_BASELINE = True  #@param {type:\"boolean\"}\n\n#@markdown **Hyperparameters:**\nLANG_WEIGHT = 0.01  #@param {type:\"number\"}\nPERIOD_WEIGHT = 0.01  #@param {type:\"number\"}\nN_EPOCHS = 5  #@param {type:\"integer\"}\n\n#@markdown **Context-Aware Training:**\nUSE_CONFIDENCE_WEIGHTING = True  #@param {type:\"boolean\"}\n#@markdown Weight prescriptive (high confidence) examples 2x in loss\n\nUSE_CONTEXT_AUXILIARY = True  #@param {type:\"boolean\"}\n#@markdown Add context prediction as auxiliary training target\n\nCONTEXT_LOSS_WEIGHT = 0.1  #@param {type:\"number\"}\n#@markdown Weight for context prediction loss\n\nSTRICT_PRESCRIPTIVE_TEST = False  #@param {type:\"boolean\"}\n#@markdown Only evaluate on prescriptive examples (strict test)\n\nprint(\"=\"*60)\nprint(\"TRAINING BIP MODEL\")\nprint(\"=\"*60)\nprint(f\"\\nHardware-optimized settings:\")\nprint(f\"  GPU Tier:     {GPU_TIER}\")\nprint(f\"  Batch size:   {BATCH_SIZE}\")\nprint(f\"  Workers:      {NUM_WORKERS}\")\nprint(f\"  Learning rate: {LR:.2e}\")\nprint(f\"  Adv weights:  lang={LANG_WEIGHT}, period={PERIOD_WEIGHT}\")\nprint(\"(0.01 prevents loss explosion while maintaining invariance)\")\nprint(f\"  Confidence weighting: {USE_CONFIDENCE_WEIGHTING}\")\nprint(f\"  Context auxiliary: {USE_CONTEXT_AUXILIARY} (weight={CONTEXT_LOSS_WEIGHT})\")\nprint(f\"  Strict prescriptive test: {STRICT_PRESCRIPTIVE_TEST}\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n\nwith open('data/splits/all_splits.json') as f:\n    all_splits = json.load(f)\n\nsplits_to_train = []\nif TRAIN_HEBREW_TO_OTHERS: splits_to_train.append('hebrew_to_others')\nif TRAIN_SEMITIC_TO_NON_SEMITIC: splits_to_train.append('semitic_to_non_semitic')\nif TRAIN_ANCIENT_TO_MODERN: splits_to_train.append('ancient_to_modern')\nif TRAIN_MIXED_BASELINE: splits_to_train.append('mixed_baseline')\n\nprint(f\"\\nTraining {len(splits_to_train)} splits: {splits_to_train}\")\n\nall_results = {}\n\nfor split_idx, split_name in enumerate(splits_to_train):\n    split_start = time.time()\n    print(\"\\n\" + \"=\"*60)\n    print(f\"[{split_idx+1}/{len(splits_to_train)}] {split_name}\")\n    print(\"=\"*60)\n\n    split = all_splits[split_name]\n    print(f\"Train: {split['train_size']:,} | Test: {split['test_size']:,}\")\n\n    MIN_TEST_SIZE = 500  # Minimum for reliable evaluation\n\n    if split['test_size'] < MIN_TEST_SIZE:\n        print(f\"WARNING: Test set only {split['test_size']} samples (need {MIN_TEST_SIZE})\")\n        print(\"Skipping this split - results would be unreliable\")\n        print(\"To fix: Add more data to the test languages/periods\")\n        continue\n\n    model = BIPModel().to(device)\n\n    train_dataset = NativeDataset(set(split['train_ids']), 'data/processed/passages.jsonl',\n                                   'data/processed/bonds.jsonl', tokenizer)\n\n    test_ids_to_use = split['test_ids'][:MAX_TEST_SAMPLES]\n\n    # Optional: strict prescriptive-only test\n    if STRICT_PRESCRIPTIVE_TEST:\n        print(\"Filtering to prescriptive examples only...\")\n        # Load bonds to filter\n        prescriptive_ids = set()\n        with open('data/processed/bonds.jsonl') as f:\n            for line in f:\n                b = json.loads(line)\n                if b['bonds'].get('context') == 'prescriptive':\n                    prescriptive_ids.add(b['passage_id'])\n        test_ids_to_use = [tid for tid in test_ids_to_use if tid in prescriptive_ids]\n        print(f\"  Filtered to {len(test_ids_to_use):,} prescriptive samples\")\n\n    test_dataset = NativeDataset(set(test_ids_to_use), 'data/processed/passages.jsonl',\n                                  'data/processed/bonds.jsonl', tokenizer)\n\n    if len(train_dataset) == 0:\n        print(\"ERROR: No training data!\")\n        continue\n\n    # Use hardware-optimized batch size\n    actual_batch = min(BATCH_SIZE, max(32, len(train_dataset) // 20))\n    print(f\"Actual batch size: {actual_batch}\")\n\n    train_loader = DataLoader(train_dataset, batch_size=actual_batch, shuffle=True,\n                              collate_fn=collate_fn, drop_last=True, num_workers=NUM_WORKERS, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=actual_batch*2, shuffle=False,\n                             collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n\n    def get_adv_lambda(epoch, warmup=2):\n        if epoch <= warmup:\n            return 0.1 + 0.9 * (epoch / warmup)\n        return 1.0\n\n    best_loss = float('inf')\n\n    for epoch in range(1, N_EPOCHS + 1):\n        model.train()\n        total_loss = 0\n        n_batches = 0\n\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n            optimizer.zero_grad()\n\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            bond_labels = batch['bond_labels'].to(device)\n            language_labels = batch['language_labels'].to(device)\n            period_labels = batch['period_labels'].to(device)\n\n            adv_lambda = get_adv_lambda(epoch)\n\n            # Use new autocast API\n            with torch.amp.autocast('cuda', enabled=USE_AMP):\n                out = model(input_ids, attention_mask, adv_lambda=adv_lambda)\n\n                # Weighted bond loss\n                if USE_CONFIDENCE_WEIGHTING:\n                    sample_weights = batch['sample_weights'].to(device)\n                    loss_bond = F.cross_entropy(out['bond_pred'], bond_labels, reduction='none')\n                    loss_bond = (loss_bond * sample_weights).mean()\n                else:\n                    loss_bond = F.cross_entropy(out['bond_pred'], bond_labels)\n\n                # Context auxiliary loss\n                if USE_CONTEXT_AUXILIARY:\n                    context_labels = batch['context_labels'].to(device)\n                    loss_context = F.cross_entropy(out['context_pred'], context_labels)\n                else:\n                    loss_context = 0\n\n                loss_lang = F.cross_entropy(out['language_pred'], language_labels)\n                loss_period = F.cross_entropy(out['period_pred'], period_labels)\n\n            loss = loss_bond + LANG_WEIGHT * loss_lang + PERIOD_WEIGHT * loss_period + CONTEXT_LOSS_WEIGHT * loss_context\n\n            if USE_AMP and scaler:\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n\n            total_loss += loss.item()\n            n_batches += 1\n\n        avg_loss = total_loss / n_batches\n        print(f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f})\")\n\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            torch.save(model.state_dict(), f'models/checkpoints/best_{split_name}.pt')\n            torch.save(model.state_dict(), f'{SAVE_DIR}/best_{split_name}.pt')\n\n    # Evaluate\n    print(\"\\nEvaluating...\")\n    model.load_state_dict(torch.load(f'models/checkpoints/best_{split_name}.pt'))\n    model.eval()\n\n    all_preds = {'bond': [], 'lang': []}\n    all_labels = {'bond': [], 'lang': []}\n    all_languages = []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Testing\"):\n            out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n            all_preds['bond'].extend(out['bond_pred'].argmax(-1).cpu().tolist())\n            all_preds['lang'].extend(out['language_pred'].argmax(-1).cpu().tolist())\n            all_labels['bond'].extend(batch['bond_labels'].tolist())\n            all_labels['lang'].extend(batch['language_labels'].tolist())\n            all_languages.extend(batch['languages'])\n\n    bond_f1 = f1_score(all_labels['bond'], all_preds['bond'], average='macro', zero_division=0)\n    bond_acc = sum(p == l for p, l in zip(all_preds['bond'], all_labels['bond'])) / len(all_preds['bond'])\n    lang_acc = sum(p == l for p, l in zip(all_preds['lang'], all_labels['lang'])) / len(all_preds['lang'])\n\n    # Per-language F1\n    lang_f1 = {}\n    for lang in set(all_languages):\n        mask = [l == lang for l in all_languages]\n        if sum(mask) > 10:\n            preds = [p for p, m in zip(all_preds['bond'], mask) if m]\n            labels = [l for l, m in zip(all_labels['bond'], mask) if m]\n            lang_f1[lang] = {'f1': f1_score(labels, preds, average='macro', zero_division=0), 'n': sum(mask)}\n\n    all_results[split_name] = {\n        'bond_f1_macro': bond_f1,\n        'bond_acc': bond_acc,\n        'language_acc': lang_acc,\n        'per_language_f1': lang_f1,\n        'training_time': time.time() - split_start\n    }\n\n    print(f\"\\n{split_name} RESULTS:\")\n    print(f\"  Bond F1 (macro): {bond_f1:.3f} ({bond_f1/0.1:.1f}x chance)\")\n    print(f\"  Bond accuracy:   {bond_acc:.1%}\")\n    print(f\"  Language acc:    {lang_acc:.1%} (want ~20% = invariant)\")\n    print(\"  Per-language:\")\n    for lang, m in sorted(lang_f1.items(), key=lambda x: -x[1]['n']):\n        print(f\"    {lang:20s}: F1={m['f1']:.3f} (n={m['n']:,})\")\n\n    # Context analysis\n    high_conf = sum(1 for c in test_dataset.data if c['confidence'] == 'high')\n    prescriptive = sum(1 for c in test_dataset.data if c['context'] == 'prescriptive')\n    print(f\"  Context: {prescriptive:,}/{len(test_dataset):,} prescriptive ({prescriptive/len(test_dataset)*100:.1f}%)\")\n    print(f\"  High confidence: {high_conf:,}/{len(test_dataset):,} ({high_conf/len(test_dataset)*100:.1f}%)\")\n\n    # GPU memory usage\n    if torch.cuda.is_available():\n        mem = torch.cuda.memory_allocated() / 1e9\n        print(f\"\\n  GPU memory: {mem:.1f} GB / {VRAM_GB:.1f} GB ({mem/VRAM_GB*100:.0f}%)\")\n\n    del model, train_dataset, test_dataset\n    gc.collect()\n    torch.cuda.empty_cache()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8. Linear Probe Test { display-mode: \"form\" }\n",
    "#@markdown Tests if z_bond encodes language/period (should be low = invariant)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LINEAR PROBE TEST\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nIf probe accuracy is NEAR CHANCE, representation is INVARIANT\")\n",
    "print(\"(This is what we want for BIP)\")\n",
    "\n",
    "probe_results = {}\n",
    "\n",
    "for split_name in ['hebrew_to_others', 'semitic_to_non_semitic']:\n",
    "    model_path = f'{SAVE_DIR}/best_{split_name}.pt'\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"\\nSkipping {split_name} - no saved model\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PROBE: {split_name}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    model = BIPModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    test_ids = set(all_splits[split_name]['test_ids'][:5000])\n",
    "    test_dataset = NativeDataset(test_ids, 'data/processed/passages.jsonl',\n",
    "                                  'data/processed/bonds.jsonl', tokenizer)\n",
    "    \n",
    "    if len(test_dataset) < 50:\n",
    "        print(f\"  Skip - only {len(test_dataset)} samples\")\n",
    "        continue\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    all_z, all_lang, all_period = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Extract\"):\n",
    "            out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n",
    "            all_z.append(out['z'].cpu().numpy())\n",
    "            all_lang.extend(batch['language_labels'].tolist())\n",
    "            all_period.extend(batch['period_labels'].tolist())\n",
    "    \n",
    "    X = np.vstack(all_z)\n",
    "    y_lang = np.array(all_lang)\n",
    "    y_period = np.array(all_period)\n",
    "    \n",
    "    scaler_probe = StandardScaler()\n",
    "    X_scaled = scaler_probe.fit_transform(X)\n",
    "    \n",
    "    # Train/test split for probes\n",
    "    n = len(X)\n",
    "    idx = np.random.permutation(n)\n",
    "    train_idx, test_idx = idx[:int(0.7*n)], idx[int(0.7*n):]\n",
    "    \n",
    "    # Language probe - check for multiple classes\n",
    "    unique_langs = np.unique(y_lang[test_idx])\n",
    "    if len(unique_langs) < 2:\n",
    "        print(f\"  SKIP language probe - only {len(unique_langs)} class\")\n",
    "        lang_acc = 1.0 / max(1, len(np.unique(y_lang)))\n",
    "        lang_chance = lang_acc\n",
    "    else:\n",
    "        lang_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        lang_probe.fit(X_scaled[train_idx], y_lang[train_idx])\n",
    "        lang_acc = (lang_probe.predict(X_scaled[test_idx]) == y_lang[test_idx]).mean()\n",
    "        lang_chance = 1.0 / len(unique_langs)\n",
    "    \n",
    "    # Period probe - same check\n",
    "    unique_periods = np.unique(y_period[test_idx])\n",
    "    if len(unique_periods) < 2:\n",
    "        print(f\"  SKIP period probe - only {len(unique_periods)} class\")\n",
    "        period_acc = 1.0 / max(1, len(np.unique(y_period)))\n",
    "        period_chance = period_acc\n",
    "    else:\n",
    "        period_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        period_probe.fit(X_scaled[train_idx], y_period[train_idx])\n",
    "        period_acc = (period_probe.predict(X_scaled[test_idx]) == y_period[test_idx]).mean()\n",
    "        period_chance = 1.0 / len(unique_periods)\n",
    "    \n",
    "    lang_status = \"INVARIANT\" if lang_acc < lang_chance + 0.15 else \"NOT invariant\"\n",
    "    period_status = \"INVARIANT\" if period_acc < period_chance + 0.15 else \"NOT invariant\"\n",
    "    \n",
    "    probe_results[split_name] = {\n",
    "        'language_acc': lang_acc,\n",
    "        'language_chance': lang_chance,\n",
    "        'language_status': lang_status,\n",
    "        'period_acc': period_acc,\n",
    "        'period_chance': period_chance,\n",
    "        'period_status': period_status,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nRESULTS:\")\n",
    "    print(f\"  Language: {lang_acc:.1%} (chance: {lang_chance:.1%}) -> {lang_status}\")\n",
    "    print(f\"  Period:   {period_acc:.1%} (chance: {period_chance:.1%}) -> {period_status}\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Probe tests complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9. Final Results { display-mode: \"form\" }\n",
    "#@markdown Comprehensive summary with verdict\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL BIP EVALUATION (v10.2)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nHardware: {GPU_TIER} ({VRAM_GB:.0f}GB VRAM, {RAM_GB:.0f}GB RAM)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"CROSS-DOMAIN TRANSFER RESULTS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "successful_splits = []\n",
    "for name, r in all_results.items():\n",
    "    ratio = r['bond_f1_macro'] / 0.1\n",
    "    lang_acc = r['language_acc']\n",
    "    \n",
    "    transfer_ok = ratio > 1.3\n",
    "    invariant_ok = lang_acc < 0.35  # Near chance (20%)\n",
    "    \n",
    "    status = \"SUCCESS\" if (transfer_ok and invariant_ok) else \"PARTIAL\" if transfer_ok else \"FAIL\"\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Bond F1:     {r['bond_f1_macro']:.3f} ({ratio:.1f}x chance) {'OK' if transfer_ok else 'WEAK'}\")\n",
    "    print(f\"  Language:    {lang_acc:.1%} {'INVARIANT' if invariant_ok else 'LEAKING'}\")\n",
    "    print(f\"  -> {status}\")\n",
    "    \n",
    "    if transfer_ok and invariant_ok:\n",
    "        successful_splits.append(name)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"VERDICT\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "n_success = len(successful_splits)\n",
    "if n_success >= 2:\n",
    "    verdict = \"STRONGLY_SUPPORTED\"\n",
    "    msg = \"Multiple independent transfer paths demonstrate universal structure\"\n",
    "elif n_success >= 1:\n",
    "    verdict = \"SUPPORTED\"\n",
    "    msg = \"At least one transfer path works\"\n",
    "elif any(r['bond_f1_macro'] > 0.13 for r in all_results.values()):\n",
    "    verdict = \"PARTIAL\"\n",
    "    msg = \"Some transfer signal, but not robust\"\n",
    "else:\n",
    "    verdict = \"INCONCLUSIVE\"\n",
    "    msg = \"No clear transfer demonstrated\"\n",
    "\n",
    "print(f\"\\n  Successful transfers: {n_success}/{len(all_results)}\")\n",
    "print(f\"  Splits: {successful_splits if successful_splits else 'None'}\")\n",
    "print(f\"\\n  VERDICT: {verdict}\")\n",
    "print(f\"  {msg}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'all_results': all_results,\n",
    "    'probe_results': probe_results if 'probe_results' in dir() else {},\n",
    "    'successful_splits': successful_splits,\n",
    "    'verdict': verdict,\n",
    "    'hardware': {'gpu': GPU_TIER, 'vram_gb': VRAM_GB, 'ram_gb': RAM_GB},\n",
    "    'settings': {'batch_size': BATCH_SIZE, 'max_per_lang': MAX_PER_LANG, 'num_workers': NUM_WORKERS},\n",
    "    'experiment_time': time.time() - EXPERIMENT_START,\n",
    "}\n",
    "\n",
    "with open('results/final_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "shutil.copy('results/final_results.json', f'{SAVE_DIR}/final_results.json')\n",
    "\n",
    "print(f\"\\nTotal time: {(time.time() - EXPERIMENT_START)/60:.1f} minutes\")\n",
    "print(\"Results saved to Drive!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10. Download Results { display-mode: \"form\" }\n",
    "#@markdown Download all models and results\n",
    "\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "print(\"Creating download package...\")\n",
    "\n",
    "with zipfile.ZipFile('BIP_v10.2_results.zip', 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    # Results\n",
    "    if os.path.exists('results/final_results.json'):\n",
    "        zf.write('results/final_results.json')\n",
    "    \n",
    "    # Models (from Drive)\n",
    "    for f in os.listdir(SAVE_DIR):\n",
    "        if f.endswith('.pt'):\n",
    "            zf.write(f'{SAVE_DIR}/{f}', f'models/{f}')\n",
    "    \n",
    "    # Config\n",
    "    if os.path.exists('data/splits/all_splits.json'):\n",
    "        zf.write('data/splits/all_splits.json')\n",
    "\n",
    "print(\"\\nDownload ready!\")\n",
    "files.download('BIP_v10.2_results.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}