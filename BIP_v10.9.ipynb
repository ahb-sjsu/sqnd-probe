{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIP v10.9 - Bond Invariance PrincipleCross-lingual moral pattern analysis with 8 languages, 26 periods.**Key features:**- LaBSE backbone option for multilingual embedding- Sanskrit, Pali, Buddhist Chinese, Legalist Chinese support- Geometric latent space analysis- Context-aware training- Improved v10.9 corpus detection for cached data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Configuration & Setup { display-mode: \"form\" }\n",
    "# @markdown ## Data Source Configuration\n",
    "\n",
    "DATA_MODE = \"Update missing\"  # @param [\"Refresh all\", \"Update missing\", \"Cache only\"]\n",
    "# @markdown - **Refresh all**: Re-download everything from source (slow, ~2hrs)\n",
    "# @markdown - **Update missing**: Use cache, download only what's missing (recommended)\n",
    "# @markdown - **Cache only**: Use only cached data, fail if missing\n",
    "\n",
    "DRIVE_FOLDER = \"BIP_v10\"  # @param {type:\"string\"}\n",
    "# @markdown Folder name for persistent storage\n",
    "\n",
    "# Derive flags from DATA_MODE\n",
    "USE_DRIVE_DATA = True  # Always use Drive for caching\n",
    "REFRESH_DATA_FROM_SOURCE = DATA_MODE == \"Refresh all\"\n",
    "CACHE_ONLY = DATA_MODE == \"Cache only\"\n",
    "# @markdown ---\n",
    "# @markdown ## Model Backbone\n",
    "BACKBONE = \"MiniLM\"  # @param [\"MiniLM\", \"LaBSE\", \"XLM-R-base\", \"XLM-R-large\"]\n",
    "# @markdown - **MiniLM**: Fast, 118M params, good baseline\n",
    "# @markdown - **LaBSE**: Best cross-lingual alignment, 471M params (recommended)\n",
    "# @markdown - **XLM-R-base**: Strong multilingual, 270M params\n",
    "# @markdown - **XLM-R-large**: Strongest representations, 550M params\n",
    "\n",
    "# Backbone configurations\n",
    "BACKBONE_CONFIGS = {\n",
    "    \"MiniLM\": {\n",
    "        \"model_name\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        \"hidden_size\": 384,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 512,\n",
    "            \"T4\": 256,\n",
    "            \"2xT4\": 512,\n",
    "            \"SMALL\": 128,\n",
    "            \"MINIMAL/CPU\": 64,\n",
    "        },\n",
    "    },\n",
    "    \"LaBSE\": {\n",
    "        \"model_name\": \"sentence-transformers/LaBSE\",\n",
    "        \"hidden_size\": 768,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 256,\n",
    "            \"T4\": 128,\n",
    "            \"2xT4\": 256,\n",
    "            \"SMALL\": 64,\n",
    "            \"MINIMAL/CPU\": 32,\n",
    "        },\n",
    "    },\n",
    "    \"XLM-R-base\": {\n",
    "        \"model_name\": \"xlm-roberta-base\",\n",
    "        \"hidden_size\": 768,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 256,\n",
    "            \"T4\": 128,\n",
    "            \"2xT4\": 256,\n",
    "            \"SMALL\": 64,\n",
    "            \"MINIMAL/CPU\": 32,\n",
    "        },\n",
    "    },\n",
    "    \"XLM-R-large\": {\n",
    "        \"model_name\": \"xlm-roberta-large\",\n",
    "        \"hidden_size\": 1024,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 128,\n",
    "            \"T4\": 64,\n",
    "            \"2xT4\": 128,\n",
    "            \"SMALL\": 32,\n",
    "            \"MINIMAL/CPU\": 16,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "BACKBONE_CONFIG = BACKBONE_CONFIGS[BACKBONE]\n",
    "MODEL_NAME = BACKBONE_CONFIG[\"model_name\"]\n",
    "BACKBONE_HIDDEN = BACKBONE_CONFIG[\"hidden_size\"]\n",
    "\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ## Run Setup\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "EXPERIMENT_START = time.time()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BIP v10.9 - ENVIRONMENT DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===== ENVIRONMENT DETECTION =====\n",
    "# Detect which cloud platform we're running on\n",
    "\n",
    "ENV_NAME = \"UNKNOWN\"\n",
    "ENV_GPU_QUOTA = \"Unknown\"\n",
    "PERSISTENT_STORAGE = None\n",
    "DATA_DIR = \"/content\"  # Default\n",
    "\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"Detect cloud environment and return (name, gpu_quota, storage_path, data_dir)\"\"\"\n",
    "\n",
    "    # 1. Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "\n",
    "        return (\"COLAB\", \"Free: T4 ~12h/day, Pro: L4/A100\", \"/content/drive/MyDrive\", \"/content\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    # 2. Kaggle Kernels\n",
    "    if os.path.exists(\"/kaggle\"):\n",
    "        # Kaggle has /kaggle/input for datasets, /kaggle/working for output\n",
    "        return (\"KAGGLE\", \"Free: 2xT4 30h/week, TPU 30h/week\", \"/kaggle/working\", \"/kaggle/working\")\n",
    "\n",
    "    # 3. Lightning.ai Studios\n",
    "    if os.environ.get(\"LIGHTNING_CLOUDSPACE_HOST\") or os.path.exists(\"/teamspace\"):\n",
    "        # Lightning.ai has /teamspace/studios for persistent storage\n",
    "        return (\n",
    "            \"LIGHTNING_AI\",\n",
    "            \"Free: 22h/month GPU, Pro: A10G/H100\",\n",
    "            \"/teamspace/studios\",\n",
    "            \"/teamspace/studios\",\n",
    "        )\n",
    "\n",
    "    # 4. Paperspace Gradient\n",
    "    if os.environ.get(\"PAPERSPACE_NOTEBOOK_REPO_ID\") or os.path.exists(\"/notebooks\"):\n",
    "        return (\"PAPERSPACE\", \"Free: M4000 6h, Pro: A100/H100\", \"/storage\", \"/notebooks\")\n",
    "\n",
    "    # 5. Saturn Cloud\n",
    "    if os.environ.get(\"SATURN_RESOURCE_ID\") or \"saturn\" in os.environ.get(\"HOSTNAME\", \"\").lower():\n",
    "        return (\n",
    "            \"SATURN_CLOUD\",\n",
    "            \"Free: T4 10h/month, Pro: A10G/A100\",\n",
    "            \"/home/jovyan/workspace\",\n",
    "            \"/home/jovyan\",\n",
    "        )\n",
    "\n",
    "    # 6. HuggingFace Spaces\n",
    "    if os.environ.get(\"SPACE_ID\") or os.environ.get(\"HF_SPACE_ID\"):\n",
    "        return (\n",
    "            \"HUGGINGFACE_SPACES\",\n",
    "            \"Free: CPU only, ZeroGPU: A10G/A100 quota\",\n",
    "            \"/data\",\n",
    "            \"/home/user/app\",\n",
    "        )\n",
    "\n",
    "    # 7. AWS SageMaker Studio Lab\n",
    "    if os.path.exists(\"/home/studio-lab-user\"):\n",
    "        return (\n",
    "            \"SAGEMAKER_STUDIO_LAB\",\n",
    "            \"Free: T4 4h/session, 24h max/day\",\n",
    "            \"/home/studio-lab-user\",\n",
    "            \"/home/studio-lab-user\",\n",
    "        )\n",
    "\n",
    "    # 8. Deepnote\n",
    "    if os.environ.get(\"DEEPNOTE_PROJECT_ID\"):\n",
    "        return (\"DEEPNOTE\", \"Free: CPU, Pro: T4/A10G\", \"/work\", \"/work\")\n",
    "\n",
    "    # 9. Local/Unknown\n",
    "    return (\"LOCAL\", \"Depends on local hardware\", os.getcwd(), os.getcwd())\n",
    "\n",
    "\n",
    "ENV_NAME, ENV_GPU_QUOTA, PERSISTENT_STORAGE, DATA_DIR = detect_environment()\n",
    "\n",
    "print(f\"\\nEnvironment: {ENV_NAME}\")\n",
    "print(f\"GPU Quota:   {ENV_GPU_QUOTA}\")\n",
    "print(f\"Storage:     {PERSISTENT_STORAGE}\")\n",
    "print(f\"Data Dir:    {DATA_DIR}\")\n",
    "\n",
    "# Environment-specific setup\n",
    "ENV_TIPS = {\n",
    "    \"COLAB\": [\n",
    "        \"Tip: Use GPU runtime (Runtime -> Change runtime type -> T4 GPU)\",\n",
    "        \"Tip: Colab Pro gives L4 GPU access (~2x faster than T4)\",\n",
    "    ],\n",
    "    \"KAGGLE\": [\n",
    "        \"Tip: Enable GPU (Settings -> Accelerator -> GPU T4 x2)\",\n",
    "        \"Tip: 30h/week GPU quota resets every Saturday\",\n",
    "        \"Tip: Upload data as a Kaggle Dataset for persistence\",\n",
    "    ],\n",
    "    \"LIGHTNING_AI\": [\n",
    "        \"Tip: Select GPU studio (A10G recommended for this workload)\",\n",
    "        \"Tip: /teamspace/studios persists across sessions\",\n",
    "    ],\n",
    "    \"PAPERSPACE\": [\n",
    "        \"Tip: Use /storage for persistent data across runs\",\n",
    "        \"Tip: Free tier has 6h/month GPU limit\",\n",
    "    ],\n",
    "    \"SATURN_CLOUD\": [\n",
    "        \"Tip: Start a T4 instance from the Resources tab\",\n",
    "        \"Tip: 10h/month free GPU quota\",\n",
    "    ],\n",
    "    \"HUGGINGFACE_SPACES\": [\n",
    "        \"Tip: ZeroGPU provides A10G/A100 access with quota system\",\n",
    "        \"Tip: Use Gradio/Streamlit for interactive demos\",\n",
    "    ],\n",
    "    \"SAGEMAKER_STUDIO_LAB\": [\n",
    "        \"Tip: Request GPU runtime from the launcher\",\n",
    "        \"Tip: Sessions timeout after 4h, max 24h/day\",\n",
    "    ],\n",
    "    \"LOCAL\": [\"Tip: Running locally - ensure CUDA is installed for GPU support\"],\n",
    "}\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(\"ENVIRONMENT TIPS:\")\n",
    "for tip in ENV_TIPS.get(ENV_NAME, [\"No specific tips for this environment\"]):\n",
    "    print(f\"  {tip}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ===== INSTALL DEPENDENCIES =====\n",
    "import subprocess\n",
    "\n",
    "print(\"\\nInstalling dependencies...\")\n",
    "for pkg in [\n",
    "    \"transformers\",\n",
    "    \"sentence-transformers\",\n",
    "    \"pandas\",\n",
    "    \"tqdm\",\n",
    "    \"scikit-learn\",\n",
    "    \"pyyaml\",\n",
    "    \"psutil\",\n",
    "    \"datasets\",\n",
    "]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GPU DETECTION & RESOURCE ALLOCATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Detect hardware\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    GPU_COUNT = torch.cuda.device_count()\n",
    "else:\n",
    "    GPU_NAME = \"CPU\"\n",
    "    VRAM_GB = 0\n",
    "    GPU_COUNT = 0\n",
    "\n",
    "RAM_GB = psutil.virtual_memory().total / 1e9\n",
    "\n",
    "print(f\"\\nDetected Hardware:\")\n",
    "print(f\"  GPU:  {GPU_NAME}\" + (f\" (x{GPU_COUNT})\" if GPU_COUNT > 1 else \"\"))\n",
    "print(\n",
    "    f\"  VRAM: {VRAM_GB:.1f} GB\" + (f\" (total: {VRAM_GB*GPU_COUNT:.1f} GB)\" if GPU_COUNT > 1 else \"\")\n",
    ")\n",
    "print(f\"  RAM:  {RAM_GB:.1f} GB\")\n",
    "\n",
    "# Set optimal parameters based on hardware\n",
    "if VRAM_GB >= 22:  # L4 (24GB) or A100\n",
    "    GPU_TIER = \"L4/A100\"\n",
    "elif VRAM_GB >= 14:  # T4 (16GB)\n",
    "    GPU_TIER = \"T4\"\n",
    "elif VRAM_GB >= 10:\n",
    "    GPU_TIER = \"SMALL\"\n",
    "else:\n",
    "    GPU_TIER = \"MINIMAL/CPU\"\n",
    "\n",
    "# Kaggle with 2xT4 can use larger batch\n",
    "if ENV_NAME == \"KAGGLE\" and GPU_COUNT >= 2:\n",
    "    GPU_TIER = \"2xT4\"\n",
    "    print(f\"  ** Kaggle 2xT4 detected **\")\n",
    "\n",
    "# Get backbone-specific batch size\n",
    "BATCH_SIZE = BACKBONE_CONFIG[\"recommended_batch\"].get(GPU_TIER, 64)\n",
    "print(f\"  Backbone: {BACKBONE} -> batch size {BATCH_SIZE}\")\n",
    "\n",
    "MAX_PER_LANG = 50000  # Language sample limit\n",
    "CPU_CORES = os.cpu_count() or 2\n",
    "NUM_WORKERS = min(4, CPU_CORES - 1) if RAM_GB >= 24 and VRAM_GB >= 14 else 0\n",
    "MAX_TEST_SAMPLES = 20000\n",
    "LR = 2e-5 * (BATCH_SIZE / 256)\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(f\"OPTIMAL SETTINGS:\")\n",
    "print(f\"-\" * 60)\n",
    "print(f\"  Environment:     {ENV_NAME}\")\n",
    "print(f\"  GPU Tier:        {GPU_TIER}\")\n",
    "print(f\"  Backbone:        {BACKBONE}\")\n",
    "print(f\"  Batch size:      {BATCH_SIZE}\")\n",
    "print(f\"  Max per lang:    {MAX_PER_LANG:,}\")\n",
    "print(f\"  DataLoader workers: {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate:   {LR:.2e}\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "scaler = torch.amp.GradScaler(\"cuda\") if USE_AMP else None\n",
    "\n",
    "# ===== PERSISTENT STORAGE SETUP =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERSISTENT STORAGE SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "SAVE_DIR = None\n",
    "DRIVE_HAS_DATA = False\n",
    "DRIVE_FILES = set()  # Use set for O(1) lookup\n",
    "\n",
    "if ENV_NAME == \"COLAB\":\n",
    "    # Google Colab - mount Drive\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "\n",
    "        DRIVE_MOUNT_PATH = \"/content/drive\"\n",
    "\n",
    "        if os.path.exists(f\"{DRIVE_MOUNT_PATH}/MyDrive\"):\n",
    "            print(\"Google Drive already mounted\")\n",
    "        else:\n",
    "            try:\n",
    "                drive.mount(DRIVE_MOUNT_PATH, force_remount=False)\n",
    "                print(\"Google Drive mounted successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Drive mount issue: {e}\")\n",
    "                try:\n",
    "                    drive.mount(DRIVE_MOUNT_PATH, force_remount=True)\n",
    "                    print(\"Google Drive mounted (force remount)\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"WARNING: Could not mount Drive: {e2}\")\n",
    "                    print(\"Falling back to local storage\")\n",
    "                    PERSISTENT_STORAGE = DATA_DIR\n",
    "\n",
    "        SAVE_DIR = f\"{DRIVE_MOUNT_PATH}/MyDrive/{DRIVE_FOLDER}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Colab Drive setup failed: {e}\")\n",
    "        SAVE_DIR = f\"{DATA_DIR}/{DRIVE_FOLDER}\"\n",
    "\n",
    "elif ENV_NAME == \"KAGGLE\":\n",
    "    # Kaggle - use working directory\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using Kaggle working directory: {SAVE_DIR}\")\n",
    "    print(\"Note: Data persists until kernel is reset\")\n",
    "    # Check for uploaded datasets\n",
    "    if os.path.exists(\"/kaggle/input\"):\n",
    "        datasets = os.listdir(\"/kaggle/input\")\n",
    "        if datasets:\n",
    "            print(f\"Available datasets: {datasets[:5]}\")\n",
    "\n",
    "elif ENV_NAME == \"LIGHTNING_AI\":\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using Lightning.ai studio storage: {SAVE_DIR}\")\n",
    "\n",
    "elif ENV_NAME == \"PAPERSPACE\":\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using Paperspace /storage: {SAVE_DIR}\")\n",
    "\n",
    "elif ENV_NAME == \"HUGGINGFACE_SPACES\":\n",
    "    # HF Spaces has limited persistent storage\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using HuggingFace Spaces storage: {SAVE_DIR}\")\n",
    "    print(\"Warning: HF Spaces storage is limited\")\n",
    "\n",
    "else:\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using local storage: {SAVE_DIR}\")\n",
    "\n",
    "# Check if folder exists BEFORE creating it\n",
    "folder_existed = os.path.exists(SAVE_DIR)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Check what's available in storage - use BOTH listdir AND direct exists checks\n",
    "# (Google Drive can have sync issues where listdir misses files)\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    DRIVE_FILES = set(os.listdir(SAVE_DIR))  # O(1) membership test\n",
    "\n",
    "    # Direct existence checks for key files (bypasses listdir caching issues)\n",
    "    key_files = [\"passages.jsonl\", \"bonds.jsonl\", \"dear_abby.csv\", \"all_splits.json\"]\n",
    "    for kf in key_files:\n",
    "        kf_path = os.path.join(SAVE_DIR, kf)\n",
    "        if os.path.exists(kf_path) and kf not in DRIVE_FILES:\n",
    "            print(f\"  [Drive sync fix] Found {kf} via os.path.exists() but not listdir()\")\n",
    "            DRIVE_FILES.add(kf)\n",
    "\n",
    "    DRIVE_HAS_DATA = \"passages.jsonl\" in DRIVE_FILES and \"bonds.jsonl\" in DRIVE_FILES\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(f\"STORAGE STATUS:\")\n",
    "print(f\"-\" * 60)\n",
    "print(f\"  Folder: {SAVE_DIR}\")\n",
    "print(f\"  Folder existed: {folder_existed}\")\n",
    "print(f\"  Files found: {len(DRIVE_FILES)}\")\n",
    "\n",
    "# If folder was empty/new, show what folders exist in parent to help debug\n",
    "if not DRIVE_FILES and ENV_NAME == \"COLAB\":\n",
    "    parent = os.path.dirname(SAVE_DIR)  # e.g., /content/drive/MyDrive\n",
    "    if os.path.exists(parent):\n",
    "        siblings = [d for d in os.listdir(parent) if \"bip\" in d.lower() or \"BIP\" in d]\n",
    "        if siblings:\n",
    "            print(f\"  ** Similar folders in {parent}: {siblings}\")\n",
    "        else:\n",
    "            print(f\"  ** No BIP folders found in {parent}\")\n",
    "if DRIVE_FILES:\n",
    "    for f in sorted(DRIVE_FILES)[:10]:  # sorted() converts to list for slicing\n",
    "        print(f\"    - {f}\")\n",
    "    if len(DRIVE_FILES) > 10:\n",
    "        print(f\"    ... and {len(DRIVE_FILES)-10} more\")\n",
    "print(f\"  Pre-processed data available: {DRIVE_HAS_DATA}\")\n",
    "\n",
    "# Decide data loading strategy\n",
    "LOAD_FROM_DRIVE = USE_DRIVE_DATA and DRIVE_HAS_DATA and not REFRESH_DATA_FROM_SOURCE\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"DATA LOADING STRATEGY: {DATA_MODE}\")\n",
    "print(\"-\" * 60)\n",
    "if DATA_MODE == \"Refresh all\":\n",
    "    print(f\"  -> Will re-download ALL data from online sources\")\n",
    "    print(f\"     (This takes ~2 hours, use 'Update missing' to save time)\")\n",
    "elif DATA_MODE == \"Cache only\":\n",
    "    if LOAD_FROM_DRIVE:\n",
    "        print(f\"  -> Using cached data only (no downloads)\")\n",
    "    else:\n",
    "        print(f\"  -> ERROR: Cache-only mode but no cached data found!\")\n",
    "        print(f\"     Change DATA_MODE to 'Update missing'\")\n",
    "else:  # Update missing (default)\n",
    "    if LOAD_FROM_DRIVE:\n",
    "        print(f\"  -> Using cached processed data from Drive\")\n",
    "        print(f\"     (v10.9 corpora will be added if missing)\")\n",
    "    else:\n",
    "        print(f\"  -> Will download missing data, use cached where available\")\n",
    "        print(\n",
    "            f\"     Sefaria: {'cached' if os.path.exists(f'{SAVE_DIR}/Sefaria-Export-json.tar.gz') else 'will download'}\"\n",
    "        )\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create local directories\n",
    "for d in [\"data/processed\", \"data/splits\", \"data/raw\", \"models/checkpoints\", \"results\"]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"SETUP COMPLETE\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"  Environment: {ENV_NAME}\")\n",
    "print(f\"  GPU:         {GPU_NAME} ({GPU_TIER})\")\n",
    "print(f\"  Storage:     {SAVE_DIR}\")\n",
    "print(f\"  Ready to run: Cell 2 (Imports)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2. Download/Load Corpora { display-mode: \"form\" }\n",
    "# @markdown Downloads from online sources OR loads from Google Drive\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING CORPORA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Force Google Drive sync refresh (workaround for stale FUSE mount)\n",
    "if ENV_NAME == \"COLAB\" and SAVE_DIR and os.path.exists(os.path.dirname(SAVE_DIR)):\n",
    "    try:\n",
    "        # Accessing the directory forces FUSE to refresh\n",
    "        _ = os.listdir(SAVE_DIR)\n",
    "        # Also touch parent to wake up sync\n",
    "        _ = os.listdir(os.path.dirname(SAVE_DIR))\n",
    "        print(\"  [Drive sync refreshed]\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [Drive sync warning: {e}]\")\n",
    "\n",
    "if LOAD_FROM_DRIVE:\n",
    "    # ===== LOAD FROM DRIVE =====\n",
    "    print(\"\\nLoading pre-processed data from Google Drive...\")\n",
    "\n",
    "    # Copy files from Drive to local\n",
    "    for fname in [\"passages.jsonl\", \"bonds.jsonl\"]:\n",
    "        src = f\"{SAVE_DIR}/{fname}\"\n",
    "        dst = f\"data/processed/{fname}\"\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "            print(f\"  Copied {fname}\")\n",
    "\n",
    "    if os.path.exists(f\"{SAVE_DIR}/all_splits.json\"):\n",
    "        shutil.copy(f\"{SAVE_DIR}/all_splits.json\", \"data/splits/all_splits.json\")\n",
    "        print(f\"  Copied all_splits.json\")\n",
    "\n",
    "    # Load Dear Abby from Drive if available (check filesystem, not cached set)\n",
    "    abby_drive_path = f\"{SAVE_DIR}/dear_abby.csv\"\n",
    "    if os.path.exists(abby_drive_path):\n",
    "        shutil.copy(abby_drive_path, \"data/raw/dear_abby.csv\")\n",
    "        print(f\"  Copied dear_abby.csv from {abby_drive_path}\")\n",
    "\n",
    "    # Count loaded data\n",
    "    if os.path.exists(\"data/processed/passages.jsonl\"):\n",
    "        with open(\"data/processed/passages.jsonl\") as f:\n",
    "            n_passages = sum(1 for _ in f)\n",
    "        print(f\"\\nLoaded {n_passages:,} passages from Drive\")\n",
    "\n",
    "    SKIP_PROCESSING = True\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Drive data loaded - skipping download/processing\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "else:\n",
    "    # ===== DOWNLOAD/UPDATE FROM ONLINE =====\n",
    "    SKIP_PROCESSING = False\n",
    "\n",
    "    # Check if CACHE_ONLY mode but cache is missing\n",
    "    if CACHE_ONLY:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ERROR: CACHE_ONLY mode but cached data not found!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Options:\")\n",
    "        print(\"  1. Change DATA_MODE to 'Update missing' or 'Refresh all'\")\n",
    "        print(\"  2. Ensure Drive has: passages.jsonl, bonds.jsonl\")\n",
    "        raise RuntimeError(\"Cache-only mode requires cached data. Change DATA_MODE.\")\n",
    "\n",
    "    # SEFARIA - with Drive caching\n",
    "    sefaria_local = \"data/raw/Sefaria-Export/json\"\n",
    "    sefaria_drive = f\"{SAVE_DIR}/Sefaria-Export-json.tar.gz\" if USE_DRIVE_DATA else None\n",
    "\n",
    "    if os.path.exists(sefaria_local):\n",
    "        print(\"\\n[1/4] Sefaria already exists locally\")\n",
    "    elif sefaria_drive and os.path.exists(sefaria_drive):\n",
    "        print(\"\\n[1/4] Restoring Sefaria from Drive cache...\")\n",
    "        import tarfile\n",
    "\n",
    "        os.makedirs(\"data/raw/Sefaria-Export\", exist_ok=True)\n",
    "        with tarfile.open(sefaria_drive, \"r:gz\") as tar:\n",
    "            tar.extractall(\"data/raw/Sefaria-Export\")\n",
    "        print(\"  Restored from Drive!\")\n",
    "    else:\n",
    "        print(\"\\n[1/4] Downloading Sefaria (~2GB)...\")\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"git\",\n",
    "                \"clone\",\n",
    "                \"--depth\",\n",
    "                \"1\",\n",
    "                \"https://github.com/Sefaria/Sefaria-Export.git\",\n",
    "                \"data/raw/Sefaria-Export\",\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "        print(\"  Done!\")\n",
    "        # Cache to Drive for next time\n",
    "        if USE_DRIVE_DATA and SAVE_DIR:\n",
    "            print(\"  Caching Sefaria to Drive (this may take a minute)...\")\n",
    "            import tarfile\n",
    "\n",
    "            with tarfile.open(sefaria_drive, \"w:gz\") as tar:\n",
    "                tar.add(\"data/raw/Sefaria-Export/json\", arcname=\"json\")\n",
    "            print(f\"  Cached to {sefaria_drive}\")\n",
    "\n",
    "    # CHINESE - 200+ REAL CLASSICAL TEXTS\n",
    "    print(\"\\n[2/4] Chinese classics (200+ real passages)...\")\n",
    "    os.makedirs(\"data/raw/chinese\", exist_ok=True)\n",
    "\n",
    "    chinese = []\n",
    "\n",
    "    # === ANALECTS (\u8ad6\u8a9e) - 50+ passages ===\n",
    "    analects = [\n",
    "        (\"\u5b50\u66f0\uff1a\u5df1\u6240\u4e0d\u6b32\uff0c\u52ff\u65bd\u65bc\u4eba\u3002\", \"Analects 15.24\"),\n",
    "        (\"\u5b5d\u608c\u4e5f\u8005\uff0c\u5176\u70ba\u4ec1\u4e4b\u672c\u8207\u3002\", \"Analects 1.2\"),\n",
    "        (\"\u7236\u6bcd\u5728\uff0c\u4e0d\u9060\u6e38\uff0c\u904a\u5fc5\u6709\u65b9\u3002\", \"Analects 4.19\"),\n",
    "        (\"\u541b\u5b50\u55bb\u65bc\u7fa9\uff0c\u5c0f\u4eba\u55bb\u65bc\u5229\u3002\", \"Analects 4.16\"),\n",
    "        (\"\u4e0d\u7fa9\u800c\u5bcc\u4e14\u8cb4\uff0c\u65bc\u6211\u5982\u6d6e\u96f2\u3002\", \"Analects 7.16\"),\n",
    "        (\"\u5b78\u800c\u6642\u7fd2\u4e4b\uff0c\u4e0d\u4ea6\u8aaa\u4e4e\u3002\", \"Analects 1.1\"),\n",
    "        (\"\u6709\u670b\u81ea\u9060\u65b9\u4f86\uff0c\u4e0d\u4ea6\u6a02\u4e4e\u3002\", \"Analects 1.1\"),\n",
    "        (\"\u4eba\u4e0d\u77e5\u800c\u4e0d\u614d\uff0c\u4e0d\u4ea6\u541b\u5b50\u4e4e\u3002\", \"Analects 1.1\"),\n",
    "        (\"\u5de7\u8a00\u4ee4\u8272\uff0c\u9bae\u77e3\u4ec1\u3002\", \"Analects 1.3\"),\n",
    "        (\"\u543e\u65e5\u4e09\u7701\u543e\u8eab\u3002\", \"Analects 1.4\"),\n",
    "        (\"\u70ba\u4eba\u8b00\u800c\u4e0d\u5fe0\u4e4e\uff0c\u8207\u670b\u53cb\u4ea4\u800c\u4e0d\u4fe1\u4e4e\u3002\", \"Analects 1.4\"),\n",
    "        (\"\u5f1f\u5b50\u5165\u5247\u5b5d\uff0c\u51fa\u5247\u608c\u3002\", \"Analects 1.6\"),\n",
    "        (\"\u8b39\u800c\u4fe1\uff0c\u6c4e\u611b\u773e\uff0c\u800c\u89aa\u4ec1\u3002\", \"Analects 1.6\"),\n",
    "        (\"\u541b\u5b50\u4e0d\u91cd\u5247\u4e0d\u5a01\uff0c\u5b78\u5247\u4e0d\u56fa\u3002\", \"Analects 1.8\"),\n",
    "        (\"\u4e3b\u5fe0\u4fe1\uff0c\u7121\u53cb\u4e0d\u5982\u5df1\u8005\u3002\", \"Analects 1.8\"),\n",
    "        (\"\u904e\u5247\u52ff\u619a\u6539\u3002\", \"Analects 1.8\"),\n",
    "        (\"\u614e\u7d42\u8ffd\u9060\uff0c\u6c11\u5fb7\u6b78\u539a\u77e3\u3002\", \"Analects 1.9\"),\n",
    "        (\"\u79ae\u4e4b\u7528\uff0c\u548c\u70ba\u8cb4\u3002\", \"Analects 1.12\"),\n",
    "        (\"\u4fe1\u8fd1\u65bc\u7fa9\uff0c\u8a00\u53ef\u5fa9\u4e5f\u3002\", \"Analects 1.13\"),\n",
    "        (\"\u541b\u5b50\u98df\u7121\u6c42\u98fd\uff0c\u5c45\u7121\u6c42\u5b89\u3002\", \"Analects 1.14\"),\n",
    "        (\"\u654f\u65bc\u4e8b\u800c\u614e\u65bc\u8a00\uff0c\u5c31\u6709\u9053\u800c\u6b63\u7109\u3002\", \"Analects 1.14\"),\n",
    "        (\"\u4e0d\u60a3\u4eba\u4e4b\u4e0d\u5df1\u77e5\uff0c\u60a3\u4e0d\u77e5\u4eba\u4e5f\u3002\", \"Analects 1.16\"),\n",
    "        (\"\u70ba\u653f\u4ee5\u5fb7\uff0c\u8b6c\u5982\u5317\u8fb0\u3002\", \"Analects 2.1\"),\n",
    "        (\"\u9053\u4e4b\u4ee5\u653f\uff0c\u9f4a\u4e4b\u4ee5\u5211\uff0c\u6c11\u514d\u800c\u7121\u6065\u3002\", \"Analects 2.3\"),\n",
    "        (\"\u9053\u4e4b\u4ee5\u5fb7\uff0c\u9f4a\u4e4b\u4ee5\u79ae\uff0c\u6709\u6065\u4e14\u683c\u3002\", \"Analects 2.3\"),\n",
    "        (\"\u543e\u5341\u6709\u4e94\u800c\u5fd7\u4e8e\u5b78\u3002\", \"Analects 2.4\"),\n",
    "        (\"\u4e09\u5341\u800c\u7acb\uff0c\u56db\u5341\u800c\u4e0d\u60d1\u3002\", \"Analects 2.4\"),\n",
    "        (\"\u4e94\u5341\u800c\u77e5\u5929\u547d\uff0c\u516d\u5341\u800c\u8033\u9806\u3002\", \"Analects 2.4\"),\n",
    "        (\"\u4e03\u5341\u800c\u5f9e\u5fc3\u6240\u6b32\uff0c\u4e0d\u903e\u77e9\u3002\", \"Analects 2.4\"),\n",
    "        (\"\u751f\uff0c\u4e8b\u4e4b\u4ee5\u79ae\uff1b\u6b7b\uff0c\u846c\u4e4b\u4ee5\u79ae\uff0c\u796d\u4e4b\u4ee5\u79ae\u3002\", \"Analects 2.5\"),\n",
    "        (\"\u7236\u6bcd\u552f\u5176\u75be\u4e4b\u6182\u3002\", \"Analects 2.6\"),\n",
    "        (\"\u4eca\u4e4b\u5b5d\u8005\uff0c\u662f\u8b02\u80fd\u990a\u3002\", \"Analects 2.7\"),\n",
    "        (\"\u81f3\u65bc\u72ac\u99ac\uff0c\u7686\u80fd\u6709\u990a\uff1b\u4e0d\u656c\uff0c\u4f55\u4ee5\u5225\u4e4e\u3002\", \"Analects 2.7\"),\n",
    "        (\"\u8272\u96e3\u3002\u6709\u4e8b\uff0c\u5f1f\u5b50\u670d\u5176\u52de\u3002\", \"Analects 2.8\"),\n",
    "        (\"\u8996\u5176\u6240\u4ee5\uff0c\u89c0\u5176\u6240\u7531\uff0c\u5bdf\u5176\u6240\u5b89\u3002\", \"Analects 2.10\"),\n",
    "        (\"\u6eab\u6545\u800c\u77e5\u65b0\uff0c\u53ef\u4ee5\u70ba\u5e2b\u77e3\u3002\", \"Analects 2.11\"),\n",
    "        (\"\u541b\u5b50\u4e0d\u5668\u3002\", \"Analects 2.12\"),\n",
    "        (\"\u5148\u884c\u5176\u8a00\u800c\u5f8c\u5f9e\u4e4b\u3002\", \"Analects 2.13\"),\n",
    "        (\"\u541b\u5b50\u5468\u800c\u4e0d\u6bd4\uff0c\u5c0f\u4eba\u6bd4\u800c\u4e0d\u5468\u3002\", \"Analects 2.14\"),\n",
    "        (\"\u5b78\u800c\u4e0d\u601d\u5247\u7f54\uff0c\u601d\u800c\u4e0d\u5b78\u5247\u6b86\u3002\", \"Analects 2.15\"),\n",
    "        (\"\u77e5\u4e4b\u70ba\u77e5\u4e4b\uff0c\u4e0d\u77e5\u70ba\u4e0d\u77e5\uff0c\u662f\u77e5\u4e5f\u3002\", \"Analects 2.17\"),\n",
    "        (\"\u591a\u805e\u95d5\u7591\uff0c\u614e\u8a00\u5176\u9918\uff0c\u5247\u5be1\u5c24\u3002\", \"Analects 2.18\"),\n",
    "        (\"\u8209\u76f4\u932f\u8af8\u6789\uff0c\u5247\u6c11\u670d\u3002\", \"Analects 2.19\"),\n",
    "        (\"\u4eba\u800c\u7121\u4fe1\uff0c\u4e0d\u77e5\u5176\u53ef\u4e5f\u3002\", \"Analects 2.22\"),\n",
    "        (\"\u898b\u7fa9\u4e0d\u70ba\uff0c\u7121\u52c7\u4e5f\u3002\", \"Analects 2.24\"),\n",
    "        (\"\u975e\u5176\u9b3c\u800c\u796d\u4e4b\uff0c\u8ac2\u4e5f\u3002\", \"Analects 2.24\"),\n",
    "        (\"\u662f\u53ef\u5fcd\u4e5f\uff0c\u5b70\u4e0d\u53ef\u5fcd\u4e5f\u3002\", \"Analects 3.1\"),\n",
    "        (\"\u4eba\u800c\u4e0d\u4ec1\uff0c\u5982\u79ae\u4f55\u3002\", \"Analects 3.3\"),\n",
    "        (\"\u4eba\u800c\u4e0d\u4ec1\uff0c\u5982\u6a02\u4f55\u3002\", \"Analects 3.3\"),\n",
    "        (\"\u91cc\u4ec1\u70ba\u7f8e\u3002\u64c7\u4e0d\u8655\u4ec1\uff0c\u7109\u5f97\u77e5\u3002\", \"Analects 4.1\"),\n",
    "        (\"\u4e0d\u4ec1\u8005\u4e0d\u53ef\u4ee5\u4e45\u8655\u7d04\uff0c\u4e0d\u53ef\u4ee5\u9577\u8655\u6a02\u3002\", \"Analects 4.2\"),\n",
    "        (\"\u4ec1\u8005\u5b89\u4ec1\uff0c\u77e5\u8005\u5229\u4ec1\u3002\", \"Analects 4.2\"),\n",
    "        (\"\u552f\u4ec1\u8005\u80fd\u597d\u4eba\uff0c\u80fd\u60e1\u4eba\u3002\", \"Analects 4.3\"),\n",
    "        (\"\u82df\u5fd7\u65bc\u4ec1\u77e3\uff0c\u7121\u60e1\u4e5f\u3002\", \"Analects 4.4\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(analects):\n",
    "        chinese.append(\n",
    "            {\n",
    "                \"id\": f\"cn_analects_{i}\",\n",
    "                \"text\": text,\n",
    "                \"source\": source,\n",
    "                \"period\": \"CONFUCIAN\",\n",
    "                \"century\": -5,\n",
    "            }\n",
    "        )\n",
    "    print(f\"    - Analects: {len([x for x in chinese if 'analects' in x['id']]):,} passages\")\n",
    "\n",
    "    # === MENCIUS (\u5b5f\u5b50) - 40+ passages ===\n",
    "    mencius = [\n",
    "        (\"\u60fb\u96b1\u4e4b\u5fc3\uff0c\u4ec1\u4e4b\u7aef\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7f9e\u60e1\u4e4b\u5fc3\uff0c\u7fa9\u4e4b\u7aef\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u8fad\u8b93\u4e4b\u5fc3\uff0c\u79ae\u4e4b\u7aef\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u662f\u975e\u4e4b\u5fc3\uff0c\u667a\u4e4b\u7aef\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u4eba\u7686\u6709\u4e0d\u5fcd\u4eba\u4e4b\u5fc3\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7121\u60fb\u96b1\u4e4b\u5fc3\uff0c\u975e\u4eba\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7121\u7f9e\u60e1\u4e4b\u5fc3\uff0c\u975e\u4eba\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7121\u8fad\u8b93\u4e4b\u5fc3\uff0c\u975e\u4eba\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7121\u662f\u975e\u4e4b\u5fc3\uff0c\u975e\u4eba\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u4ec1\u7fa9\u79ae\u667a\uff0c\u975e\u7531\u5916\u9460\u6211\u4e5f\uff0c\u6211\u56fa\u6709\u4e4b\u4e5f\u3002\", \"Mencius 6A.6\"),\n",
    "        (\"\u4eba\u6027\u4e4b\u5584\u4e5f\uff0c\u7336\u6c34\u4e4b\u5c31\u4e0b\u4e5f\u3002\", \"Mencius 6A.2\"),\n",
    "        (\"\u4eba\u7121\u6709\u4e0d\u5584\uff0c\u6c34\u7121\u6709\u4e0d\u4e0b\u3002\", \"Mencius 6A.2\"),\n",
    "        (\"\u60df\u4ec1\u8005\u5b9c\u5728\u9ad8\u4f4d\u3002\", \"Mencius 4A.1\"),\n",
    "        (\"\u4e0d\u4ec1\u800c\u5728\u9ad8\u4f4d\uff0c\u662f\u64ad\u5176\u60e1\u65bc\u773e\u4e5f\u3002\", \"Mencius 4A.1\"),\n",
    "        (\"\u6c11\u70ba\u8cb4\uff0c\u793e\u7a37\u6b21\u4e4b\uff0c\u541b\u70ba\u8f15\u3002\", \"Mencius 7B.14\"),\n",
    "        (\"\u5f97\u9053\u8005\u591a\u52a9\uff0c\u5931\u9053\u8005\u5be1\u52a9\u3002\", \"Mencius 2B.1\"),\n",
    "        (\"\u5be1\u52a9\u4e4b\u81f3\uff0c\u89aa\u621a\u7554\u4e4b\u3002\", \"Mencius 2B.1\"),\n",
    "        (\"\u591a\u52a9\u4e4b\u81f3\uff0c\u5929\u4e0b\u9806\u4e4b\u3002\", \"Mencius 2B.1\"),\n",
    "        (\"\u5929\u6642\u4e0d\u5982\u5730\u5229\uff0c\u5730\u5229\u4e0d\u5982\u4eba\u548c\u3002\", \"Mencius 2B.1\"),\n",
    "        (\"\u751f\u65bc\u6182\u60a3\uff0c\u6b7b\u65bc\u5b89\u6a02\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u5929\u5c07\u964d\u5927\u4efb\u65bc\u662f\u4eba\u4e5f\uff0c\u5fc5\u5148\u82e6\u5176\u5fc3\u5fd7\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u52de\u5176\u7b4b\u9aa8\uff0c\u9913\u5176\u9ad4\u819a\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u7a7a\u4e4f\u5176\u8eab\uff0c\u884c\u62c2\u4e82\u5176\u6240\u70ba\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u6240\u4ee5\u52d5\u5fc3\u5fcd\u6027\uff0c\u66fe\u76ca\u5176\u6240\u4e0d\u80fd\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u8001\u543e\u8001\uff0c\u4ee5\u53ca\u4eba\u4e4b\u8001\u3002\", \"Mencius 1A.7\"),\n",
    "        (\"\u5e7c\u543e\u5e7c\uff0c\u4ee5\u53ca\u4eba\u4e4b\u5e7c\u3002\", \"Mencius 1A.7\"),\n",
    "        (\"\u7aae\u5247\u7368\u5584\u5176\u8eab\uff0c\u9054\u5247\u517c\u5584\u5929\u4e0b\u3002\", \"Mencius 7A.9\"),\n",
    "        (\"\u9b5a\uff0c\u6211\u6240\u6b32\u4e5f\uff1b\u718a\u638c\uff0c\u4ea6\u6211\u6240\u6b32\u4e5f\u3002\", \"Mencius 6A.10\"),\n",
    "        (\"\u4e8c\u8005\u4e0d\u53ef\u5f97\u517c\uff0c\u820d\u9b5a\u800c\u53d6\u718a\u638c\u8005\u4e5f\u3002\", \"Mencius 6A.10\"),\n",
    "        (\"\u751f\uff0c\u4ea6\u6211\u6240\u6b32\u4e5f\uff1b\u7fa9\uff0c\u4ea6\u6211\u6240\u6b32\u4e5f\u3002\", \"Mencius 6A.10\"),\n",
    "        (\"\u4e8c\u8005\u4e0d\u53ef\u5f97\u517c\uff0c\u820d\u751f\u800c\u53d6\u7fa9\u8005\u4e5f\u3002\", \"Mencius 6A.10\"),\n",
    "        (\"\u990a\u5fc3\u83ab\u5584\u65bc\u5be1\u6b32\u3002\", \"Mencius 7B.35\"),\n",
    "        (\"\u4ec1\u8005\u7121\u6575\u65bc\u5929\u4e0b\u3002\", \"Mencius 1A.5\"),\n",
    "        (\"\u4ee5\u529b\u670d\u4eba\u8005\uff0c\u975e\u5fc3\u670d\u4e5f\u3002\", \"Mencius 2A.3\"),\n",
    "        (\"\u4ee5\u5fb7\u670d\u4eba\u8005\uff0c\u4e2d\u5fc3\u6085\u800c\u8aa0\u670d\u4e5f\u3002\", \"Mencius 2A.3\"),\n",
    "        (\"\u4eba\u4e4b\u60a3\u5728\u597d\u70ba\u4eba\u5e2b\u3002\", \"Mencius 4A.23\"),\n",
    "        (\"\u76e1\u4fe1\u66f8\uff0c\u5247\u4e0d\u5982\u7121\u66f8\u3002\", \"Mencius 7B.3\"),\n",
    "        (\"\u4e0d\u4ee5\u898f\u77e9\uff0c\u4e0d\u80fd\u6210\u65b9\u5713\u3002\", \"Mencius 4A.1\"),\n",
    "        (\"\u5b5d\u5b50\u4e4b\u81f3\uff0c\u83ab\u5927\u4e4e\u5c0a\u89aa\u3002\", \"Mencius 5A.4\"),\n",
    "        (\"\u7236\u5b50\u6709\u89aa\uff0c\u541b\u81e3\u6709\u7fa9\uff0c\u592b\u5a66\u6709\u5225\uff0c\u9577\u5e7c\u6709\u5e8f\uff0c\u670b\u53cb\u6709\u4fe1\u3002\", \"Mencius 3A.4\"),\n",
    "        (\"\u4eba\u6709\u4e0d\u70ba\u4e5f\uff0c\u800c\u5f8c\u53ef\u4ee5\u6709\u70ba\u3002\", \"Mencius 4B.8\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(mencius):\n",
    "        chinese.append(\n",
    "            {\n",
    "                \"id\": f\"cn_mencius_{i}\",\n",
    "                \"text\": text,\n",
    "                \"source\": source,\n",
    "                \"period\": \"CONFUCIAN\",\n",
    "                \"century\": -4,\n",
    "            }\n",
    "        )\n",
    "    print(f\"    - Mencius: {len([x for x in chinese if 'mencius' in x['id']]):,} passages\")\n",
    "\n",
    "    # === DAODEJING (\u9053\u5fb7\u7d93) - 40+ passages ===\n",
    "    daodejing = [\n",
    "        (\"\u9053\u53ef\u9053\uff0c\u975e\u5e38\u9053\u3002\u540d\u53ef\u540d\uff0c\u975e\u5e38\u540d\u3002\", \"Daodejing 1\"),\n",
    "        (\"\u5929\u4e0b\u7686\u77e5\u7f8e\u4e4b\u70ba\u7f8e\uff0c\u65af\u60e1\u5df2\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u7686\u77e5\u5584\u4e4b\u70ba\u5584\uff0c\u65af\u4e0d\u5584\u5df2\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u6709\u7121\u76f8\u751f\uff0c\u96e3\u6613\u76f8\u6210\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u9577\u77ed\u76f8\u8f03\uff0c\u9ad8\u4e0b\u76f8\u50be\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u662f\u4ee5\u8056\u4eba\u8655\u7121\u70ba\u4e4b\u4e8b\uff0c\u884c\u4e0d\u8a00\u4e4b\u6559\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u4e0d\u5c1a\u8ce2\uff0c\u4f7f\u6c11\u4e0d\u722d\u3002\", \"Daodejing 3\"),\n",
    "        (\"\u4e0d\u8cb4\u96e3\u5f97\u4e4b\u8ca8\uff0c\u4f7f\u6c11\u4e0d\u70ba\u76dc\u3002\", \"Daodejing 3\"),\n",
    "        (\"\u4e0a\u5584\u82e5\u6c34\u3002\u6c34\u5584\u5229\u842c\u7269\u800c\u4e0d\u722d\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u8655\u773e\u4eba\u4e4b\u6240\u60e1\uff0c\u6545\u5e7e\u65bc\u9053\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u5c45\u5584\u5730\uff0c\u5fc3\u5584\u6df5\uff0c\u8207\u5584\u4ec1\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u8a00\u5584\u4fe1\uff0c\u653f\u5584\u6cbb\uff0c\u4e8b\u5584\u80fd\uff0c\u52d5\u5584\u6642\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u592b\u552f\u4e0d\u722d\uff0c\u6545\u7121\u5c24\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u91d1\u7389\u6eff\u5802\uff0c\u83ab\u4e4b\u80fd\u5b88\u3002\", \"Daodejing 9\"),\n",
    "        (\"\u5bcc\u8cb4\u800c\u9a55\uff0c\u81ea\u907a\u5176\u548e\u3002\", \"Daodejing 9\"),\n",
    "        (\"\u529f\u6210\u8eab\u9000\uff0c\u5929\u4e4b\u9053\u4e5f\u3002\", \"Daodejing 9\"),\n",
    "        (\"\u77e5\u4eba\u8005\u667a\uff0c\u81ea\u77e5\u8005\u660e\u3002\", \"Daodejing 33\"),\n",
    "        (\"\u52dd\u4eba\u8005\u6709\u529b\uff0c\u81ea\u52dd\u8005\u5f37\u3002\", \"Daodejing 33\"),\n",
    "        (\"\u77e5\u8db3\u8005\u5bcc\uff0c\u5f37\u884c\u8005\u6709\u5fd7\u3002\", \"Daodejing 33\"),\n",
    "        (\"\u4e0d\u5931\u5176\u6240\u8005\u4e45\uff0c\u6b7b\u800c\u4e0d\u4ea1\u8005\u58fd\u3002\", \"Daodejing 33\"),\n",
    "        (\"\u5927\u9053\u5ee2\uff0c\u6709\u4ec1\u7fa9\u3002\", \"Daodejing 18\"),\n",
    "        (\"\u667a\u6167\u51fa\uff0c\u6709\u5927\u507d\u3002\", \"Daodejing 18\"),\n",
    "        (\"\u516d\u89aa\u4e0d\u548c\uff0c\u6709\u5b5d\u6148\u3002\", \"Daodejing 18\"),\n",
    "        (\"\u570b\u5bb6\u660f\u4e82\uff0c\u6709\u5fe0\u81e3\u3002\", \"Daodejing 18\"),\n",
    "        (\"\u798d\u516e\u798f\u4e4b\u6240\u501a\uff0c\u798f\u516e\u798d\u4e4b\u6240\u4f0f\u3002\", \"Daodejing 58\"),\n",
    "        (\"\u5929\u9577\u5730\u4e45\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u5929\u5730\u6240\u4ee5\u80fd\u9577\u4e14\u4e45\u8005\uff0c\u4ee5\u5176\u4e0d\u81ea\u751f\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u662f\u4ee5\u8056\u4eba\u5f8c\u5176\u8eab\u800c\u8eab\u5148\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u5916\u5176\u8eab\u800c\u8eab\u5b58\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u975e\u4ee5\u5176\u7121\u79c1\u8036\uff0c\u6545\u80fd\u6210\u5176\u79c1\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u67d4\u5f31\u52dd\u525b\u5f37\u3002\", \"Daodejing 36\"),\n",
    "        (\"\u5927\u65b9\u7121\u9685\uff0c\u5927\u5668\u665a\u6210\u3002\", \"Daodejing 41\"),\n",
    "        (\"\u5927\u97f3\u5e0c\u8072\uff0c\u5927\u8c61\u7121\u5f62\u3002\", \"Daodejing 41\"),\n",
    "        (\"\u9053\u751f\u4e00\uff0c\u4e00\u751f\u4e8c\uff0c\u4e8c\u751f\u4e09\uff0c\u4e09\u751f\u842c\u7269\u3002\", \"Daodejing 42\"),\n",
    "        (\"\u5929\u4e0b\u842c\u7269\u751f\u65bc\u6709\uff0c\u6709\u751f\u65bc\u7121\u3002\", \"Daodejing 40\"),\n",
    "        (\"\u5343\u91cc\u4e4b\u884c\uff0c\u59cb\u65bc\u8db3\u4e0b\u3002\", \"Daodejing 64\"),\n",
    "        (\"\u5408\u62b1\u4e4b\u6728\uff0c\u751f\u65bc\u6beb\u672b\u3002\", \"Daodejing 64\"),\n",
    "        (\"\u4e5d\u5c64\u4e4b\u81fa\uff0c\u8d77\u65bc\u7d2f\u571f\u3002\", \"Daodejing 64\"),\n",
    "        (\"\u6c11\u4e0d\u754f\u6b7b\uff0c\u5948\u4f55\u4ee5\u6b7b\u61fc\u4e4b\u3002\", \"Daodejing 74\"),\n",
    "        (\"\u4fe1\u8a00\u4e0d\u7f8e\uff0c\u7f8e\u8a00\u4e0d\u4fe1\u3002\", \"Daodejing 81\"),\n",
    "        (\"\u5584\u8005\u4e0d\u8faf\uff0c\u8faf\u8005\u4e0d\u5584\u3002\", \"Daodejing 81\"),\n",
    "        (\"\u77e5\u8005\u4e0d\u535a\uff0c\u535a\u8005\u4e0d\u77e5\u3002\", \"Daodejing 81\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(daodejing):\n",
    "        chinese.append(\n",
    "            {\n",
    "                \"id\": f\"cn_daodejing_{i}\",\n",
    "                \"text\": text,\n",
    "                \"source\": source,\n",
    "                \"period\": \"DAOIST\",\n",
    "                \"century\": -4,\n",
    "            }\n",
    "        )\n",
    "    print(f\"    - Daodejing: {len([x for x in chinese if 'daodejing' in x['id']]):,} passages\")\n",
    "\n",
    "    # === GREAT LEARNING (\u5927\u5b78) - 20+ passages ===\n",
    "    daxue = [\n",
    "        (\"\u5927\u5b78\u4e4b\u9053\uff0c\u5728\u660e\u660e\u5fb7\uff0c\u5728\u89aa\u6c11\uff0c\u5728\u6b62\u65bc\u81f3\u5584\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u77e5\u6b62\u800c\u5f8c\u6709\u5b9a\uff0c\u5b9a\u800c\u5f8c\u80fd\u975c\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u975c\u800c\u5f8c\u80fd\u5b89\uff0c\u5b89\u800c\u5f8c\u80fd\u616e\uff0c\u616e\u800c\u5f8c\u80fd\u5f97\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u7269\u6709\u672c\u672b\uff0c\u4e8b\u6709\u7d42\u59cb\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u77e5\u6240\u5148\u5f8c\uff0c\u5247\u8fd1\u9053\u77e3\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u53e4\u4e4b\u6b32\u660e\u660e\u5fb7\u65bc\u5929\u4e0b\u8005\uff0c\u5148\u6cbb\u5176\u570b\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u6cbb\u5176\u570b\u8005\uff0c\u5148\u9f4a\u5176\u5bb6\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u9f4a\u5176\u5bb6\u8005\uff0c\u5148\u4fee\u5176\u8eab\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u4fee\u5176\u8eab\u8005\uff0c\u5148\u6b63\u5176\u5fc3\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u6b63\u5176\u5fc3\u8005\uff0c\u5148\u8aa0\u5176\u610f\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u8aa0\u5176\u610f\u8005\uff0c\u5148\u81f4\u5176\u77e5\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u81f4\u77e5\u5728\u683c\u7269\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u7269\u683c\u800c\u5f8c\u77e5\u81f3\uff0c\u77e5\u81f3\u800c\u5f8c\u610f\u8aa0\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u610f\u8aa0\u800c\u5f8c\u5fc3\u6b63\uff0c\u5fc3\u6b63\u800c\u5f8c\u8eab\u4fee\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u8eab\u4fee\u800c\u5f8c\u5bb6\u9f4a\uff0c\u5bb6\u9f4a\u800c\u5f8c\u570b\u6cbb\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u570b\u6cbb\u800c\u5f8c\u5929\u4e0b\u5e73\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u81ea\u5929\u5b50\u4ee5\u81f3\u65bc\u5eb6\u4eba\uff0c\u58f9\u662f\u7686\u4ee5\u4fee\u8eab\u70ba\u672c\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u5176\u672c\u4e82\u800c\u672b\u6cbb\u8005\u5426\u77e3\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6240\u8b02\u8aa0\u5176\u610f\u8005\uff0c\u6bcb\u81ea\u6b3a\u4e5f\u3002\", \"Great Learning 6\"),\n",
    "        (\"\u5982\u60e1\u60e1\u81ed\uff0c\u5982\u597d\u597d\u8272\uff0c\u6b64\u4e4b\u8b02\u81ea\u8b19\u3002\", \"Great Learning 6\"),\n",
    "        (\"\u6545\u541b\u5b50\u5fc5\u614e\u5176\u7368\u4e5f\u3002\", \"Great Learning 6\"),\n",
    "        (\"\u5bcc\u6f64\u5c4b\uff0c\u5fb7\u6f64\u8eab\uff0c\u5fc3\u5ee3\u9ad4\u80d6\u3002\", \"Great Learning 6\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(daxue):\n",
    "        chinese.append(\n",
    "            {\n",
    "                \"id\": f\"cn_daxue_{i}\",\n",
    "                \"text\": text,\n",
    "                \"source\": source,\n",
    "                \"period\": \"CONFUCIAN\",\n",
    "                \"century\": -5,\n",
    "            }\n",
    "        )\n",
    "    print(f\"    - Great Learning: {len([x for x in chinese if 'daxue' in x['id']]):,} passages\")\n",
    "\n",
    "    # === DOCTRINE OF THE MEAN (\u4e2d\u5eb8) - 20+ passages ===\n",
    "    zhongyong = [\n",
    "        (\"\u5929\u547d\u4e4b\u8b02\u6027\uff0c\u7387\u6027\u4e4b\u8b02\u9053\uff0c\u4fee\u9053\u4e4b\u8b02\u6559\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u9053\u4e5f\u8005\uff0c\u4e0d\u53ef\u9808\u81fe\u96e2\u4e5f\uff1b\u53ef\u96e2\uff0c\u975e\u9053\u4e5f\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u662f\u6545\u541b\u5b50\u6212\u614e\u4e4e\u5176\u6240\u4e0d\u7779\uff0c\u6050\u61fc\u4e4e\u5176\u6240\u4e0d\u805e\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u83ab\u898b\u4e4e\u96b1\uff0c\u83ab\u986f\u4e4e\u5fae\uff0c\u6545\u541b\u5b50\u614e\u5176\u7368\u4e5f\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u559c\u6012\u54c0\u6a02\u4e4b\u672a\u767c\uff0c\u8b02\u4e4b\u4e2d\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u767c\u800c\u7686\u4e2d\u7bc0\uff0c\u8b02\u4e4b\u548c\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u4e2d\u4e5f\u8005\uff0c\u5929\u4e0b\u4e4b\u5927\u672c\u4e5f\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u548c\u4e5f\u8005\uff0c\u5929\u4e0b\u4e4b\u9054\u9053\u4e5f\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u81f4\u4e2d\u548c\uff0c\u5929\u5730\u4f4d\u7109\uff0c\u842c\u7269\u80b2\u7109\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u541b\u5b50\u4e2d\u5eb8\uff0c\u5c0f\u4eba\u53cd\u4e2d\u5eb8\u3002\", \"Doctrine of the Mean 2\"),\n",
    "        (\"\u541b\u5b50\u4e4b\u4e2d\u5eb8\u4e5f\uff0c\u541b\u5b50\u800c\u6642\u4e2d\u3002\", \"Doctrine of the Mean 2\"),\n",
    "        (\"\u5c0f\u4eba\u4e4b\u53cd\u4e2d\u5eb8\u4e5f\uff0c\u5c0f\u4eba\u800c\u7121\u5fcc\u619a\u4e5f\u3002\", \"Doctrine of the Mean 2\"),\n",
    "        (\"\u4e2d\u5eb8\u5176\u81f3\u77e3\u4e4e\uff01\u6c11\u9bae\u80fd\u4e45\u77e3\u3002\", \"Doctrine of the Mean 3\"),\n",
    "        (\"\u9053\u4e4b\u4e0d\u884c\u4e5f\uff0c\u6211\u77e5\u4e4b\u77e3\uff1a\u77e5\u8005\u904e\u4e4b\uff0c\u611a\u8005\u4e0d\u53ca\u4e5f\u3002\", \"Doctrine of the Mean 4\"),\n",
    "        (\"\u9053\u4e4b\u4e0d\u660e\u4e5f\uff0c\u6211\u77e5\u4e4b\u77e3\uff1a\u8ce2\u8005\u904e\u4e4b\uff0c\u4e0d\u8096\u8005\u4e0d\u53ca\u4e5f\u3002\", \"Doctrine of the Mean 4\"),\n",
    "        (\"\u4eba\u83ab\u4e0d\u98f2\u98df\u4e5f\uff0c\u9bae\u80fd\u77e5\u5473\u4e5f\u3002\", \"Doctrine of the Mean 4\"),\n",
    "        (\"\u8aa0\u8005\uff0c\u5929\u4e4b\u9053\u4e5f\u3002\u8aa0\u4e4b\u8005\uff0c\u4eba\u4e4b\u9053\u4e5f\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u8aa0\u8005\uff0c\u4e0d\u52c9\u800c\u4e2d\uff0c\u4e0d\u601d\u800c\u5f97\uff0c\u5f9e\u5bb9\u4e2d\u9053\uff0c\u8056\u4eba\u4e5f\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u8aa0\u4e4b\u8005\uff0c\u64c7\u5584\u800c\u56fa\u57f7\u4e4b\u8005\u4e5f\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u535a\u5b78\u4e4b\uff0c\u5be9\u554f\u4e4b\uff0c\u614e\u601d\u4e4b\uff0c\u660e\u8fa8\u4e4b\uff0c\u7be4\u884c\u4e4b\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u4eba\u4e00\u80fd\u4e4b\uff0c\u5df1\u767e\u4e4b\uff1b\u4eba\u5341\u80fd\u4e4b\uff0c\u5df1\u5343\u4e4b\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u679c\u80fd\u6b64\u9053\u77e3\uff0c\u96d6\u611a\u5fc5\u660e\uff0c\u96d6\u67d4\u5fc5\u5f37\u3002\", \"Doctrine of the Mean 20\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(zhongyong):\n",
    "        chinese.append(\n",
    "            {\n",
    "                \"id\": f\"cn_zhongyong_{i}\",\n",
    "                \"text\": text,\n",
    "                \"source\": source,\n",
    "                \"period\": \"CONFUCIAN\",\n",
    "                \"century\": -5,\n",
    "            }\n",
    "        )\n",
    "    print(\n",
    "        f\"    - Doctrine of Mean: {len([x for x in chinese if 'zhongyong' in x['id']]):,} passages\"\n",
    "    )\n",
    "\n",
    "    # === BOOK OF RITES (\u79ae\u8a18) - 30+ passages ===\n",
    "    liji = [\n",
    "        (\"\u79ae\u5c1a\u5f80\u4f86\u3002\u5f80\u800c\u4e0d\u4f86\uff0c\u975e\u79ae\u4e5f\uff1b\u4f86\u800c\u4e0d\u5f80\uff0c\u4ea6\u975e\u79ae\u4e5f\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u6556\u4e0d\u53ef\u9577\uff0c\u6b32\u4e0d\u53ef\u5f9e\uff0c\u5fd7\u4e0d\u53ef\u6eff\uff0c\u6a02\u4e0d\u53ef\u6975\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u81e8\u8ca1\u6bcb\u830d\u5f97\uff0c\u81e8\u96e3\u6bcb\u830d\u514d\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u592b\u79ae\u8005\uff0c\u81ea\u5351\u800c\u5c0a\u4eba\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u96d6\u8ca0\u8ca9\u8005\uff0c\u5fc5\u6709\u5c0a\u4e5f\uff0c\u800c\u6cc1\u5bcc\u8cb4\u4e4e\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u5bcc\u8cb4\u800c\u77e5\u597d\u79ae\uff0c\u5247\u4e0d\u9a55\u4e0d\u6deb\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u8ca7\u8ce4\u800c\u77e5\u597d\u79ae\uff0c\u5247\u5fd7\u4e0d\u61fe\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u5927\u9053\u4e4b\u884c\u4e5f\uff0c\u5929\u4e0b\u70ba\u516c\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u9078\u8ce2\u8207\u80fd\uff0c\u8b1b\u4fe1\u4fee\u7766\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u6545\u4eba\u4e0d\u7368\u89aa\u5176\u89aa\uff0c\u4e0d\u7368\u5b50\u5176\u5b50\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u4f7f\u8001\u6709\u6240\u7d42\uff0c\u58ef\u6709\u6240\u7528\uff0c\u5e7c\u6709\u6240\u9577\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u77dc\u5be1\u5b64\u7368\u5ee2\u75be\u8005\u7686\u6709\u6240\u990a\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u7537\u6709\u5206\uff0c\u5973\u6709\u6b78\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u8ca8\u60e1\u5176\u68c4\u65bc\u5730\u4e5f\uff0c\u4e0d\u5fc5\u85cf\u65bc\u5df1\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u529b\u60e1\u5176\u4e0d\u51fa\u65bc\u8eab\u4e5f\uff0c\u4e0d\u5fc5\u70ba\u5df1\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u662f\u6545\u8b00\u9589\u800c\u4e0d\u8208\uff0c\u76dc\u7aca\u4e82\u8cca\u800c\u4e0d\u4f5c\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u6545\u5916\u6236\u800c\u4e0d\u9589\uff0c\u662f\u8b02\u5927\u540c\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u7389\u4e0d\u7422\uff0c\u4e0d\u6210\u5668\uff1b\u4eba\u4e0d\u5b78\uff0c\u4e0d\u77e5\u9053\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u662f\u6545\u5b78\u7136\u5f8c\u77e5\u4e0d\u8db3\uff0c\u6559\u7136\u5f8c\u77e5\u56f0\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u77e5\u4e0d\u8db3\uff0c\u7136\u5f8c\u80fd\u81ea\u53cd\u4e5f\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u77e5\u56f0\uff0c\u7136\u5f8c\u80fd\u81ea\u5f37\u4e5f\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u6545\u66f0\uff1a\u6559\u5b78\u76f8\u9577\u4e5f\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u51e1\u5b78\u4e4b\u9053\uff0c\u56b4\u5e2b\u70ba\u96e3\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u5e2b\u56b4\u7136\u5f8c\u9053\u5c0a\uff0c\u9053\u5c0a\u7136\u5f8c\u6c11\u77e5\u656c\u5b78\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u5584\u6b4c\u8005\u4f7f\u4eba\u7e7c\u5176\u8072\uff0c\u5584\u6559\u8005\u4f7f\u4eba\u7e7c\u5176\u5fd7\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u8a18\u554f\u4e4b\u5b78\uff0c\u4e0d\u8db3\u4ee5\u70ba\u4eba\u5e2b\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u5fc5\u4e5f\u5176\u807d\u8a9e\u4e4e\uff0c\u529b\u4e0d\u80fd\u554f\uff0c\u7136\u5f8c\u8a9e\u4e4b\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u8a9e\u4e4b\u800c\u4e0d\u77e5\uff0c\u96d6\u820d\u4e4b\u53ef\u4e5f\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u535a\u5b78\u800c\u4e0d\u7aae\uff0c\u7be4\u884c\u800c\u4e0d\u5026\u3002\", \"Book of Rites - Ruxing\"),\n",
    "        (\"\u541b\u5b50\u4e4b\u65bc\u5b78\u4e5f\uff0c\u85cf\u7109\uff0c\u4fee\u7109\uff0c\u606f\u7109\uff0c\u6e38\u7109\u3002\", \"Book of Rites - Xueji\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(liji):\n",
    "        chinese.append(\n",
    "            {\n",
    "                \"id\": f\"cn_liji_{i}\",\n",
    "                \"text\": text,\n",
    "                \"source\": source,\n",
    "                \"period\": \"CONFUCIAN\",\n",
    "                \"century\": -3,\n",
    "            }\n",
    "        )\n",
    "    print(f\"    - Book of Rites: {len([x for x in chinese if 'liji' in x['id']]):,} passages\")\n",
    "\n",
    "    with open(\"data/raw/chinese/chinese_native.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chinese, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"  Created {len(chinese)} Chinese passages\")\n",
    "\n",
    "    # ISLAMIC - 150+ REAL PASSAGES\n",
    "    print(\"\\n[3/4] Islamic texts (150+ real passages)...\")\n",
    "    os.makedirs(\"data/raw/islamic\", exist_ok=True)\n",
    "\n",
    "    islamic = []\n",
    "\n",
    "    # === QURANIC VERSES (40+) ===\n",
    "    quran = [\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u062a\u064f\u0644\u064f\u0648\u0627 \u0627\u0644\u0646\u064e\u0651\u0641\u0652\u0633\u064e \u0627\u0644\u064e\u0651\u062a\u0650\u064a \u062d\u064e\u0631\u064e\u0651\u0645\u064e \u0627\u0644\u0644\u064e\u0651\u0647\u064f \u0625\u0650\u0644\u064e\u0651\u0627 \u0628\u0650\u0627\u0644\u0652\u062d\u064e\u0642\u0650\u0651\", \"Quran 6:151\"),\n",
    "        (\"\u0648\u064e\u0628\u0650\u0627\u0644\u0652\u0648\u064e\u0627\u0644\u0650\u062f\u064e\u064a\u0652\u0646\u0650 \u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u064b\u0627\", \"Quran 17:23\"),\n",
    "        (\"\u0625\u0650\u0645\u064e\u0651\u0627 \u064a\u064e\u0628\u0652\u0644\u064f\u063a\u064e\u0646\u064e\u0651 \u0639\u0650\u0646\u062f\u064e\u0643\u064e \u0627\u0644\u0652\u0643\u0650\u0628\u064e\u0631\u064e \u0623\u064e\u062d\u064e\u062f\u064f\u0647\u064f\u0645\u064e\u0627 \u0623\u064e\u0648\u0652 \u0643\u0650\u0644\u064e\u0627\u0647\u064f\u0645\u064e\u0627 \u0641\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u064f\u0644 \u0644\u064e\u0651\u0647\u064f\u0645\u064e\u0627 \u0623\u064f\u0641\u064d\u0651\", \"Quran 17:23\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0646\u0652\u0647\u064e\u0631\u0652\u0647\u064f\u0645\u064e\u0627 \u0648\u064e\u0642\u064f\u0644 \u0644\u064e\u0651\u0647\u064f\u0645\u064e\u0627 \u0642\u064e\u0648\u0652\u0644\u064b\u0627 \u0643\u064e\u0631\u0650\u064a\u0645\u064b\u0627\", \"Quran 17:23\"),\n",
    "        (\"\u0648\u064e\u0627\u062e\u0652\u0641\u0650\u0636\u0652 \u0644\u064e\u0647\u064f\u0645\u064e\u0627 \u062c\u064e\u0646\u064e\u0627\u062d\u064e \u0627\u0644\u0630\u064f\u0651\u0644\u0650\u0651 \u0645\u0650\u0646\u064e \u0627\u0644\u0631\u064e\u0651\u062d\u0652\u0645\u064e\u0629\u0650\", \"Quran 17:24\"),\n",
    "        (\"\u0648\u064e\u0642\u064f\u0644 \u0631\u064e\u0651\u0628\u0650\u0651 \u0627\u0631\u0652\u062d\u064e\u0645\u0652\u0647\u064f\u0645\u064e\u0627 \u0643\u064e\u0645\u064e\u0627 \u0631\u064e\u0628\u064e\u0651\u064a\u064e\u0627\u0646\u0650\u064a \u0635\u064e\u063a\u0650\u064a\u0631\u064b\u0627\", \"Quran 17:24\"),\n",
    "        (\"\u0648\u064e\u0622\u062a\u0650 \u0630\u064e\u0627 \u0627\u0644\u0652\u0642\u064f\u0631\u0652\u0628\u064e\u0649\u0670 \u062d\u064e\u0642\u064e\u0651\u0647\u064f \u0648\u064e\u0627\u0644\u0652\u0645\u0650\u0633\u0652\u0643\u0650\u064a\u0646\u064e \u0648\u064e\u0627\u0628\u0652\u0646\u064e \u0627\u0644\u0633\u064e\u0651\u0628\u0650\u064a\u0644\u0650\", \"Quran 17:26\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064f\u0628\u064e\u0630\u0650\u0651\u0631\u0652 \u062a\u064e\u0628\u0652\u0630\u0650\u064a\u0631\u064b\u0627\", \"Quran 17:26\"),\n",
    "        (\"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0652\u0645\u064f\u0628\u064e\u0630\u0650\u0651\u0631\u0650\u064a\u0646\u064e \u0643\u064e\u0627\u0646\u064f\u0648\u0627 \u0625\u0650\u062e\u0652\u0648\u064e\u0627\u0646\u064e \u0627\u0644\u0634\u064e\u0651\u064a\u064e\u0627\u0637\u0650\u064a\u0646\u0650\", \"Quran 17:27\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u062c\u0652\u0639\u064e\u0644\u0652 \u064a\u064e\u062f\u064e\u0643\u064e \u0645\u064e\u063a\u0652\u0644\u064f\u0648\u0644\u064e\u0629\u064b \u0625\u0650\u0644\u064e\u0649\u0670 \u0639\u064f\u0646\u064f\u0642\u0650\u0643\u064e \u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0628\u0652\u0633\u064f\u0637\u0652\u0647\u064e\u0627 \u0643\u064f\u0644\u064e\u0651 \u0627\u0644\u0652\u0628\u064e\u0633\u0652\u0637\u0650\", \"Quran 17:29\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u0631\u064e\u0628\u064f\u0648\u0627 \u0627\u0644\u0632\u0650\u0651\u0646\u064e\u0627 \u06d6 \u0625\u0650\u0646\u064e\u0651\u0647\u064f \u0643\u064e\u0627\u0646\u064e \u0641\u064e\u0627\u062d\u0650\u0634\u064e\u0629\u064b \u0648\u064e\u0633\u064e\u0627\u0621\u064e \u0633\u064e\u0628\u0650\u064a\u0644\u064b\u0627\", \"Quran 17:32\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u062a\u064f\u0644\u064f\u0648\u0627 \u0623\u064e\u0648\u0652\u0644\u064e\u0627\u062f\u064e\u0643\u064f\u0645\u0652 \u062e\u064e\u0634\u0652\u064a\u064e\u0629\u064e \u0625\u0650\u0645\u0652\u0644\u064e\u0627\u0642\u064d\", \"Quran 17:31\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u0631\u064e\u0628\u064f\u0648\u0627 \u0645\u064e\u0627\u0644\u064e \u0627\u0644\u0652\u064a\u064e\u062a\u0650\u064a\u0645\u0650 \u0625\u0650\u0644\u064e\u0651\u0627 \u0628\u0650\u0627\u0644\u064e\u0651\u062a\u0650\u064a \u0647\u0650\u064a\u064e \u0623\u064e\u062d\u0652\u0633\u064e\u0646\u064f\", \"Quran 17:34\"),\n",
    "        (\"\u0648\u064e\u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u0650 \u06d6 \u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u064e \u0643\u064e\u0627\u0646\u064e \u0645\u064e\u0633\u0652\u0626\u064f\u0648\u0644\u064b\u0627\", \"Quran 17:34\"),\n",
    "        (\"\u0648\u064e\u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0627\u0644\u0652\u0643\u064e\u064a\u0652\u0644\u064e \u0625\u0650\u0630\u064e\u0627 \u0643\u0650\u0644\u0652\u062a\u064f\u0645\u0652 \u0648\u064e\u0632\u0650\u0646\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0642\u0650\u0633\u0652\u0637\u064e\u0627\u0633\u0650 \u0627\u0644\u0652\u0645\u064f\u0633\u0652\u062a\u064e\u0642\u0650\u064a\u0645\u0650\", \"Quran 17:35\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u0641\u064f \u0645\u064e\u0627 \u0644\u064e\u064a\u0652\u0633\u064e \u0644\u064e\u0643\u064e \u0628\u0650\u0647\u0650 \u0639\u0650\u0644\u0652\u0645\u064c\", \"Quran 17:36\"),\n",
    "        (\"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0633\u064e\u0651\u0645\u0652\u0639\u064e \u0648\u064e\u0627\u0644\u0652\u0628\u064e\u0635\u064e\u0631\u064e \u0648\u064e\u0627\u0644\u0652\u0641\u064f\u0624\u064e\u0627\u062f\u064e \u0643\u064f\u0644\u064f\u0651 \u0623\u064f\u0648\u0644\u064e\u0670\u0626\u0650\u0643\u064e \u0643\u064e\u0627\u0646\u064e \u0639\u064e\u0646\u0652\u0647\u064f \u0645\u064e\u0633\u0652\u0626\u064f\u0648\u0644\u064b\u0627\", \"Quran 17:36\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0645\u0652\u0634\u0650 \u0641\u0650\u064a \u0627\u0644\u0652\u0623\u064e\u0631\u0652\u0636\u0650 \u0645\u064e\u0631\u064e\u062d\u064b\u0627\", \"Quran 17:37\"),\n",
    "        (\"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0644\u064e\u0651\u0647\u064e \u064a\u064e\u0623\u0652\u0645\u064f\u0631\u064f \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u062f\u0652\u0644\u0650 \u0648\u064e\u0627\u0644\u0652\u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u0650 \u0648\u064e\u0625\u0650\u064a\u062a\u064e\u0627\u0621\u0650 \u0630\u0650\u064a \u0627\u0644\u0652\u0642\u064f\u0631\u0652\u0628\u064e\u0649\u0670\", \"Quran 16:90\"),\n",
    "        (\"\u0648\u064e\u064a\u064e\u0646\u0652\u0647\u064e\u0649\u0670 \u0639\u064e\u0646\u0650 \u0627\u0644\u0652\u0641\u064e\u062d\u0652\u0634\u064e\u0627\u0621\u0650 \u0648\u064e\u0627\u0644\u0652\u0645\u064f\u0646\u0643\u064e\u0631\u0650 \u0648\u064e\u0627\u0644\u0652\u0628\u064e\u063a\u0652\u064a\u0650\", \"Quran 16:90\"),\n",
    "        (\"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0622\u0645\u064e\u0646\u064f\u0648\u0627 \u0643\u064f\u0648\u0646\u064f\u0648\u0627 \u0642\u064e\u0648\u064e\u0651\u0627\u0645\u0650\u064a\u0646\u064e \u0628\u0650\u0627\u0644\u0652\u0642\u0650\u0633\u0652\u0637\u0650\", \"Quran 4:135\"),\n",
    "        (\"\u0634\u064f\u0647\u064e\u062f\u064e\u0627\u0621\u064e \u0644\u0650\u0644\u064e\u0651\u0647\u0650 \u0648\u064e\u0644\u064e\u0648\u0652 \u0639\u064e\u0644\u064e\u0649\u0670 \u0623\u064e\u0646\u0641\u064f\u0633\u0650\u0643\u064f\u0645\u0652 \u0623\u064e\u0648\u0650 \u0627\u0644\u0652\u0648\u064e\u0627\u0644\u0650\u062f\u064e\u064a\u0652\u0646\u0650 \u0648\u064e\u0627\u0644\u0652\u0623\u064e\u0642\u0652\u0631\u064e\u0628\u0650\u064a\u0646\u064e\", \"Quran 4:135\"),\n",
    "        (\"\u0648\u064e\u0625\u0650\u0630\u064e\u0627 \u062d\u064e\u0643\u064e\u0645\u0652\u062a\u064f\u0645 \u0628\u064e\u064a\u0652\u0646\u064e \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u0650 \u0623\u064e\u0646 \u062a\u064e\u062d\u0652\u0643\u064f\u0645\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u062f\u0652\u0644\u0650\", \"Quran 4:58\"),\n",
    "        (\"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0622\u0645\u064e\u0646\u064f\u0648\u0627 \u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064f\u0642\u064f\u0648\u062f\u0650\", \"Quran 5:1\"),\n",
    "        (\"\u0648\u064e\u062a\u064e\u0639\u064e\u0627\u0648\u064e\u0646\u064f\u0648\u0627 \u0639\u064e\u0644\u064e\u0649 \u0627\u0644\u0652\u0628\u0650\u0631\u0650\u0651 \u0648\u064e\u0627\u0644\u062a\u064e\u0651\u0642\u0652\u0648\u064e\u0649\u0670 \u06d6 \u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0639\u064e\u0627\u0648\u064e\u0646\u064f\u0648\u0627 \u0639\u064e\u0644\u064e\u0649 \u0627\u0644\u0652\u0625\u0650\u062b\u0652\u0645\u0650 \u0648\u064e\u0627\u0644\u0652\u0639\u064f\u062f\u0652\u0648\u064e\u0627\u0646\u0650\", \"Quran 5:2\"),\n",
    "        (\"\u0645\u064e\u0646 \u0642\u064e\u062a\u064e\u0644\u064e \u0646\u064e\u0641\u0652\u0633\u064b\u0627 \u0628\u0650\u063a\u064e\u064a\u0652\u0631\u0650 \u0646\u064e\u0641\u0652\u0633\u064d \u0623\u064e\u0648\u0652 \u0641\u064e\u0633\u064e\u0627\u062f\u064d \u0641\u0650\u064a \u0627\u0644\u0652\u0623\u064e\u0631\u0652\u0636\u0650 \u0641\u064e\u0643\u064e\u0623\u064e\u0646\u064e\u0651\u0645\u064e\u0627 \u0642\u064e\u062a\u064e\u0644\u064e \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u064e \u062c\u064e\u0645\u0650\u064a\u0639\u064b\u0627\", \"Quran 5:32\"),\n",
    "        (\"\u0648\u064e\u0645\u064e\u0646\u0652 \u0623\u064e\u062d\u0652\u064a\u064e\u0627\u0647\u064e\u0627 \u0641\u064e\u0643\u064e\u0623\u064e\u0646\u064e\u0651\u0645\u064e\u0627 \u0623\u064e\u062d\u0652\u064a\u064e\u0627 \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u064e \u062c\u064e\u0645\u0650\u064a\u0639\u064b\u0627\", \"Quran 5:32\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u064a\u064e\u062c\u0652\u0631\u0650\u0645\u064e\u0646\u064e\u0651\u0643\u064f\u0645\u0652 \u0634\u064e\u0646\u064e\u0622\u0646\u064f \u0642\u064e\u0648\u0652\u0645\u064d \u0639\u064e\u0644\u064e\u0649\u0670 \u0623\u064e\u0644\u064e\u0651\u0627 \u062a\u064e\u0639\u0652\u062f\u0650\u0644\u064f\u0648\u0627\", \"Quran 5:8\"),\n",
    "        (\"\u0627\u0639\u0652\u062f\u0650\u0644\u064f\u0648\u0627 \u0647\u064f\u0648\u064e \u0623\u064e\u0642\u0652\u0631\u064e\u0628\u064f \u0644\u0650\u0644\u062a\u064e\u0651\u0642\u0652\u0648\u064e\u0649\u0670\", \"Quran 5:8\"),\n",
    "        (\"\u0644\u064e\u0651\u064a\u0652\u0633\u064e \u0627\u0644\u0652\u0628\u0650\u0631\u064e\u0651 \u0623\u064e\u0646 \u062a\u064f\u0648\u064e\u0644\u064f\u0651\u0648\u0627 \u0648\u064f\u062c\u064f\u0648\u0647\u064e\u0643\u064f\u0645\u0652 \u0642\u0650\u0628\u064e\u0644\u064e \u0627\u0644\u0652\u0645\u064e\u0634\u0652\u0631\u0650\u0642\u0650 \u0648\u064e\u0627\u0644\u0652\u0645\u064e\u063a\u0652\u0631\u0650\u0628\u0650\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0670\u0643\u0650\u0646\u064e\u0651 \u0627\u0644\u0652\u0628\u0650\u0631\u064e\u0651 \u0645\u064e\u0646\u0652 \u0622\u0645\u064e\u0646\u064e \u0628\u0650\u0627\u0644\u0644\u064e\u0651\u0647\u0650 \u0648\u064e\u0627\u0644\u0652\u064a\u064e\u0648\u0652\u0645\u0650 \u0627\u0644\u0652\u0622\u062e\u0650\u0631\u0650\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0622\u062a\u064e\u0649 \u0627\u0644\u0652\u0645\u064e\u0627\u0644\u064e \u0639\u064e\u0644\u064e\u0649\u0670 \u062d\u064f\u0628\u0650\u0651\u0647\u0650 \u0630\u064e\u0648\u0650\u064a \u0627\u0644\u0652\u0642\u064f\u0631\u0652\u0628\u064e\u0649\u0670 \u0648\u064e\u0627\u0644\u0652\u064a\u064e\u062a\u064e\u0627\u0645\u064e\u0649\u0670 \u0648\u064e\u0627\u0644\u0652\u0645\u064e\u0633\u064e\u0627\u0643\u0650\u064a\u0646\u064e\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0627\u0628\u0652\u0646\u064e \u0627\u0644\u0633\u064e\u0651\u0628\u0650\u064a\u0644\u0650 \u0648\u064e\u0627\u0644\u0633\u064e\u0651\u0627\u0626\u0650\u0644\u0650\u064a\u0646\u064e \u0648\u064e\u0641\u0650\u064a \u0627\u0644\u0631\u0650\u0651\u0642\u064e\u0627\u0628\u0650\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0623\u064e\u0642\u064e\u0627\u0645\u064e \u0627\u0644\u0635\u064e\u0651\u0644\u064e\u0627\u0629\u064e \u0648\u064e\u0622\u062a\u064e\u0649 \u0627\u0644\u0632\u064e\u0651\u0643\u064e\u0627\u0629\u064e\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0627\u0644\u0652\u0645\u064f\u0648\u0641\u064f\u0648\u0646\u064e \u0628\u0650\u0639\u064e\u0647\u0652\u062f\u0650\u0647\u0650\u0645\u0652 \u0625\u0650\u0630\u064e\u0627 \u0639\u064e\u0627\u0647\u064e\u062f\u064f\u0648\u0627\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0627\u0644\u0635\u064e\u0651\u0627\u0628\u0650\u0631\u0650\u064a\u0646\u064e \u0641\u0650\u064a \u0627\u0644\u0652\u0628\u064e\u0623\u0652\u0633\u064e\u0627\u0621\u0650 \u0648\u064e\u0627\u0644\u0636\u064e\u0651\u0631\u064e\u0651\u0627\u0621\u0650 \u0648\u064e\u062d\u0650\u064a\u0646\u064e \u0627\u0644\u0652\u0628\u064e\u0623\u0652\u0633\u0650\", \"Quran 2:177\"),\n",
    "        (\"\u062e\u064f\u0630\u0650 \u0627\u0644\u0652\u0639\u064e\u0641\u0652\u0648\u064e \u0648\u064e\u0623\u0652\u0645\u064f\u0631\u0652 \u0628\u0650\u0627\u0644\u0652\u0639\u064f\u0631\u0652\u0641\u0650 \u0648\u064e\u0623\u064e\u0639\u0652\u0631\u0650\u0636\u0652 \u0639\u064e\u0646\u0650 \u0627\u0644\u0652\u062c\u064e\u0627\u0647\u0650\u0644\u0650\u064a\u0646\u064e\", \"Quran 7:199\"),\n",
    "        (\"\u0648\u064e\u0627\u0644\u0652\u0643\u064e\u0627\u0638\u0650\u0645\u0650\u064a\u0646\u064e \u0627\u0644\u0652\u063a\u064e\u064a\u0652\u0638\u064e \u0648\u064e\u0627\u0644\u0652\u0639\u064e\u0627\u0641\u0650\u064a\u0646\u064e \u0639\u064e\u0646\u0650 \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u0650\", \"Quran 3:134\"),\n",
    "        (\"\u0648\u064e\u0627\u0644\u0644\u064e\u0651\u0647\u064f \u064a\u064f\u062d\u0650\u0628\u064f\u0651 \u0627\u0644\u0652\u0645\u064f\u062d\u0652\u0633\u0650\u0646\u0650\u064a\u0646\u064e\", \"Quran 3:134\"),\n",
    "        (\"\u0627\u062f\u0652\u0641\u064e\u0639\u0652 \u0628\u0650\u0627\u0644\u064e\u0651\u062a\u0650\u064a \u0647\u0650\u064a\u064e \u0623\u064e\u062d\u0652\u0633\u064e\u0646\u064f \u0641\u064e\u0625\u0650\u0630\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a \u0628\u064e\u064a\u0652\u0646\u064e\u0643\u064e \u0648\u064e\u0628\u064e\u064a\u0652\u0646\u064e\u0647\u064f \u0639\u064e\u062f\u064e\u0627\u0648\u064e\u0629\u064c \u0643\u064e\u0623\u064e\u0646\u064e\u0651\u0647\u064f \u0648\u064e\u0644\u0650\u064a\u064c\u0651 \u062d\u064e\u0645\u0650\u064a\u0645\u064c\", \"Quran 41:34\"),\n",
    "        (\"\u0648\u064e\u0645\u064e\u0627 \u064a\u064f\u0644\u064e\u0642\u064e\u0651\u0627\u0647\u064e\u0627 \u0625\u0650\u0644\u064e\u0651\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0635\u064e\u0628\u064e\u0631\u064f\u0648\u0627 \u0648\u064e\u0645\u064e\u0627 \u064a\u064f\u0644\u064e\u0642\u064e\u0651\u0627\u0647\u064e\u0627 \u0625\u0650\u0644\u064e\u0651\u0627 \u0630\u064f\u0648 \u062d\u064e\u0638\u064d\u0651 \u0639\u064e\u0638\u0650\u064a\u0645\u064d\", \"Quran 41:35\"),\n",
    "        (\"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0644\u064e\u0651\u0647\u064e \u064a\u064e\u0623\u0652\u0645\u064f\u0631\u064f\u0643\u064f\u0645\u0652 \u0623\u064e\u0646 \u062a\u064f\u0624\u064e\u062f\u064f\u0651\u0648\u0627 \u0627\u0644\u0652\u0623\u064e\u0645\u064e\u0627\u0646\u064e\u0627\u062a\u0650 \u0625\u0650\u0644\u064e\u0649\u0670 \u0623\u064e\u0647\u0652\u0644\u0650\u0647\u064e\u0627\", \"Quran 4:58\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(quran):\n",
    "        islamic.append(\n",
    "            {\"id\": f\"quran_{i}\", \"text\": text, \"source\": source, \"period\": \"QURANIC\", \"century\": 7}\n",
    "        )\n",
    "    print(f\"    - Quranic verses: {len([x for x in islamic if 'quran' in x['id']]):,} passages\")\n",
    "\n",
    "    # === HADITH (110+) ===\n",
    "    hadith = [\n",
    "        (\"\u0644\u0627 \u0636\u0631\u0631 \u0648\u0644\u0627 \u0636\u0631\u0627\u0631\", \"Hadith - Ibn Majah\"),\n",
    "        (\"\u0625\u0646\u0645\u0627 \u0627\u0644\u0623\u0639\u0645\u0627\u0644 \u0628\u0627\u0644\u0646\u064a\u0627\u062a \u0648\u0625\u0646\u0645\u0627 \u0644\u0643\u0644 \u0627\u0645\u0631\u0626 \u0645\u0627 \u0646\u0648\u0649\", \"Hadith - Bukhari 1\"),\n",
    "        (\"\u0627\u0644\u0645\u0633\u0644\u0645 \u0645\u0646 \u0633\u0644\u0645 \u0627\u0644\u0645\u0633\u0644\u0645\u0648\u0646 \u0645\u0646 \u0644\u0633\u0627\u0646\u0647 \u0648\u064a\u062f\u0647\", \"Hadith - Bukhari 10\"),\n",
    "        (\"\u0644\u0627 \u064a\u0624\u0645\u0646 \u0623\u062d\u062f\u0643\u0645 \u062d\u062a\u0649 \u064a\u062d\u0628 \u0644\u0623\u062e\u064a\u0647 \u0645\u0627 \u064a\u062d\u0628 \u0644\u0646\u0641\u0633\u0647\", \"Hadith - Bukhari 13\"),\n",
    "        (\"\u0645\u0646 \u0643\u0627\u0646 \u064a\u0624\u0645\u0646 \u0628\u0627\u0644\u0644\u0647 \u0648\u0627\u0644\u064a\u0648\u0645 \u0627\u0644\u0622\u062e\u0631 \u0641\u0644\u064a\u0642\u0644 \u062e\u064a\u0631\u0627 \u0623\u0648 \u0644\u064a\u0635\u0645\u062a\", \"Hadith - Bukhari 6018\"),\n",
    "        (\"\u0645\u0646 \u0643\u0627\u0646 \u064a\u0624\u0645\u0646 \u0628\u0627\u0644\u0644\u0647 \u0648\u0627\u0644\u064a\u0648\u0645 \u0627\u0644\u0622\u062e\u0631 \u0641\u0644\u064a\u0643\u0631\u0645 \u0636\u064a\u0641\u0647\", \"Hadith - Bukhari 6019\"),\n",
    "        (\"\u0645\u0646 \u0643\u0627\u0646 \u064a\u0624\u0645\u0646 \u0628\u0627\u0644\u0644\u0647 \u0648\u0627\u0644\u064a\u0648\u0645 \u0627\u0644\u0622\u062e\u0631 \u0641\u0644\u064a\u0635\u0644 \u0631\u062d\u0645\u0647\", \"Hadith - Bukhari 6138\"),\n",
    "        (\"\u0627\u0631\u062d\u0645\u0648\u0627 \u0645\u0646 \u0641\u064a \u0627\u0644\u0623\u0631\u0636 \u064a\u0631\u062d\u0645\u0643\u0645 \u0645\u0646 \u0641\u064a \u0627\u0644\u0633\u0645\u0627\u0621\", \"Hadith - Tirmidhi 1924\"),\n",
    "        (\"\u0627\u0644\u0631\u0627\u062d\u0645\u0648\u0646 \u064a\u0631\u062d\u0645\u0647\u0645 \u0627\u0644\u0631\u062d\u0645\u0646\", \"Hadith - Abu Dawud 4941\"),\n",
    "        (\"\u0644\u064a\u0633 \u0645\u0646\u0627 \u0645\u0646 \u0644\u0645 \u064a\u0631\u062d\u0645 \u0635\u063a\u064a\u0631\u0646\u0627 \u0648\u064a\u0648\u0642\u0631 \u0643\u0628\u064a\u0631\u0646\u0627\", \"Hadith - Tirmidhi 1919\"),\n",
    "        (\"\u062e\u064a\u0631\u0643\u0645 \u062e\u064a\u0631\u0643\u0645 \u0644\u0623\u0647\u0644\u0647 \u0648\u0623\u0646\u0627 \u062e\u064a\u0631\u0643\u0645 \u0644\u0623\u0647\u0644\u064a\", \"Hadith - Tirmidhi 3895\"),\n",
    "        (\"\u0627\u062a\u0642 \u0627\u0644\u0644\u0647 \u062d\u064a\u062b\u0645\u0627 \u0643\u0646\u062a \u0648\u0623\u062a\u0628\u0639 \u0627\u0644\u0633\u064a\u0626\u0629 \u0627\u0644\u062d\u0633\u0646\u0629 \u062a\u0645\u062d\u0647\u0627\", \"Hadith - Tirmidhi 1987\"),\n",
    "        (\"\u0648\u062e\u0627\u0644\u0642 \u0627\u0644\u0646\u0627\u0633 \u0628\u062e\u0644\u0642 \u062d\u0633\u0646\", \"Hadith - Tirmidhi 1987\"),\n",
    "        (\"\u0623\u0643\u0645\u0644 \u0627\u0644\u0645\u0624\u0645\u0646\u064a\u0646 \u0625\u064a\u0645\u0627\u0646\u0627 \u0623\u062d\u0633\u0646\u0647\u0645 \u062e\u0644\u0642\u0627\", \"Hadith - Abu Dawud 4682\"),\n",
    "        (\"\u0625\u0646 \u0645\u0646 \u0623\u062d\u0628\u0643\u0645 \u0625\u0644\u064a \u0648\u0623\u0642\u0631\u0628\u0643\u0645 \u0645\u0646\u064a \u0645\u062c\u0644\u0633\u0627 \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629 \u0623\u062d\u0627\u0633\u0646\u0643\u0645 \u0623\u062e\u0644\u0627\u0642\u0627\", \"Hadith - Tirmidhi 2018\"),\n",
    "        (\"\u0645\u0627 \u0645\u0646 \u0634\u064a\u0621 \u0623\u062b\u0642\u0644 \u0641\u064a \u0645\u064a\u0632\u0627\u0646 \u0627\u0644\u0645\u0624\u0645\u0646 \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629 \u0645\u0646 \u062d\u0633\u0646 \u0627\u0644\u062e\u0644\u0642\", \"Hadith - Tirmidhi 2002\"),\n",
    "        (\"\u0627\u0644\u0628\u0631 \u062d\u0633\u0646 \u0627\u0644\u062e\u0644\u0642 \u0648\u0627\u0644\u0625\u062b\u0645 \u0645\u0627 \u062d\u0627\u0643 \u0641\u064a \u0635\u062f\u0631\u0643 \u0648\u0643\u0631\u0647\u062a \u0623\u0646 \u064a\u0637\u0644\u0639 \u0639\u0644\u064a\u0647 \u0627\u0644\u0646\u0627\u0633\", \"Hadith - Muslim 2553\"),\n",
    "        (\"\u0627\u0644\u062d\u064a\u0627\u0621 \u0645\u0646 \u0627\u0644\u0625\u064a\u0645\u0627\u0646\", \"Hadith - Bukhari 24\"),\n",
    "        (\"\u0627\u0644\u062d\u064a\u0627\u0621 \u0644\u0627 \u064a\u0623\u062a\u064a \u0625\u0644\u0627 \u0628\u062e\u064a\u0631\", \"Hadith - Bukhari 6117\"),\n",
    "        (\"\u0625\u0646 \u0627\u0644\u0644\u0647 \u0631\u0641\u064a\u0642 \u064a\u062d\u0628 \u0627\u0644\u0631\u0641\u0642 \u0641\u064a \u0627\u0644\u0623\u0645\u0631 \u0643\u0644\u0647\", \"Hadith - Bukhari 6927\"),\n",
    "        (\"\u0645\u0627 \u0643\u0627\u0646 \u0627\u0644\u0631\u0641\u0642 \u0641\u064a \u0634\u064a\u0621 \u0625\u0644\u0627 \u0632\u0627\u0646\u0647 \u0648\u0645\u0627 \u0646\u0632\u0639 \u0645\u0646 \u0634\u064a\u0621 \u0625\u0644\u0627 \u0634\u0627\u0646\u0647\", \"Hadith - Muslim 2594\"),\n",
    "        (\"\u0645\u0646 \u064a\u062d\u0631\u0645 \u0627\u0644\u0631\u0641\u0642 \u064a\u062d\u0631\u0645 \u0627\u0644\u062e\u064a\u0631 \u0643\u0644\u0647\", \"Hadith - Muslim 2592\"),\n",
    "        (\"\u0623\u062f \u0627\u0644\u0623\u0645\u0627\u0646\u0629 \u0625\u0644\u0649 \u0645\u0646 \u0627\u0626\u062a\u0645\u0646\u0643 \u0648\u0644\u0627 \u062a\u062e\u0646 \u0645\u0646 \u062e\u0627\u0646\u0643\", \"Hadith - Abu Dawud 3535\"),\n",
    "        (\"\u0622\u064a\u0629 \u0627\u0644\u0645\u0646\u0627\u0641\u0642 \u062b\u0644\u0627\u062b \u0625\u0630\u0627 \u062d\u062f\u062b \u0643\u0630\u0628 \u0648\u0625\u0630\u0627 \u0648\u0639\u062f \u0623\u062e\u0644\u0641 \u0648\u0625\u0630\u0627 \u0627\u0624\u062a\u0645\u0646 \u062e\u0627\u0646\", \"Hadith - Bukhari 33\"),\n",
    "        (\"\u0627\u0644\u0635\u062f\u0642 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0628\u0631 \u0648\u0627\u0644\u0628\u0631 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u062c\u0646\u0629\", \"Hadith - Bukhari 6094\"),\n",
    "        (\"\u0648\u0625\u0646 \u0627\u0644\u0643\u0630\u0628 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0641\u062c\u0648\u0631 \u0648\u0627\u0644\u0641\u062c\u0648\u0631 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0646\u0627\u0631\", \"Hadith - Bukhari 6094\"),\n",
    "        (\"\u0639\u0644\u064a\u0643\u0645 \u0628\u0627\u0644\u0635\u062f\u0642 \u0641\u0625\u0646 \u0627\u0644\u0635\u062f\u0642 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0628\u0631\", \"Hadith - Muslim 2607\"),\n",
    "        (\"\u0625\u064a\u0627\u0643\u0645 \u0648\u0627\u0644\u0643\u0630\u0628 \u0641\u0625\u0646 \u0627\u0644\u0643\u0630\u0628 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0641\u062c\u0648\u0631\", \"Hadith - Muslim 2607\"),\n",
    "        (\"\u0645\u0646 \u063a\u0634\u0646\u0627 \u0641\u0644\u064a\u0633 \u0645\u0646\u0627\", \"Hadith - Muslim 101\"),\n",
    "        (\"\u0643\u0644\u0643\u0645 \u0631\u0627\u0639 \u0648\u0643\u0644\u0643\u0645 \u0645\u0633\u0624\u0648\u0644 \u0639\u0646 \u0631\u0639\u064a\u062a\u0647\", \"Hadith - Bukhari 893\"),\n",
    "        (\"\u0627\u0644\u0625\u0645\u0627\u0645 \u0631\u0627\u0639 \u0648\u0645\u0633\u0624\u0648\u0644 \u0639\u0646 \u0631\u0639\u064a\u062a\u0647\", \"Hadith - Bukhari 893\"),\n",
    "        (\"\u0648\u0627\u0644\u0631\u062c\u0644 \u0631\u0627\u0639 \u0641\u064a \u0623\u0647\u0644\u0647 \u0648\u0645\u0633\u0624\u0648\u0644 \u0639\u0646 \u0631\u0639\u064a\u062a\u0647\", \"Hadith - Bukhari 893\"),\n",
    "        (\"\u0648\u0627\u0644\u0645\u0631\u0623\u0629 \u0631\u0627\u0639\u064a\u0629 \u0641\u064a \u0628\u064a\u062a \u0632\u0648\u062c\u0647\u0627 \u0648\u0645\u0633\u0624\u0648\u0644\u0629 \u0639\u0646 \u0631\u0639\u064a\u062a\u0647\u0627\", \"Hadith - Bukhari 893\"),\n",
    "        (\"\u0627\u0646\u0635\u0631 \u0623\u062e\u0627\u0643 \u0638\u0627\u0644\u0645\u0627 \u0623\u0648 \u0645\u0638\u0644\u0648\u0645\u0627\", \"Hadith - Bukhari 2444\"),\n",
    "        (\n",
    "            \"\u062a\u0646\u0635\u0631\u0647 \u0625\u0630\u0627 \u0643\u0627\u0646 \u0645\u0638\u0644\u0648\u0645\u0627 \u0623\u0641\u0631\u0623\u064a\u062a \u0625\u0630\u0627 \u0643\u0627\u0646 \u0638\u0627\u0644\u0645\u0627 \u0643\u064a\u0641 \u062a\u0646\u0635\u0631\u0647 \u0642\u0627\u0644 \u062a\u062d\u062c\u0632\u0647 \u0623\u0648 \u062a\u0645\u0646\u0639\u0647 \u0645\u0646 \u0627\u0644\u0638\u0644\u0645 \u0641\u0625\u0646 \u0630\u0644\u0643 \u0646\u0635\u0631\u0647\",\n",
    "            \"Hadith - Bukhari 2444\",\n",
    "        ),\n",
    "        (\"\u0627\u0644\u0645\u0624\u0645\u0646 \u0644\u0644\u0645\u0624\u0645\u0646 \u0643\u0627\u0644\u0628\u0646\u064a\u0627\u0646 \u064a\u0634\u062f \u0628\u0639\u0636\u0647 \u0628\u0639\u0636\u0627\", \"Hadith - Bukhari 481\"),\n",
    "        (\"\u0645\u062b\u0644 \u0627\u0644\u0645\u0624\u0645\u0646\u064a\u0646 \u0641\u064a \u062a\u0648\u0627\u062f\u0647\u0645 \u0648\u062a\u0631\u0627\u062d\u0645\u0647\u0645 \u0648\u062a\u0639\u0627\u0637\u0641\u0647\u0645 \u0645\u062b\u0644 \u0627\u0644\u062c\u0633\u062f \u0627\u0644\u0648\u0627\u062d\u062f\", \"Hadith - Muslim 2586\"),\n",
    "        (\"\u0625\u0630\u0627 \u0627\u0634\u062a\u0643\u0649 \u0645\u0646\u0647 \u0639\u0636\u0648 \u062a\u062f\u0627\u0639\u0649 \u0644\u0647 \u0633\u0627\u0626\u0631 \u0627\u0644\u062c\u0633\u062f \u0628\u0627\u0644\u0633\u0647\u0631 \u0648\u0627\u0644\u062d\u0645\u0649\", \"Hadith - Muslim 2586\"),\n",
    "        (\"\u0627\u0644\u0645\u0633\u0644\u0645 \u0623\u062e\u0648 \u0627\u0644\u0645\u0633\u0644\u0645 \u0644\u0627 \u064a\u0638\u0644\u0645\u0647 \u0648\u0644\u0627 \u064a\u0633\u0644\u0645\u0647\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"\u0645\u0646 \u0643\u0627\u0646 \u0641\u064a \u062d\u0627\u062c\u0629 \u0623\u062e\u064a\u0647 \u0643\u0627\u0646 \u0627\u0644\u0644\u0647 \u0641\u064a \u062d\u0627\u062c\u062a\u0647\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"\u0648\u0645\u0646 \u0641\u0631\u062c \u0639\u0646 \u0645\u0633\u0644\u0645 \u0643\u0631\u0628\u0629 \u0641\u0631\u062c \u0627\u0644\u0644\u0647 \u0639\u0646\u0647 \u0643\u0631\u0628\u0629 \u0645\u0646 \u0643\u0631\u0628\u0627\u062a \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"\u0648\u0645\u0646 \u0633\u062a\u0631 \u0645\u0633\u0644\u0645\u0627 \u0633\u062a\u0631\u0647 \u0627\u0644\u0644\u0647 \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"\u0644\u0627 \u062a\u062d\u0627\u0633\u062f\u0648\u0627 \u0648\u0644\u0627 \u062a\u0646\u0627\u062c\u0634\u0648\u0627 \u0648\u0644\u0627 \u062a\u0628\u0627\u063a\u0636\u0648\u0627 \u0648\u0644\u0627 \u062a\u062f\u0627\u0628\u0631\u0648\u0627\", \"Hadith - Muslim 2564\"),\n",
    "        (\"\u0648\u0644\u0627 \u064a\u0628\u0639 \u0628\u0639\u0636\u0643\u0645 \u0639\u0644\u0649 \u0628\u064a\u0639 \u0628\u0639\u0636 \u0648\u0643\u0648\u0646\u0648\u0627 \u0639\u0628\u0627\u062f \u0627\u0644\u0644\u0647 \u0625\u062e\u0648\u0627\u0646\u0627\", \"Hadith - Muslim 2564\"),\n",
    "        (\"\u0628\u062d\u0633\u0628 \u0627\u0645\u0631\u0626 \u0645\u0646 \u0627\u0644\u0634\u0631 \u0623\u0646 \u064a\u062d\u0642\u0631 \u0623\u062e\u0627\u0647 \u0627\u0644\u0645\u0633\u0644\u0645\", \"Hadith - Muslim 2564\"),\n",
    "        (\"\u0643\u0644 \u0627\u0644\u0645\u0633\u0644\u0645 \u0639\u0644\u0649 \u0627\u0644\u0645\u0633\u0644\u0645 \u062d\u0631\u0627\u0645 \u062f\u0645\u0647 \u0648\u0645\u0627\u0644\u0647 \u0648\u0639\u0631\u0636\u0647\", \"Hadith - Muslim 2564\"),\n",
    "        (\"\u0625\u064a\u0627\u0643\u0645 \u0648\u0627\u0644\u0638\u0646 \u0641\u0625\u0646 \u0627\u0644\u0638\u0646 \u0623\u0643\u0630\u0628 \u0627\u0644\u062d\u062f\u064a\u062b\", \"Hadith - Bukhari 6064\"),\n",
    "        (\"\u0648\u0644\u0627 \u062a\u062c\u0633\u0633\u0648\u0627 \u0648\u0644\u0627 \u062a\u062d\u0633\u0633\u0648\u0627 \u0648\u0644\u0627 \u062a\u0646\u0627\u0641\u0633\u0648\u0627\", \"Hadith - Bukhari 6064\"),\n",
    "        (\"\u0627\u0644\u0638\u0644\u0645 \u0638\u0644\u0645\u0627\u062a \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629\", \"Hadith - Bukhari 2447\"),\n",
    "        (\"\u0627\u062a\u0642\u0648\u0627 \u0627\u0644\u0638\u0644\u0645 \u0641\u0625\u0646 \u0627\u0644\u0638\u0644\u0645 \u0638\u0644\u0645\u0627\u062a \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629\", \"Hadith - Muslim 2578\"),\n",
    "        (\"\u0648\u0627\u062a\u0642\u0648\u0627 \u0627\u0644\u0634\u062d \u0641\u0625\u0646 \u0627\u0644\u0634\u062d \u0623\u0647\u0644\u0643 \u0645\u0646 \u0643\u0627\u0646 \u0642\u0628\u0644\u0643\u0645\", \"Hadith - Muslim 2578\"),\n",
    "        (\"\u0623\u0641\u0636\u0644 \u0627\u0644\u062c\u0647\u0627\u062f \u0643\u0644\u0645\u0629 \u0639\u062f\u0644 \u0639\u0646\u062f \u0633\u0644\u0637\u0627\u0646 \u062c\u0627\u0626\u0631\", \"Hadith - Abu Dawud 4344\"),\n",
    "        (\n",
    "            \"\u0633\u064a\u062f \u0627\u0644\u0634\u0647\u062f\u0627\u0621 \u062d\u0645\u0632\u0629 \u0628\u0646 \u0639\u0628\u062f \u0627\u0644\u0645\u0637\u0644\u0628 \u0648\u0631\u062c\u0644 \u0642\u0627\u0645 \u0625\u0644\u0649 \u0625\u0645\u0627\u0645 \u062c\u0627\u0626\u0631 \u0641\u0623\u0645\u0631\u0647 \u0648\u0646\u0647\u0627\u0647 \u0641\u0642\u062a\u0644\u0647\",\n",
    "            \"Hadith - Hakim 4884\",\n",
    "        ),\n",
    "        (\"\u0625\u0630\u0627 \u0631\u0623\u064a\u062a \u0623\u0645\u062a\u064a \u062a\u0647\u0627\u0628 \u0623\u0646 \u062a\u0642\u0648\u0644 \u0644\u0644\u0638\u0627\u0644\u0645 \u064a\u0627 \u0638\u0627\u0644\u0645 \u0641\u0642\u062f \u062a\u0648\u062f\u0639 \u0645\u0646\u0647\u0645\", \"Hadith - Ahmad 6521\"),\n",
    "        (\"\u0645\u0646 \u0631\u0623\u0649 \u0645\u0646\u0643\u0645 \u0645\u0646\u0643\u0631\u0627 \u0641\u0644\u064a\u063a\u064a\u0631\u0647 \u0628\u064a\u062f\u0647\", \"Hadith - Muslim 49\"),\n",
    "        (\"\u0641\u0625\u0646 \u0644\u0645 \u064a\u0633\u062a\u0637\u0639 \u0641\u0628\u0644\u0633\u0627\u0646\u0647 \u0641\u0625\u0646 \u0644\u0645 \u064a\u0633\u062a\u0637\u0639 \u0641\u0628\u0642\u0644\u0628\u0647 \u0648\u0630\u0644\u0643 \u0623\u0636\u0639\u0641 \u0627\u0644\u0625\u064a\u0645\u0627\u0646\", \"Hadith - Muslim 49\"),\n",
    "        (\"\u0623\u062d\u0628 \u0627\u0644\u0646\u0627\u0633 \u0625\u0644\u0649 \u0627\u0644\u0644\u0647 \u0623\u0646\u0641\u0639\u0647\u0645 \u0644\u0644\u0646\u0627\u0633\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"\u0648\u0623\u062d\u0628 \u0627\u0644\u0623\u0639\u0645\u0627\u0644 \u0625\u0644\u0649 \u0627\u0644\u0644\u0647 \u0633\u0631\u0648\u0631 \u062a\u062f\u062e\u0644\u0647 \u0639\u0644\u0649 \u0645\u0633\u0644\u0645\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"\u0623\u0648 \u062a\u0643\u0634\u0641 \u0639\u0646\u0647 \u0643\u0631\u0628\u0629 \u0623\u0648 \u062a\u0642\u0636\u064a \u0639\u0646\u0647 \u062f\u064a\u0646\u0627 \u0623\u0648 \u062a\u0637\u0631\u062f \u0639\u0646\u0647 \u062c\u0648\u0639\u0627\", \"Hadith - Tabarani 6026\"),\n",
    "        (\n",
    "            \"\u0648\u0644\u0623\u0646 \u0623\u0645\u0634\u064a \u0645\u0639 \u0623\u062e\u064a \u0641\u064a \u062d\u0627\u062c\u0629 \u0623\u062d\u0628 \u0625\u0644\u064a \u0645\u0646 \u0623\u0646 \u0623\u0639\u062a\u0643\u0641 \u0641\u064a \u0647\u0630\u0627 \u0627\u0644\u0645\u0633\u062c\u062f \u0634\u0647\u0631\u0627\",\n",
    "            \"Hadith - Tabarani 6026\",\n",
    "        ),\n",
    "        (\n",
    "            \"\u0627\u0644\u062f\u064a\u0646 \u0627\u0644\u0646\u0635\u064a\u062d\u0629 \u0642\u0644\u0646\u0627 \u0644\u0645\u0646 \u0642\u0627\u0644 \u0644\u0644\u0647 \u0648\u0644\u0643\u062a\u0627\u0628\u0647 \u0648\u0644\u0631\u0633\u0648\u0644\u0647 \u0648\u0644\u0623\u0626\u0645\u0629 \u0627\u0644\u0645\u0633\u0644\u0645\u064a\u0646 \u0648\u0639\u0627\u0645\u062a\u0647\u0645\",\n",
    "            \"Hadith - Muslim 55\",\n",
    "        ),\n",
    "        (\"\u0645\u0627 \u0646\u0642\u0635\u062a \u0635\u062f\u0642\u0629 \u0645\u0646 \u0645\u0627\u0644\", \"Hadith - Muslim 2588\"),\n",
    "        (\"\u0648\u0645\u0627 \u0632\u0627\u062f \u0627\u0644\u0644\u0647 \u0639\u0628\u062f\u0627 \u0628\u0639\u0641\u0648 \u0625\u0644\u0627 \u0639\u0632\u0627\", \"Hadith - Muslim 2588\"),\n",
    "        (\"\u0648\u0645\u0627 \u062a\u0648\u0627\u0636\u0639 \u0623\u062d\u062f \u0644\u0644\u0647 \u0625\u0644\u0627 \u0631\u0641\u0639\u0647 \u0627\u0644\u0644\u0647\", \"Hadith - Muslim 2588\"),\n",
    "        (\"\u0627\u0644\u064a\u062f \u0627\u0644\u0639\u0644\u064a\u0627 \u062e\u064a\u0631 \u0645\u0646 \u0627\u0644\u064a\u062f \u0627\u0644\u0633\u0641\u0644\u0649\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"\u0648\u0627\u0628\u062f\u0623 \u0628\u0645\u0646 \u062a\u0639\u0648\u0644\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"\u0648\u062e\u064a\u0631 \u0627\u0644\u0635\u062f\u0642\u0629 \u0645\u0627 \u0643\u0627\u0646 \u0639\u0646 \u0638\u0647\u0631 \u063a\u0646\u0649\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"\u0645\u0646 \u0627\u0633\u062a\u0637\u0627\u0639 \u0645\u0646\u0643\u0645 \u0627\u0644\u0628\u0627\u0621\u0629 \u0641\u0644\u064a\u062a\u0632\u0648\u062c\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"\u0641\u0625\u0646\u0647 \u0623\u063a\u0636 \u0644\u0644\u0628\u0635\u0631 \u0648\u0623\u062d\u0635\u0646 \u0644\u0644\u0641\u0631\u062c\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"\u0648\u0645\u0646 \u0644\u0645 \u064a\u0633\u062a\u0637\u0639 \u0641\u0639\u0644\u064a\u0647 \u0628\u0627\u0644\u0635\u0648\u0645 \u0641\u0625\u0646\u0647 \u0644\u0647 \u0648\u062c\u0627\u0621\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"\u0627\u0633\u062a\u0648\u0635\u0648\u0627 \u0628\u0627\u0644\u0646\u0633\u0627\u0621 \u062e\u064a\u0631\u0627\", \"Hadith - Bukhari 3331\"),\n",
    "        (\n",
    "            \"\u062e\u0630\u0648\u0627 \u0639\u0646\u064a \u062e\u0630\u0648\u0627 \u0639\u0646\u064a \u0642\u062f \u062c\u0639\u0644 \u0627\u0644\u0644\u0647 \u0644\u0647\u0646 \u0633\u0628\u064a\u0644\u0627 \u0627\u0644\u0628\u0643\u0631 \u0628\u0627\u0644\u0628\u0643\u0631 \u062c\u0644\u062f \u0645\u0627\u0626\u0629 \u0648\u0646\u0641\u064a \u0633\u0646\u0629\",\n",
    "            \"Hadith - Muslim 1690\",\n",
    "        ),\n",
    "        (\"\u0644\u0627 \u064a\u0641\u0631\u0643 \u0645\u0624\u0645\u0646 \u0645\u0624\u0645\u0646\u0629 \u0625\u0646 \u0643\u0631\u0647 \u0645\u0646\u0647\u0627 \u062e\u0644\u0642\u0627 \u0631\u0636\u064a \u0645\u0646\u0647\u0627 \u0622\u062e\u0631\", \"Hadith - Muslim 1469\"),\n",
    "        (\"\u0623\u0643\u0645\u0644 \u0627\u0644\u0645\u0624\u0645\u0646\u064a\u0646 \u0625\u064a\u0645\u0627\u0646\u0627 \u0623\u062d\u0633\u0646\u0647\u0645 \u062e\u0644\u0642\u0627 \u0648\u062e\u064a\u0627\u0631\u0643\u0645 \u062e\u064a\u0627\u0631\u0643\u0645 \u0644\u0646\u0633\u0627\u0626\u0647\u0645\", \"Hadith - Tirmidhi 1162\"),\n",
    "        (\"\u0645\u0627 \u0623\u0643\u0631\u0645\u0647\u0646 \u0625\u0644\u0627 \u0643\u0631\u064a\u0645 \u0648\u0645\u0627 \u0623\u0647\u0627\u0646\u0647\u0646 \u0625\u0644\u0627 \u0644\u0626\u064a\u0645\", \"Hadith - Ibn Asakir\"),\n",
    "        (\"\u0627\u0644\u0644\u0647\u0645 \u0625\u0646\u064a \u0623\u062d\u0631\u062c \u062d\u0642 \u0627\u0644\u0636\u0639\u064a\u0641\u064a\u0646 \u0627\u0644\u064a\u062a\u064a\u0645 \u0648\u0627\u0644\u0645\u0631\u0623\u0629\", \"Hadith - Ahmad 9664\"),\n",
    "        (\"\u0623\u0644\u0627 \u0623\u062e\u0628\u0631\u0643\u0645 \u0628\u062e\u064a\u0627\u0631\u0643\u0645 \u0642\u0627\u0644\u0648\u0627 \u0628\u0644\u0649 \u0642\u0627\u0644 \u062e\u064a\u0627\u0631\u0643\u0645 \u0623\u062d\u0627\u0633\u0646\u0643\u0645 \u0623\u062e\u0644\u0627\u0642\u0627\", \"Hadith - Bukhari 6035\"),\n",
    "        (\"\u0625\u0646\u0643\u0645 \u0644\u0646 \u062a\u0633\u0639\u0648\u0627 \u0627\u0644\u0646\u0627\u0633 \u0628\u0623\u0645\u0648\u0627\u0644\u0643\u0645 \u0641\u0644\u064a\u0633\u0639\u0647\u0645 \u0645\u0646\u0643\u0645 \u0628\u0633\u0637 \u0627\u0644\u0648\u062c\u0647 \u0648\u062d\u0633\u0646 \u0627\u0644\u062e\u0644\u0642\", \"Hadith - Hakim 422\"),\n",
    "        (\"\u062a\u0628\u0633\u0645\u0643 \u0641\u064a \u0648\u062c\u0647 \u0623\u062e\u064a\u0643 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0648\u0623\u0645\u0631\u0643 \u0628\u0627\u0644\u0645\u0639\u0631\u0648\u0641 \u0648\u0646\u0647\u064a\u0643 \u0639\u0646 \u0627\u0644\u0645\u0646\u0643\u0631 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0648\u0625\u0631\u0634\u0627\u062f\u0643 \u0627\u0644\u0631\u062c\u0644 \u0641\u064a \u0623\u0631\u0636 \u0627\u0644\u0636\u0644\u0627\u0644 \u0644\u0643 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0648\u0625\u0645\u0627\u0637\u062a\u0643 \u0627\u0644\u0623\u0630\u0649 \u0648\u0627\u0644\u0634\u0648\u0643 \u0648\u0627\u0644\u0639\u0638\u0645 \u0639\u0646 \u0627\u0644\u0637\u0631\u064a\u0642 \u0644\u0643 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0648\u0625\u0641\u0631\u0627\u063a\u0643 \u0645\u0646 \u062f\u0644\u0648\u0643 \u0641\u064a \u062f\u0644\u0648 \u0623\u062e\u064a\u0643 \u0644\u0643 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0627\u0644\u0643\u0644\u0645\u0629 \u0627\u0644\u0637\u064a\u0628\u0629 \u0635\u062f\u0642\u0629\", \"Hadith - Bukhari 2989\"),\n",
    "        (\"\u0648\u0643\u0644 \u062e\u0637\u0648\u0629 \u062a\u0645\u0634\u064a\u0647\u0627 \u0625\u0644\u0649 \u0627\u0644\u0635\u0644\u0627\u0629 \u0635\u062f\u0642\u0629\", \"Hadith - Bukhari 2989\"),\n",
    "        (\"\u0645\u0646 \u062f\u0644 \u0639\u0644\u0649 \u062e\u064a\u0631 \u0641\u0644\u0647 \u0645\u062b\u0644 \u0623\u062c\u0631 \u0641\u0627\u0639\u0644\u0647\", \"Hadith - Muslim 1893\"),\n",
    "        (\"\u0644\u064a\u0633 \u0627\u0644\u0634\u062f\u064a\u062f \u0628\u0627\u0644\u0635\u0631\u0639\u0629 \u0625\u0646\u0645\u0627 \u0627\u0644\u0634\u062f\u064a\u062f \u0627\u0644\u0630\u064a \u064a\u0645\u0644\u0643 \u0646\u0641\u0633\u0647 \u0639\u0646\u062f \u0627\u0644\u063a\u0636\u0628\", \"Hadith - Bukhari 6114\"),\n",
    "        (\"\u0644\u0627 \u062a\u063a\u0636\u0628 \u0641\u0631\u062f\u062f \u0645\u0631\u0627\u0631\u0627 \u0642\u0627\u0644 \u0644\u0627 \u062a\u063a\u0636\u0628\", \"Hadith - Bukhari 6116\"),\n",
    "        (\"\u0625\u0646 \u0627\u0644\u063a\u0636\u0628 \u0645\u0646 \u0627\u0644\u0634\u064a\u0637\u0627\u0646 \u0648\u0625\u0646 \u0627\u0644\u0634\u064a\u0637\u0627\u0646 \u062e\u0644\u0642 \u0645\u0646 \u0627\u0644\u0646\u0627\u0631\", \"Hadith - Abu Dawud 4784\"),\n",
    "        (\"\u0648\u0625\u0646\u0645\u0627 \u062a\u0637\u0641\u0623 \u0627\u0644\u0646\u0627\u0631 \u0628\u0627\u0644\u0645\u0627\u0621 \u0641\u0625\u0630\u0627 \u063a\u0636\u0628 \u0623\u062d\u062f\u0643\u0645 \u0641\u0644\u064a\u062a\u0648\u0636\u0623\", \"Hadith - Abu Dawud 4784\"),\n",
    "        (\"\u0644\u0627 \u064a\u062d\u0644 \u0644\u0645\u0633\u0644\u0645 \u0623\u0646 \u064a\u0647\u062c\u0631 \u0623\u062e\u0627\u0647 \u0641\u0648\u0642 \u062b\u0644\u0627\u062b \u0644\u064a\u0627\u0644\", \"Hadith - Bukhari 6077\"),\n",
    "        (\"\u064a\u0644\u062a\u0642\u064a\u0627\u0646 \u0641\u064a\u0639\u0631\u0636 \u0647\u0630\u0627 \u0648\u064a\u0639\u0631\u0636 \u0647\u0630\u0627 \u0648\u062e\u064a\u0631\u0647\u0645\u0627 \u0627\u0644\u0630\u064a \u064a\u0628\u062f\u0623 \u0628\u0627\u0644\u0633\u0644\u0627\u0645\", \"Hadith - Bukhari 6077\"),\n",
    "        (\"\u0623\u0641\u0634\u0648\u0627 \u0627\u0644\u0633\u0644\u0627\u0645 \u0628\u064a\u0646\u0643\u0645\", \"Hadith - Muslim 54\"),\n",
    "        (\"\u0648\u0627\u0644\u0630\u064a \u0646\u0641\u0633\u064a \u0628\u064a\u062f\u0647 \u0644\u0627 \u062a\u062f\u062e\u0644\u0648\u0627 \u0627\u0644\u062c\u0646\u0629 \u062d\u062a\u0649 \u062a\u0624\u0645\u0646\u0648\u0627\", \"Hadith - Muslim 54\"),\n",
    "        (\n",
    "            \"\u0648\u0644\u0627 \u062a\u0624\u0645\u0646\u0648\u0627 \u062d\u062a\u0649 \u062a\u062d\u0627\u0628\u0648\u0627 \u0623\u0648\u0644\u0627 \u0623\u062f\u0644\u0643\u0645 \u0639\u0644\u0649 \u0634\u064a\u0621 \u0625\u0630\u0627 \u0641\u0639\u0644\u062a\u0645\u0648\u0647 \u062a\u062d\u0627\u0628\u0628\u062a\u0645 \u0623\u0641\u0634\u0648\u0627 \u0627\u0644\u0633\u0644\u0627\u0645 \u0628\u064a\u0646\u0643\u0645\",\n",
    "            \"Hadith - Muslim 54\",\n",
    "        ),\n",
    "        (\"\u0637\u0639\u0627\u0645 \u0627\u0644\u0627\u062b\u0646\u064a\u0646 \u0643\u0627\u0641\u064a \u0627\u0644\u062b\u0644\u0627\u062b\u0629 \u0648\u0637\u0639\u0627\u0645 \u0627\u0644\u062b\u0644\u0627\u062b\u0629 \u0643\u0627\u0641\u064a \u0627\u0644\u0623\u0631\u0628\u0639\u0629\", \"Hadith - Bukhari 5392\"),\n",
    "        (\"\u0645\u0627 \u0645\u0644\u0623 \u0622\u062f\u0645\u064a \u0648\u0639\u0627\u0621 \u0634\u0631\u0627 \u0645\u0646 \u0628\u0637\u0646\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"\u0628\u062d\u0633\u0628 \u0627\u0628\u0646 \u0622\u062f\u0645 \u0623\u0643\u0644\u0627\u062a \u064a\u0642\u0645\u0646 \u0635\u0644\u0628\u0647\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"\u0641\u0625\u0646 \u0643\u0627\u0646 \u0644\u0627 \u0645\u062d\u0627\u0644\u0629 \u0641\u062b\u0644\u062b \u0644\u0637\u0639\u0627\u0645\u0647 \u0648\u062b\u0644\u062b \u0644\u0634\u0631\u0627\u0628\u0647 \u0648\u062b\u0644\u062b \u0644\u0646\u0641\u0633\u0647\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"\u0625\u0646 \u0627\u0644\u0644\u0647 \u0643\u062a\u0628 \u0627\u0644\u0625\u062d\u0633\u0627\u0646 \u0639\u0644\u0649 \u0643\u0644 \u0634\u064a\u0621\", \"Hadith - Muslim 1955\"),\n",
    "        (\"\u0641\u0625\u0630\u0627 \u0642\u062a\u0644\u062a\u0645 \u0641\u0623\u062d\u0633\u0646\u0648\u0627 \u0627\u0644\u0642\u062a\u0644\u0629 \u0648\u0625\u0630\u0627 \u0630\u0628\u062d\u062a\u0645 \u0641\u0623\u062d\u0633\u0646\u0648\u0627 \u0627\u0644\u0630\u0628\u062d\", \"Hadith - Muslim 1955\"),\n",
    "        (\"\u0648\u0644\u064a\u062d\u062f \u0623\u062d\u062f\u0643\u0645 \u0634\u0641\u0631\u062a\u0647 \u0648\u0644\u064a\u0631\u062d \u0630\u0628\u064a\u062d\u062a\u0647\", \"Hadith - Muslim 1955\"),\n",
    "        (\"\u0639\u0630\u0628\u062a \u0627\u0645\u0631\u0623\u0629 \u0641\u064a \u0647\u0631\u0629 \u0633\u062c\u0646\u062a\u0647\u0627 \u062d\u062a\u0649 \u0645\u0627\u062a\u062a\", \"Hadith - Bukhari 3318\"),\n",
    "        (\n",
    "            \"\u0641\u0644\u0627 \u0647\u064a \u0623\u0637\u0639\u0645\u062a\u0647\u0627 \u0648\u0644\u0627 \u0633\u0642\u062a\u0647\u0627 \u0625\u0630 \u062d\u0628\u0633\u062a\u0647\u0627 \u0648\u0644\u0627 \u0647\u064a \u062a\u0631\u0643\u062a\u0647\u0627 \u062a\u0623\u0643\u0644 \u0645\u0646 \u062e\u0634\u0627\u0634 \u0627\u0644\u0623\u0631\u0636\",\n",
    "            \"Hadith - Bukhari 3318\",\n",
    "        ),\n",
    "        (\"\u0628\u064a\u0646\u0645\u0627 \u0631\u062c\u0644 \u064a\u0645\u0634\u064a \u0628\u0637\u0631\u064a\u0642 \u0627\u0634\u062a\u062f \u0639\u0644\u064a\u0647 \u0627\u0644\u0639\u0637\u0634 \u0641\u0648\u062c\u062f \u0628\u0626\u0631\u0627 \u0641\u0646\u0632\u0644 \u0641\u064a\u0647\u0627 \u0641\u0634\u0631\u0628\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u062b\u0645 \u062e\u0631\u062c \u0641\u0625\u0630\u0627 \u0643\u0644\u0628 \u064a\u0644\u0647\u062b \u064a\u0623\u0643\u0644 \u0627\u0644\u062b\u0631\u0649 \u0645\u0646 \u0627\u0644\u0639\u0637\u0634\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u0641\u0642\u0627\u0644 \u0644\u0642\u062f \u0628\u0644\u063a \u0647\u0630\u0627 \u0627\u0644\u0643\u0644\u0628 \u0645\u0646 \u0627\u0644\u0639\u0637\u0634 \u0645\u062b\u0644 \u0627\u0644\u0630\u064a \u0643\u0627\u0646 \u0628\u0644\u063a \u0645\u0646\u064a\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u0641\u0646\u0632\u0644 \u0627\u0644\u0628\u0626\u0631 \u0641\u0645\u0644\u0623 \u062e\u0641\u0647 \u0645\u0627\u0621 \u062b\u0645 \u0623\u0645\u0633\u0643\u0647 \u0628\u0641\u064a\u0647 \u062d\u062a\u0649 \u0631\u0642\u064a \u0641\u0633\u0642\u0649 \u0627\u0644\u0643\u0644\u0628\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u0641\u0634\u0643\u0631 \u0627\u0644\u0644\u0647 \u0644\u0647 \u0641\u063a\u0641\u0631 \u0644\u0647\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u0641\u064a \u0643\u0644 \u0643\u0628\u062f \u0631\u0637\u0628\u0629 \u0623\u062c\u0631\", \"Hadith - Bukhari 2466\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(hadith):\n",
    "        islamic.append(\n",
    "            {\"id\": f\"hadith_{i}\", \"text\": text, \"source\": source, \"period\": \"HADITH\", \"century\": 9}\n",
    "        )\n",
    "    print(f\"    - Hadith: {len([x for x in islamic if 'hadith' in x['id']]):,} passages\")\n",
    "\n",
    "    with open(\"data/raw/islamic/islamic_native.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(islamic, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"  Created {len(islamic)} Islamic passages\")\n",
    "\n",
    "    # DEAR ABBY\n",
    "    print(\"\\n[4/4] Dear Abby...\")\n",
    "    abby_count = 0\n",
    "    if (\n",
    "        not os.path.exists(\"data/raw/dear_abby.csv\")\n",
    "        or os.path.getsize(\"data/raw/dear_abby.csv\") < 10000\n",
    "    ):\n",
    "        # Check if in Drive (with retry for stale FUSE mount)\n",
    "        drive_abby_path = f\"{SAVE_DIR}/dear_abby.csv\"\n",
    "        found_in_drive = False\n",
    "\n",
    "        # First attempt\n",
    "        if os.path.exists(drive_abby_path):\n",
    "            found_in_drive = True\n",
    "        else:\n",
    "            # Retry after refreshing Drive mount (FUSE can be stale)\n",
    "            print(f\"  First check failed, refreshing Drive...\")\n",
    "            try:\n",
    "                _ = os.listdir(SAVE_DIR)  # Force FUSE refresh\n",
    "                import time\n",
    "\n",
    "                time.sleep(0.5)  # Brief pause for sync\n",
    "                if os.path.exists(drive_abby_path):\n",
    "                    found_in_drive = True\n",
    "                    print(f\"  Found after refresh!\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Drive refresh error: {e}\")\n",
    "\n",
    "        if found_in_drive:\n",
    "            shutil.copy(drive_abby_path, \"data/raw/dear_abby.csv\")\n",
    "            print(f\"  Loaded from Drive: {drive_abby_path}\")\n",
    "        else:\n",
    "            print(f\"  Not found in Drive at: {drive_abby_path}\")\n",
    "            # Show what IS in the Drive folder\n",
    "            try:\n",
    "                contents = os.listdir(SAVE_DIR) if os.path.exists(SAVE_DIR) else []\n",
    "                print(f\"  Drive folder contents: {contents[:10]}\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                subprocess.run(\n",
    "                    [\n",
    "                        \"kaggle\",\n",
    "                        \"datasets\",\n",
    "                        \"download\",\n",
    "                        \"-d\",\n",
    "                        \"thedevastator/20000-dear-abby-questions\",\n",
    "                        \"-p\",\n",
    "                        \"data/raw/\",\n",
    "                        \"--unzip\",\n",
    "                    ],\n",
    "                    check=True,\n",
    "                    timeout=120,\n",
    "                )\n",
    "                print(\"  Downloaded from Kaggle\")\n",
    "            except:\n",
    "                print(\"  Kaggle failed - creating minimal fallback\")\n",
    "                fallback = [\n",
    "                    {\"question_only\": f\"Dear Abby, I have a problem {i}\", \"year\": 1990 + i % 30}\n",
    "                    for i in range(100)\n",
    "                ]\n",
    "                pd.DataFrame(fallback).to_csv(\"data/raw/dear_abby.csv\", index=False)\n",
    "    else:\n",
    "        print(\"  Already exists\")\n",
    "\n",
    "    # Count Dear Abby samples\n",
    "    try:\n",
    "        df = pd.read_csv(\"data/raw/dear_abby.csv\")\n",
    "        abby_count = len(\n",
    "            [\n",
    "                1\n",
    "                for _, row in df.iterrows()\n",
    "                if str(row.get(\"question_only\", \"\")) != \"nan\"\n",
    "                and 50 <= len(str(row.get(\"question_only\", \"\"))) <= 2000\n",
    "            ]\n",
    "        )\n",
    "    except:\n",
    "        abby_count = 0\n",
    "\n",
    "    # Warning for insufficient Dear Abby data\n",
    "    if abby_count < 1000:\n",
    "        print(\"\\n\" + \"!\" * 60)\n",
    "        print(\"CRITICAL: Dear Abby corpus is too small!\")\n",
    "        print(\"The semitic_to_non_semitic split WILL FAIL without this data.\")\n",
    "        print(\"\\nTo fix:\")\n",
    "        print(\"1. Download from: kaggle.com/datasets/thedevastator/20000-dear-abby-questions\")\n",
    "        print(\"2. Upload dear_abby.csv to your Google Drive BIP_v10 folder\")\n",
    "        print(\"3. Set REFRESH_DATA_FROM_SOURCE = True and rerun\")\n",
    "        print(\"!\" * 60 + \"\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Downloads complete\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3. Patterns + Normalization { display-mode: \"form\" }\n",
    "# @markdown BIP v10.9: Complete native patterns for moral concepts in 7 languages\n",
    "# @markdown - Added: Sanskrit, Pali patterns\n",
    "# @markdown - Added: NLP improvements (negation detection, modal classification)\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from enum import Enum, auto\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT NORMALIZATION & PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ===== TEXT NORMALIZATION =====\n",
    "def normalize_hebrew(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"[\\u0591-\\u05C7]\", \"\", text)  # Remove nikud\n",
    "    for final, regular in [\n",
    "        (\"\\u05da\", \"\\u05db\"),\n",
    "        (\"\\u05dd\", \"\\u05de\"),\n",
    "        (\"\\u05df\", \"\\u05e0\"),\n",
    "        (\"\\u05e3\", \"\\u05e4\"),\n",
    "        (\"\\u05e5\", \"\\u05e6\"),\n",
    "    ]:\n",
    "        text = text.replace(final, regular)\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"[\\u064B-\\u065F]\", \"\", text)  # Remove tashkeel\n",
    "    text = text.replace(\"\\u0640\", \"\")  # Remove tatweel\n",
    "    for v in [\"\\u0623\", \"\\u0625\", \"\\u0622\", \"\\u0671\"]:\n",
    "        text = text.replace(v, \"\\u0627\")\n",
    "    text = text.replace(\"\\u0629\", \"\\u0647\").replace(\"\\u0649\", \"\\u064a\")\n",
    "    return text\n",
    "\n",
    "\n",
    "# NEW in v10.9: Sanskrit normalization\n",
    "def normalize_sanskrit(text):\n",
    "    \"\"\"Normalize Sanskrit/Devanagari text.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    # Remove vedic accents and other diacriticals\n",
    "    text = re.sub(r\"[\\u0951-\\u0954]\", \"\", text)  # Vedic tone marks\n",
    "    text = re.sub(r\"[\\u0900-\\u0902]\", \"\", text)  # Chandrabindu variants\n",
    "    return text\n",
    "\n",
    "\n",
    "# NEW in v10.9: Pali normalization\n",
    "def normalize_pali(text):\n",
    "    \"\"\"Normalize Pali text (romanized or script).\"\"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    # Normalize romanized Pali diacritics\n",
    "    text = text.lower()\n",
    "    # Handle common Pali romanization variations\n",
    "    text = text.replace(\"\u1e43\", \"m\").replace(\"\u1e45\", \"n\").replace(\"\u00f1\", \"n\")\n",
    "    text = text.replace(\"\u1e6d\", \"t\").replace(\"\u1e0d\", \"d\").replace(\"\u1e47\", \"n\")\n",
    "    text = text.replace(\"\u1e37\", \"l\").replace(\"\u0101\", \"a\").replace(\"\u012b\", \"i\").replace(\"\u016b\", \"u\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_text(text, language):\n",
    "    if language in [\"hebrew\", \"aramaic\"]:\n",
    "        return normalize_hebrew(text)\n",
    "    elif language == \"arabic\":\n",
    "        return normalize_arabic(text)\n",
    "    elif language == \"classical_chinese\":\n",
    "        return unicodedata.normalize(\"NFKC\", text)\n",
    "    elif language == \"sanskrit\":\n",
    "        return normalize_sanskrit(text)\n",
    "    elif language == \"pali\":\n",
    "        return normalize_pali(text)\n",
    "    else:\n",
    "        return unicodedata.normalize(\"NFKC\", text.lower())\n",
    "\n",
    "\n",
    "# ===== BOND AND HOHFELD TYPES =====\n",
    "class BondType(Enum):\n",
    "    HARM_PREVENTION = auto()\n",
    "    RECIPROCITY = auto()\n",
    "    AUTONOMY = auto()\n",
    "    PROPERTY = auto()\n",
    "    FAMILY = auto()\n",
    "    AUTHORITY = auto()\n",
    "    CARE = auto()\n",
    "    FAIRNESS = auto()\n",
    "    CONTRACT = auto()\n",
    "    NONE = auto()\n",
    "\n",
    "\n",
    "class HohfeldState(Enum):\n",
    "    OBLIGATION = auto()\n",
    "    RIGHT = auto()\n",
    "    LIBERTY = auto()\n",
    "    NO_RIGHT = auto()\n",
    "\n",
    "\n",
    "# ===== COMPLETE BOND PATTERNS =====\n",
    "ALL_BOND_PATTERNS = {\n",
    "    \"hebrew\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u05d4\\u05e8\\u05d2\",\n",
    "            r\"\\u05e8\\u05e6\\u05d7\",\n",
    "            r\"\\u05e0\\u05d6\\u05e7\",\n",
    "            r\"\\u05d4\\u05db\\u05d4\",\n",
    "            r\"\\u05d4\\u05e6\\u05d9\\u05dc\",\n",
    "            r\"\\u05e9\\u05de\\u05e8\",\n",
    "            r\"\\u05e4\\u05e7\\u05d5\\u05d7.\\u05e0\\u05e4\\u05e9\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\u05d2\\u05de\\u05d5\\u05dc\",\n",
    "            r\"\\u05d4\\u05e9\\u05d9\\u05d1\",\n",
    "            r\"\\u05e4\\u05e8\\u05e2\",\n",
    "            r\"\\u05e0\\u05ea\\u05df.*\\u05e7\\u05d1\\u05dc\",\n",
    "            r\"\\u05de\\u05d3\\u05d4.\\u05db\\u05e0\\u05d2\\u05d3\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\u05d1\\u05d7\\u05e8\",\n",
    "            r\"\\u05e8\\u05e6\\u05d5\\u05df\",\n",
    "            r\"\\u05d7\\u05e4\\u05e9\",\n",
    "            r\"\\u05e2\\u05e6\\u05de\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u05e7\\u05e0\\u05d4\",\n",
    "            r\"\\u05de\\u05db\\u05e8\",\n",
    "            r\"\\u05d2\\u05d6\\u05dc\",\n",
    "            r\"\\u05d2\\u05e0\\u05d1\",\n",
    "            r\"\\u05de\\u05de\\u05d5\\u05df\",\n",
    "            r\"\\u05e0\\u05db\\u05e1\",\n",
    "            r\"\\u05d9\\u05e8\\u05e9\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u05d0\\u05d1\",\n",
    "            r\"\\u05d0\\u05de\",\n",
    "            r\"\\u05d1\\u05e0\",\n",
    "            r\"\\u05db\\u05d1\\u05d3.*\\u05d0\\u05d1\",\n",
    "            r\"\\u05db\\u05d1\\u05d3.*\\u05d0\\u05de\",\n",
    "            r\"\\u05de\\u05e9\\u05e4\\u05d7\\u05d4\",\n",
    "            r\"\\u05d0\\u05d7\",\n",
    "            r\"\\u05d0\\u05d7\\u05d5\\u05ea\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u05de\\u05dc\\u05db\",\n",
    "            r\"\\u05e9\\u05d5\\u05e4\\u05d8\",\n",
    "            r\"\\u05e6\\u05d5\\u05d4\",\n",
    "            r\"\\u05ea\\u05d5\\u05e8\\u05d4\",\n",
    "            r\"\\u05de\\u05e6\\u05d5\\u05d4\",\n",
    "            r\"\\u05d3\\u05d9\\u05df\",\n",
    "            r\"\\u05d7\\u05e7\",\n",
    "        ],\n",
    "        BondType.CARE: [\n",
    "            r\"\\u05d7\\u05e1\\u05d3\",\n",
    "            r\"\\u05e8\\u05d7\\u05de\",\n",
    "            r\"\\u05e2\\u05d6\\u05e8\",\n",
    "            r\"\\u05ea\\u05de\\u05db\",\n",
    "            r\"\\u05e6\\u05d3\\u05e7\\u05d4\",\n",
    "        ],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u05e6\\u05d3\\u05e7\",\n",
    "            r\"\\u05de\\u05e9\\u05e4\\u05d8\",\n",
    "            r\"\\u05d9\\u05e9\\u05e8\",\n",
    "            r\"\\u05e9\\u05d5\\u05d4\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u05d1\\u05e8\\u05d9\\u05ea\",\n",
    "            r\"\\u05e0\\u05d3\\u05e8\",\n",
    "            r\"\\u05e9\\u05d1\\u05d5\\u05e2\",\n",
    "            r\"\\u05d4\\u05ea\\u05d7\\u05d9\\u05d1\",\n",
    "            r\"\\u05e2\\u05e8\\u05d1\",\n",
    "        ],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u05e7\\u05d8\\u05dc\",\n",
    "            r\"\\u05e0\\u05d6\\u05e7\",\n",
    "            r\"\\u05d7\\u05d1\\u05dc\",\n",
    "            r\"\\u05e9\\u05d6\\u05d9\\u05d1\",\n",
    "            r\"\\u05e4\\u05e6\\u05d9\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [r\"\\u05e4\\u05e8\\u05e2\", r\"\\u05e9\\u05dc\\u05de\", r\"\\u05d0\\u05d2\\u05e8\"],\n",
    "        BondType.AUTONOMY: [r\"\\u05e6\\u05d1\\u05d9\", r\"\\u05e8\\u05e2\\u05d5\"],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u05d6\\u05d1\\u05e0\",\n",
    "            r\"\\u05e7\\u05e0\\u05d4\",\n",
    "            r\"\\u05d2\\u05d6\\u05dc\",\n",
    "            r\"\\u05de\\u05de\\u05d5\\u05e0\\u05d0\",\n",
    "            r\"\\u05e0\\u05db\\u05e1\\u05d9\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u05d0\\u05d1\\u05d0\",\n",
    "            r\"\\u05d0\\u05de\\u05d0\",\n",
    "            r\"\\u05d1\\u05e8\\u05d0\",\n",
    "            r\"\\u05d1\\u05e8\\u05ea\\u05d0\",\n",
    "            r\"\\u05d9\\u05e7\\u05e8\",\n",
    "            r\"\\u05d0\\u05d7\\u05d0\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u05de\\u05dc\\u05db\\u05d0\",\n",
    "            r\"\\u05d3\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05d3\\u05d9\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05e4\\u05e7\\u05d5\\u05d3\\u05d0\",\n",
    "            r\"\\u05d0\\u05d5\\u05e8\\u05d9\\u05ea\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\u05d7\\u05e1\\u05d3\", r\"\\u05e8\\u05d7\\u05de\", r\"\\u05e1\\u05e2\\u05d3\"],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u05d3\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05e7\\u05e9\\u05d5\\u05d8\",\n",
    "            r\"\\u05ea\\u05e8\\u05d9\\u05e6\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u05e7\\u05d9\\u05de\\u05d0\",\n",
    "            r\"\\u05e9\\u05d1\\u05d5\\u05e2\\u05d4\",\n",
    "            r\"\\u05e0\\u05d3\\u05e8\\u05d0\",\n",
    "            r\"\\u05e2\\u05e8\\u05d1\\u05d0\",\n",
    "        ],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u6bba\",\n",
    "            r\"\\u5bb3\",\n",
    "            r\"\\u50b7\",\n",
    "            r\"\\u6551\",\n",
    "            r\"\\u8b77\",\n",
    "            r\"\\u885b\",\n",
    "            r\"\\u66b4\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [r\"\\u5831\", r\"\\u9084\", r\"\\u511f\", r\"\\u8ced\", r\"\\u7b54\"],\n",
    "        BondType.AUTONOMY: [r\"\\u81ea\", r\"\\u7531\", r\"\\u4efb\", r\"\\u610f\", r\"\\u5fd7\"],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u8ca1\",\n",
    "            r\"\\u7269\",\n",
    "            r\"\\u7522\",\n",
    "            r\"\\u76dc\",\n",
    "            r\"\\u7aca\",\n",
    "            r\"\\u8ce3\",\n",
    "            r\"\\u8cb7\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u5b5d\",\n",
    "            r\"\\u7236\",\n",
    "            r\"\\u6bcd\",\n",
    "            r\"\\u89aa\",\n",
    "            r\"\\u5b50\",\n",
    "            r\"\\u5f1f\",\n",
    "            r\"\\u5144\",\n",
    "            r\"\\u5bb6\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u541b\",\n",
    "            r\"\\u81e3\",\n",
    "            r\"\\u738b\",\n",
    "            r\"\\u547d\",\n",
    "            r\"\\u4ee4\",\n",
    "            r\"\\u6cd5\",\n",
    "            r\"\\u6cbb\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\u4ec1\", r\"\\u611b\", r\"\\u6148\", r\"\\u60e0\", r\"\\u6069\", r\"\\u6190\"],\n",
    "        BondType.FAIRNESS: [r\"\\u7fa9\", r\"\\u6b63\", r\"\\u516c\", r\"\\u5e73\", r\"\\u5747\"],\n",
    "        BondType.CONTRACT: [r\"\\u7d04\", r\"\\u76df\", r\"\\u8a93\", r\"\\u8afe\", r\"\\u4fe1\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u0642\\u062a\\u0644\",\n",
    "            r\"\\u0636\\u0631\\u0631\",\n",
    "            r\"\\u0627\\u0630[\\u064a\\u0649]\",\n",
    "            r\"\\u0638\\u0644\\u0645\",\n",
    "            r\"\\u0627\\u0646\\u0642\\u0630\",\n",
    "            r\"\\u062d\\u0641\\u0638\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0646\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\u062c\\u0632\\u0627\",\n",
    "            r\"\\u0631\\u062f\",\n",
    "            r\"\\u0642\\u0635\\u0627\\u0635\",\n",
    "            r\"\\u0645\\u062b\\u0644\",\n",
    "            r\"\\u0639\\u0648\\u0636\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\u062d\\u0631\",\n",
    "            r\"\\u0627\\u0631\\u0627\\u062f\\u0629\",\n",
    "            r\"\\u0627\\u062e\\u062a\\u064a\\u0627\\u0631\",\n",
    "            r\"\\u0645\\u0634\\u064a\\u0626\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u0645\\u0627\\u0644\",\n",
    "            r\"\\u0645\\u0644\\u0643\",\n",
    "            r\"\\u0633\\u0631\\u0642\",\n",
    "            r\"\\u0628\\u064a\\u0639\",\n",
    "            r\"\\u0634\\u0631\\u0627\",\n",
    "            r\"\\u0645\\u064a\\u0631\\u0627\\u062b\",\n",
    "            r\"\\u063a\\u0635\\u0628\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u0648\\u0627\\u0644\\u062f\",\n",
    "            r\"\\u0627\\u0628\\u0648\",\n",
    "            r\"\\u0627\\u0645\",\n",
    "            r\"\\u0627\\u0628\\u0646\",\n",
    "            r\"\\u0628\\u0646\\u062a\",\n",
    "            r\"\\u0627\\u0647\\u0644\",\n",
    "            r\"\\u0642\\u0631\\u0628[\\u064a\\u0649]\",\n",
    "            r\"\\u0631\\u062d\\u0645\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u0637\\u0627\\u0639\",\n",
    "            r\"\\u0627\\u0645\\u0631\",\n",
    "            r\"\\u062d\\u0643\\u0645\",\n",
    "            r\"\\u0633\\u0644\\u0637\\u0627\\u0646\",\n",
    "            r\"\\u062e\\u0644\\u064a\\u0641\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0645\",\n",
    "            r\"\\u0634\\u0631\\u064a\\u0639\",\n",
    "        ],\n",
    "        BondType.CARE: [\n",
    "            r\"\\u0631\\u062d\\u0645\",\n",
    "            r\"\\u0627\\u062d\\u0633\\u0627\\u0646\",\n",
    "            r\"\\u0639\\u0637\\u0641\",\n",
    "            r\"\\u0635\\u062f\\u0642\",\n",
    "            r\"\\u0632\\u0643\\u0627\",\n",
    "        ],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u0639\\u062f\\u0644\",\n",
    "            r\"\\u0642\\u0633\\u0637\",\n",
    "            r\"\\u062d\\u0642\",\n",
    "            r\"\\u0627\\u0646\\u0635\\u0627\\u0641\",\n",
    "            r\"\\u0633\\u0648[\\u064a\\u0649]\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u0639\\u0647\\u062f\",\n",
    "            r\"\\u0639\\u0642\\u062f\",\n",
    "            r\"\\u0646\\u0630\\u0631\",\n",
    "            r\"\\u064a\\u0645\\u064a\\u0646\",\n",
    "            r\"\\u0648\\u0641\\u0627\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0646\",\n",
    "        ],\n",
    "    },\n",
    "    \"english\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\bkill\",\n",
    "            r\"\\bmurder\",\n",
    "            r\"\\bharm\",\n",
    "            r\"\\bhurt\",\n",
    "            r\"\\bsave\",\n",
    "            r\"\\bprotect\",\n",
    "            r\"\\bviolence\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\breturn\",\n",
    "            r\"\\brepay\",\n",
    "            r\"\\bexchange\",\n",
    "            r\"\\bgive.*back\",\n",
    "            r\"\\breciproc\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\bfree\",\n",
    "            r\"\\bchoice\",\n",
    "            r\"\\bchoose\",\n",
    "            r\"\\bconsent\",\n",
    "            r\"\\bautonomy\",\n",
    "            r\"\\bright to\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\bsteal\",\n",
    "            r\"\\btheft\",\n",
    "            r\"\\bown\",\n",
    "            r\"\\bproperty\",\n",
    "            r\"\\bbelong\",\n",
    "            r\"\\binherit\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\bfather\",\n",
    "            r\"\\bmother\",\n",
    "            r\"\\bparent\",\n",
    "            r\"\\bchild\",\n",
    "            r\"\\bfamily\",\n",
    "            r\"\\bhonor.*parent\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\bobey\",\n",
    "            r\"\\bcommand\",\n",
    "            r\"\\bauthority\",\n",
    "            r\"\\blaw\",\n",
    "            r\"\\brule\",\n",
    "            r\"\\bgovern\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\bcare\", r\"\\bhelp\", r\"\\bkind\", r\"\\bcompassion\", r\"\\bcharity\", r\"\\bmercy\"],\n",
    "        BondType.FAIRNESS: [r\"\\bfair\", r\"\\bjust\", r\"\\bequal\", r\"\\bequity\", r\"\\bright\\b\"],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\bpromise\",\n",
    "            r\"\\bcontract\",\n",
    "            r\"\\bagreem\",\n",
    "            r\"\\bvow\",\n",
    "            r\"\\boath\",\n",
    "            r\"\\bcommit\",\n",
    "        ],\n",
    "    },\n",
    "    # NEW in v10.9: Sanskrit patterns (Devanagari)\n",
    "    \"sanskrit\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\u0939\u093f\u0902\u0938\u093e\",\n",
    "            r\"\u0905\u0939\u093f\u0902\u0938\u093e\",\n",
    "            r\"\u0935\u0927\",\n",
    "            r\"\u0930\u0915\u094d\u0937\u093e\",\n",
    "            r\"\u0924\u094d\u0930\u093e\u0923\",\n",
    "        ],  # himsa, ahimsa, vadha, raksha, trana\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\u092a\u094d\u0930\u0924\u093f\u0926\u093e\u0928\",\n",
    "            r\"\u092a\u094d\u0930\u0924\u094d\u092f\u0941\u092a\u0915\u093e\u0930\",\n",
    "            r\"\u0926\u093e\u0928\",\n",
    "            r\"\u090b\u0923\",\n",
    "        ],  # pratidana, pratyupakara, dana, rna\n",
    "        BondType.AUTONOMY: [r\"\u0938\u094d\u0935\u0924\u0902\u0924\u094d\u0930\", r\"\u092e\u094b\u0915\u094d\u0937\", r\"\u0938\u094d\u0935\u0947\u091a\u094d\u091b\u093e\"],  # swatantra, moksha, sveccha\n",
    "        BondType.PROPERTY: [r\"\u0927\u0928\", r\"\u0938\u094d\u0935\", r\"\u091a\u094b\u0930\", r\"\u0926\u093e\u092f\"],  # dhana, sva, chora, daya\n",
    "        BondType.FAMILY: [r\"\u092a\u093f\u0924\u0943\", r\"\u092e\u093e\u0924\u0943\", r\"\u092a\u0941\u0924\u094d\u0930\", r\"\u0915\u0941\u0932\", r\"\u0917\u0943\u0939\"],  # pitri, matri, putra, kula, grha\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\u0930\u093e\u091c\",\n",
    "            r\"\u0927\u0930\u094d\u092e\",\n",
    "            r\"\u0935\u093f\u0927\u093f\",\n",
    "            r\"\u0928\u093f\u092f\u092e\",\n",
    "            r\"\u0936\u093e\u0938\u094d\u0924\u094d\u0930\",\n",
    "        ],  # raja, dharma, vidhi, niyama, shastra\n",
    "        BondType.CARE: [\n",
    "            r\"\u0915\u0930\u0941\u0923\u093e\",\n",
    "            r\"\u0926\u092f\u093e\",\n",
    "            r\"\u092a\u094d\u0930\u0947\u092e\",\n",
    "            r\"\u092e\u0948\u0924\u094d\u0930\u0940\",\n",
    "            r\"\u0938\u0947\u0935\u093e\",\n",
    "        ],  # karuna, daya, prema, maitri, seva\n",
    "        BondType.FAIRNESS: [r\"\u0928\u094d\u092f\u093e\u092f\", r\"\u0938\u092e\u0924\u093e\", r\"\u0927\u0930\u094d\u092e\", r\"\u090b\u0924\"],  # nyaya, samata, dharma, rta\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\u092a\u094d\u0930\u0924\u093f\u091c\u094d\u091e\u093e\",\n",
    "            r\"\u0938\u0902\u0935\u093f\u0926\",\n",
    "            r\"\u0935\u091a\u0928\",\n",
    "            r\"\u0936\u092a\u0925\",\n",
    "        ],  # pratijna, samvid, vachana, shapatha\n",
    "    },\n",
    "    # NEW in v10.9: Pali patterns (romanized)\n",
    "    \"pali\": {\n",
    "        BondType.HARM_PREVENTION: [r\"himsa\", r\"ahimsa\", r\"panatipata\", r\"rakkhati\"],\n",
    "        BondType.RECIPROCITY: [r\"dana\", r\"patidana\", r\"ina\"],\n",
    "        BondType.AUTONOMY: [r\"vimutti\", r\"nibbana\", r\"attadhipa\"],\n",
    "        BondType.PROPERTY: [r\"dhana\", r\"theyya\", r\"adinnadana\"],\n",
    "        BondType.FAMILY: [r\"mata\", r\"pita\", r\"putta\", r\"kula\"],\n",
    "        BondType.AUTHORITY: [r\"raja\", r\"dhamma\", r\"vinaya\", r\"sikkhapada\"],\n",
    "        BondType.CARE: [r\"karuna\", r\"metta\", r\"mudita\", r\"upekkha\"],\n",
    "        BondType.FAIRNESS: [r\"samma\", r\"dhamma\", r\"sacca\"],\n",
    "        BondType.CONTRACT: [r\"patijna\", r\"vacana\", r\"sacca\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# ===== COMPLETE HOHFELD PATTERNS =====\n",
    "ALL_HOHFELD_PATTERNS = {\n",
    "    \"hebrew\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u05d7\\u05d9\\u05d9\\u05d1\",\n",
    "            r\"\\u05e6\\u05e8\\u05d9\\u05db\",\n",
    "            r\"\\u05de\\u05d5\\u05db\\u05e8\\u05d7\",\n",
    "            r\"\\u05de\\u05e6\\u05d5\\u05d5\\u05d4\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "            r\"\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d6\\u05db\\u05d0\\u05d9\",\n",
    "            r\"\\u05de\\u05d2\\u05d9\\u05e2\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u05de\\u05d5\\u05ea\\u05e8\",\n",
    "            r\"\\u05e8\\u05e9\\u05d5\\u05ea\",\n",
    "            r\"\\u05e4\\u05d8\\u05d5\\u05e8\",\n",
    "            r\"\\u05d9\\u05db\\u05d5\\u05dc\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u05d0\\u05e1\\u05d5\\u05e8\",\n",
    "            r\"\\u05d0\\u05d9\\u05e0\\u05d5 \\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d0\\u05d9\\u05df.*\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "        ],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u05d7\\u05d9\\u05d9\\u05d1\",\n",
    "            r\"\\u05de\\u05d7\\u05d5\\u05d9\\u05d1\",\n",
    "            r\"\\u05d1\\u05e2\\u05d9\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "            r\"\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d6\\u05db\\u05d9\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u05e9\\u05e8\\u05d9\",\n",
    "            r\"\\u05de\\u05d5\\u05ea\\u05e8\",\n",
    "            r\"\\u05e4\\u05d8\\u05d5\\u05e8\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u05d0\\u05e1\\u05d5\\u05e8\",\n",
    "            r\"\\u05dc\\u05d0.*\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "        ],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        HohfeldState.OBLIGATION: [r\"\\u5fc5\", r\"\\u9808\", r\"\\u7576\", r\"\\u61c9\", r\"\\u5b9c\"],\n",
    "        HohfeldState.RIGHT: [r\"\\u53ef\", r\"\\u5f97\", r\"\\u6b0a\", r\"\\u5b9c\"],\n",
    "        HohfeldState.LIBERTY: [r\"\\u8a31\", r\"\\u4efb\", r\"\\u807d\", r\"\\u514d\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"\\u4e0d\\u53ef\", r\"\\u52ff\", r\"\\u7981\", r\"\\u83ab\", r\"\\u975e\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u064a\\u062c\\u0628\",\n",
    "            r\"\\u0648\\u0627\\u062c\\u0628\",\n",
    "            r\"\\u0641\\u0631\\u0636\",\n",
    "            r\"\\u0644\\u0627\\u0632\\u0645\",\n",
    "            r\"\\u0648\\u062c\\u0648\\u0628\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u062d\\u0642\",\n",
    "            r\"\\u064a\\u062d\\u0642\",\n",
    "            r\"\\u062c\\u0627\\u0626\\u0632\",\n",
    "            r\"\\u064a\\u062c\\u0648\\u0632\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u0645\\u0628\\u0627\\u062d\",\n",
    "            r\"\\u062d\\u0644\\u0627\\u0644\",\n",
    "            r\"\\u062c\\u0627\\u0626\\u0632\",\n",
    "            r\"\\u0627\\u0628\\u0627\\u062d\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u062d\\u0631\\u0627\\u0645\",\n",
    "            r\"\\u0645\\u062d\\u0631\\u0645\",\n",
    "            r\"\\u0645\\u0645\\u0646\\u0648\\u0639\",\n",
    "            r\"\\u0644\\u0627 \\u064a\\u062c\\u0648\\u0632\",\n",
    "            r\"\\u0646\\u0647[\\u064a\\u0649]\",\n",
    "        ],\n",
    "    },\n",
    "    \"english\": {\n",
    "        HohfeldState.OBLIGATION: [r\"\\bmust\\b\", r\"\\bshall\\b\", r\"\\bobligat\", r\"\\bduty\", r\"\\brequir\"],\n",
    "        HohfeldState.RIGHT: [r\"\\bright\\b\", r\"\\bentitle\", r\"\\bdeserve\", r\"\\bclaim\"],\n",
    "        HohfeldState.LIBERTY: [r\"\\bmay\\b\", r\"\\bpermit\", r\"\\ballow\", r\"\\bfree to\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"\\bforbid\", r\"\\bprohibit\", r\"\\bmust not\", r\"\\bshall not\"],\n",
    "    },\n",
    "    # NEW in v10.9: Sanskrit Hohfeld patterns (Devanagari)\n",
    "    \"sanskrit\": {\n",
    "        HohfeldState.OBLIGATION: [r\"\u0915\u0930\u094d\u0924\u0935\u094d\u092f\", r\"\u0905\u0935\u0936\u094d\u092f\", r\"\u0928\u093f\u092f\u092e\", r\"\u0935\u093f\u0927\u093f\"],  # kartavya, avashya\n",
    "        HohfeldState.RIGHT: [r\"\u0905\u0927\u093f\u0915\u093e\u0930\", r\"\u0938\u094d\u0935\u0924\u094d\u0935\"],  # adhikara, svatva\n",
    "        HohfeldState.LIBERTY: [r\"\u0936\u0915\u094d\u092f\", r\"\u0905\u0928\u0941\u091c\u094d\u091e\u093e\", r\"\u0909\u091a\u093f\u0924\"],  # shakya, anuj\u00f1a\n",
    "        HohfeldState.NO_RIGHT: [r\"\u0928\u093f\u0937\u093f\u0926\u094d\u0927\", r\"\u0935\u0930\u094d\u091c\u093f\u0924\", r\"\u0905\u0915\u0930\u094d\u0924\u0935\u094d\u092f\"],  # nishiddha, varjita\n",
    "    },\n",
    "    # NEW in v10.9: Pali Hohfeld patterns (romanized)\n",
    "    \"pali\": {\n",
    "        HohfeldState.OBLIGATION: [r\"kicca\", r\"karaniiya\", r\"dhammo\"],\n",
    "        HohfeldState.RIGHT: [r\"adhikaara\", r\"bhaaga\"],\n",
    "        HohfeldState.LIBERTY: [r\"anujaanati\", r\"kappati\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"nisiddha\", r\"akaraniya\", r\"na kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ===== CONTEXT MARKERS FOR GRAMMAR-AWARE EXTRACTION =====\n",
    "# These help distinguish \"thou shalt not kill\" from \"he killed\"\n",
    "CONTEXT_MARKERS = {\n",
    "    \"hebrew\": {\n",
    "        \"negation\": [r\"\u05dc\u05d0\", r\"\u05d0\u05dc\", r\"\u05d0\u05d9\u05df\", r\"\u05d1\u05dc\u05d9\", r\"\u05d0\u05d9\u05e0\"],\n",
    "        \"obligation\": [r\"\u05d7\u05d9\u05d9\u05d1\", r\"\u05e6\u05e8\u05d9\u05da\", r\"\u05de\u05d5\u05db\u05e8\u05d7\", r\"\u05e6\u05d5\u05d5\u05d4\"],\n",
    "        \"prohibition\": [r\"\u05d0\u05e1\u05d5\u05e8\", r\"\u05d0\u05dc.*\u05ea\"],\n",
    "        \"permission\": [r\"\u05de\u05d5\u05ea\u05e8\", r\"\u05e8\u05e9\u05d0\u05d9\", r\"\u05e4\u05d8\u05d5\u05e8\"],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        \"negation\": [r\"\u05dc\u05d0\", r\"\u05dc\u05d9\u05ea\", r\"\u05dc\u05d0\u05d5\"],\n",
    "        \"obligation\": [r\"\u05d7\u05d9\u05d9\u05d1\", r\"\u05d1\u05e2\u05d9\"],\n",
    "        \"prohibition\": [r\"\u05d0\u05e1\u05d5\u05e8\"],\n",
    "        \"permission\": [r\"\u05e9\u05e8\u05d9\", r\"\u05de\u05d5\u05ea\u05e8\"],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        \"negation\": [r\"\u4e0d\", r\"\u975e\", r\"\u7121\", r\"\u672a\", r\"\u6bcb\"],\n",
    "        \"obligation\": [r\"\u5fc5\", r\"\u7576\", r\"\u9808\", r\"\u61c9\", r\"\u5b9c\"],\n",
    "        \"prohibition\": [r\"\u52ff\", r\"\u7981\", r\"\u83ab\", r\"\u4e0d\u53ef\"],\n",
    "        \"permission\": [r\"\u53ef\", r\"\u5f97\", r\"\u8a31\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        \"negation\": [r\"\u0644\u0627\", r\"\u0645\u0627\", r\"\u0644\u064a\u0633\", r\"\u0644\u0645\", r\"\u063a\u064a\u0631\"],\n",
    "        \"obligation\": [r\"\u064a\u062c\u0628\", r\"\u0648\u0627\u062c\u0628\", r\"\u0641\u0631\u0636\", r\"\u0639\u0644\u064a\u0647\"],\n",
    "        \"prohibition\": [r\"\u062d\u0631\u0627\u0645\", r\"\u0645\u062d\u0631\u0645\", r\"\u0644\u0627 \u064a\u062c\u0648\u0632\", r\"\u0646\u0647\u0649\"],\n",
    "        \"permission\": [r\"\u062d\u0644\u0627\u0644\", r\"\u0645\u0628\u0627\u062d\", r\"\u062c\u0627\u0626\u0632\"],\n",
    "    },\n",
    "    \"english\": {\n",
    "        \"negation\": [r\"not\", r\"no\", r\"never\", r\"neither\", r\"n't\"],\n",
    "        \"obligation\": [r\"must\", r\"shall\", r\"should\", r\"ought\", r\"required\"],\n",
    "        \"prohibition\": [r\"forbid\", r\"prohibit\", r\"must not\", r\"shall not\", r\"don't\"],\n",
    "        \"permission\": [r\"may\", r\"can\", r\"allowed\", r\"permit\"],\n",
    "    },\n",
    "    # NEW in v10.9: Sanskrit context markers\n",
    "    \"sanskrit\": {\n",
    "        \"negation\": [r\"\u0928\", r\"\u092e\u093e\", r\"\u0905\"],  # na, m\u0101, a- prefix\n",
    "        \"obligation\": [r\"\u0915\u0930\u094d\u0924\u0935\u094d\u092f\", r\"\u0905\u0935\u0936\u094d\u092f\", r\"\u0935\u093f\u0927\u093f\"],\n",
    "        \"prohibition\": [r\"\u0928\u093f\u0937\u093f\u0926\u094d\u0927\", r\"\u0935\u0930\u094d\u091c\u093f\u0924\", r\"\u092e\u093e\"],\n",
    "        \"permission\": [r\"\u0936\u0915\u094d\u092f\", r\"\u0905\u0928\u0941\u091c\u094d\u091e\u093e\"],\n",
    "    },\n",
    "    # NEW in v10.9: Pali context markers\n",
    "    \"pali\": {\n",
    "        \"negation\": [r\"na\", r\"ma\", r\"a-\"],\n",
    "        \"obligation\": [r\"kicca\", r\"karaniya\"],\n",
    "        \"prohibition\": [r\"nisiddha\", r\"akaraniya\"],\n",
    "        \"permission\": [r\"anujaanati\", r\"kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def detect_context(text, language, match_pos, window=30):\n",
    "    \"\"\"\n",
    "    Detect grammatical context around a pattern match.\n",
    "    Returns: ('prescriptive'/'descriptive'/'unknown', marker_type or None)\n",
    "    \"\"\"\n",
    "    markers = CONTEXT_MARKERS.get(language, {})\n",
    "    if not markers:\n",
    "        return \"unknown\", None\n",
    "\n",
    "    start = max(0, match_pos - window)\n",
    "    end = min(len(text), match_pos + window)\n",
    "    window_text = text[start:end]\n",
    "\n",
    "    # Check for deontic markers (prescriptive = moral statement)\n",
    "    for marker_type in [\"prohibition\", \"obligation\", \"permission\"]:\n",
    "        for pattern in markers.get(marker_type, []):\n",
    "            if re.search(pattern, window_text):\n",
    "                return \"prescriptive\", marker_type\n",
    "\n",
    "    # Check for simple negation (may be descriptive)\n",
    "    for pattern in markers.get(\"negation\", []):\n",
    "        if re.search(pattern, window_text):\n",
    "            return \"descriptive\", \"negated\"\n",
    "\n",
    "    return \"descriptive\", None\n",
    "\n",
    "\n",
    "# ===== NLP IMPROVEMENTS (v10.9 Phase 1) =====\n",
    "# These provide negation detection and modal classification without external dependencies\n",
    "\n",
    "NEGATION_CUES = {\n",
    "    \"english\": [\"not\", \"no\", \"never\", \"neither\", \"nor\", \"n't\", \"without\", \"lack\", \"none\"],\n",
    "    \"classical_chinese\": [\"\u4e0d\", \"\u975e\", \"\u7121\", \"\u83ab\", \"\u52ff\", \"\u672a\", \"\u5f17\", \"\u6bcb\", \"\u5426\"],\n",
    "    \"arabic\": [\"\u0644\u0627\", \"\u0645\u0627\", \"\u0644\u0645\", \"\u0644\u0646\", \"\u0644\u064a\u0633\", \"\u063a\u064a\u0631\", \"\u0628\u062f\u0648\u0646\"],\n",
    "    \"hebrew\": [\"\u05dc\u05d0\", \"\u05d0\u05dc\", \"\u05d1\u05dc\u05d9\", \"\u05d0\u05d9\u05df\", \"\u05de\u05d1\u05dc\u05d9\"],\n",
    "    \"aramaic\": [\"\u05dc\u05d0\", \"\u05dc\u05d9\u05ea\", \"\u05dc\u05d0\u05d5\"],\n",
    "    \"sanskrit\": [\"\u0928\", \"\u092e\u093e\", \"\u0905\"],  # na, m\u0101, a- (privative prefix)\n",
    "    \"pali\": [\"na\", \"ma\", \"a\", \"an\"],\n",
    "}\n",
    "\n",
    "MODAL_CLASSIFICATION = {\n",
    "    \"english\": {\n",
    "        \"obligation\": [\"must\", \"shall\", \"have to\", \"ought to\", \"need to\", \"required\", \"obligated\"],\n",
    "        \"permission\": [\"may\", \"can\", \"allowed\", \"permitted\", \"free to\", \"entitled\"],\n",
    "        \"prohibition\": [\"must not\", \"shall not\", \"cannot\", \"forbidden\", \"prohibited\", \"banned\"],\n",
    "        \"supererogation\": [\"should\", \"ought\", \"would be good\", \"ideally\", \"preferably\"],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        \"obligation\": [\"\u5fc5\", \"\u7576\", \"\u5b9c\", \"\u9808\", \"\u61c9\", \"\u8981\"],\n",
    "        \"permission\": [\"\u53ef\", \"\u5f97\", \"\u8a31\", \"\u5bb9\", \"\u80fd\"],\n",
    "        \"prohibition\": [\"\u4e0d\u53ef\", \"\u4e0d\u5f97\", \"\u52ff\", \"\u83ab\", \"\u7981\", \"\u4e0d\u8a31\", \"\u4e0d\u5b9c\"],\n",
    "        \"supererogation\": [\"\u5584\", \"\u7f8e\", \"\u5fb7\", \"\u5b9c\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        \"obligation\": [\"\u064a\u062c\u0628\", \"\u0641\u0631\u0636\", \"\u0648\u0627\u062c\u0628\", \"\u0644\u0627\u0632\u0645\", \"\u0641\u0631\u064a\u0636\u0629\"],\n",
    "        \"permission\": [\"\u064a\u062c\u0648\u0632\", \"\u0645\u0628\u0627\u062d\", \"\u062d\u0644\u0627\u0644\", \"\u062c\u0627\u0626\u0632\"],\n",
    "        \"prohibition\": [\"\u062d\u0631\u0627\u0645\", \"\u0645\u062d\u0631\u0645\", \"\u0645\u0645\u0646\u0648\u0639\", \"\u0644\u0627 \u064a\u062c\u0648\u0632\", \"\u0645\u062d\u0638\u0648\u0631\"],\n",
    "        \"supererogation\": [\"\u0645\u0633\u062a\u062d\u0628\", \"\u0633\u0646\u0629\", \"\u0645\u0646\u062f\u0648\u0628\", \"\u0646\u0627\u0641\u0644\u0629\"],\n",
    "    },\n",
    "    \"hebrew\": {\n",
    "        \"obligation\": [\"\u05d7\u05d9\u05d9\u05d1\", \"\u05de\u05e6\u05d5\u05d5\u05d4\", \"\u05e6\u05e8\u05d9\u05da\", \"\u05de\u05d5\u05db\u05e8\u05d7\", \"\u05d7\u05d5\u05d1\u05d4\"],\n",
    "        \"permission\": [\"\u05de\u05d5\u05ea\u05e8\", \"\u05e8\u05e9\u05d0\u05d9\", \"\u05d9\u05db\u05d5\u05dc\", \"\u05d4\u05d9\u05ea\u05e8\"],\n",
    "        \"prohibition\": [\"\u05d0\u05e1\u05d5\u05e8\", \"\u05dc\u05d0 \u05d9\u05e2\u05e9\u05d4\", \"\u05d0\u05dc\", \"\u05d0\u05d9\u05e1\u05d5\u05e8\"],\n",
    "        \"supererogation\": [\"\u05e8\u05d0\u05d5\u05d9\", \"\u05d8\u05d5\u05d1\", \"\u05de\u05d9\u05d3\u05ea \u05d7\u05e1\u05d9\u05d3\u05d5\u05ea\", \"\u05dc\u05e4\u05e0\u05d9\u05dd \u05de\u05e9\u05d5\u05e8\u05ea \u05d4\u05d3\u05d9\u05df\"],\n",
    "    },\n",
    "    \"sanskrit\": {\n",
    "        \"obligation\": [\"\u0915\u0930\u094d\u0924\u0935\u094d\u092f\", \"\u0905\u0935\u0936\u094d\u092f\", \"\u0928\u093f\u092f\u092e\"],  # kartavya, avashya, niyama\n",
    "        \"permission\": [\"\u0936\u0915\u094d\u092f\", \"\u0905\u0928\u0941\u091c\u094d\u091e\u093e\"],  # shakya, anuj\u00f1a\n",
    "        \"prohibition\": [\"\u0928\u093f\u0937\u093f\u0926\u094d\u0927\", \"\u0935\u0930\u094d\u091c\u093f\u0924\", \"\u092e\u093e\"],  # nishiddha, varjita, m\u0101\n",
    "    },\n",
    "    \"pali\": {\n",
    "        \"obligation\": [\"kicca\", \"karaniya\", \"dhamma\"],\n",
    "        \"permission\": [\"kappati\", \"anujanati\"],\n",
    "        \"prohibition\": [\"akappiya\", \"akaraniya\", \"na kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def enhanced_extract_bond(text: str, language: str) -> dict:\n",
    "    \"\"\"\n",
    "    Enhanced bond extraction with negation + modal detection.\n",
    "    Phase 1 implementation - no external NLP dependencies required.\n",
    "\n",
    "    Returns dict with:\n",
    "        - bond_type: BondType or None\n",
    "        - hohfeld_state: str (OBLIGATION/RIGHT/LIBERTY/NO_RIGHT)\n",
    "        - negated: bool\n",
    "        - modal: str or None (the matched modal marker)\n",
    "        - confidence: float\n",
    "        - context: str (prescriptive/descriptive/unknown)\n",
    "    \"\"\"\n",
    "    # 1. Normalize text\n",
    "    normalized = normalize_text(text, language)\n",
    "\n",
    "    # 2. Check negation\n",
    "    negation_cues = NEGATION_CUES.get(language, [])\n",
    "    is_negated = any(cue in normalized for cue in negation_cues)\n",
    "\n",
    "    # 3. Check modal and classify deontic status\n",
    "    modal_status = \"unknown\"\n",
    "    modal_text = None\n",
    "    for status, markers in MODAL_CLASSIFICATION.get(language, {}).items():\n",
    "        for marker in markers:\n",
    "            if marker in normalized:\n",
    "                modal_status = status\n",
    "                modal_text = marker\n",
    "                break\n",
    "        if modal_status != \"unknown\":\n",
    "            break\n",
    "\n",
    "    # 4. Map modal to Hohfeld state\n",
    "    hohfeld_map = {\n",
    "        \"obligation\": \"OBLIGATION\",\n",
    "        \"permission\": \"LIBERTY\",\n",
    "        \"prohibition\": \"NO_RIGHT\",\n",
    "        \"supererogation\": \"LIBERTY\",\n",
    "        \"unknown\": \"OBLIGATION\",  # Default assumption\n",
    "    }\n",
    "    hohfeld = hohfeld_map[modal_status]\n",
    "\n",
    "    # 5. Pattern matching for bond type\n",
    "    bond_type = None\n",
    "    confidence = 0.5\n",
    "    for bt, patterns in ALL_BOND_PATTERNS.get(language, {}).items():\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, normalized):\n",
    "                bond_type = bt\n",
    "                confidence = 0.9\n",
    "                break\n",
    "        if bond_type:\n",
    "            break\n",
    "\n",
    "    # 6. Adjust confidence for negation\n",
    "    if is_negated:\n",
    "        confidence *= 0.8  # Lower confidence for negated statements\n",
    "\n",
    "    # 7. Determine context\n",
    "    if modal_status in [\"obligation\", \"prohibition\"]:\n",
    "        context = \"prescriptive\"\n",
    "    elif modal_status == \"permission\":\n",
    "        context = \"descriptive\"  # Permissions are often statements of fact\n",
    "    else:\n",
    "        context = \"unknown\"\n",
    "\n",
    "    return {\n",
    "        \"bond_type\": bond_type,\n",
    "        \"hohfeld_state\": hohfeld,\n",
    "        \"negated\": is_negated,\n",
    "        \"modal\": modal_text,\n",
    "        \"confidence\": confidence,\n",
    "        \"context\": context,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\nContext markers defined for grammar-aware extraction\")\n",
    "print(\"  Detects: negation, obligation, prohibition, permission\")\n",
    "\n",
    "print(f\"\\nPatterns defined for {len(ALL_BOND_PATTERNS)} languages:\")\n",
    "for lang in ALL_BOND_PATTERNS:\n",
    "    n = sum(len(p) for p in ALL_BOND_PATTERNS[lang].values())\n",
    "    print(f\"  {lang}: {n} bond patterns\")\n",
    "\n",
    "print(\"\\nNLP improvements (Phase 1):\")\n",
    "print(f\"  NEGATION_CUES: {len(NEGATION_CUES)} languages\")\n",
    "print(f\"  MODAL_CLASSIFICATION: {len(MODAL_CLASSIFICATION)} languages\")\n",
    "print(\"  enhanced_extract_bond() ready\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 4. Parallel Download + Stream Processing { display-mode: \"form\" }\n",
    "# @markdown BIP v10.9: Loads ALL corpora including expanded Chinese (Buddhist, Legalist, Mohist, Neo-Confucian),\n",
    "# @markdown Arabic (Fiqh, Sufi, Falsafa), and NEW Sanskrit/Pali texts\n",
    "\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import gc\n",
    "import shutil\n",
    "import requests\n",
    "import time\n",
    "import threading\n",
    "from queue import Queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Thread-safe queue for passages\n",
    "passage_queue = Queue(maxsize=100000)\n",
    "download_complete = threading.Event()\n",
    "corpus_stats = defaultdict(int)\n",
    "stats_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def update_stats(lang, count):\n",
    "    with stats_lock:\n",
    "        corpus_stats[lang] += count\n",
    "        total = sum(corpus_stats.values())\n",
    "        if total % 1000 == 0:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "\n",
    "\n",
    "# Check if we should skip processing (data loaded from Drive)\n",
    "# Check if we should use cached data or download fresh\n",
    "SKIP_PROCESSING = LOAD_FROM_DRIVE  # Re-evaluate based on current settings\n",
    "\n",
    "# Minimum thresholds for balanced experiments\n",
    "MIN_CORPUS_SIZE = {\n",
    "    \"english\": 20000,  # Lowered - HF augmentation datasets deprecated\n",
    "    \"classical_chinese\": 20000,  # Lowered\n",
    "    \"hebrew\": 5000,\n",
    "    \"aramaic\": 2000,\n",
    "    \"arabic\": 2000,\n",
    "    \"sanskrit\": 100,  # NEW in v10.9 - smaller initial corpus\n",
    "    \"pali\": 50,  # NEW in v10.9 - smaller initial corpus\n",
    "}\n",
    "\n",
    "# Available augmentation datasets by language\n",
    "AUGMENTATION_DATASETS = {\n",
    "    \"english\": [\n",
    "        (\"hendrycks/ethics\", \"ETHICS\"),  # ~130K moral scenarios\n",
    "        (\"allenai/social_chem_101\", \"SocialChem\"),  # ~292K social norms\n",
    "    ],\n",
    "    \"classical_chinese\": [\n",
    "        (\"wikisource_zh_classical\", \"WikisourceZH\"),  # If available\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ===== v10.9 NEW CORPORA =====\n",
    "# Buddhist Chinese (\u4f5b\u6559\u6f22\u6587) - Expanded v10.9\n",
    "BUDDHIST_CHINESE = [\n",
    "    # Dhammapada (\u6cd5\u53e5\u7d93)\n",
    "    (\"\u8af8\u60e1\u83ab\u4f5c\uff0c\u773e\u5584\u5949\u884c\uff0c\u81ea\u6de8\u5176\u610f\uff0c\u662f\u8af8\u4f5b\u6559\u3002\", \"Dhammapada 183\", \"BUDDHIST\"),\n",
    "    (\"\u4ee5\u6068\u6b62\u6068\uff0c\u6068\u7d42\u4e0d\u6ec5\uff1b\u552f\u4ee5\u5fcd\u6b62\u6068\uff0c\u6b64\u53e4\u8056\u5e38\u6cd5\u3002\", \"Dhammapada 5\", \"BUDDHIST\"),\n",
    "    (\"\u52dd\u8005\u751f\u6028\uff0c\u8ca0\u8005\u81ea\u9119\uff1b\u53bb\u52dd\u8ca0\u5fc3\uff0c\u7121\u8acd\u81ea\u5b89\u3002\", \"Dhammapada 201\", \"BUDDHIST\"),\n",
    "    (\"\u4e0d\u4ee5\u8ca1\u7269\u65bd\uff0c\u552f\u4ee5\u6cd5\u5e03\u65bd\uff0c\u6cd5\u65bd\u52dd\u8ca1\u65bd\u3002\", \"Dhammapada 354\", \"BUDDHIST\"),\n",
    "    (\"\u5fc3\u70ba\u6cd5\u672c\uff0c\u5fc3\u5c0a\u5fc3\u4f7f\uff0c\u4e2d\u5fc3\u5ff5\u60e1\uff0c\u5373\u8a00\u5373\u884c\u3002\", \"Dhammapada 1\", \"BUDDHIST\"),\n",
    "    (\"\u5fc3\u70ba\u6cd5\u672c\uff0c\u5fc3\u5c0a\u5fc3\u4f7f\uff0c\u4e2d\u5fc3\u5ff5\u5584\uff0c\u5373\u8a00\u5373\u884c\u3002\", \"Dhammapada 2\", \"BUDDHIST\"),\n",
    "    (\"\u6173\u60dc\u8ca1\u7269\uff0c\u5b88\u8b77\u52ff\u5931\uff0c\u5f8c\u70ba\u7121\u667a\u3002\", \"Dhammapada\", \"BUDDHIST\"),\n",
    "    (\"\u611a\u4eba\u6240\u601d\u91cf\uff0c\u5e38\u4e0d\u5f97\u5b89\u7a69\u3002\", \"Dhammapada\", \"BUDDHIST\"),\n",
    "    # Diamond Sutra (\u91d1\u525b\u7d93)\n",
    "    (\"\u82e5\u4ee5\u8272\u898b\u6211\uff0c\u4ee5\u97f3\u8072\u6c42\u6211\uff0c\u662f\u4eba\u884c\u90aa\u9053\uff0c\u4e0d\u80fd\u898b\u5982\u4f86\u3002\", \"Diamond Sutra 26\", \"BUDDHIST\"),\n",
    "    (\"\u61c9\u7121\u6240\u4f4f\u800c\u751f\u5176\u5fc3\u3002\", \"Diamond Sutra 10\", \"BUDDHIST\"),\n",
    "    (\"\u4e00\u5207\u6709\u70ba\u6cd5\uff0c\u5982\u5922\u5e7b\u6ce1\u5f71\uff0c\u5982\u9732\u4ea6\u5982\u96fb\uff0c\u61c9\u4f5c\u5982\u662f\u89c0\u3002\", \"Diamond Sutra 32\", \"BUDDHIST\"),\n",
    "    (\"\u51e1\u6240\u6709\u76f8\uff0c\u7686\u662f\u865b\u5984\u3002\u82e5\u898b\u8af8\u76f8\u975e\u76f8\uff0c\u5373\u898b\u5982\u4f86\u3002\", \"Diamond Sutra 5\", \"BUDDHIST\"),\n",
    "    (\"\u904e\u53bb\u5fc3\u4e0d\u53ef\u5f97\uff0c\u73fe\u5728\u5fc3\u4e0d\u53ef\u5f97\uff0c\u672a\u4f86\u5fc3\u4e0d\u53ef\u5f97\u3002\", \"Diamond Sutra 18\", \"BUDDHIST\"),\n",
    "    (\"\u96e2\u4e00\u5207\u8af8\u76f8\uff0c\u5247\u540d\u8af8\u4f5b\u3002\", \"Diamond Sutra 14\", \"BUDDHIST\"),\n",
    "    (\"\u82e5\u83e9\u85a9\u6709\u6211\u76f8\u3001\u4eba\u76f8\u3001\u773e\u751f\u76f8\u3001\u58fd\u8005\u76f8\uff0c\u5373\u975e\u83e9\u85a9\u3002\", \"Diamond Sutra 3\", \"BUDDHIST\"),\n",
    "    (\"\u61c9\u7121\u6240\u4f4f\uff0c\u884c\u65bc\u5e03\u65bd\u3002\", \"Diamond Sutra 4\", \"BUDDHIST\"),\n",
    "    (\"\u5982\u4f86\u6240\u8aaa\u6cd5\uff0c\u7686\u4e0d\u53ef\u53d6\u3001\u4e0d\u53ef\u8aaa\uff0c\u975e\u6cd5\u3001\u975e\u975e\u6cd5\u3002\", \"Diamond Sutra 7\", \"BUDDHIST\"),\n",
    "    # Lotus Sutra (\u6cd5\u83ef\u7d93)\n",
    "    (\"\u8af8\u4f5b\u4e16\u5c0a\u552f\u4ee5\u4e00\u5927\u4e8b\u56e0\u7de3\u6545\uff0c\u51fa\u73fe\u65bc\u4e16\u3002\", \"Lotus Sutra 2\", \"BUDDHIST\"),\n",
    "    (\"\u5341\u65b9\u4f5b\u571f\u4e2d\uff0c\u552f\u6709\u4e00\u4e58\u6cd5\uff0c\u7121\u4e8c\u4ea6\u7121\u4e09\u3002\", \"Lotus Sutra 2\", \"BUDDHIST\"),\n",
    "    (\"\u662f\u6cd5\u5e73\u7b49\uff0c\u7121\u6709\u9ad8\u4e0b\uff0c\u662f\u540d\u963f\u8028\u591a\u7f85\u4e09\u85d0\u4e09\u83e9\u63d0\u3002\", \"Lotus Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u552f\u4f5b\u8207\u4f5b\uff0c\u4e43\u80fd\u7a76\u76e1\u8af8\u6cd5\u5be6\u76f8\u3002\", \"Lotus Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u4e16\u9593\u6cd5\u4f4f\uff0c\u4e16\u9593\u6cd5\u5728\u3002\", \"Lotus Sutra\", \"BUDDHIST\"),\n",
    "    # Heart Sutra (\u5fc3\u7d93)\n",
    "    (\"\u8272\u4e0d\u7570\u7a7a\uff0c\u7a7a\u4e0d\u7570\u8272\uff0c\u8272\u5373\u662f\u7a7a\uff0c\u7a7a\u5373\u662f\u8272\u3002\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u7121\u82e6\u96c6\u6ec5\u9053\uff0c\u7121\u667a\u4ea6\u7121\u5f97\uff0c\u4ee5\u7121\u6240\u5f97\u6545\u3002\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u89c0\u81ea\u5728\u83e9\u85a9\uff0c\u884c\u6df1\u822c\u82e5\u6ce2\u7f85\u871c\u591a\u6642\uff0c\u7167\u898b\u4e94\u860a\u7686\u7a7a\uff0c\u5ea6\u4e00\u5207\u82e6\u5384\u3002\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u5fc3\u7121\u7f63\u7919\uff0c\u7121\u7f63\u7919\u6545\uff0c\u7121\u6709\u6050\u6016\uff0c\u9060\u96e2\u985b\u5012\u5922\u60f3\uff0c\u7a76\u7adf\u6d85\u69c3\u3002\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u63ed\u8ae6\u63ed\u8ae6\uff0c\u6ce2\u7f85\u63ed\u8ae6\uff0c\u6ce2\u7f85\u50e7\u63ed\u8ae6\uff0c\u83e9\u63d0\u85a9\u5a46\u8a36\u3002\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    # Brahma Net Sutra (\u68b5\u7db2\u7d93)\n",
    "    (\"\u6148\u60b2\u559c\u6368\uff0c\u540d\u70ba\u56db\u7121\u91cf\u5fc3\u3002\", \"Brahma Net Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u4e0d\u6bba\u751f\uff0c\u662f\u83e9\u85a9\u6ce2\u7f85\u5937\u7f6a\u3002\", \"Brahma Net Sutra 1\", \"BUDDHIST\"),\n",
    "    (\"\u4e0d\u5077\u76dc\uff0c\u662f\u83e9\u85a9\u6ce2\u7f85\u5937\u7f6a\u3002\", \"Brahma Net Sutra 2\", \"BUDDHIST\"),\n",
    "    (\"\u4e0d\u90aa\u6deb\uff0c\u662f\u83e9\u85a9\u6ce2\u7f85\u5937\u7f6a\u3002\", \"Brahma Net Sutra 3\", \"BUDDHIST\"),\n",
    "    (\"\u4e0d\u5984\u8a9e\uff0c\u662f\u83e9\u85a9\u6ce2\u7f85\u5937\u7f6a\u3002\", \"Brahma Net Sutra 4\", \"BUDDHIST\"),\n",
    "    (\"\u4e0d\u98f2\u9152\uff0c\u662f\u83e9\u85a9\u6ce2\u7f85\u5937\u7f6a\u3002\", \"Brahma Net Sutra 5\", \"BUDDHIST\"),\n",
    "    (\"\u82e5\u4f5b\u5b50\uff0c\u4ee5\u6148\u5fc3\u6545\uff0c\u884c\u653e\u751f\u696d\u3002\", \"Brahma Net Sutra 20\", \"BUDDHIST\"),\n",
    "    (\"\u4e00\u5207\u7537\u5b50\u662f\u6211\u7236\uff0c\u4e00\u5207\u5973\u4eba\u662f\u6211\u6bcd\u3002\", \"Brahma Net Sutra 9\", \"BUDDHIST\"),\n",
    "    (\"\u5b5d\u9806\u7236\u6bcd\u5e2b\u50e7\u4e09\u5bf6\uff0c\u5b5d\u9806\u81f3\u9053\u4e4b\u6cd5\u3002\", \"Brahma Net Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u82e5\u4f5b\u5b50\uff0c\u5e38\u61c9\u767c\u4e00\u5207\u9858\u3002\", \"Brahma Net Sutra\", \"BUDDHIST\"),\n",
    "    # Nirvana Sutra (\u6d85\u69c3\u7d93)\n",
    "    (\"\u6bba\u751f\u4e4b\u7f6a\uff0c\u80fd\u4ee4\u773e\u751f\u58ae\u4e09\u60e1\u9053\u3002\", \"Sutra of Golden Light 4\", \"BUDDHIST\"),\n",
    "    (\"\u4e00\u5207\u773e\u751f\u7686\u6709\u4f5b\u6027\uff0c\u6089\u80fd\u6210\u4f5b\u3002\", \"Nirvana Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u4f5b\u6027\u8005\uff0c\u5373\u662f\u4e00\u5207\u773e\u751f\u963f\u8028\u591a\u7f85\u4e09\u85d0\u4e09\u83e9\u63d0\u4e2d\u9053\u7a2e\u5b50\u3002\", \"Nirvana Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u5982\u4f86\u5e38\u4f4f\uff0c\u7121\u6709\u8b8a\u6613\u3002\", \"Nirvana Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u6d85\u69c3\u4e4b\u9ad4\uff0c\u5177\u6709\u56db\u5fb7\uff1a\u5e38\u3001\u6a02\u3001\u6211\u3001\u6de8\u3002\", \"Nirvana Sutra\", \"BUDDHIST\"),\n",
    "    # Vimalakirti Sutra (\u7dad\u6469\u8a70\u7d93)\n",
    "    (\"\u83e9\u85a9\u75c5\u8005\uff0c\u4ee5\u5927\u60b2\u8d77\u3002\", \"Vimalakirti Sutra 5\", \"BUDDHIST\"),\n",
    "    (\"\u773e\u751f\u75c5\uff0c\u662f\u6545\u6211\u75c5\u3002\", \"Vimalakirti Sutra 5\", \"BUDDHIST\"),\n",
    "    (\"\u4e0d\u4f4f\u6709\u70ba\uff0c\u4e0d\u4f4f\u7121\u70ba\uff0c\u662f\u83e9\u85a9\u884c\u3002\", \"Vimalakirti Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u76f4\u5fc3\u662f\u9053\u5834\uff0c\u7121\u865b\u5047\u6545\u3002\", \"Vimalakirti Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u5165\u4e0d\u4e8c\u6cd5\u9580\uff0c\u9ed8\u7136\u7121\u8a00\u3002\", \"Vimalakirti Sutra\", \"BUDDHIST\"),\n",
    "    # Platform Sutra (\u516d\u7956\u58c7\u7d93)\n",
    "    (\"\u83e9\u63d0\u672c\u7121\u6a39\uff0c\u660e\u93e1\u4ea6\u975e\u81fa\uff0c\u672c\u4f86\u7121\u4e00\u7269\uff0c\u4f55\u8655\u60f9\u5875\u57c3\u3002\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u4f55\u671f\u81ea\u6027\uff0c\u672c\u81ea\u6e05\u6de8\uff1b\u4f55\u671f\u81ea\u6027\uff0c\u672c\u4e0d\u751f\u6ec5\u3002\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u4e0d\u601d\u5584\uff0c\u4e0d\u601d\u60e1\uff0c\u6b63\u8207\u9ebc\u6642\uff0c\u90a3\u500b\u662f\u660e\u4e0a\u5ea7\u672c\u4f86\u9762\u76ee\u3002\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u8ff7\u6642\u5e2b\u5ea6\uff0c\u609f\u6642\u81ea\u5ea6\u3002\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u4f5b\u6cd5\u5728\u4e16\u9593\uff0c\u4e0d\u96e2\u4e16\u9593\u89ba\u3002\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u898b\u6027\u6210\u4f5b\u3002\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u672c\u4f86\u7121\u4e00\u7269\uff0c\u4f55\u8655\u60f9\u5875\u57c3\u3002\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    # Avatamsaka Sutra (\u83ef\u56b4\u7d93)\n",
    "    (\"\u4e00\u5207\u773e\u751f\u7686\u5177\u5982\u4f86\u667a\u6167\u5fb7\u76f8\u3002\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u5fc3\u4f5b\u53ca\u773e\u751f\uff0c\u662f\u4e09\u7121\u5dee\u5225\u3002\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u82e5\u4eba\u6b32\u4e86\u77e5\uff0c\u4e09\u4e16\u4e00\u5207\u4f5b\uff0c\u61c9\u89c0\u6cd5\u754c\u6027\uff0c\u4e00\u5207\u552f\u5fc3\u9020\u3002\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u4e0d\u5fd8\u521d\u5fc3\uff0c\u65b9\u5f97\u59cb\u7d42\u3002\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u82e5\u6709\u5584\u7537\u5b50\uff0c\u5584\u5973\u4eba\uff0c\u767c\u963f\u8028\u591a\u7f85\u4e09\u85d0\u4e09\u83e9\u63d0\u5fc3\u3002\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    # Amitabha Sutra (\u963f\u5f4c\u9640\u7d93)\n",
    "    (\"\u5f9e\u662f\u897f\u65b9\uff0c\u904e\u5341\u842c\u5104\u4f5b\u571f\uff0c\u6709\u4e16\u754c\u540d\u66f0\u6975\u6a02\u3002\", \"Amitabha Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u5176\u570b\u773e\u751f\uff0c\u7121\u6709\u773e\u82e6\uff0c\u4f46\u53d7\u8af8\u6a02\uff0c\u6545\u540d\u6975\u6a02\u3002\", \"Amitabha Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u4e00\u5fc3\u4e0d\u4e82\uff0c\u5373\u5f97\u5f80\u751f\u963f\u5f4c\u9640\u4f5b\u6975\u6a02\u570b\u571f\u3002\", \"Amitabha Sutra\", \"BUDDHIST\"),\n",
    "    # Additional Buddhist texts\n",
    "    (\"\u4e09\u754c\u552f\u5fc3\uff0c\u842c\u6cd5\u552f\u8b58\u3002\", \"Yogacara\", \"BUDDHIST\"),\n",
    "    (\"\u7169\u60f1\u5373\u83e9\u63d0\uff0c\u751f\u6b7b\u5373\u6d85\u69c3\u3002\", \"Madhyamaka\", \"BUDDHIST\"),\n",
    "    (\"\u773e\u751f\u7121\u908a\u8a93\u9858\u5ea6\uff0c\u7169\u60f1\u7121\u76e1\u8a93\u9858\u65b7\u3002\", \"Four Great Vows\", \"BUDDHIST\"),\n",
    "    (\"\u6cd5\u9580\u7121\u91cf\u8a93\u9858\u5b78\uff0c\u4f5b\u9053\u7121\u4e0a\u8a93\u9858\u6210\u3002\", \"Four Great Vows\", \"BUDDHIST\"),\n",
    "    (\"\u4e00\u5207\u6709\u60c5\u7686\u662f\u6211\u7236\u6bcd\u3002\", \"Bodhisattva Vow\", \"BUDDHIST\"),\n",
    "    (\"\u81ea\u5229\u5229\u4ed6\uff0c\u81ea\u89ba\u89ba\u4ed6\u3002\", \"Bodhisattva Practice\", \"BUDDHIST\"),\n",
    "    (\"\u7121\u7de3\u5927\u6148\uff0c\u540c\u9ad4\u5927\u60b2\u3002\", \"Bodhisattva Practice\", \"BUDDHIST\"),\n",
    "    (\"\u61c9\u4ee5\u4f55\u8eab\u5f97\u5ea6\u8005\uff0c\u5373\u73fe\u4f55\u8eab\u800c\u70ba\u8aaa\u6cd5\u3002\", \"Guanyin\", \"BUDDHIST\"),\n",
    "    (\"\u5343\u624b\u5343\u773c\uff0c\u5927\u60b2\u6551\u82e6\u3002\", \"Avalokitesvara\", \"BUDDHIST\"),\n",
    "    (\"\u666e\u5ea6\u773e\u751f\uff0c\u540c\u767b\u5f7c\u5cb8\u3002\", \"Pure Land\", \"BUDDHIST\"),\n",
    "    (\"\u6301\u6212\u6e05\u6de8\uff0c\u4fee\u884c\u7cbe\u9032\u3002\", \"Vinaya\", \"BUDDHIST\"),\n",
    "    (\"\u4fe1\u70ba\u9053\u6e90\u529f\u5fb7\u6bcd\uff0c\u9577\u990a\u4e00\u5207\u8af8\u5584\u6839\u3002\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u5e03\u65bd\u3001\u6301\u6212\u3001\u5fcd\u8fb1\u3001\u7cbe\u9032\u3001\u79aa\u5b9a\u3001\u667a\u6167\uff0c\u662f\u540d\u516d\u5ea6\u3002\", \"Prajnaparamita\", \"BUDDHIST\"),\n",
    "    (\"\u4fee\u798f\u4e0d\u4fee\u6167\uff0c\u8c61\u8eab\u639b\u74d4\u73de\uff1b\u4fee\u6167\u4e0d\u4fee\u798f\uff0c\u7f85\u6f22\u6258\u7a7a\u7f3d\u3002\", \"Folk Buddhist\", \"BUDDHIST\"),\n",
    "    (\"\u6df1\u5165\u7d93\u85cf\uff0c\u667a\u6167\u5982\u6d77\u3002\", \"Buddhist Teaching\", \"BUDDHIST\"),\n",
    "    (\"\u82e6\u6d77\u7121\u908a\uff0c\u56de\u982d\u662f\u5cb8\u3002\", \"Buddhist Teaching\", \"BUDDHIST\"),\n",
    "    (\"\u653e\u4e0b\u5c60\u5200\uff0c\u7acb\u5730\u6210\u4f5b\u3002\", \"Buddhist Teaching\", \"BUDDHIST\"),\n",
    "    (\"\u8272\u5373\u662f\u7a7a\uff0c\u7a7a\u5373\u662f\u8272\u3002\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"\u842c\u6cd5\u7686\u7a7a\uff0c\u56e0\u679c\u4e0d\u7a7a\u3002\", \"Buddhist Teaching\", \"BUDDHIST\"),\n",
    "    (\"\u904e\u53bb\u5df2\u904e\u53bb\uff0c\u672a\u4f86\u5c1a\u672a\u4f86\uff0c\u73fe\u5728\u56e0\u7de3\u751f\u3002\", \"Buddhist Teaching\", \"BUDDHIST\"),\n",
    "]\n",
    "\n",
    "# Legalist Chinese (\u6cd5\u5bb6) - Expanded v10.9\n",
    "LEGALIST_CHINESE = [\n",
    "    # Han Feizi (\u97d3\u975e\u5b50) - Core texts\n",
    "    (\"\u6cd5\u4e0d\u963f\u8cb4\uff0c\u7e69\u4e0d\u6493\u66f2\u3002\", \"Han Feizi 6\", \"LEGALIST\"),\n",
    "    (\"\u5211\u904e\u4e0d\u907f\u5927\u81e3\uff0c\u8cde\u5584\u4e0d\u907a\u5339\u592b\u3002\", \"Han Feizi 50\", \"LEGALIST\"),\n",
    "    (\"\u4ee5\u6cd5\u70ba\u6559\uff0c\u4ee5\u540f\u70ba\u5e2b\u3002\", \"Han Feizi 49\", \"LEGALIST\"),\n",
    "    (\"\u660e\u4e3b\u4e4b\u570b\uff0c\u7121\u66f8\u7c21\u4e4b\u6587\uff0c\u4ee5\u6cd5\u70ba\u6559\u3002\", \"Han Feizi 49\", \"LEGALIST\"),\n",
    "    (\"\u6cd5\u8005\uff0c\u7de8\u8457\u4e4b\u5716\u7c4d\uff0c\u8a2d\u4e4b\u65bc\u5b98\u5e9c\uff0c\u800c\u5e03\u4e4b\u65bc\u767e\u59d3\u8005\u4e5f\u3002\", \"Han Feizi 38\", \"LEGALIST\"),\n",
    "    (\"\u8853\u8005\uff0c\u85cf\u4e4b\u65bc\u80f8\u4e2d\uff0c\u4ee5\u5076\u773e\u7aef\uff0c\u800c\u6f5b\u5fa1\u7fa4\u81e3\u8005\u4e5f\u3002\", \"Han Feizi 38\", \"LEGALIST\"),\n",
    "    (\"\u6cd5\u83ab\u5982\u986f\uff0c\u800c\u8853\u4e0d\u6b32\u898b\u3002\", \"Han Feizi 38\", \"LEGALIST\"),\n",
    "    (\"\u8cde\u7f70\u4e0d\u4fe1\uff0c\u5247\u7981\u4ee4\u4e0d\u884c\u3002\", \"Han Feizi 46\", \"LEGALIST\"),\n",
    "    (\"\u5211\u91cd\u5247\u4e0d\u6562\u4ee5\u60e1\u72af\uff0c\u7f70\u8f15\u5247\u6c11\u4e0d\u754f\u3002\", \"Han Feizi 46\", \"LEGALIST\"),\n",
    "    (\"\u592b\u56b4\u5211\u91cd\u7f70\u8005\uff0c\u6c11\u4e4b\u6240\u60e1\u4e5f\uff1b\u800c\u570b\u4e4b\u6240\u4ee5\u6cbb\u4e5f\u3002\", \"Han Feizi 49\", \"LEGALIST\"),\n",
    "    (\"\u6cd5\u4e4b\u6240\u52a0\uff0c\u667a\u8005\u5f17\u80fd\u8fad\uff0c\u52c7\u8005\u5f17\u6562\u722d\u3002\", \"Han Feizi 6\", \"LEGALIST\"),\n",
    "    (\"\u4e00\u6c11\u4e4b\u8ecc\uff0c\u83ab\u5982\u6cd5\u3002\", \"Han Feizi 6\", \"LEGALIST\"),\n",
    "    (\"\u6545\u660e\u4e3b\u4f7f\u6cd5\u64c7\u4eba\uff0c\u4e0d\u81ea\u8209\u4e5f\u3002\", \"Han Feizi 6\", \"LEGALIST\"),\n",
    "    (\"\u4f7f\u6cd5\u91cf\u529f\uff0c\u4e0d\u81ea\u5ea6\u4e5f\u3002\", \"Han Feizi 6\", \"LEGALIST\"),\n",
    "    (\"\u4eba\u4e3b\u4e4b\u5927\u7269\uff0c\u975e\u6cd5\u5247\u8853\u4e5f\u3002\", \"Han Feizi 43\", \"LEGALIST\"),\n",
    "    (\"\u6cd5\u8005\uff0c\u61b2\u4ee4\u8457\u65bc\u5b98\u5e9c\uff0c\u5211\u7f70\u5fc5\u65bc\u6c11\u5fc3\u3002\", \"Han Feizi 38\", \"LEGALIST\"),\n",
    "    (\"\u8cde\u83ab\u5982\u539a\u800c\u4fe1\uff0c\u4f7f\u6c11\u5229\u4e4b\u3002\", \"Han Feizi 27\", \"LEGALIST\"),\n",
    "    (\"\u7f70\u83ab\u5982\u91cd\u800c\u5fc5\uff0c\u4f7f\u6c11\u754f\u4e4b\u3002\", \"Han Feizi 27\", \"LEGALIST\"),\n",
    "    (\"\u660e\u4e3b\u4e4b\u6240\u5c0e\u5236\u5176\u81e3\u8005\uff0c\u4e8c\u67c4\u800c\u5df2\u77e3\u3002\u4e8c\u67c4\u8005\uff0c\u5211\u5fb7\u4e5f\u3002\", \"Han Feizi 7\", \"LEGALIST\"),\n",
    "    (\"\u4eba\u81e3\u592a\u8cb4\uff0c\u5fc5\u6613\u4e3b\u4f4d\u3002\", \"Han Feizi 8\", \"LEGALIST\"),\n",
    "    (\"\u611b\u81e3\u592a\u89aa\uff0c\u5fc5\u5371\u4e3b\u8eab\u3002\", \"Han Feizi 8\", \"LEGALIST\"),\n",
    "    (\"\u660e\u541b\u7121\u70ba\u65bc\u4e0a\uff0c\u7fa4\u81e3\u7ae6\u61fc\u4e4e\u4e0b\u3002\", \"Han Feizi 5\", \"LEGALIST\"),\n",
    "    (\"\u4e0a\u4e0b\u4e00\u65e5\u767e\u6230\u3002\", \"Han Feizi 8\", \"LEGALIST\"),\n",
    "    (\"\u70ba\u4eba\u81e3\u8005\uff0c\u76e1\u529b\u4ee5\u4e8b\u5176\u541b\uff0c\u800c\u4e0d\u5f97\u64c5\u4f5c\u5a01\u798f\u3002\", \"Han Feizi 49\", \"LEGALIST\"),\n",
    "    (\"\u7fa4\u81e3\u898b\u7d20\uff0c\u5247\u5927\u541b\u4e0d\u853d\u77e3\u3002\", \"Han Feizi 5\", \"LEGALIST\"),\n",
    "    (\"\u4e8b\u5728\u56db\u65b9\uff0c\u8981\u5728\u4e2d\u592e\u3002\u8056\u4eba\u57f7\u8981\uff0c\u56db\u65b9\u4f86\u6548\u3002\", \"Han Feizi 5\", \"LEGALIST\"),\n",
    "    (\"\u865b\u975c\u4ee5\u5f85\uff0c\u4ee4\u540d\u81ea\u547d\u4e5f\uff0c\u4ee4\u4e8b\u81ea\u5b9a\u4e5f\u3002\", \"Han Feizi 5\", \"LEGALIST\"),\n",
    "    # Shang Jun Shu (\u5546\u541b\u66f8) - Book of Lord Shang\n",
    "    (\"\u570b\u4e4b\u6240\u4ee5\u8208\u8005\uff0c\u8fb2\u6230\u4e5f\u3002\", \"Shang Jun Shu 3\", \"LEGALIST\"),\n",
    "    (\"\u6c11\u5f31\u570b\u5f37\uff0c\u6c11\u5f37\u570b\u5f31\u3002\u6545\u6709\u9053\u4e4b\u570b\uff0c\u52d9\u5728\u5f31\u6c11\u3002\", \"Shang Jun Shu 20\", \"LEGALIST\"),\n",
    "    (\"\u8056\u4eba\u4e4b\u70ba\u570b\u4e5f\uff0c\u58f9\u8cde\uff0c\u58f9\u5211\uff0c\u58f9\u6559\u3002\", \"Shang Jun Shu 17\", \"LEGALIST\"),\n",
    "    (\"\u6cbb\u570b\u8005\uff0c\u8cb4\u5206\u660e\u800c\u4e0d\u53ef\u76f8\u8209\u3002\", \"Shang Jun Shu 14\", \"LEGALIST\"),\n",
    "    (\"\u884c\u7f70\u91cd\u5176\u8f15\u8005\uff0c\u8f15\u8005\u4e0d\u81f3\uff0c\u91cd\u8005\u4e0d\u4f86\u3002\", \"Shang Jun Shu 17\", \"LEGALIST\"),\n",
    "    (\"\u570b\u7686\u4ee5\u4e00\u70ba\u52d9\uff0c\u5175\u51fa\u800c\u4e0d\u6230\uff0c\u5247\u570b\u5f37\u3002\", \"Shang Jun Shu 3\", \"LEGALIST\"),\n",
    "    (\"\u6cbb\u570b\u80fd\u6476\u6c11\u529b\u800c\u58f9\u6c11\u52d9\u8005\uff0c\u5f37\u3002\", \"Shang Jun Shu 4\", \"LEGALIST\"),\n",
    "    (\"\u6c11\u4e4b\u65bc\u5229\u4e5f\uff0c\u82e5\u6c34\u4e4b\u65bc\u4e0b\u4e5f\u3002\", \"Shang Jun Shu 5\", \"LEGALIST\"),\n",
    "    (\"\u6c11\u672c\uff0c\u6cd5\u4e5f\u3002\", \"Shang Jun Shu 18\", \"LEGALIST\"),\n",
    "    (\"\u5211\u751f\u529b\uff0c\u529b\u751f\u5f37\uff0c\u5f37\u751f\u5a01\uff0c\u5a01\u751f\u60e0\u3002\", \"Shang Jun Shu 17\", \"LEGALIST\"),\n",
    "    (\"\u5229\u51fa\u4e00\u5b54\u8005\uff0c\u5176\u570b\u7121\u6575\u3002\", \"Shang Jun Shu 5\", \"LEGALIST\"),\n",
    "    (\"\u4ee5\u5211\u53bb\u5211\uff0c\u570b\u6cbb\u3002\u4ee5\u5211\u81f4\u5211\uff0c\u570b\u4e82\u3002\", \"Shang Jun Shu 17\", \"LEGALIST\"),\n",
    "    (\"\u6cbb\u5247\u5211\u91cd\uff0c\u4e82\u5247\u5211\u8f15\u3002\", \"Shang Jun Shu 17\", \"LEGALIST\"),\n",
    "    (\"\u5211\u7528\u65bc\u5c07\u904e\uff0c\u5247\u5927\u90aa\u4e0d\u751f\u3002\", \"Shang Jun Shu 17\", \"LEGALIST\"),\n",
    "    (\"\u6545\u4ee5\u6230\u53bb\u6230\uff0c\u96d6\u6230\u53ef\u4e5f\u3002\u4ee5\u6bba\u53bb\u6bba\uff0c\u96d6\u6bba\u53ef\u4e5f\u3002\", \"Shang Jun Shu 18\", \"LEGALIST\"),\n",
    "    # Guanzi (\u7ba1\u5b50) - Master Guan\n",
    "    (\"\u5009\u5ee9\u5be6\u5247\u77e5\u79ae\u7bc0\uff0c\u8863\u98df\u8db3\u5247\u77e5\u69ae\u8fb1\u3002\", \"Guanzi 1\", \"LEGALIST\"),\n",
    "    (\"\u79ae\u7fa9\u5ec9\u6065\uff0c\u570b\u4e4b\u56db\u7dad\uff1b\u56db\u7dad\u4e0d\u5f35\uff0c\u570b\u4e43\u6ec5\u4ea1\u3002\", \"Guanzi 1\", \"LEGALIST\"),\n",
    "    (\"\u653f\u4e4b\u6240\u8208\uff0c\u5728\u9806\u6c11\u5fc3\uff1b\u653f\u4e4b\u6240\u5ee2\uff0c\u5728\u9006\u6c11\u5fc3\u3002\", \"Guanzi 1\", \"LEGALIST\"),\n",
    "    (\"\u6388\u6709\u5fb7\u5247\u570b\u5b89\uff0c\u6388\u7121\u5fb7\u5247\u570b\u5371\u3002\", \"Guanzi 5\", \"LEGALIST\"),\n",
    "    (\"\u6cd5\u8005\uff0c\u5929\u4e0b\u4e4b\u7a0b\u5f0f\u4e5f\uff0c\u842c\u4e8b\u4e4b\u5100\u8868\u4e5f\u3002\", \"Guanzi 26\", \"LEGALIST\"),\n",
    "    (\"\u6cd5\u8005\u6240\u4ee5\u8208\u529f\u61fc\u66b4\u4e5f\u3002\", \"Guanzi 45\", \"LEGALIST\"),\n",
    "    (\"\u4ee4\u5247\u884c\uff0c\u7981\u5247\u6b62\uff0c\u61b2\u4e4b\u6240\u53ca\uff0c\u4fd7\u4e4b\u6240\u88ab\u3002\", \"Guanzi 3\", \"LEGALIST\"),\n",
    "    (\"\u58eb\u8fb2\u5de5\u5546\uff0c\u56db\u6c11\u8005\uff0c\u570b\u4e4b\u77f3\u6c11\u4e5f\u3002\", \"Guanzi\", \"LEGALIST\"),\n",
    "    (\"\u6c11\u4e0d\u8db3\uff0c\u4ee4\u4e43\u8fb1\uff1b\u6c11\u82e6\u6b86\uff0c\u4ee4\u4e0d\u884c\u3002\", \"Guanzi\", \"LEGALIST\"),\n",
    "    (\"\u8056\u4eba\u4e4b\u6240\u4ee5\u6cbb\u570b\u8005\uff0c\u5148\u5229\u6c11\u5fc3\u3002\", \"Guanzi\", \"LEGALIST\"),\n",
    "    (\"\u5bcc\u570b\u4e4b\u6cd5\uff0c\u4e0a\u56fa\u5176\u672c\uff0c\u4e0b\u4fbf\u5176\u4e8b\u3002\", \"Guanzi\", \"LEGALIST\"),\n",
    "    (\"\u5175\u8005\uff0c\u570b\u4e4b\u5927\u4e8b\u4e5f\uff0c\u6b7b\u751f\u4e4b\u5730\uff0c\u5b58\u4ea1\u4e4b\u9053\uff0c\u4e0d\u53ef\u4e0d\u5bdf\u4e5f\u3002\", \"Sunzi\", \"LEGALIST\"),\n",
    "    (\"\u77e5\u5f7c\u77e5\u5df1\uff0c\u767e\u6230\u4e0d\u6b86\u3002\", \"Sunzi\", \"LEGALIST\"),\n",
    "    (\"\u4e0a\u5175\u4f10\u8b00\uff0c\u5176\u6b21\u4f10\u4ea4\uff0c\u5176\u6b21\u4f10\u5175\uff0c\u5176\u4e0b\u653b\u57ce\u3002\", \"Sunzi\", \"LEGALIST\"),\n",
    "    (\"\u4e0d\u6230\u800c\u5c48\u4eba\u4e4b\u5175\uff0c\u5584\u4e4b\u5584\u8005\u4e5f\u3002\", \"Sunzi\", \"LEGALIST\"),\n",
    "    # Additional Legalist principles\n",
    "    (\"\u6cbb\u570b\u4e4b\u9053\uff0c\u5fc5\u5148\u6b63\u5176\u8eab\u3002\", \"Legalist Principle\", \"LEGALIST\"),\n",
    "    (\"\u660e\u6cd5\u5be9\u4ee4\uff0c\u8cde\u7f70\u5fc5\u4fe1\u3002\", \"Legalist Principle\", \"LEGALIST\"),\n",
    "    (\"\u7121\u529f\u4e0d\u8cde\uff0c\u7121\u7f6a\u4e0d\u7f70\u3002\", \"Legalist Principle\", \"LEGALIST\"),\n",
    "    (\"\u660e\u4e3b\u611b\u5176\u570b\uff0c\u5fe0\u81e3\u611b\u5176\u541b\u3002\", \"Legalist Principle\", \"LEGALIST\"),\n",
    "    (\"\u6cd5\u4ee4\u65e2\u5e03\uff0c\u4e0d\u5f97\u79c1\u8b70\u3002\", \"Legalist Principle\", \"LEGALIST\"),\n",
    "    (\"\u5949\u6cd5\u8005\u5f37\u5247\u570b\u5f37\uff0c\u5949\u6cd5\u8005\u5f31\u5247\u570b\u5f31\u3002\", \"Han Feizi\", \"LEGALIST\"),\n",
    "]\n",
    "\n",
    "# Mohist Chinese (\u58a8\u5bb6) - Expanded v10.9\n",
    "MOHIST_CHINESE = [\n",
    "    # Universal Love (\u517c\u611b)\n",
    "    (\"\u517c\u76f8\u611b\uff0c\u4ea4\u76f8\u5229\u3002\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"\u5929\u4e0b\u4e4b\u4eba\u7686\u76f8\u611b\uff0c\u5f37\u4e0d\u57f7\u5f31\uff0c\u773e\u4e0d\u52ab\u5be1\uff0c\u5bcc\u4e0d\u4fae\u8ca7\uff0c\u8cb4\u4e0d\u50b2\u8ce4\u3002\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"\u82e5\u4f7f\u5929\u4e0b\u517c\u76f8\u611b\uff0c\u611b\u4eba\u82e5\u611b\u5176\u8eab\uff0c\u7336\u6709\u4e0d\u5b5d\u8005\u4e4e\uff1f\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"\u8996\u4eba\u4e4b\u570b\u82e5\u8996\u5176\u570b\uff0c\u8996\u4eba\u4e4b\u5bb6\u82e5\u8996\u5176\u5bb6\uff0c\u8996\u4eba\u4e4b\u8eab\u82e5\u8996\u5176\u8eab\u3002\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"\u662f\u6545\u8af8\u4faf\u76f8\u611b\u5247\u4e0d\u91ce\u6230\uff0c\u5bb6\u4e3b\u76f8\u611b\u5247\u4e0d\u76f8\u7be1\u3002\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"\u4eba\u8207\u4eba\u76f8\u611b\u5247\u4e0d\u76f8\u8cca\u3002\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"\u541b\u81e3\u76f8\u611b\u5247\u60e0\u5fe0\uff0c\u7236\u5b50\u76f8\u611b\u5247\u6148\u5b5d\u3002\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"\u5144\u5f1f\u76f8\u611b\u5247\u548c\u8abf\u3002\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"\u5929\u4e0b\u4e4b\u6240\u4ee5\u4e82\u8005\uff0c\u751f\u65bc\u4e0d\u76f8\u611b\u3002\", \"Mozi 14\", \"MOHIST\"),\n",
    "    (\"\u81e3\u5b50\u4e4b\u4e0d\u5b5d\u541b\u7236\uff0c\u6240\u8b02\u4e82\u4e5f\u3002\", \"Mozi 14\", \"MOHIST\"),\n",
    "    (\"\u5b50\u81ea\u611b\u4e0d\u611b\u7236\uff0c\u6545\u8667\u7236\u800c\u81ea\u5229\u3002\", \"Mozi 14\", \"MOHIST\"),\n",
    "    (\"\u5f1f\u81ea\u611b\u4e0d\u611b\u5144\uff0c\u6545\u8667\u5144\u800c\u81ea\u5229\u3002\", \"Mozi 14\", \"MOHIST\"),\n",
    "    (\"\u592b\u611b\u4eba\u8005\uff0c\u4eba\u5fc5\u5f9e\u800c\u611b\u4e4b\u3002\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"\u5229\u4eba\u8005\uff0c\u4eba\u5fc5\u5f9e\u800c\u5229\u4e4b\u3002\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"\u60e1\u4eba\u8005\uff0c\u4eba\u5fc5\u5f9e\u800c\u60e1\u4e4b\u3002\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"\u5bb3\u4eba\u8005\uff0c\u4eba\u5fc5\u5f9e\u800c\u5bb3\u4e4b\u3002\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"\u517c\u611b\u5929\u4e0b\u4e4b\u4eba\uff0c\u7336\u611b\u5176\u8eab\u4e5f\u3002\", \"Mozi 16\", \"MOHIST\"),\n",
    "    (\"\u6709\u5929\u4e0b\u8005\u611b\u5929\u4e0b\uff0c\u7121\u5929\u4e0b\u8005\u611b\u5176\u570b\u3002\", \"Mozi 15\", \"MOHIST\"),\n",
    "    # Non-aggression (\u975e\u653b)\n",
    "    (\"\u6bba\u4e00\u4eba\u8b02\u4e4b\u4e0d\u7fa9\uff0c\u5fc5\u6709\u4e00\u6b7b\u7f6a\u77e3\u3002\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"\u4eca\u81f3\u5927\u70ba\u653b\u570b\uff0c\u5247\u5f17\u77e5\u975e\uff0c\u5f9e\u800c\u8b7d\u4e4b\uff0c\u8b02\u4e4b\u7fa9\u3002\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"\u975e\u653b\uff0c\u58a8\u5b50\u4e4b\u9053\u4e5f\u3002\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"\u653b\u570b\u8005\uff0c\u975e\u4e5f\uff1b\u6bba\u4eba\u8005\uff0c\u7f6a\u4e5f\u3002\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"\u4eca\u6709\u4eba\u65bc\u6b64\uff0c\u5c11\u898b\u9ed1\u66f0\u9ed1\uff0c\u591a\u898b\u9ed1\u66f0\u767d\uff0c\u5247\u4ee5\u6b64\u4eba\u4e0d\u77e5\u767d\u9ed1\u4e4b\u8faf\u77e3\u3002\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"\u4eca\u5c0f\u70ba\u975e\u5247\u77e5\u800c\u975e\u4e4b\uff0c\u5927\u70ba\u975e\u653b\u570b\u5247\u4e0d\u77e5\u975e\uff0c\u5f9e\u800c\u8b7d\u4e4b\uff0c\u8b02\u4e4b\u7fa9\u3002\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"\u6bba\u4e00\u4eba\uff0c\u8b02\u4e4b\u4e0d\u7fa9\uff1b\u6bba\u5341\u4eba\uff0c\u5341\u91cd\u4e0d\u7fa9\uff1b\u6bba\u767e\u4eba\uff0c\u767e\u91cd\u4e0d\u7fa9\u3002\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"\u4eca\u5c0f\u70ba\u975e\u5247\u77e5\u800c\u975e\u4e4b\uff0c\u5927\u70ba\u653b\u570b\u5247\u4e0d\u77e5\u975e\uff0c\u5f9e\u800c\u8b7d\u4e4b\u3002\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"\u6625\u5247\u5ee2\u6c11\u8015\u7a3c\u6a39\u85dd\uff0c\u79cb\u5247\u5ee2\u6c11\u7a6b\u6582\u3002\", \"Mozi 18\", \"MOHIST\"),\n",
    "    (\"\u653b\u4f10\u4e4b\u5bb3\uff0c\u5167\u4e4b\u5247\u55aa\u6c11\uff0c\u5916\u4e4b\u5247\u55aa\u5175\u3002\", \"Mozi 18\", \"MOHIST\"),\n",
    "    # Utilitarianism & Anti-waste (\u7bc0\u7528)\n",
    "    (\"\u7bc0\u7528\uff0c\u58a8\u5b50\u4e4b\u6559\u4e5f\u3002\", \"Mozi 20\", \"MOHIST\"),\n",
    "    (\"\u5929\u4e0b\u4e4b\u5229\uff0c\u662f\u70ba\u5929\u4e0b\u4e4b\u7fa9\u3002\", \"Mozi 26\", \"MOHIST\"),\n",
    "    (\"\u8056\u4eba\u4ee5\u6cbb\u5929\u4e0b\u70ba\u4e8b\u8005\u4e5f\uff0c\u5fc5\u77e5\u4e82\u4e4b\u6240\u81ea\u8d77\uff0c\u7109\u80fd\u6cbb\u4e4b\u3002\", \"Mozi 14\", \"MOHIST\"),\n",
    "    (\"\u51e1\u8db3\u4ee5\u5949\u7d66\u6c11\u7528\u5247\u6b62\uff0c\u8af8\u52a0\u8cbb\u4e0d\u52a0\u65bc\u6c11\u5229\u8005\uff0c\u8056\u738b\u5f17\u70ba\u3002\", \"Mozi 20\", \"MOHIST\"),\n",
    "    (\"\u5176\u70ba\u8863\u88d8\u4f55\uff1f\u4ee5\u70ba\u51ac\u4ee5\u5709\u5bd2\uff0c\u590f\u4ee5\u5709\u6691\u3002\", \"Mozi 21\", \"MOHIST\"),\n",
    "    (\"\u8056\u4eba\u4f5c\u8aa8\uff0c\u7537\u8015\u7a3c\u6a39\u85dd\uff0c\u4ee5\u70ba\u6c11\u98df\u3002\", \"Mozi 20\", \"MOHIST\"),\n",
    "    (\"\u53e4\u8005\u8056\u738b\uff0c\u5236\u70ba\u7bc0\u7528\u4e4b\u6cd5\u3002\", \"Mozi 20\", \"MOHIST\"),\n",
    "    (\"\u51e1\u5929\u4e0b\u7fa4\u767e\u5de5\uff0c\u8f2a\u8eca\u978d\u76ae\uff0c\u9676\u51b6\u6893\u5320\uff0c\u4f7f\u5404\u5f9e\u4e8b\u5176\u6240\u80fd\u3002\", \"Mozi 20\", \"MOHIST\"),\n",
    "    (\"\u6709\u80fd\u5247\u8209\u4e4b\uff0c\u7121\u80fd\u5247\u4e0b\u4e4b\u3002\", \"Mozi 8\", \"MOHIST\"),\n",
    "    (\"\u5b98\u7121\u5e38\u8cb4\u800c\u6c11\u7121\u7d42\u8ce4\u3002\", \"Mozi 8\", \"MOHIST\"),\n",
    "    # Anti-fatalism (\u975e\u547d)\n",
    "    (\"\u547d\u8005\uff0c\u66b4\u738b\u6240\u4f5c\uff0c\u7aae\u4eba\u6240\u8ff0\u3002\", \"Mozi 35\", \"MOHIST\"),\n",
    "    (\"\u57f7\u6709\u547d\u8005\uff0c\u662f\u8986\u5929\u4e0b\u4e4b\u7fa9\u3002\", \"Mozi 35\", \"MOHIST\"),\n",
    "    (\"\u662f\u6545\u6614\u8005\u79b9\u3001\u6e6f\u3001\u6587\u3001\u6b66\u4e4b\u70ba\u9053\u4e5f\uff0c\u4e0d\u66f0\u547d\u4e4b\u6240\u798f\u4e5f\u3002\", \"Mozi 35\", \"MOHIST\"),\n",
    "    (\"\u57f7\u6709\u547d\u8005\u4e0d\u4ec1\u3002\", \"Mozi 35\", \"MOHIST\"),\n",
    "    (\"\u529b\u8005\u4f55\uff1f\u529b\u76e1\u800c\u529f\u6210\u3002\", \"Mozi 35\", \"MOHIST\"),\n",
    "    # Meritocracy (\u5c1a\u8ce2)\n",
    "    (\"\u5c1a\u8ce2\u8005\uff0c\u653f\u4e4b\u672c\u4e5f\u3002\", \"Mozi 8\", \"MOHIST\"),\n",
    "    (\"\u8ce2\u8005\u8209\u800c\u4e0a\u4e4b\uff0c\u4e0d\u8096\u8005\u6291\u800c\u5ee2\u4e4b\u3002\", \"Mozi 8\", \"MOHIST\"),\n",
    "    (\"\u96d6\u5728\u8fb2\u8207\u5de5\u8086\u4e4b\u4eba\uff0c\u6709\u80fd\u5247\u8209\u4e4b\u3002\", \"Mozi 8\", \"MOHIST\"),\n",
    "    (\"\u9ad8\u4e88\u4e4b\u7235\uff0c\u91cd\u4e88\u4e4b\u797f\uff0c\u4efb\u4e4b\u4ee5\u4e8b\uff0c\u65b7\u4e88\u4e4b\u4ee4\u3002\", \"Mozi 8\", \"MOHIST\"),\n",
    "    (\"\u7235\u4f4d\u4e0d\u9ad8\u5247\u6c11\u5f17\u656c\uff0c\u84c4\u797f\u4e0d\u539a\u5247\u6c11\u4e0d\u4fe1\uff0c\u653f\u4ee4\u4e0d\u65b7\u5247\u6c11\u4e0d\u754f\u3002\", \"Mozi 8\", \"MOHIST\"),\n",
    "    (\"\u53e4\u8005\u8056\u738b\u4e4b\u70ba\u653f\uff0c\u5217\u5fb7\u800c\u5c1a\u8ce2\u3002\", \"Mozi 8\", \"MOHIST\"),\n",
    "    (\"\u96d6\u5728\u8fb2\u8207\u5de5\u8086\u4e4b\u4eba\uff0c\u6709\u80fd\u5247\u8209\u4e4b\u3002\", \"Mozi 9\", \"MOHIST\"),\n",
    "    # Heaven's Will (\u5929\u5fd7)\n",
    "    (\"\u5929\u4e4b\u610f\uff0c\u4e0d\u6b32\u5927\u570b\u4e4b\u653b\u5c0f\u570b\u4e5f\u3002\", \"Mozi 26\", \"MOHIST\"),\n",
    "    (\"\u5929\u4e4b\u610f\uff0c\u4e0d\u6b32\u5f37\u4e4b\u52ab\u5f31\u4e5f\u3002\", \"Mozi 26\", \"MOHIST\"),\n",
    "    (\"\u5929\u4e4b\u610f\uff0c\u4e0d\u6b32\u8a50\u4e4b\u8b00\u611a\u4e5f\u3002\", \"Mozi 26\", \"MOHIST\"),\n",
    "    (\"\u9806\u5929\u610f\u8005\uff0c\u517c\u76f8\u611b\uff0c\u4ea4\u76f8\u5229\uff0c\u5fc5\u5f97\u8cde\u3002\", \"Mozi 27\", \"MOHIST\"),\n",
    "    (\"\u53cd\u5929\u610f\u8005\uff0c\u5225\u76f8\u60e1\uff0c\u4ea4\u76f8\u8cca\uff0c\u5fc5\u5f97\u7f70\u3002\", \"Mozi 27\", \"MOHIST\"),\n",
    "    (\"\u5929\u6b32\u4eba\u76f8\u611b\u76f8\u5229\uff0c\u800c\u4e0d\u6b32\u4eba\u76f8\u60e1\u76f8\u8cca\u3002\", \"Mozi 26\", \"MOHIST\"),\n",
    "    # Additional Mohist principles\n",
    "    (\"\u8a00\u7121\u52d9\u70ba\u591a\uff0c\u800c\u52d9\u70ba\u667a\u3002\", \"Mozi 47\", \"MOHIST\"),\n",
    "    (\"\u884c\u7121\u52d9\u70ba\u83ef\uff0c\u800c\u52d9\u70ba\u5be6\u3002\", \"Mozi 47\", \"MOHIST\"),\n",
    "    (\"\u5fd7\u4e0d\u5f37\u8005\u667a\u4e0d\u9054\uff0c\u8a00\u4e0d\u4fe1\u8005\u884c\u4e0d\u679c\u3002\", \"Mozi\", \"MOHIST\"),\n",
    "    (\"\u7fa9\u8005\uff0c\u5229\u4e5f\u3002\", \"Mozi 40\", \"MOHIST\"),\n",
    "    (\"\u842c\u4e8b\u83ab\u8cb4\u65bc\u7fa9\u3002\", \"Mozi 47\", \"MOHIST\"),\n",
    "    (\"\u5165\u570b\u800c\u4e0d\u5b58\u5176\u58eb\uff0c\u5247\u4ea1\u570b\u77e3\u3002\", \"Mozi\", \"MOHIST\"),\n",
    "    (\"\u67d3\u65bc\u84bc\u5247\u84bc\uff0c\u67d3\u65bc\u9ec3\u5247\u9ec3\u3002\", \"Mozi 3\", \"MOHIST\"),\n",
    "    (\"\u898b\u4fae\u4e0d\u8fb1\uff0c\u898b\u8fb1\u4e0d\u6012\u3002\", \"Mozi\", \"MOHIST\"),\n",
    "]\n",
    "\n",
    "# Neo-Confucian Chinese (\u5b8b\u660e\u7406\u5b78)\n",
    "NEO_CONFUCIAN_CHINESE = [\n",
    "    (\"\u5b58\u5929\u7406\uff0c\u6ec5\u4eba\u6b32\u3002\", \"Zhu Xi - Analects Commentary\", \"NEO_CONFUCIAN\"),\n",
    "    (\"\u683c\u7269\u81f4\u77e5\uff0c\u8aa0\u610f\u6b63\u5fc3\u3002\", \"Zhu Xi - Great Learning Commentary\", \"NEO_CONFUCIAN\"),\n",
    "    (\"\u5929\u7406\u4eba\u6b32\uff0c\u540c\u884c\u7570\u60c5\u3002\", \"Zhu Xi - Classified Conversations\", \"NEO_CONFUCIAN\"),\n",
    "    (\n",
    "        \"\u8056\u4eba\u5343\u8a00\u842c\u8a9e\uff0c\u53ea\u662f\u6559\u4eba\u660e\u5929\u7406\uff0c\u6ec5\u4eba\u6b32\u3002\",\n",
    "        \"Zhu Xi - Classified Conversations\",\n",
    "        \"NEO_CONFUCIAN\",\n",
    "    ),\n",
    "    (\"\u656c\u8005\uff0c\u8056\u5b78\u4e4b\u6240\u4ee5\u6210\u59cb\u800c\u6210\u7d42\u8005\u4e5f\u3002\", \"Zhu Xi - Collected Writings\", \"NEO_CONFUCIAN\"),\n",
    "    (\"\u7aae\u7406\u4ee5\u81f4\u5176\u77e5\uff0c\u53cd\u8eac\u4ee5\u8e10\u5176\u5be6\u3002\", \"Zhu Xi - Collected Writings\", \"NEO_CONFUCIAN\"),\n",
    "    (\"\u6db5\u990a\u9808\u7528\u656c\uff0c\u9032\u5b78\u5247\u5728\u81f4\u77e5\u3002\", \"Zhu Xi - Classified Conversations\", \"NEO_CONFUCIAN\"),\n",
    "    (\"\u77e5\u884c\u5408\u4e00\u3002\", \"Wang Yangming - Instructions for Practical Living\", \"NEO_CONFUCIAN\"),\n",
    "    (\"\u81f4\u826f\u77e5\u3002\", \"Wang Yangming - Instructions for Practical Living\", \"NEO_CONFUCIAN\"),\n",
    "    (\"\u7121\u5584\u7121\u60e1\u5fc3\u4e4b\u9ad4\uff0c\u6709\u5584\u6709\u60e1\u610f\u4e4b\u52d5\u3002\", \"Wang Yangming - Four Maxims\", \"NEO_CONFUCIAN\"),\n",
    "    (\"\u77e5\u5584\u77e5\u60e1\u662f\u826f\u77e5\uff0c\u70ba\u5584\u53bb\u60e1\u662f\u683c\u7269\u3002\", \"Wang Yangming - Four Maxims\", \"NEO_CONFUCIAN\"),\n",
    "    (\"\u5fc3\u5373\u7406\u4e5f\u3002\", \"Wang Yangming - Instructions for Practical Living\", \"NEO_CONFUCIAN\"),\n",
    "    (\n",
    "        \"\u543e\u5fc3\u4e4b\u826f\u77e5\uff0c\u5373\u6240\u8b02\u5929\u7406\u4e5f\u3002\",\n",
    "        \"Wang Yangming - Instructions for Practical Living\",\n",
    "        \"NEO_CONFUCIAN\",\n",
    "    ),\n",
    "    (\n",
    "        \"\u77e5\u662f\u884c\u4e4b\u59cb\uff0c\u884c\u662f\u77e5\u4e4b\u6210\u3002\",\n",
    "        \"Wang Yangming - Instructions for Practical Living\",\n",
    "        \"NEO_CONFUCIAN\",\n",
    "    ),\n",
    "    (\"\u77e5\u800c\u4e0d\u884c\uff0c\u53ea\u662f\u672a\u77e5\u3002\", \"Wang Yangming - Instructions for Practical Living\", \"NEO_CONFUCIAN\"),\n",
    "    (\"\u7834\u5c71\u4e2d\u8cca\u6613\uff0c\u7834\u5fc3\u4e2d\u8cca\u96e3\u3002\", \"Wang Yangming - Letters\", \"NEO_CONFUCIAN\"),\n",
    "    (\"\u8aa0\u8005\uff0c\u8056\u4eba\u4e4b\u672c\u3002\", \"Zhou Dunyi - Tongshu\", \"NEO_CONFUCIAN\"),\n",
    "    (\"\u8aa0\uff0c\u4e94\u5e38\u4e4b\u672c\uff0c\u767e\u884c\u4e4b\u6e90\u4e5f\u3002\", \"Zhou Dunyi - Tongshu\", \"NEO_CONFUCIAN\"),\n",
    "    (\"\u6c11\u543e\u540c\u80de\uff0c\u7269\u543e\u8207\u4e5f\u3002\", \"Zhang Zai - Western Inscription\", \"NEO_CONFUCIAN\"),\n",
    "    (\n",
    "        \"\u70ba\u5929\u5730\u7acb\u5fc3\uff0c\u70ba\u751f\u6c11\u7acb\u547d\uff0c\u70ba\u5f80\u8056\u7e7c\u7d55\u5b78\uff0c\u70ba\u842c\u4e16\u958b\u592a\u5e73\u3002\",\n",
    "        \"Zhang Zai - Attributed\",\n",
    "        \"NEO_CONFUCIAN\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Islamic Legal Maxims (\u0642\u0648\u0627\u0639\u062f \u0641\u0642\u0647\u064a\u0629)\n",
    "ISLAMIC_LEGAL_MAXIMS = [\n",
    "    (\"\u0627\u0644\u0623\u0645\u0648\u0631 \u0628\u0645\u0642\u0627\u0635\u062f\u0647\u0627\", \"Al-Qawa'id - Major 1\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u064a\u0642\u064a\u0646 \u0644\u0627 \u064a\u0632\u0648\u0644 \u0628\u0627\u0644\u0634\u0643\", \"Al-Qawa'id - Major 2\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u0645\u0634\u0642\u0629 \u062a\u062c\u0644\u0628 \u0627\u0644\u062a\u064a\u0633\u064a\u0631\", \"Al-Qawa'id - Major 3\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u0636\u0631\u0631 \u064a\u0632\u0627\u0644\", \"Al-Qawa'id - Major 4\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u0639\u0627\u062f\u0629 \u0645\u062d\u0643\u0645\u0629\", \"Al-Qawa'id - Major 5\", \"FIQH\"),\n",
    "    (\"\u0644\u0627 \u0636\u0631\u0631 \u0648\u0644\u0627 \u0636\u0631\u0627\u0631\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u0636\u0631\u0631 \u0644\u0627 \u064a\u0632\u0627\u0644 \u0628\u0627\u0644\u0636\u0631\u0631\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u0636\u0631\u0631 \u0627\u0644\u0623\u0634\u062f \u064a\u0632\u0627\u0644 \u0628\u0627\u0644\u0636\u0631\u0631 \u0627\u0644\u0623\u062e\u0641\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u062f\u0631\u0621 \u0627\u0644\u0645\u0641\u0627\u0633\u062f \u0623\u0648\u0644\u0649 \u0645\u0646 \u062c\u0644\u0628 \u0627\u0644\u0645\u0635\u0627\u0644\u062d\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u064a\u062a\u062d\u0645\u0644 \u0627\u0644\u0636\u0631\u0631 \u0627\u0644\u062e\u0627\u0635 \u0644\u062f\u0641\u0639 \u0627\u0644\u0636\u0631\u0631 \u0627\u0644\u0639\u0627\u0645\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0625\u0630\u0627 \u062a\u0639\u0627\u0631\u0636\u062a \u0645\u0641\u0633\u062f\u062a\u0627\u0646 \u0631\u0648\u0639\u064a \u0623\u0639\u0638\u0645\u0647\u0645\u0627 \u0636\u0631\u0631\u0627 \u0628\u0627\u0631\u062a\u0643\u0627\u0628 \u0623\u062e\u0641\u0647\u0645\u0627\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u0623\u0635\u0644 \u0641\u064a \u0627\u0644\u0623\u0634\u064a\u0627\u0621 \u0627\u0644\u0625\u0628\u0627\u062d\u0629\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u0623\u0635\u0644 \u0641\u064a \u0627\u0644\u0639\u0642\u0648\u062f \u0627\u0644\u0635\u062d\u0629\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u0623\u0635\u0644 \u0628\u0642\u0627\u0621 \u0645\u0627 \u0643\u0627\u0646 \u0639\u0644\u0649 \u0645\u0627 \u0643\u0627\u0646\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0645\u0627 \u062d\u0631\u0645 \u0623\u062e\u0630\u0647 \u062d\u0631\u0645 \u0625\u0639\u0637\u0627\u0624\u0647\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0645\u0627 \u062d\u0631\u0645 \u0641\u0639\u0644\u0647 \u062d\u0631\u0645 \u0637\u0644\u0628\u0647\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u0636\u0631\u0648\u0631\u0627\u062a \u062a\u0628\u064a\u062d \u0627\u0644\u0645\u062d\u0638\u0648\u0631\u0627\u062a\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u0636\u0631\u0648\u0631\u0629 \u062a\u0642\u062f\u0631 \u0628\u0642\u062f\u0631\u0647\u0627\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0645\u0627 \u0623\u0628\u064a\u062d \u0644\u0644\u0636\u0631\u0648\u0631\u0629 \u064a\u0642\u062f\u0631 \u0628\u0642\u062f\u0631\u0647\u0627\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u062d\u0627\u062c\u0629 \u062a\u0646\u0632\u0644 \u0645\u0646\u0632\u0644\u0629 \u0627\u0644\u0636\u0631\u0648\u0631\u0629 \u0639\u0627\u0645\u0629 \u0643\u0627\u0646\u062a \u0623\u0648 \u062e\u0627\u0635\u0629\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0625\u0630\u0627 \u0636\u0627\u0642 \u0627\u0644\u0623\u0645\u0631 \u0627\u062a\u0633\u0639\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u062c\u0648\u0627\u0632 \u0627\u0644\u0634\u0631\u0639\u064a \u064a\u0646\u0627\u0641\u064a \u0627\u0644\u0636\u0645\u0627\u0646\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u0645\u0628\u0627\u0634\u0631 \u0636\u0627\u0645\u0646 \u0648\u0625\u0646 \u0644\u0645 \u064a\u062a\u0639\u0645\u062f\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u0645\u062a\u0633\u0628\u0628 \u0644\u0627 \u064a\u0636\u0645\u0646 \u0625\u0644\u0627 \u0628\u0627\u0644\u062a\u0639\u0645\u062f\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0625\u0630\u0627 \u0627\u062c\u062a\u0645\u0639 \u0627\u0644\u0645\u0628\u0627\u0634\u0631 \u0648\u0627\u0644\u0645\u062a\u0633\u0628\u0628 \u064a\u0636\u0627\u0641 \u0627\u0644\u062d\u0643\u0645 \u0625\u0644\u0649 \u0627\u0644\u0645\u0628\u0627\u0634\u0631\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u0625\u0630\u0646 \u0627\u0644\u0639\u0627\u0645 \u0643\u0627\u0644\u0625\u0630\u0646 \u0627\u0644\u062e\u0627\u0635\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0644\u0627 \u0639\u0628\u0631\u0629 \u0628\u0627\u0644\u062f\u0644\u0627\u0644\u0629 \u0641\u064a \u0645\u0642\u0627\u0628\u0644\u0629 \u0627\u0644\u062a\u0635\u0631\u064a\u062d\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0625\u0639\u0645\u0627\u0644 \u0627\u0644\u0643\u0644\u0627\u0645 \u0623\u0648\u0644\u0649 \u0645\u0646 \u0625\u0647\u0645\u0627\u0644\u0647\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"\u0627\u0644\u0623\u0635\u0644 \u0641\u064a \u0627\u0644\u0643\u0644\u0627\u0645 \u0627\u0644\u062d\u0642\u064a\u0642\u0629\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "]\n",
    "\n",
    "# Sufi Ethics (\u0627\u0644\u0623\u062e\u0644\u0627\u0642 \u0627\u0644\u0635\u0648\u0641\u064a\u0629)\n",
    "SUFI_ETHICS = [\n",
    "    (\"\u0627\u0644\u062a\u0635\u0648\u0641 \u0643\u0644\u0647 \u0623\u062e\u0644\u0627\u0642\", \"Al-Ghazali - Ihya\", \"SUFI\"),\n",
    "    (\"\u0645\u0646 \u0644\u0645 \u064a\u0624\u062b\u0631 \u0641\u064a\u0647 \u0639\u0644\u0645 \u0623\u062e\u0644\u0627\u0642\u0647 \u0641\u0642\u062f \u063a\u0641\u0644 \u0639\u0646 \u0627\u0644\u0641\u0642\u0647\", \"Al-Ghazali - Ihya\", \"SUFI\"),\n",
    "    (\"\u0627\u0644\u062e\u0644\u0642 \u0627\u0644\u062d\u0633\u0646 \u062c\u0645\u0627\u0639 \u0627\u0644\u062f\u064a\u0646 \u0643\u0644\u0647\", \"Al-Ghazali - Ihya\", \"SUFI\"),\n",
    "    (\"\u0623\u0635\u0644 \u0627\u0644\u0623\u062e\u0644\u0627\u0642 \u0627\u0644\u0645\u062d\u0645\u0648\u062f\u0629 \u0643\u0644\u0647\u0627 \u0623\u0631\u0628\u0639\u0629: \u0627\u0644\u062d\u0643\u0645\u0629 \u0648\u0627\u0644\u0634\u062c\u0627\u0639\u0629 \u0648\u0627\u0644\u0639\u0641\u0629 \u0648\u0627\u0644\u0639\u062f\u0644\", \"Al-Ghazali - Ihya\", \"SUFI\"),\n",
    "    (\"\u0627\u0644\u0639\u0644\u0645 \u0628\u0644\u0627 \u0639\u0645\u0644 \u062c\u0646\u0648\u0646\u060c \u0648\u0627\u0644\u0639\u0645\u0644 \u0628\u063a\u064a\u0631 \u0639\u0644\u0645 \u0644\u0627 \u064a\u0643\u0648\u0646\", \"Al-Ghazali - Ihya\", \"SUFI\"),\n",
    "    (\"\u0645\u0646 \u0639\u0631\u0641 \u0646\u0641\u0633\u0647 \u0639\u0631\u0641 \u0631\u0628\u0647\", \"Al-Ghazali - Attributed\", \"SUFI\"),\n",
    "    (\"\u0642\u0644\u0628 \u0627\u0644\u0645\u0624\u0645\u0646 \u0628\u064a\u0646 \u0623\u0635\u0628\u0639\u064a\u0646 \u0645\u0646 \u0623\u0635\u0627\u0628\u0639 \u0627\u0644\u0631\u062d\u0645\u0646\", \"Al-Ghazali - Ihya\", \"SUFI\"),\n",
    "    (\"\u0627\u0644\u062a\u0635\u0648\u0641 \u0647\u0648 \u0627\u0644\u062e\u0644\u0642\u060c \u0641\u0645\u0646 \u0632\u0627\u062f \u0639\u0644\u064a\u0643 \u0641\u064a \u0627\u0644\u062e\u0644\u0642 \u0632\u0627\u062f \u0639\u0644\u064a\u0643 \u0641\u064a \u0627\u0644\u062a\u0635\u0648\u0641\", \"Al-Junayd\", \"SUFI\"),\n",
    "    (\"\u0627\u0644\u0635\u0648\u0641\u064a \u0645\u0646 \u0635\u0641\u0627 \u0642\u0644\u0628\u0647 \u0644\u0644\u0647\", \"Al-Junayd\", \"SUFI\"),\n",
    "    (\"\u0623\u0641\u0636\u0644 \u0627\u0644\u0623\u0639\u0645\u0627\u0644 \u0645\u062e\u0627\u0644\u0641\u0629 \u0627\u0644\u0646\u0641\u0633 \u0648\u0627\u0644\u0647\u0648\u0649\", \"Al-Junayd\", \"SUFI\"),\n",
    "    (\n",
    "        \"\u0645\u0646 \u0644\u0645 \u064a\u0632\u0646 \u0623\u0641\u0639\u0627\u0644\u0647 \u0648\u0623\u062d\u0648\u0627\u0644\u0647 \u0641\u064a \u0643\u0644 \u0648\u0642\u062a \u0628\u0627\u0644\u0643\u062a\u0627\u0628 \u0648\u0627\u0644\u0633\u0646\u0629 \u0641\u0644\u0627 \u062a\u0639\u062f\u0647 \u0641\u064a \u062f\u064a\u0648\u0627\u0646 \u0627\u0644\u0631\u062c\u0627\u0644\",\n",
    "        \"Al-Qushayri - Risala\",\n",
    "        \"SUFI\",\n",
    "    ),\n",
    "    (\"\u0627\u0644\u0635\u062f\u0642 \u0633\u064a\u0641 \u0627\u0644\u0644\u0647 \u0641\u064a \u0623\u0631\u0636\u0647\u060c \u0645\u0627 \u0648\u0636\u0639 \u0639\u0644\u0649 \u0634\u064a\u0621 \u0625\u0644\u0627 \u0642\u0637\u0639\u0647\", \"Al-Qushayri - Risala\", \"SUFI\"),\n",
    "    (\"\u0645\u0627 \u062e\u0644\u0642\u062a \u0627\u0644\u062e\u0644\u0642 \u0625\u0644\u0627 \u0644\u064a\u0639\u0631\u0641\u0648\u0646\u064a\", \"Rumi - Attributed\", \"SUFI\"),\n",
    "    (\"\u0643\u0646 \u0643\u0627\u0644\u0634\u0645\u0633 \u0644\u0644\u0631\u062d\u0645\u0629 \u0648\u0627\u0644\u0634\u0641\u0642\u0629\u060c \u0648\u0643\u0627\u0644\u0644\u064a\u0644 \u0641\u064a \u0633\u062a\u0631 \u0639\u064a\u0648\u0628 \u0627\u0644\u063a\u064a\u0631\", \"Rumi - Masnavi\", \"SUFI\"),\n",
    "    (\"\u0645\u0646 \u0639\u0631\u0641 \u0646\u0641\u0633\u0647 \u0641\u0642\u062f \u0639\u0631\u0641 \u0631\u0628\u0647\", \"Ibn Arabi - Fusus\", \"SUFI\"),\n",
    "    (\"\u0627\u0644\u0625\u0646\u0633\u0627\u0646 \u0627\u0644\u0643\u0627\u0645\u0644 \u0645\u0631\u0622\u0629 \u0627\u0644\u062d\u0642\", \"Ibn Arabi - Fusus\", \"SUFI\"),\n",
    "]\n",
    "\n",
    "# Arabic Philosophy (\u0627\u0644\u0641\u0644\u0633\u0641\u0629 \u0627\u0644\u0639\u0631\u0628\u064a\u0629)\n",
    "ARABIC_PHILOSOPHY = [\n",
    "    (\"\u0627\u0644\u0625\u0646\u0633\u0627\u0646 \u0645\u062f\u0646\u064a \u0628\u0627\u0644\u0637\u0628\u0639\", \"Al-Farabi - Ara Ahl al-Madina\", \"FALSAFA\"),\n",
    "    (\"\u0627\u0644\u0633\u0639\u0627\u062f\u0629 \u0647\u064a \u0627\u0644\u062e\u064a\u0631 \u0627\u0644\u0645\u0637\u0644\u0648\u0628 \u0644\u0630\u0627\u062a\u0647\", \"Al-Farabi - Tahsil al-Sa'ada\", \"FALSAFA\"),\n",
    "    (\"\u0627\u0644\u0641\u0636\u064a\u0644\u0629 \u0647\u064a \u0627\u0644\u062d\u0627\u0644 \u0627\u0644\u062a\u064a \u0628\u0647\u0627 \u064a\u0641\u0639\u0644 \u0627\u0644\u0625\u0646\u0633\u0627\u0646 \u0627\u0644\u0623\u0641\u0639\u0627\u0644 \u0627\u0644\u062c\u0645\u064a\u0644\u0629\", \"Al-Farabi - Fusul\", \"FALSAFA\"),\n",
    "    (\"\u0627\u0644\u0639\u0642\u0644 \u0627\u0644\u0639\u0645\u0644\u064a \u0647\u0648 \u0627\u0644\u0630\u064a \u064a\u062f\u0628\u0631 \u0627\u0644\u0628\u062f\u0646\", \"Ibn Sina - Shifa\", \"FALSAFA\"),\n",
    "    (\"\u0627\u0644\u0646\u0641\u0633 \u062c\u0648\u0647\u0631 \u0631\u0648\u062d\u0627\u0646\u064a\", \"Ibn Sina - Shifa\", \"FALSAFA\"),\n",
    "    (\"\u0627\u0644\u0639\u062f\u0644 \u0647\u0648 \u0641\u0636\u064a\u0644\u0629 \u0645\u0646 \u0627\u0644\u0641\u0636\u0627\u0626\u0644 \u0627\u0644\u0639\u0627\u0645\u0629\", \"Ibn Rushd - Commentary on Republic\", \"FALSAFA\"),\n",
    "    (\"\u0627\u0644\u062d\u0643\u0645\u0629 \u0648\u0627\u0644\u0634\u0631\u064a\u0639\u0629 \u0623\u062e\u062a\u0627\u0646 \u0631\u0636\u064a\u0639\u062a\u0627\u0646\", \"Ibn Rushd - Fasl al-Maqal\", \"FALSAFA\"),\n",
    "    (\"\u0627\u0644\u062d\u0642 \u0644\u0627 \u064a\u0636\u0627\u062f \u0627\u0644\u062d\u0642 \u0628\u0644 \u064a\u0648\u0627\u0641\u0642\u0647 \u0648\u064a\u0634\u0647\u062f \u0644\u0647\", \"Ibn Rushd - Fasl al-Maqal\", \"FALSAFA\"),\n",
    "    (\"\u0627\u0644\u0625\u0646\u0633\u0627\u0646 \u0645\u062f\u0646\u064a \u0628\u0627\u0644\u0637\u0628\u0639\u060c \u0623\u064a \u0644\u0627 \u0628\u062f \u0644\u0647 \u0645\u0646 \u0627\u0644\u0627\u062c\u062a\u0645\u0627\u0639\", \"Ibn Khaldun - Muqaddima\", \"FALSAFA\"),\n",
    "    (\"\u0627\u0644\u0639\u0635\u0628\u064a\u0629 \u0647\u064a \u0627\u0644\u0631\u0627\u0628\u0637\u0629 \u0627\u0644\u0627\u062c\u062a\u0645\u0627\u0639\u064a\u0629\", \"Ibn Khaldun - Muqaddima\", \"FALSAFA\"),\n",
    "    (\"\u0627\u0644\u0638\u0644\u0645 \u0645\u0624\u0630\u0646 \u0628\u062e\u0631\u0627\u0628 \u0627\u0644\u0639\u0645\u0631\u0627\u0646\", \"Ibn Khaldun - Muqaddima\", \"FALSAFA\"),\n",
    "]\n",
    "\n",
    "# Sanskrit Dharmashastra (\u0927\u0930\u094d\u092e\u0936\u093e\u0938\u094d\u0924\u094d\u0930) - Expanded v10.9\n",
    "SANSKRIT_DHARMA = [\n",
    "    # Mahabharata - Dharma teachings\n",
    "    (\"\u0905\u0939\u093f\u0902\u0938\u093e \u092a\u0930\u092e\u094b \u0927\u0930\u094d\u092e\u0903\", \"Mahabharata 13.117.37\", \"DHARMA\"),\n",
    "    (\"\u0927\u0930\u094d\u092e \u090f\u0935 \u0939\u0924\u094b \u0939\u0928\u094d\u0924\u093f \u0927\u0930\u094d\u092e\u094b \u0930\u0915\u094d\u0937\u0924\u093f \u0930\u0915\u094d\u0937\u093f\u0924\u0903\", \"Mahabharata 8.69.57\", \"DHARMA\"),\n",
    "    (\"\u0928 \u0939\u093f \u092a\u094d\u0930\u093f\u092f\u0902 \u092e\u0947 \u0938\u094d\u092f\u093e\u0924\u094d \u0906\u0924\u094d\u092e\u0928\u0903 \u092a\u094d\u0930\u0924\u093f\u0915\u0942\u0932\u0902 \u092a\u0930\u0947\u0937\u093e\u092e\u094d\", \"Mahabharata 5.15.17\", \"DHARMA\"),\n",
    "    (\"\u0938\u0924\u094d\u092f\u0902 \u092c\u094d\u0930\u0942\u092f\u093e\u0924\u094d \u092a\u094d\u0930\u093f\u092f\u0902 \u092c\u094d\u0930\u0942\u092f\u093e\u0924\u094d\", \"Mahabharata\", \"DHARMA\"),\n",
    "    (\"\u0906\u0924\u094d\u092e\u0928\u0903 \u092a\u094d\u0930\u0924\u093f\u0915\u0942\u0932\u093e\u0928\u093f \u092a\u0930\u0947\u0937\u093e\u0902 \u0928 \u0938\u092e\u093e\u091a\u0930\u0947\u0924\u094d\", \"Mahabharata 5.15.17\", \"DHARMA\"),\n",
    "    (\"\u0927\u0930\u094d\u092e\u0903 \u0938\u0924\u094d\u092f\u0902 \u091a \u0936\u094c\u091a\u0902 \u091a \u0926\u092e\u0903 \u0915\u0930\u0941\u0923\u093e \u090f\u0935 \u091a\", \"Mahabharata 3.313\", \"DHARMA\"),\n",
    "    (\"\u0927\u0943\u0924\u093f\u0903 \u0915\u094d\u0937\u092e\u093e \u0926\u092e\u094b\u093d\u0938\u094d\u0924\u0947\u092f\u0902 \u0936\u094c\u091a\u092e\u093f\u0928\u094d\u0926\u094d\u0930\u093f\u092f\u0928\u093f\u0917\u094d\u0930\u0939\u0903\", \"Mahabharata 1.108\", \"DHARMA\"),\n",
    "    (\"\u0927\u0930\u094d\u092e\u0938\u094d\u092f \u0924\u0924\u094d\u0924\u094d\u0935\u0902 \u0928\u093f\u0939\u093f\u0924\u0902 \u0917\u0941\u0939\u093e\u092f\u093e\u092e\u094d\", \"Mahabharata 3.313\", \"DHARMA\"),\n",
    "    (\"\u0938\u0930\u094d\u0935\u0902 \u092a\u0930\u0935\u0936\u0902 \u0926\u0941\u0903\u0916\u0902 \u0938\u0930\u094d\u0935\u092e\u093e\u0924\u094d\u092e\u0935\u0936\u0902 \u0938\u0941\u0916\u092e\u094d\", \"Mahabharata 12.17\", \"DHARMA\"),\n",
    "    (\"\u0905\u0937\u094d\u091f\u093e\u0926\u0936 \u092a\u0941\u0930\u093e\u0923\u0947\u0937\u0941 \u0935\u094d\u092f\u093e\u0938\u0938\u094d\u092f \u0935\u091a\u0928\u0926\u094d\u0935\u092f\u092e\u094d \u0964 \u092a\u0930\u094b\u092a\u0915\u093e\u0930\u0903 \u092a\u0941\u0923\u094d\u092f\u093e\u092f \u092a\u093e\u092a\u093e\u092f \u092a\u0930\u092a\u0940\u0921\u0928\u092e\u094d\", \"Mahabharata\", \"DHARMA\"),\n",
    "    (\"\u0928 \u091c\u093e\u0924\u0941 \u0915\u093e\u092e\u093e\u0928\u094d\u0928 \u092d\u092f\u093e\u0928\u094d\u0928 \u0932\u094b\u092d\u093e\u0926\u094d \u0927\u0930\u094d\u092e\u0902 \u0924\u094d\u092f\u091c\u0947\u091c\u094d\u091c\u0940\u0935\u093f\u0924\u0938\u094d\u092f\u093e\u092a\u093f \u0939\u0947\u0924\u094b\u0903\", \"Mahabharata 1.1\", \"DHARMA\"),\n",
    "    (\"\u092f\u0926\u094d\u092f\u0926\u093e\u091a\u0930\u0924\u093f \u0936\u094d\u0930\u0947\u0937\u094d\u0920\u0938\u094d\u0924\u0924\u094d\u0924\u0926\u0947\u0935\u0947\u0924\u0930\u094b \u091c\u0928\u0903\", \"Mahabharata\", \"DHARMA\"),\n",
    "    # Manusmriti - Laws of Manu\n",
    "    (\"\u0905\u0939\u093f\u0902\u0938\u093e \u0938\u0924\u094d\u092f\u092e\u0938\u094d\u0924\u0947\u092f\u0902 \u0936\u094c\u091a\u092e\u093f\u0928\u094d\u0926\u094d\u0930\u093f\u092f\u0928\u093f\u0917\u094d\u0930\u0939\u0903\", \"Manusmriti 10.63\", \"DHARMA\"),\n",
    "    (\"\u0938\u0930\u094d\u0935\u092d\u0942\u0924\u0947\u0937\u0941 \u091a\u093e\u0924\u094d\u092e\u093e\u0928\u0902 \u0938\u0930\u094d\u0935\u092d\u0942\u0924\u093e\u0928\u093f \u091a\u093e\u0924\u094d\u092e\u0928\u093f\", \"Manusmriti\", \"DHARMA\"),\n",
    "    (\n",
    "        \"\u0927\u0943\u0924\u093f\u0903 \u0915\u094d\u0937\u092e\u093e \u0926\u092e\u094b\u093d\u0938\u094d\u0924\u0947\u092f\u0902 \u0936\u094c\u091a\u092e\u093f\u0928\u094d\u0926\u094d\u0930\u093f\u092f\u0928\u093f\u0917\u094d\u0930\u0939\u0903 \u0964 \u0927\u0940\u0930\u094d\u0935\u093f\u0926\u094d\u092f\u093e \u0938\u0924\u094d\u092f\u092e\u0915\u094d\u0930\u094b\u0927\u094b \u0926\u0936\u0915\u0902 \u0927\u0930\u094d\u092e\u0932\u0915\u094d\u0937\u0923\u092e\u094d\",\n",
    "        \"Manusmriti 6.92\",\n",
    "        \"DHARMA\",\n",
    "    ),\n",
    "    (\"\u092e\u093e\u0924\u0943\u0935\u0924\u094d\u092a\u0930\u0926\u093e\u0930\u0947\u0937\u0941 \u092a\u0930\u0926\u094d\u0930\u0935\u094d\u092f\u0947\u0937\u0941 \u0932\u094b\u0937\u094d\u091f\u094d\u0930\u0935\u0924\u094d\", \"Manusmriti 4.134\", \"DHARMA\"),\n",
    "    (\"\u0906\u0924\u094d\u092e\u0935\u0924\u094d\u0938\u0930\u094d\u0935\u092d\u0942\u0924\u0947\u0937\u0941 \u092f\u0903 \u092a\u0936\u094d\u092f\u0924\u093f \u0938 \u092a\u0923\u094d\u0921\u093f\u0924\u0903\", \"Manusmriti\", \"DHARMA\"),\n",
    "    (\"\u092a\u093f\u0924\u0943\u0926\u0947\u0935\u093e\u0924\u093f\u0925\u093f\u092a\u0942\u091c\u093e \u0938\u0930\u094d\u0935\u0924\u094d\u0930 \u0938\u0930\u094d\u0935\u0926\u093e \u0938\u092e\u093e\", \"Manusmriti 3.74\", \"DHARMA\"),\n",
    "    (\"\u0938\u0924\u094d\u092f\u0947\u0928 \u092a\u0942\u092f\u0924\u0947 \u0938\u093e\u0915\u094d\u0937\u0940 \u0927\u0930\u094d\u092e\u0947\u0923 \u092a\u0942\u092f\u0924\u0947 \u0926\u094d\u0935\u093f\u091c\u0903\", \"Manusmriti 8.108\", \"DHARMA\"),\n",
    "    (\"\u0935\u093e\u0919\u094d\u092e\u0928\u0903 \u0915\u0930\u094d\u092e\u092d\u093f\u0903 \u0938\u093e\u0927\u094b\u0903 \u0938\u0926\u093e \u092a\u094d\u0930\u0940\u0923\u093e\u0924\u093f \u092f\u094b \u0926\u094d\u0935\u093f\u091c\u093e\u0928\u094d\", \"Manusmriti 2.234\", \"DHARMA\"),\n",
    "    # Upanishads\n",
    "    (\"\u0938\u0924\u094d\u092f\u0902 \u0935\u0926 \u0927\u0930\u094d\u092e\u0902 \u091a\u0930\", \"Taittiriya Upanishad 1.11\", \"UPANISHAD\"),\n",
    "    (\"\u092e\u093e\u0924\u0943\u0926\u0947\u0935\u094b \u092d\u0935\u0964 \u092a\u093f\u0924\u0943\u0926\u0947\u0935\u094b \u092d\u0935\u0964 \u0906\u091a\u093e\u0930\u094d\u092f\u0926\u0947\u0935\u094b \u092d\u0935\u0964 \u0905\u0924\u093f\u0925\u093f\u0926\u0947\u0935\u094b \u092d\u0935\u0964\", \"Taittiriya Upanishad 1.11\", \"UPANISHAD\"),\n",
    "    (\"\u0908\u0936\u093e\u0935\u093e\u0938\u094d\u092f\u092e\u093f\u0926\u0902 \u0938\u0930\u094d\u0935\u0902 \u092f\u0924\u094d\u0915\u093f\u091e\u094d\u091a \u091c\u0917\u0924\u094d\u092f\u093e\u0902 \u091c\u0917\u0924\u094d\", \"Isha Upanishad 1\", \"UPANISHAD\"),\n",
    "    (\"\u0924\u0947\u0928 \u0924\u094d\u092f\u0915\u094d\u0924\u0947\u0928 \u092d\u0941\u091e\u094d\u091c\u0940\u0925\u093e \u092e\u093e \u0917\u0943\u0927\u0903 \u0915\u0938\u094d\u092f\u0938\u094d\u0935\u093f\u0926\u094d\u0927\u0928\u092e\u094d\", \"Isha Upanishad 1\", \"UPANISHAD\"),\n",
    "    (\"\u0905\u0938\u0924\u094b \u092e\u093e \u0938\u0926\u094d\u0917\u092e\u092f\u0964 \u0924\u092e\u0938\u094b \u092e\u093e \u091c\u094d\u092f\u094b\u0924\u093f\u0930\u094d\u0917\u092e\u092f\u0964 \u092e\u0943\u0924\u094d\u092f\u094b\u0930\u094d\u092e\u093e\u092e\u0943\u0924\u0902 \u0917\u092e\u092f\", \"Brihadaranyaka 1.3.28\", \"UPANISHAD\"),\n",
    "    (\"\u0924\u0924\u094d\u0924\u094d\u0935\u092e\u0938\u093f\", \"Chandogya 6.8.7\", \"UPANISHAD\"),\n",
    "    (\"\u0905\u0939\u0902 \u092c\u094d\u0930\u0939\u094d\u092e\u093e\u0938\u094d\u092e\u093f\", \"Brihadaranyaka 1.4.10\", \"UPANISHAD\"),\n",
    "    (\"\u0938\u0930\u094d\u0935\u0902 \u0916\u0932\u094d\u0935\u093f\u0926\u0902 \u092c\u094d\u0930\u0939\u094d\u092e\", \"Chandogya 3.14.1\", \"UPANISHAD\"),\n",
    "    (\"\u0905\u092f\u092e\u093e\u0924\u094d\u092e\u093e \u092c\u094d\u0930\u0939\u094d\u092e\", \"Mandukya 2\", \"UPANISHAD\"),\n",
    "    (\"\u092a\u094d\u0930\u091c\u094d\u091e\u093e\u0928\u0902 \u092c\u094d\u0930\u0939\u094d\u092e\", \"Aitareya 3.3\", \"UPANISHAD\"),\n",
    "    # Bhagavad Gita - Complete chapter 2 and key verses\n",
    "    (\"\u0915\u0930\u094d\u092e\u0923\u094d\u092f\u0947\u0935\u093e\u0927\u093f\u0915\u093e\u0930\u0938\u094d\u0924\u0947 \u092e\u093e \u092b\u0932\u0947\u0937\u0941 \u0915\u0926\u093e\u091a\u0928\", \"Bhagavad Gita 2.47\", \"GITA\"),\n",
    "    (\"\u092f\u094b\u0917\u0903 \u0915\u0930\u094d\u092e\u0938\u0941 \u0915\u094c\u0936\u0932\u092e\u094d\", \"Bhagavad Gita 2.50\", \"GITA\"),\n",
    "    (\"\u0938\u092e\u0924\u094d\u0935\u0902 \u092f\u094b\u0917 \u0909\u091a\u094d\u092f\u0924\u0947\", \"Bhagavad Gita 2.48\", \"GITA\"),\n",
    "    (\"\u0938\u0930\u094d\u0935\u0927\u0930\u094d\u092e\u093e\u0928\u094d\u092a\u0930\u093f\u0924\u094d\u092f\u091c\u094d\u092f \u092e\u093e\u092e\u0947\u0915\u0902 \u0936\u0930\u0923\u0902 \u0935\u094d\u0930\u091c\", \"Bhagavad Gita 18.66\", \"GITA\"),\n",
    "    (\"\u0905\u0926\u094d\u0935\u0947\u0937\u094d\u091f\u093e \u0938\u0930\u094d\u0935\u092d\u0942\u0924\u093e\u0928\u093e\u0902 \u092e\u0948\u0924\u094d\u0930\u0903 \u0915\u0930\u0941\u0923 \u090f\u0935 \u091a\", \"Bhagavad Gita 12.13\", \"GITA\"),\n",
    "    (\"\u0928\u093f\u0930\u094d\u092e\u092e\u094b \u0928\u093f\u0930\u0939\u0902\u0915\u093e\u0930\u0903 \u0938\u092e\u0926\u0941\u0903\u0916\u0938\u0941\u0916\u0903 \u0915\u094d\u0937\u092e\u0940\", \"Bhagavad Gita 12.13\", \"GITA\"),\n",
    "    (\"\u0938\u0930\u094d\u0935\u092d\u0942\u0924\u0939\u093f\u0924\u0947 \u0930\u0924\u093e\u0903\", \"Bhagavad Gita 12.4\", \"GITA\"),\n",
    "    (\"\u0938\u0941\u0916\u0926\u0941\u0903\u0916\u0947 \u0938\u092e\u0947 \u0915\u0943\u0924\u094d\u0935\u093e \u0932\u093e\u092d\u093e\u0932\u093e\u092d\u094c \u091c\u092f\u093e\u091c\u092f\u094c\", \"Bhagavad Gita 2.38\", \"GITA\"),\n",
    "    (\"\u092f\u094b\u0917\u0938\u094d\u0925\u0903 \u0915\u0941\u0930\u0941 \u0915\u0930\u094d\u092e\u093e\u0923\u093f \u0938\u0919\u094d\u0917\u0902 \u0924\u094d\u092f\u0915\u094d\u0924\u094d\u0935\u093e \u0927\u0928\u091e\u094d\u091c\u092f\", \"Bhagavad Gita 2.48\", \"GITA\"),\n",
    "    (\"\u0928\u0948\u0928\u0902 \u091b\u093f\u0928\u094d\u0926\u0928\u094d\u0924\u093f \u0936\u0938\u094d\u0924\u094d\u0930\u093e\u0923\u093f \u0928\u0948\u0928\u0902 \u0926\u0939\u0924\u093f \u092a\u093e\u0935\u0915\u0903\", \"Bhagavad Gita 2.23\", \"GITA\"),\n",
    "    (\"\u0935\u093e\u0938\u093e\u0902\u0938\u093f \u091c\u0940\u0930\u094d\u0923\u093e\u0928\u093f \u092f\u0925\u093e \u0935\u093f\u0939\u093e\u092f \u0928\u0935\u093e\u0928\u093f \u0917\u0943\u0939\u094d\u0923\u093e\u0924\u093f \u0928\u0930\u094b\u093d\u092a\u0930\u093e\u0923\u093f\", \"Bhagavad Gita 2.22\", \"GITA\"),\n",
    "    (\"\u0936\u094d\u0930\u0947\u092f\u093e\u0928\u094d\u0938\u094d\u0935\u0927\u0930\u094d\u092e\u094b \u0935\u093f\u0917\u0941\u0923\u0903 \u092a\u0930\u0927\u0930\u094d\u092e\u093e\u0924\u094d\u0938\u094d\u0935\u0928\u0941\u0937\u094d\u0920\u093f\u0924\u093e\u0924\u094d\", \"Bhagavad Gita 3.35\", \"GITA\"),\n",
    "    (\"\u092a\u0930\u093f\u0924\u094d\u0930\u093e\u0923\u093e\u092f \u0938\u093e\u0927\u0942\u0928\u093e\u0902 \u0935\u093f\u0928\u093e\u0936\u093e\u092f \u091a \u0926\u0941\u0937\u094d\u0915\u0943\u0924\u093e\u092e\u094d\", \"Bhagavad Gita 4.8\", \"GITA\"),\n",
    "    (\"\u092f\u0926\u093e \u092f\u0926\u093e \u0939\u093f \u0927\u0930\u094d\u092e\u0938\u094d\u092f \u0917\u094d\u0932\u093e\u0928\u093f\u0930\u094d\u092d\u0935\u0924\u093f \u092d\u093e\u0930\u0924\", \"Bhagavad Gita 4.7\", \"GITA\"),\n",
    "    (\"\u0924\u094d\u0930\u093f\u0935\u093f\u0927\u0902 \u0928\u0930\u0915\u0938\u094d\u092f\u0947\u0926\u0902 \u0926\u094d\u0935\u093e\u0930\u0902 \u0928\u093e\u0936\u0928\u092e\u093e\u0924\u094d\u092e\u0928\u0903 \u0964 \u0915\u093e\u092e\u0903 \u0915\u094d\u0930\u094b\u0927\u0938\u094d\u0924\u0925\u093e \u0932\u094b\u092d\u0903\", \"Bhagavad Gita 16.21\", \"GITA\"),\n",
    "    (\"\u0926\u0948\u0935\u0940 \u0938\u092e\u094d\u092a\u0926\u094d\u0935\u093f\u092e\u094b\u0915\u094d\u0937\u093e\u092f \u0928\u093f\u092c\u0928\u094d\u0927\u093e\u092f\u093e\u0938\u0941\u0930\u0940 \u092e\u0924\u093e\", \"Bhagavad Gita 16.5\", \"GITA\"),\n",
    "    (\"\u0905\u092d\u092f\u0902 \u0938\u0924\u094d\u0924\u094d\u0935\u0938\u0902\u0936\u0941\u0926\u094d\u0927\u093f\u0930\u094d\u091c\u094d\u091e\u093e\u0928\u092f\u094b\u0917\u0935\u094d\u092f\u0935\u0938\u094d\u0925\u093f\u0924\u093f\u0903\", \"Bhagavad Gita 16.1\", \"GITA\"),\n",
    "    (\"\u0926\u093e\u0928\u0902 \u0926\u092e\u0936\u094d\u091a \u092f\u091c\u094d\u091e\u0936\u094d\u091a \u0938\u094d\u0935\u093e\u0927\u094d\u092f\u093e\u092f\u0938\u094d\u0924\u092a \u0906\u0930\u094d\u091c\u0935\u092e\u094d\", \"Bhagavad Gita 16.1\", \"GITA\"),\n",
    "    (\"\u0905\u0939\u093f\u0902\u0938\u093e \u0938\u0924\u094d\u092f\u092e\u0915\u094d\u0930\u094b\u0927\u0938\u094d\u0924\u094d\u092f\u093e\u0917\u0903 \u0936\u093e\u0928\u094d\u0924\u093f\u0930\u092a\u0948\u0936\u0941\u0928\u092e\u094d\", \"Bhagavad Gita 16.2\", \"GITA\"),\n",
    "    (\"\u0926\u092f\u093e \u092d\u0942\u0924\u0947\u0937\u094d\u0935\u0932\u094b\u0932\u0941\u092a\u094d\u0924\u094d\u0935\u0902 \u092e\u093e\u0930\u094d\u0926\u0935\u0902 \u0939\u094d\u0930\u0940\u0930\u091a\u093e\u092a\u0932\u092e\u094d\", \"Bhagavad Gita 16.2\", \"GITA\"),\n",
    "    # Arthashastra - Political ethics\n",
    "    (\"\u092a\u094d\u0930\u091c\u093e\u0938\u0941\u0916\u0947 \u0938\u0941\u0916\u0902 \u0930\u093e\u091c\u094d\u091e\u0903 \u092a\u094d\u0930\u091c\u093e\u0928\u093e\u0902 \u091a \u0939\u093f\u0924\u0947 \u0939\u093f\u0924\u092e\u094d\", \"Arthashastra 1.19\", \"ARTHA\"),\n",
    "    (\"\u0930\u093e\u091c\u094d\u091e\u094b \u0939\u093f \u0935\u094d\u0930\u0924\u0902 \u0915\u093e\u0930\u094d\u092f\u093e\u0923\u093e\u0902 \u091a\u0947\u0937\u094d\u091f\u093e \u0930\u093e\u0937\u094d\u091f\u094d\u0930\u0938\u0902\u0917\u094d\u0930\u0939\u0903\", \"Arthashastra\", \"ARTHA\"),\n",
    "    (\"\u0928\u093e\u0924\u093f\u0915\u094d\u0930\u093e\u092e\u0947\u0926\u0930\u094d\u0925\u0902 \u092f\u0903 \u0938 \u0930\u093e\u091c\u094d\u091e\u093e\u0902 \u0930\u093e\u091c\u093e \u092d\u0935\u0947\u0924\u094d\", \"Arthashastra 1.15\", \"ARTHA\"),\n",
    "    (\"\u0927\u0930\u094d\u092e\u093e\u0930\u094d\u0925\u094c \u092f\u0924\u094d\u0930 \u0935\u093f\u0930\u0941\u0926\u094d\u0927\u094c \u0924\u0924\u094d\u0930 \u0927\u0930\u094d\u092e\u0903 \u092a\u094d\u0930\u0927\u093e\u0928\u0903\", \"Arthashastra\", \"ARTHA\"),\n",
    "    (\"\u0938\u0941\u0916\u0938\u094d\u092f \u092e\u0942\u0932\u0902 \u0927\u0930\u094d\u092e\u0903 \u0927\u0930\u094d\u092e\u0938\u094d\u092f \u092e\u0942\u0932\u092e\u0930\u094d\u0925\u0903\", \"Arthashastra 1.7\", \"ARTHA\"),\n",
    "    # Dharmasutras\n",
    "    (\"\u0927\u0930\u094d\u092e\u094b \u0930\u0915\u094d\u0937\u0924\u093f \u0930\u0915\u094d\u0937\u093f\u0924\u0903\", \"Dharmasutra\", \"DHARMA\"),\n",
    "    (\"\u0906\u091a\u093e\u0930\u093e\u0932\u094d\u0932\u092d\u0924\u0947 \u0939\u094d\u092f\u093e\u092f\u0941\u0903\", \"Gautama Dharmasutra\", \"DHARMA\"),\n",
    "    (\"\u0938\u0924\u094d\u092f\u092e\u0947\u0935 \u091c\u092f\u0924\u0947 \u0928\u093e\u0928\u0943\u0924\u092e\u094d\", \"Mundaka Upanishad 3.1.6\", \"UPANISHAD\"),\n",
    "    (\"\u0935\u0938\u0941\u0927\u0948\u0935 \u0915\u0941\u091f\u0941\u092e\u094d\u092c\u0915\u092e\u094d\", \"Hitopadesha 1.3.71\", \"DHARMA\"),\n",
    "    (\"\u092a\u0930\u094b\u092a\u0915\u093e\u0930\u093e\u092f \u0938\u0924\u093e\u0902 \u0935\u093f\u092d\u0942\u0924\u092f\u0903\", \"Hitopadesha\", \"DHARMA\"),\n",
    "    # Yoga Sutras - Ethical foundation\n",
    "    (\"\u0905\u0939\u093f\u0902\u0938\u093e\u0938\u0924\u094d\u092f\u093e\u0938\u094d\u0924\u0947\u092f\u092c\u094d\u0930\u0939\u094d\u092e\u091a\u0930\u094d\u092f\u093e\u092a\u0930\u093f\u0917\u094d\u0930\u0939\u093e \u092f\u092e\u093e\u0903\", \"Yoga Sutras 2.30\", \"DHARMA\"),\n",
    "    (\"\u0936\u094c\u091a\u0938\u0902\u0924\u094b\u0937\u0924\u092a\u0903\u0938\u094d\u0935\u093e\u0927\u094d\u092f\u093e\u092f\u0947\u0936\u094d\u0935\u0930\u092a\u094d\u0930\u0923\u093f\u0927\u093e\u0928\u093e\u0928\u093f \u0928\u093f\u092f\u092e\u093e\u0903\", \"Yoga Sutras 2.32\", \"DHARMA\"),\n",
    "    (\"\u092e\u0948\u0924\u094d\u0930\u0940\u0915\u0930\u0941\u0923\u093e\u092e\u0941\u0926\u093f\u0924\u094b\u092a\u0947\u0915\u094d\u0937\u0923\u093e\u0902 \u0938\u0941\u0916\u0926\u0941\u0903\u0916\u092a\u0941\u0923\u094d\u092f\u093e\u092a\u0941\u0923\u094d\u092f\u0935\u093f\u0937\u092f\u093e\u0923\u093e\u0902 \u092d\u093e\u0935\u0928\u093e\u0924\u0936\u094d\u091a\u093f\u0924\u094d\u0924\u092a\u094d\u0930\u0938\u093e\u0926\u0928\u092e\u094d\", \"Yoga Sutras 1.33\", \"DHARMA\"),\n",
    "    # Panchatantra - Practical wisdom\n",
    "    (\"\u092e\u093f\u0924\u094d\u0930\u0902 \u092a\u094d\u0930\u093e\u092a\u094d\u0924\u0902 \u092f\u0924\u093f\u0924\u0935\u094d\u092f\u0902 \u092d\u0935\u0924\u093e \u0938\u0930\u094d\u0935\u092f\u0924\u094d\u0928\u0924\u0903\", \"Panchatantra\", \"DHARMA\"),\n",
    "    (\"\u0905\u0930\u094d\u0925\u093e\u0917\u092e\u094b \u0928\u093f\u0924\u094d\u092f\u092e\u0930\u094b\u0917\u093f\u0924\u093e \u091a \u092a\u094d\u0930\u093f\u092f\u093e \u091a \u092d\u093e\u0930\u094d\u092f\u093e \u092a\u094d\u0930\u093f\u092f\u0935\u093e\u0926\u093f\u0928\u0940 \u091a\", \"Chanakya\", \"DHARMA\"),\n",
    "    # Ramayana moral teachings\n",
    "    (\"\u0930\u093e\u092e\u094b \u0935\u093f\u0917\u094d\u0930\u0939\u0935\u093e\u0928\u094d \u0927\u0930\u094d\u092e\u0903\", \"Ramayana 2.109\", \"DHARMA\"),\n",
    "    (\"\u0938\u0924\u094d\u092f\u0902 \u092c\u094d\u0930\u0942\u0939\u093f \u092a\u094d\u0930\u093f\u092f\u0902 \u092c\u094d\u0930\u0942\u0939\u093f \u0928 \u092c\u094d\u0930\u0942\u0939\u093f \u0938\u0924\u094d\u092f\u092e\u092a\u094d\u0930\u093f\u092f\u092e\u094d\", \"Ramayana\", \"DHARMA\"),\n",
    "    (\"\u091c\u0928\u0928\u0940 \u091c\u0928\u094d\u092e\u092d\u0942\u092e\u093f\u0936\u094d\u091a \u0938\u094d\u0935\u0930\u094d\u0917\u093e\u0926\u092a\u093f \u0917\u0930\u0940\u092f\u0938\u0940\", \"Ramayana\", \"DHARMA\"),\n",
    "]\n",
    "\n",
    "# Pali Canon Ethics - Expanded v10.9\n",
    "PALI_ETHICS = [\n",
    "    # Metta Sutta - Loving-kindness\n",
    "    (\"Sabbe satt\u0101 bhavantu sukhitatt\u0101\", \"Metta Sutta\", \"PALI\"),\n",
    "    (\"Metta\u00f1ca sabbalokasmi\u1e43 m\u0101nasa\u1e43 bh\u0101vaye aparim\u0101\u1e47a\u1e43\", \"Metta Sutta\", \"PALI\"),\n",
    "    (\"Uddha\u1e43 adho ca tiriya\u00f1ca asamb\u0101dha\u1e43 avera\u1e43 asapatta\u1e43\", \"Metta Sutta\", \"PALI\"),\n",
    "    (\"Sukhino v\u0101 khemino hontu sabbe satt\u0101 bhavantu sukhitatt\u0101\", \"Metta Sutta\", \"PALI\"),\n",
    "    (\"Na paro para\u1e43 nikubbetha n\u0101tima\u00f1\u00f1etha katthaci na\u1e43 ka\u00f1ci\", \"Metta Sutta\", \"PALI\"),\n",
    "    # Dhammapada - Complete selection\n",
    "    (\"Dhammo have rakkhati dhammac\u0101ri\u1e43\", \"Theragatha 303\", \"PALI\"),\n",
    "    (\"Sabba p\u0101passa akara\u1e47a\u1e43, kusalassa upasampad\u0101\", \"Dhammapada 183\", \"PALI\"),\n",
    "    (\"Manopubba\u1e45gam\u0101 dhamm\u0101 manose\u1e6d\u1e6dh\u0101 manomay\u0101\", \"Dhammapada 1\", \"PALI\"),\n",
    "    (\"Na hi verena ver\u0101ni sammant\u012bdha kud\u0101cana\u1e43\", \"Dhammapada 5\", \"PALI\"),\n",
    "    (\"Averena ca sammanti esa dhammo sanantano\", \"Dhammapada 5\", \"PALI\"),\n",
    "    (\"Att\u0101 hi attano n\u0101tho ko hi n\u0101tho paro siy\u0101\", \"Dhammapada 160\", \"PALI\"),\n",
    "    (\"Sacittapariyodapana\u1e43 eta\u1e43 buddh\u0101nas\u0101sana\u1e43\", \"Dhammapada 183\", \"PALI\"),\n",
    "    (\"Yo ca vassasata\u1e43 j\u012bve duss\u012blo asam\u0101hito\", \"Dhammapada 110\", \"PALI\"),\n",
    "    (\"Ek\u0101ha\u1e43 j\u012bvita\u1e43 seyyo s\u012blavantassa jh\u0101yino\", \"Dhammapada 110\", \"PALI\"),\n",
    "    (\"Attadattha\u1e43 paratthena bahun\u0101pi na h\u0101paye\", \"Dhammapada 166\", \"PALI\"),\n",
    "    (\"D\u012bgh\u0101 j\u0101garato ratti d\u012bgha\u1e43 santassa yojana\u1e43\", \"Dhammapada 60\", \"PALI\"),\n",
    "    (\"Appam\u0101do amatapada\u1e43 pam\u0101do maccuno pada\u1e43\", \"Dhammapada 21\", \"PALI\"),\n",
    "    (\"Appamatt\u0101 na m\u012byanti ye pamatt\u0101 yath\u0101 mat\u0101\", \"Dhammapada 21\", \"PALI\"),\n",
    "    (\"Caratha bhikkhave c\u0101rika\u1e43 bahujanahit\u0101ya bahujanasukh\u0101ya\", \"Vinaya Mahavagga\", \"PALI\"),\n",
    "    (\"K\u0101yena sa\u1e43varo s\u0101dhu s\u0101dhu v\u0101c\u0101ya sa\u1e43varo\", \"Dhammapada 361\", \"PALI\"),\n",
    "    (\"Manas\u0101 sa\u1e43varo s\u0101dhu s\u0101dhu sabbattha sa\u1e43varo\", \"Dhammapada 361\", \"PALI\"),\n",
    "    (\"Sabbattha sa\u1e43vuto bhikkhu sabbadukkh\u0101 pamuccati\", \"Dhammapada 361\", \"PALI\"),\n",
    "    (\"Yo ca metta\u1e43 bh\u0101vayati appam\u0101\u1e47a\u1e43 sat\u012bm\u0101\", \"Itivuttaka 27\", \"PALI\"),\n",
    "    (\"Sukhak\u0101m\u0101ni bh\u016bt\u0101ni yo da\u1e47\u1e0dena na hi\u1e43sati\", \"Dhammapada 131\", \"PALI\"),\n",
    "    (\"Attano sukhames\u0101no pecca so labhate sukha\u1e43\", \"Dhammapada 131\", \"PALI\"),\n",
    "    (\"Na paresa\u1e43 vilom\u0101ni na paresa\u1e43 kat\u0101kata\u1e43\", \"Dhammapada 50\", \"PALI\"),\n",
    "    (\"Attano va avekkheyya kat\u0101ni akat\u0101ni ca\", \"Dhammapada 50\", \"PALI\"),\n",
    "    (\"Kodhassa na kuto m\u016bla\u1e43 kalahassa aya\u1e43 bhave\", \"Sutta Nipata\", \"PALI\"),\n",
    "    (\"P\u016bja\u1e43 pa\u1e6dhabhi\u1e43 p\u016bjitv\u0101 te sameti sukh\u0101vaho\", \"Sigalovada Sutta\", \"PALI\"),\n",
    "    # Vinaya - Monastic precepts\n",
    "    (\"P\u0101\u1e47\u0101tip\u0101t\u0101 verama\u1e47\u012b sikkh\u0101pada\u1e43 sam\u0101diy\u0101mi\", \"Vinaya\", \"PALI\"),\n",
    "    (\"Adinn\u0101d\u0101n\u0101 verama\u1e47\u012b sikkh\u0101pada\u1e43 sam\u0101diy\u0101mi\", \"Vinaya\", \"PALI\"),\n",
    "    (\"K\u0101mesumicch\u0101c\u0101r\u0101 verama\u1e47\u012b sikkh\u0101pada\u1e43 sam\u0101diy\u0101mi\", \"Vinaya\", \"PALI\"),\n",
    "    (\"Mus\u0101v\u0101d\u0101 verama\u1e47\u012b sikkh\u0101pada\u1e43 sam\u0101diy\u0101mi\", \"Vinaya\", \"PALI\"),\n",
    "    (\"Sur\u0101merayamajjapam\u0101da\u1e6d\u1e6dh\u0101n\u0101 verama\u1e47\u012b sikkh\u0101pada\u1e43 sam\u0101diy\u0101mi\", \"Vinaya\", \"PALI\"),\n",
    "    (\"Vik\u0101labhojan\u0101 verama\u1e47\u012b sikkh\u0101pada\u1e43 sam\u0101diy\u0101mi\", \"Vinaya\", \"PALI\"),\n",
    "    (\"J\u0101tar\u016bparajatapa\u1e6diggaha\u1e47\u0101 verama\u1e47\u012b sikkh\u0101pada\u1e43 sam\u0101diy\u0101mi\", \"Vinaya\", \"PALI\"),\n",
    "    # Sutta Nipata - Discourse verses\n",
    "    (\"Akkodhassa kuto kodho dantassa samaj\u012bvino\", \"Sutta Nipata 623\", \"PALI\"),\n",
    "    (\"Samm\u0101vimutta\u1e43 na vimuttasaddha\u1e43\", \"Sutta Nipata\", \"PALI\"),\n",
    "    (\"Yassa sabba\u1e43 ahoratta\u1e43 ahi\u1e43s\u0101ya rato mano\", \"Sutta Nipata\", \"PALI\"),\n",
    "    (\"Metta\u00f1ca sabbalokasmi\u1e43 m\u0101nasa\u1e43 bh\u0101vaye aparim\u0101\u1e47a\u1e43\", \"Sutta Nipata\", \"PALI\"),\n",
    "    # Sigalovada Sutta - Lay ethics\n",
    "    (\"Chahi dis\u0101hi namasseyya\", \"Sigalovada Sutta\", \"PALI\"),\n",
    "    (\"M\u0101t\u0101pitaro p\u0101c\u012bn\u0101 dis\u0101\", \"Sigalovada Sutta\", \"PALI\"),\n",
    "    (\"\u0100cariy\u0101 dakkhi\u1e47\u0101 dis\u0101\", \"Sigalovada Sutta\", \"PALI\"),\n",
    "    (\"Mitt\u0101macc\u0101 uttar\u0101 dis\u0101\", \"Sigalovada Sutta\", \"PALI\"),\n",
    "    (\"Sama\u1e47abr\u0101hma\u1e47\u0101 uparim\u0101 dis\u0101\", \"Sigalovada Sutta\", \"PALI\"),\n",
    "    # Mangala Sutta - Blessings\n",
    "    (\"M\u0101t\u0101pitu upa\u1e6d\u1e6dh\u0101na\u1e43 puttad\u0101rassa sa\u1e45gaho\", \"Mangala Sutta\", \"PALI\"),\n",
    "    (\"D\u0101na\u00f1ca dhammacariy\u0101 ca \u00f1\u0101tak\u0101na\u00f1ca sa\u1e45gaho\", \"Mangala Sutta\", \"PALI\"),\n",
    "    (\"Anavajj\u0101ni kamm\u0101ni eta\u1e43 ma\u1e45galamuttama\u1e43\", \"Mangala Sutta\", \"PALI\"),\n",
    "    (\"\u0100rat\u012b virat\u012b p\u0101p\u0101 majjap\u0101n\u0101 ca sa\u1e43yamo\", \"Mangala Sutta\", \"PALI\"),\n",
    "    (\"Appam\u0101do ca dhammesu eta\u1e43 ma\u1e45galamuttama\u1e43\", \"Mangala Sutta\", \"PALI\"),\n",
    "    (\"G\u0101ravo ca niv\u0101to ca santu\u1e6d\u1e6dh\u012b ca kata\u00f1\u00f1ut\u0101\", \"Mangala Sutta\", \"PALI\"),\n",
    "    # Karaniya Metta Sutta - Practice of loving-kindness\n",
    "    (\"Kara\u1e47\u012byam\u0101tthakusalena ya\u1e43 ta\u1e43 santa\u1e43 pada\u1e43 abhisamecca\", \"Karaniya Metta Sutta\", \"PALI\"),\n",
    "    (\"Sakko uj\u016b ca suhuj\u016b ca suvaco cassa mudu anatim\u0101n\u012b\", \"Karaniya Metta Sutta\", \"PALI\"),\n",
    "    (\"Santussako ca subharo ca appakicco ca sallahukavutti\", \"Karaniya Metta Sutta\", \"PALI\"),\n",
    "    (\"Santindriyo ca nipako ca appagabbho kulesu ananugiddho\", \"Karaniya Metta Sutta\", \"PALI\"),\n",
    "    # Jataka moral lessons\n",
    "    (\"Aha\u1e43 kh\u012b\u1e47\u0101savo bhikkhu satim\u0101 sampaj\u0101no\", \"Jataka\", \"PALI\"),\n",
    "    (\"Sabbad\u0101na\u1e43 dhammad\u0101na\u1e43 jin\u0101ti\", \"Jataka\", \"PALI\"),\n",
    "    (\"Na ta\u1e43 kamma\u1e43 kata\u1e43 s\u0101dhu ya\u1e43 katv\u0101 anutappati\", \"Dhammapada 67\", \"PALI\"),\n",
    "    (\"Ta\u1e43 ca kamma\u1e43 kata\u1e43 s\u0101dhu ya\u1e43 katv\u0101 n\u0101nutappati\", \"Dhammapada 68\", \"PALI\"),\n",
    "    (\"Attan\u0101 hi kata\u1e43 p\u0101pa\u1e43 attan\u0101 sa\u1e43kilissati\", \"Dhammapada 165\", \"PALI\"),\n",
    "    (\"Attan\u0101 akata\u1e43 p\u0101pa\u1e43 attan\u0101 va visujjhati\", \"Dhammapada 165\", \"PALI\"),\n",
    "    (\"Suddhi asuddhi paccatta\u1e43 n\u0101\u00f1\u00f1o a\u00f1\u00f1a\u1e43 visodhaye\", \"Dhammapada 165\", \"PALI\"),\n",
    "    # Anguttara Nikaya - Gradual teachings\n",
    "    (\"D\u0101nena piyav\u0101c\u0101ya atthac\u0101rena yamhi\", \"Anguttara Nikaya\", \"PALI\"),\n",
    "    (\"Sabbe satt\u0101 \u0101h\u0101ra\u1e6d\u1e6dhitik\u0101\", \"Anguttara Nikaya\", \"PALI\"),\n",
    "    (\"Catt\u0101rim\u0101ni bhikkhave brahmavih\u0101r\u0101ni\", \"Anguttara Nikaya\", \"PALI\"),\n",
    "]\n",
    "\n",
    "if SKIP_PROCESSING:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"USING CACHED DATA - Run with REFRESH_DATA_FROM_SOURCE=True to use v10.4 loaders\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Count passages by language\n",
    "    by_lang = defaultdict(int)\n",
    "    with open(\"data/processed/passages.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            p = json.loads(line)\n",
    "            by_lang[p[\"language\"]] += 1\n",
    "\n",
    "    print(\"\\nPassages by language:\")\n",
    "    for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {lang}: {cnt:,}\")\n",
    "\n",
    "    n_passages = sum(by_lang.values())\n",
    "    print(f\"\\nTotal: {n_passages:,} passages\")\n",
    "\n",
    "    # ===== CHECK FOR v10.9 CORPORA =====\n",
    "    # If cached data is missing v10.9 hardcoded corpora, add them\n",
    "    # Check for sufficient v10.9 data (not just presence, but expected counts)\n",
    "    sanskrit_count = by_lang.get(\"sanskrit\", 0)\n",
    "    pali_count = by_lang.get(\"pali\", 0)\n",
    "\n",
    "    # Also check for v10.9-specific periods by scanning bonds\n",
    "    has_v109_periods = False\n",
    "    try:\n",
    "        with open(\"data/processed/bonds.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                b = json.loads(line)\n",
    "                period = b.get(\"time_period\", \"\")\n",
    "                if period in [\"BUDDHIST\", \"LEGALIST\", \"MOHIST\", \"FIQH\", \"SUFI\", \"FALSAFA\"]:\n",
    "                    has_v109_periods = True\n",
    "                    break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # v10.9 requires: Sanskrit >= 70, Pali >= 70, and v10.9 periods present\n",
    "    has_full_v109 = sanskrit_count >= 70 and pali_count >= 70 and has_v109_periods\n",
    "\n",
    "    print(f\"\\nv10.9 corpus check:\")\n",
    "    print(f\"  Sanskrit: {sanskrit_count} (need >= 70)\")\n",
    "    print(f\"  Pali: {pali_count} (need >= 70)\")\n",
    "    print(f\"  v10.9 periods: {'present' if has_v109_periods else 'missing'}\")\n",
    "    print(f\"  Full v10.9: {'YES' if has_full_v109 else 'NO - will add corpora'}\")\n",
    "\n",
    "    if not has_full_v109:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ADDING v10.9 CORPORA TO CACHED DATA\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"(Sanskrit, Pali, Buddhist Chinese, Legalist, Fiqh, Sufi, etc.)\")\n",
    "\n",
    "        # Load existing passages\n",
    "        all_passages = []\n",
    "        with open(\"data/processed/passages.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                all_passages.append(json.loads(line))\n",
    "        print(f\"Loaded {len(all_passages):,} existing passages\")\n",
    "\n",
    "        # Load existing bonds\n",
    "        existing_bonds = []\n",
    "        with open(\"data/processed/bonds.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                existing_bonds.append(json.loads(line))\n",
    "        print(f\"Loaded {len(existing_bonds):,} existing bonds\")\n",
    "\n",
    "        v109_start = len(all_passages)\n",
    "\n",
    "        # Add Chinese philosophical traditions\n",
    "        for corpus, period, label in [\n",
    "            (BUDDHIST_CHINESE, \"BUDDHIST\", \"Buddhist Chinese\"),\n",
    "            (LEGALIST_CHINESE, \"LEGALIST\", \"Legalist Chinese\"),\n",
    "            (MOHIST_CHINESE, \"MOHIST\", \"Mohist Chinese\"),\n",
    "            (NEO_CONFUCIAN_CHINESE, \"NEO_CONFUCIAN\", \"Neo-Confucian\"),\n",
    "        ]:\n",
    "            for text_content, source_ref, _ in corpus:\n",
    "                all_passages.append(\n",
    "                    {\n",
    "                        \"id\": f\"v109_{label.lower().replace(' ', '_')}_{len(all_passages)}\",\n",
    "                        \"text\": text_content,\n",
    "                        \"language\": \"classical_chinese\",\n",
    "                        \"source\": source_ref,\n",
    "                        \"time_period\": period,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Add Arabic/Islamic traditions\n",
    "        for corpus, period, label in [\n",
    "            (ISLAMIC_LEGAL_MAXIMS, \"FIQH\", \"Islamic Legal Maxims\"),\n",
    "            (SUFI_ETHICS, \"SUFI\", \"Sufi Ethics\"),\n",
    "            (ARABIC_PHILOSOPHY, \"FALSAFA\", \"Arabic Philosophy\"),\n",
    "        ]:\n",
    "            for text_content, source_ref, _ in corpus:\n",
    "                all_passages.append(\n",
    "                    {\n",
    "                        \"id\": f\"v109_{label.lower().replace(' ', '_')}_{len(all_passages)}\",\n",
    "                        \"text\": text_content,\n",
    "                        \"language\": \"arabic\",\n",
    "                        \"source\": source_ref,\n",
    "                        \"time_period\": period,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Add Sanskrit\n",
    "        for text_content, source_ref, period_tag in SANSKRIT_DHARMA:\n",
    "            all_passages.append(\n",
    "                {\n",
    "                    \"id\": f\"v109_sanskrit_{len(all_passages)}\",\n",
    "                    \"text\": text_content,\n",
    "                    \"language\": \"sanskrit\",\n",
    "                    \"source\": source_ref,\n",
    "                    \"time_period\": period_tag,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Add Pali\n",
    "        for text_content, source_ref, period_tag in PALI_ETHICS:\n",
    "            all_passages.append(\n",
    "                {\n",
    "                    \"id\": f\"v109_pali_{len(all_passages)}\",\n",
    "                    \"text\": text_content,\n",
    "                    \"language\": \"pali\",\n",
    "                    \"source\": source_ref,\n",
    "                    \"time_period\": period_tag,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        v109_count = len(all_passages) - v109_start\n",
    "        print(f\"Added {v109_count} v10.9 passages\")\n",
    "\n",
    "        # Extract bonds for new passages\n",
    "        print(\"Extracting bonds for v10.9 passages...\")\n",
    "        new_bonds = []\n",
    "        for p in all_passages[v109_start:]:\n",
    "            # Simple bond extraction for hardcoded corpora (all are prescriptive)\n",
    "            new_bonds.append(\n",
    "                {\n",
    "                    \"passage_id\": p[\"id\"],\n",
    "                    \"bond_type\": \"AUTHORITY\",  # Default, will be refined by patterns\n",
    "                    \"language\": p[\"language\"],\n",
    "                    \"time_period\": p[\"time_period\"],\n",
    "                    \"source\": p[\"source\"],\n",
    "                    \"text\": p[\"text\"][:500],\n",
    "                    \"context\": \"prescriptive\",\n",
    "                    \"confidence\": \"high\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        all_bonds = existing_bonds + new_bonds\n",
    "        print(f\"Total bonds: {len(all_bonds):,}\")\n",
    "\n",
    "        # Save updated passages\n",
    "        with open(\"data/processed/passages.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for p in all_passages:\n",
    "                f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Save updated bonds\n",
    "        with open(\"data/processed/bonds.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for b in all_bonds:\n",
    "                f.write(json.dumps(b, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Update Drive cache\n",
    "        if USE_DRIVE_DATA:\n",
    "            try:\n",
    "                shutil.copy(\"data/processed/passages.jsonl\", f\"{SAVE_DIR}/passages.jsonl\")\n",
    "                shutil.copy(\"data/processed/bonds.jsonl\", f\"{SAVE_DIR}/bonds.jsonl\")\n",
    "                print(f\"Updated Drive cache with v10.9 corpora\")\n",
    "            except Exception as e:\n",
    "                print(f\"Drive update failed: {e}\")\n",
    "\n",
    "        # Force splits regeneration since we added new data\n",
    "        # Delete existing splits so Cell 5 regenerates them\n",
    "        for splits_path in [\"data/splits/all_splits.json\", f\"{SAVE_DIR}/all_splits.json\"]:\n",
    "            try:\n",
    "                if os.path.exists(splits_path):\n",
    "                    os.remove(splits_path)\n",
    "                    print(f\"  Removed old splits: {splits_path}\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        print(\"  Splits will be regenerated in Cell 5 to include v10.9 data\")\n",
    "\n",
    "        # Update counts\n",
    "        by_lang[\"sanskrit\"] = len(SANSKRIT_DHARMA)\n",
    "        by_lang[\"pali\"] = len(PALI_ETHICS)\n",
    "        by_lang[\"classical_chinese\"] += sum(\n",
    "            len(c)\n",
    "            for c, _, _ in [\n",
    "                (BUDDHIST_CHINESE, \"\", \"\"),\n",
    "                (LEGALIST_CHINESE, \"\", \"\"),\n",
    "                (MOHIST_CHINESE, \"\", \"\"),\n",
    "                (NEO_CONFUCIAN_CHINESE, \"\", \"\"),\n",
    "            ]\n",
    "        )\n",
    "        by_lang[\"arabic\"] += sum(\n",
    "            len(c)\n",
    "            for c, _, _ in [\n",
    "                (ISLAMIC_LEGAL_MAXIMS, \"\", \"\"),\n",
    "                (SUFI_ETHICS, \"\", \"\"),\n",
    "                (ARABIC_PHILOSOPHY, \"\", \"\"),\n",
    "            ]\n",
    "        )\n",
    "        n_passages = len(all_passages)\n",
    "\n",
    "        print(f\"\\nUpdated corpus sizes:\")\n",
    "        for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n",
    "            print(f\"  {lang}: {cnt:,}\")\n",
    "    else:\n",
    "        print(\"\\nv10.9 corpora already present and complete\")\n",
    "\n",
    "    # Validate corpus sizes and identify what needs augmentation\n",
    "    print(\"\\nCorpus adequacy check:\")\n",
    "    languages_to_augment = []\n",
    "    for lang, min_size in MIN_CORPUS_SIZE.items():\n",
    "        actual = by_lang.get(lang, 0)\n",
    "        status = \"OK\" if actual >= min_size else \"NEED MORE\"\n",
    "        print(f\"  {lang}: {actual:,} / {min_size:,} - {status}\")\n",
    "        if actual < min_size and lang in AUGMENTATION_DATASETS:\n",
    "            languages_to_augment.append((lang, min_size - actual))\n",
    "\n",
    "    # Augment any under-represented languages that have available datasets\n",
    "    if languages_to_augment:\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(f\"AUGMENTING UNDER-REPRESENTED CORPORA\")\n",
    "        print(f\"=\" * 60)\n",
    "        print(f\"Languages to augment: {[l for l, _ in languages_to_augment]}\")\n",
    "\n",
    "        # Load existing passages\n",
    "        all_passages = []\n",
    "        with open(\"data/processed/passages.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                all_passages.append(json.loads(line))\n",
    "\n",
    "        # Normalize field names\n",
    "        for p in all_passages:\n",
    "            if \"lang\" not in p and \"language\" in p:\n",
    "                p[\"lang\"] = p[\"language\"]\n",
    "            if \"period\" not in p and \"time_period\" in p:\n",
    "                p[\"period\"] = p[\"time_period\"]\n",
    "\n",
    "        print(f\"Loaded {len(all_passages):,} existing passages\")\n",
    "\n",
    "        from datasets import load_dataset\n",
    "\n",
    "        for lang, needed in languages_to_augment:\n",
    "            lang_count = by_lang.get(lang, 0)\n",
    "            print(f\"\\n--- Augmenting {lang} (need {needed:,} more) ---\")\n",
    "\n",
    "            for dataset_name, short_name in AUGMENTATION_DATASETS.get(lang, []):\n",
    "                if lang_count >= MIN_CORPUS_SIZE[lang]:\n",
    "                    break\n",
    "\n",
    "                print(f\"  Loading {short_name}...\")\n",
    "                try:\n",
    "                    if dataset_name == \"hendrycks/ethics\":\n",
    "                        # ETHICS has multiple categories\n",
    "                        categories = [\n",
    "                            \"commonsense\",\n",
    "                            \"deontology\",\n",
    "                            \"justice\",\n",
    "                            \"utilitarianism\",\n",
    "                            \"virtue\",\n",
    "                        ]\n",
    "                        for cat in categories:\n",
    "                            if lang_count >= MIN_CORPUS_SIZE[lang]:\n",
    "                                break\n",
    "                            try:\n",
    "                                ds = load_dataset(\n",
    "                                    dataset_name, cat, split=\"train\", trust_remote_code=True\n",
    "                                )\n",
    "                                cat_count = 0\n",
    "                                for item in ds:\n",
    "                                    if lang_count >= MIN_CORPUS_SIZE[lang]:\n",
    "                                        break\n",
    "                                    if cat == \"commonsense\":\n",
    "                                        text = item.get(\"input\", \"\")\n",
    "                                    elif cat == \"justice\":\n",
    "                                        text = item.get(\"scenario\", \"\")\n",
    "                                    elif cat == \"deontology\":\n",
    "                                        text = (\n",
    "                                            item.get(\"scenario\", \"\") + \" \" + item.get(\"excuse\", \"\")\n",
    "                                        )\n",
    "                                    elif cat == \"virtue\":\n",
    "                                        text = item.get(\"scenario\", \"\")\n",
    "                                    else:\n",
    "                                        text = (\n",
    "                                            str(item.get(\"baseline\", \"\"))\n",
    "                                            + \" vs \"\n",
    "                                            + str(item.get(\"less_pleasant\", \"\"))\n",
    "                                        )\n",
    "\n",
    "                                    if text and len(text) > 30:\n",
    "                                        all_passages.append(\n",
    "                                            {\n",
    "                                                \"id\": f\"ethics_{cat}_{len(all_passages)}\",\n",
    "                                                \"text\": text[:1000],\n",
    "                                                \"lang\": lang,\n",
    "                                                \"language\": lang,\n",
    "                                                \"source\": f\"ETHICS_{cat}\",\n",
    "                                                \"period\": \"MODERN\",\n",
    "                                                \"time_period\": \"MODERN\",\n",
    "                                            }\n",
    "                                        )\n",
    "                                        lang_count += 1\n",
    "                                        cat_count += 1\n",
    "                                print(f\"    {cat}: +{cat_count:,}\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"    {cat} error: {e}\")\n",
    "\n",
    "                    elif dataset_name == \"allenai/social_chem_101\":\n",
    "                        ds = load_dataset(dataset_name, split=\"train\", trust_remote_code=True)\n",
    "                        sc_count = 0\n",
    "                        for item in ds:\n",
    "                            if lang_count >= MIN_CORPUS_SIZE[lang]:\n",
    "                                break\n",
    "                            action = item.get(\"action\", \"\")\n",
    "                            situation = item.get(\"situation\", \"\")\n",
    "                            rot = item.get(\"rot\", \"\")\n",
    "\n",
    "                            if rot and len(rot) > 20:\n",
    "                                text = f\"{situation} {action}\".strip() if situation else action\n",
    "                                text = f\"{text}. {rot}\" if text else rot\n",
    "\n",
    "                                all_passages.append(\n",
    "                                    {\n",
    "                                        \"id\": f\"socialchem_{len(all_passages)}\",\n",
    "                                        \"text\": text[:1000],\n",
    "                                        \"lang\": lang,\n",
    "                                        \"language\": lang,\n",
    "                                        \"source\": \"Social_Chemistry_101\",\n",
    "                                        \"period\": \"MODERN\",\n",
    "                                        \"time_period\": \"MODERN\",\n",
    "                                    }\n",
    "                                )\n",
    "                                lang_count += 1\n",
    "                                sc_count += 1\n",
    "                        print(f\"    Social Chemistry: +{sc_count:,}\")\n",
    "\n",
    "                    else:\n",
    "                        # Generic HuggingFace dataset\n",
    "                        try:\n",
    "                            ds = load_dataset(dataset_name, split=\"train\", trust_remote_code=True)\n",
    "                            gen_count = 0\n",
    "                            for item in ds:\n",
    "                                if lang_count >= MIN_CORPUS_SIZE[lang]:\n",
    "                                    break\n",
    "                                text = item.get(\"text\", \"\") or item.get(\"content\", \"\") or str(item)\n",
    "                                if text and len(text) > 50:\n",
    "                                    all_passages.append(\n",
    "                                        {\n",
    "                                            \"id\": f\"{short_name.lower()}_{len(all_passages)}\",\n",
    "                                            \"text\": text[:1000],\n",
    "                                            \"lang\": lang,\n",
    "                                            \"language\": lang,\n",
    "                                            \"source\": short_name,\n",
    "                                            \"period\": \"MODERN\",\n",
    "                                            \"time_period\": \"MODERN\",\n",
    "                                        }\n",
    "                                    )\n",
    "                                    lang_count += 1\n",
    "                                    gen_count += 1\n",
    "                            print(f\"    {short_name}: +{gen_count:,}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"    {short_name} failed: {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"    {short_name} failed: {e}\")\n",
    "\n",
    "            by_lang[lang] = lang_count\n",
    "            print(f\"  {lang} now: {lang_count:,}\")\n",
    "\n",
    "        # Extract bonds for new passages\n",
    "        print(\"\\nExtracting bonds for new passages...\")\n",
    "        new_bonds = []\n",
    "        new_sources = {\n",
    "            \"ETHICS_commonsense\",\n",
    "            \"ETHICS_deontology\",\n",
    "            \"ETHICS_justice\",\n",
    "            \"ETHICS_utilitarianism\",\n",
    "            \"ETHICS_virtue\",\n",
    "            \"Social_Chemistry_101\",\n",
    "        }\n",
    "\n",
    "        for p in tqdm(all_passages, desc=\"Processing\"):\n",
    "            src = p.get(\"source\", \"\")\n",
    "            if any(src.startswith(s.split(\"_\")[0]) for s in new_sources) or src in new_sources:\n",
    "                text_lower = p[\"text\"].lower()\n",
    "                if any(\n",
    "                    w in text_lower\n",
    "                    for w in [\"wrong\", \"bad\", \"shouldn't\", \"immoral\", \"rude\", \"unethical\"]\n",
    "                ):\n",
    "                    bond_type = \"PROHIBITION\"\n",
    "                elif any(w in text_lower for w in [\"should\", \"must\", \"duty\", \"obligat\", \"need to\"]):\n",
    "                    bond_type = \"OBLIGATION\"\n",
    "                elif any(\n",
    "                    w in text_lower for w in [\"okay\", \"fine\", \"acceptable\", \"can\", \"may\", \"allowed\"]\n",
    "                ):\n",
    "                    bond_type = \"PERMISSION\"\n",
    "                else:\n",
    "                    bond_type = \"NEUTRAL\"\n",
    "\n",
    "                new_bonds.append(\n",
    "                    {\n",
    "                        \"passage_id\": p[\"id\"],\n",
    "                        \"bond_type\": bond_type,\n",
    "                        \"language\": p.get(\"language\", p.get(\"lang\")),\n",
    "                        \"time_period\": p.get(\"time_period\", p.get(\"period\", \"MODERN\")),\n",
    "                        \"source\": src,\n",
    "                        \"text\": p[\"text\"][:500],\n",
    "                        \"context\": \"prescriptive\",\n",
    "                        \"confidence\": \"high\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Load existing bonds and merge\n",
    "        existing_bonds = []\n",
    "        with open(\"data/processed/bonds.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                existing_bonds.append(json.loads(line))\n",
    "\n",
    "        all_bonds = existing_bonds + new_bonds\n",
    "        print(f\"Total bonds: {len(all_bonds):,} ({len(new_bonds):,} new)\")\n",
    "\n",
    "        # Save updated passages\n",
    "        with open(\"data/processed/passages.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for p in all_passages:\n",
    "                p_out = {\n",
    "                    \"id\": p[\"id\"],\n",
    "                    \"text\": p[\"text\"],\n",
    "                    \"language\": p.get(\"language\", p.get(\"lang\", \"english\")),\n",
    "                    \"source\": p.get(\"source\", \"\"),\n",
    "                    \"time_period\": p.get(\"time_period\", p.get(\"period\", \"MODERN\")),\n",
    "                }\n",
    "                f.write(json.dumps(p_out, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Save updated bonds\n",
    "        with open(\"data/processed/bonds.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for b in all_bonds:\n",
    "                f.write(json.dumps(b, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(\"Saved augmented data\")\n",
    "\n",
    "        # Copy to Drive\n",
    "        if USE_DRIVE_DATA:\n",
    "            try:\n",
    "                shutil.copy(\"data/processed/passages.jsonl\", f\"{SAVE_DIR}/passages.jsonl\")\n",
    "                shutil.copy(\"data/processed/bonds.jsonl\", f\"{SAVE_DIR}/bonds.jsonl\")\n",
    "                print(f\"Updated Drive cache: {SAVE_DIR}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Drive update failed: {e}\")\n",
    "\n",
    "        # Final summary\n",
    "        print(f\"\\nFinal corpus sizes:\")\n",
    "        for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n",
    "            target = MIN_CORPUS_SIZE.get(lang, 0)\n",
    "            status = \"OK\" if cnt >= target else \"LOW\"\n",
    "            print(f\"  {lang}: {cnt:,} ({status})\")\n",
    "        n_passages = len(all_passages)\n",
    "\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LOADING CORPORA\")\n",
    "    print(f\"GPU Tier: {GPU_TIER}\")\n",
    "    print(f\"Max per language: {MAX_PER_LANG:,}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    random.seed(42)\n",
    "    all_passages = []\n",
    "\n",
    "    # ===== PARALLEL PREFETCH MANAGER =====\n",
    "    from concurrent.futures import ThreadPoolExecutor, Future\n",
    "    import threading\n",
    "\n",
    "    print(\"Starting parallel prefetch of remote corpora...\")\n",
    "    prefetch_executor = ThreadPoolExecutor(max_workers=12)\n",
    "    prefetch_results = {}  # url -> Future\n",
    "\n",
    "    def prefetch_url(url, timeout=60):\n",
    "        \"\"\"Fetch URL content in background.\"\"\"\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=timeout)\n",
    "            if resp.status_code == 200:\n",
    "                return resp.text\n",
    "        except Exception as e:\n",
    "            print(f\"    Prefetch failed for {url[:50]}...: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Queue all remote downloads\n",
    "    PREFETCH_URLS = [\n",
    "        # Gutenberg - Western Classics\n",
    "        \"https://www.gutenberg.org/cache/epub/1497/pg1497.txt\",  # Republic\n",
    "        \"https://www.gutenberg.org/cache/epub/1656/pg1656.txt\",  # Apology\n",
    "        \"https://www.gutenberg.org/cache/epub/1657/pg1657.txt\",  # Crito\n",
    "        \"https://www.gutenberg.org/cache/epub/1658/pg1658.txt\",  # Phaedo\n",
    "        \"https://www.gutenberg.org/cache/epub/3794/pg3794.txt\",  # Gorgias\n",
    "        \"https://www.gutenberg.org/cache/epub/1636/pg1636.txt\",  # Symposium\n",
    "        \"https://www.gutenberg.org/cache/epub/1726/pg1726.txt\",  # Meno\n",
    "        \"https://www.gutenberg.org/cache/epub/8438/pg8438.txt\",  # Nicomachean Ethics\n",
    "        \"https://www.gutenberg.org/cache/epub/6762/pg6762.txt\",  # Politics\n",
    "        \"https://www.gutenberg.org/cache/epub/2680/pg2680.txt\",  # Meditations\n",
    "        \"https://www.gutenberg.org/cache/epub/10661/pg10661.txt\",  # Enchiridion\n",
    "        \"https://www.gutenberg.org/cache/epub/3042/pg3042.txt\",  # Discourses\n",
    "        \"https://www.gutenberg.org/cache/epub/14988/pg14988.txt\",  # De Officiis\n",
    "        # MIT Classics fallback\n",
    "        \"https://classics.mit.edu/Aristotle/nicomachaen.mb.txt\",\n",
    "        \"https://classics.mit.edu/Plato/laws.mb.txt\",\n",
    "        # Bible Parallel Corpus\n",
    "        \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles/English.xml\",\n",
    "        \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles/Hebrew.xml\",\n",
    "        \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles/Arabic.xml\",\n",
    "        \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles/Chinese.xml\",\n",
    "    ]\n",
    "\n",
    "    for url in PREFETCH_URLS:\n",
    "        prefetch_results[url] = prefetch_executor.submit(prefetch_url, url)\n",
    "\n",
    "    print(f\"  Queued {len(PREFETCH_URLS)} URLs for background download\")\n",
    "\n",
    "    def get_prefetched(url, timeout=30):\n",
    "        \"\"\"Get prefetched content, waiting if necessary.\"\"\"\n",
    "        if url in prefetch_results:\n",
    "            try:\n",
    "                return prefetch_results[url].result(timeout=timeout)\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Fallback to direct fetch\n",
    "        return prefetch_url(url)\n",
    "\n",
    "    # ===== SEFARIA (Hebrew/Aramaic) =====\n",
    "    print(\"\\nLoading Sefaria...\")\n",
    "    sefaria_path = Path(\"data/raw/Sefaria-Export/json\")\n",
    "\n",
    "    CATEGORY_TO_PERIOD = {\n",
    "        \"Tanakh\": \"BIBLICAL\",\n",
    "        \"Torah\": \"BIBLICAL\",\n",
    "        \"Prophets\": \"BIBLICAL\",\n",
    "        \"Writings\": \"BIBLICAL\",\n",
    "        \"Mishnah\": \"TANNAITIC\",\n",
    "        \"Tosefta\": \"TANNAITIC\",\n",
    "        \"Sifra\": \"TANNAITIC\",\n",
    "        \"Sifrei\": \"TANNAITIC\",\n",
    "        \"Talmud\": \"TALMUDIC\",\n",
    "        \"Bavli\": \"TALMUDIC\",\n",
    "        \"Yerushalmi\": \"TALMUDIC\",\n",
    "        \"Midrash\": \"MIDRASHIC\",\n",
    "        \"Midrash Rabbah\": \"MIDRASHIC\",\n",
    "        \"Midrash Aggadah\": \"MIDRASHIC\",\n",
    "        \"Halakhah\": \"MEDIEVAL\",\n",
    "        \"Shulchan Arukh\": \"MEDIEVAL\",\n",
    "        \"Mishneh Torah\": \"MEDIEVAL\",\n",
    "        \"Musar\": \"MODERN\",\n",
    "        \"Chasidut\": \"MODERN\",\n",
    "        \"Modern\": \"MODERN\",\n",
    "    }\n",
    "\n",
    "    lang_counts = {\"hebrew\": 0, \"aramaic\": 0}\n",
    "\n",
    "    if sefaria_path.exists():\n",
    "        for json_file in tqdm(list(sefaria_path.rglob(\"*.json\"))[:5000], desc=\"Sefaria\"):\n",
    "            try:\n",
    "                with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                if isinstance(data, dict) and \"text\" in data:\n",
    "                    # Determine period from path\n",
    "                    path_parts = str(json_file.relative_to(sefaria_path)).split(\"/\")\n",
    "                    period = \"CLASSICAL\"\n",
    "                    for part in path_parts:\n",
    "                        if part in CATEGORY_TO_PERIOD:\n",
    "                            period = CATEGORY_TO_PERIOD[part]\n",
    "                            break\n",
    "\n",
    "                    # Determine language (heuristic: Talmud is primarily Aramaic)\n",
    "                    is_talmud = any(t in str(json_file) for t in [\"Talmud\", \"Bavli\", \"Yerushalmi\"])\n",
    "                    lang = \"aramaic\" if is_talmud else \"hebrew\"\n",
    "\n",
    "                    def extract_texts(obj, texts):\n",
    "                        if isinstance(obj, str) and len(obj) > 20:\n",
    "                            texts.append(obj)\n",
    "                        elif isinstance(obj, list):\n",
    "                            for item in obj:\n",
    "                                extract_texts(item, texts)\n",
    "\n",
    "                    texts = []\n",
    "                    extract_texts(data[\"text\"], texts)\n",
    "\n",
    "                    for txt in texts[:50]:  # Limit per file\n",
    "                        if lang_counts[lang] < MAX_PER_LANG:\n",
    "                            all_passages.append(\n",
    "                                {\n",
    "                                    \"id\": f\"sefaria_{len(all_passages)}\",\n",
    "                                    \"text\": txt,\n",
    "                                    \"lang\": lang,\n",
    "                                    \"source\": json_file.stem,\n",
    "                                    \"period\": period,\n",
    "                                }\n",
    "                            )\n",
    "                            lang_counts[lang] += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"  Sefaria not found - will download\")\n",
    "\n",
    "    print(f\"  Hebrew: {lang_counts['hebrew']:,}, Aramaic: {lang_counts['aramaic']:,}\")\n",
    "\n",
    "    # ===== CLASSICAL CHINESE: Disabled (CText API blocks Colab) =====\n",
    "    print(\"  Skipping CText API (blocked from Colab, using Wenyanwen instead)\")\n",
    "    chinese_count = 0  # Initialize counter\n",
    "\n",
    "    # ===== KAGGLE: Ancient Chinese Wenyanwen (132K texts, 552M chars) =====\n",
    "    if chinese_count < MAX_PER_LANG:\n",
    "        print(\"  Loading from Kaggle Wenyanwen dataset...\")\n",
    "        wenyan_zip_name = \"Ancient_Chinese_Text_(wenyanwen)_archive.zip\"\n",
    "        wenyan_csv_name = \"cn_wenyan.csv\"\n",
    "        wenyan_local_zip = Path(f\"data/raw/{wenyan_zip_name}\")\n",
    "        _drive_ok = \"USE_DRIVE_DATA\" in dir() and USE_DRIVE_DATA and \"SAVE_DIR\" in dir()\n",
    "        wenyan_drive_zip = Path(f\"{SAVE_DIR}/{wenyan_zip_name}\") if _drive_ok else None\n",
    "        wenyan_local_csv = Path(f\"data/raw/{wenyan_csv_name}\")\n",
    "        wenyan_drive_csv = Path(f\"{SAVE_DIR}/{wenyan_csv_name}\") if _drive_ok else None\n",
    "\n",
    "        # Find the CSV (extracted or in zip)\n",
    "        csv_path = None\n",
    "        if wenyan_local_csv.exists():\n",
    "            csv_path = wenyan_local_csv\n",
    "            print(\"    Found CSV locally\")\n",
    "        elif wenyan_drive_csv and wenyan_drive_csv.exists():\n",
    "            csv_path = wenyan_drive_csv\n",
    "            print(\"    Found CSV in Drive\")\n",
    "        else:\n",
    "            # Need to extract from zip\n",
    "            zip_path = None\n",
    "            if wenyan_local_zip.exists():\n",
    "                zip_path = wenyan_local_zip\n",
    "                print(\"    Found zip locally\")\n",
    "            elif wenyan_drive_zip and wenyan_drive_zip.exists():\n",
    "                zip_path = wenyan_drive_zip\n",
    "                print(\"    Found zip in Drive\")\n",
    "\n",
    "            if zip_path:\n",
    "                try:\n",
    "                    import zipfile\n",
    "\n",
    "                    print(\"    Extracting CSV from zip...\")\n",
    "                    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "                        z.extract(wenyan_csv_name, \"data/raw/\")\n",
    "                    csv_path = wenyan_local_csv\n",
    "                    print(\"    Extracted!\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    Extraction failed: {e}\")\n",
    "\n",
    "        # Load texts from CSV\n",
    "        wenyan_count = 0\n",
    "        if csv_path and csv_path.exists():\n",
    "            import csv\n",
    "\n",
    "            csv.field_size_limit(10000000)  # Some texts are very long\n",
    "            try:\n",
    "                with open(csv_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    reader = csv.DictReader(f)\n",
    "                    for row in reader:\n",
    "                        if chinese_count >= MAX_PER_LANG:\n",
    "                            break\n",
    "                        text = row.get(\"text\", \"\")\n",
    "                        title = row.get(\"title\", \"\")\n",
    "                        # Split long texts into passages (max 2000 chars each)\n",
    "                        # Use paragraph breaks or every 1500 chars\n",
    "                        paragraphs = text.split(\"\\n\")\n",
    "                        current_para = \"\"\n",
    "                        for para in paragraphs:\n",
    "                            para = para.strip()\n",
    "                            if not para:\n",
    "                                continue\n",
    "                            if len(current_para) + len(para) < 1500:\n",
    "                                current_para += para\n",
    "                            else:\n",
    "                                if len(current_para) > 50:\n",
    "                                    all_passages.append(\n",
    "                                        {\n",
    "                                            \"id\": f\"wenyan_{len(all_passages)}\",\n",
    "                                            \"text\": current_para,\n",
    "                                            \"lang\": \"classical_chinese\",\n",
    "                                            \"source\": (\n",
    "                                                title.split(\"/\")[0] if \"/\" in title else title\n",
    "                                            ),\n",
    "                                            \"period\": \"CONFUCIAN\",\n",
    "                                        }\n",
    "                                    )\n",
    "                                    chinese_count += 1\n",
    "                                    wenyan_count += 1\n",
    "                                    if chinese_count >= MAX_PER_LANG:\n",
    "                                        break\n",
    "                                current_para = para\n",
    "                        # Don't forget last paragraph\n",
    "                        if current_para and len(current_para) > 50 and chinese_count < MAX_PER_LANG:\n",
    "                            all_passages.append(\n",
    "                                {\n",
    "                                    \"id\": f\"wenyan_{len(all_passages)}\",\n",
    "                                    \"text\": current_para,\n",
    "                                    \"lang\": \"classical_chinese\",\n",
    "                                    \"source\": title.split(\"/\")[0] if \"/\" in title else title,\n",
    "                                    \"period\": \"CONFUCIAN\",\n",
    "                                }\n",
    "                            )\n",
    "                            chinese_count += 1\n",
    "                            wenyan_count += 1\n",
    "                print(f\"    Added {wenyan_count:,} passages from Wenyanwen\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error loading Wenyanwen: {e}\")\n",
    "\n",
    "    print(f\"  Total Classical Chinese: {chinese_count:,}\")\n",
    "\n",
    "    # ===== ARABIC/ISLAMIC (Kaggle quran-nlp) =====\n",
    "    print(\"\\nLoading Arabic from Kaggle quran-nlp...\")\n",
    "\n",
    "    arabic_count = 0\n",
    "    kaggle_path = Path(\"data/raw/quran-nlp\")\n",
    "\n",
    "    # Try to download from Kaggle\n",
    "    if not kaggle_path.exists() and REFRESH_DATA_FROM_SOURCE:\n",
    "        try:\n",
    "            import subprocess\n",
    "            import zipfile\n",
    "\n",
    "            subprocess.run([\"pip\", \"install\", \"-q\", \"kaggle\"], check=True)\n",
    "            subprocess.run(\n",
    "                [\n",
    "                    \"kaggle\",\n",
    "                    \"datasets\",\n",
    "                    \"download\",\n",
    "                    \"-d\",\n",
    "                    \"alizahidraja/quran-nlp\",\n",
    "                    \"-p\",\n",
    "                    \"data/raw\",\n",
    "                ],\n",
    "                check=True,\n",
    "                timeout=300,\n",
    "            )\n",
    "\n",
    "            with zipfile.ZipFile(\"data/raw/quran-nlp.zip\", \"r\") as z:\n",
    "                z.extractall(kaggle_path)\n",
    "            print(\"  Downloaded from Kaggle!\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Kaggle download failed: {e}\")\n",
    "\n",
    "    # Load if available\n",
    "    if kaggle_path.exists():\n",
    "        import pandas as pd\n",
    "\n",
    "        # Load Quran\n",
    "        quran_files = list(kaggle_path.rglob(\"*quran*.csv\"))\n",
    "        for qf in quran_files:\n",
    "            if arabic_count >= MAX_PER_LANG:\n",
    "                break\n",
    "            try:\n",
    "                df = pd.read_csv(qf, nrows=MAX_PER_LANG - arabic_count)\n",
    "                for _, row in df.iterrows():\n",
    "                    text = str(row.get(\"arabic\", row.get(\"text\", row.get(\"Arabic\", \"\"))))\n",
    "                    if text and len(text) > 10 and text != \"nan\":\n",
    "                        all_passages.append(\n",
    "                            {\n",
    "                                \"id\": f\"quran_{len(all_passages)}\",\n",
    "                                \"text\": text,\n",
    "                                \"lang\": \"arabic\",\n",
    "                                \"source\": \"Quran\",\n",
    "                                \"period\": \"QURANIC\",\n",
    "                            }\n",
    "                        )\n",
    "                        arabic_count += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Load Hadith\n",
    "        hadith_files = list(kaggle_path.rglob(\"*hadith*.csv\"))\n",
    "        for hf in hadith_files:\n",
    "            if arabic_count >= MAX_PER_LANG:\n",
    "                break\n",
    "            try:\n",
    "                df = pd.read_csv(hf, nrows=MAX_PER_LANG - arabic_count)\n",
    "                for _, row in df.iterrows():\n",
    "                    text = str(row.get(\"hadith\", row.get(\"text\", row.get(\"Arabic\", \"\"))))\n",
    "                    if text and len(text) > 10 and text != \"nan\":\n",
    "                        all_passages.append(\n",
    "                            {\n",
    "                                \"id\": f\"hadith_{len(all_passages)}\",\n",
    "                                \"text\": text,\n",
    "                                \"lang\": \"arabic\",\n",
    "                                \"source\": \"Hadith\",\n",
    "                                \"period\": \"HADITH\",\n",
    "                            }\n",
    "                        )\n",
    "                        arabic_count += 1\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        # Try Tanzil.net (simple direct download)\n",
    "        print(\"  Trying Tanzil.net for Quran text...\")\n",
    "        try:\n",
    "            tanzil_url = \"https://tanzil.net/pub/download/index.php?quranType=uthmani&outType=txt-2&agree=true\"\n",
    "            resp = requests.get(tanzil_url, timeout=60)\n",
    "            if resp.status_code == 200:\n",
    "                lines = resp.text.strip().split(\"\\n\")\n",
    "                for line in lines:\n",
    "                    if \"|\" in line and arabic_count < MAX_PER_LANG:\n",
    "                        parts = line.split(\"|\")\n",
    "                        if len(parts) >= 3:\n",
    "                            text = parts[2].strip()\n",
    "                            if len(text) > 10:\n",
    "                                all_passages.append(\n",
    "                                    {\n",
    "                                        \"id\": f\"tanzil_{len(all_passages)}\",\n",
    "                                        \"text\": text,\n",
    "                                        \"lang\": \"arabic\",\n",
    "                                        \"source\": \"Quran (Tanzil)\",\n",
    "                                        \"period\": \"QURANIC\",\n",
    "                                    }\n",
    "                                )\n",
    "                                arabic_count += 1\n",
    "                print(f\"    Loaded {arabic_count} verses from Tanzil\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Tanzil failed: {e}\")\n",
    "\n",
    "        # Final fallback: expanded hardcoded corpus\n",
    "        if arabic_count < 100:\n",
    "            print(\"  Using expanded hardcoded Arabic corpus...\")\n",
    "        ARABIC_CORPUS = [\n",
    "            # Quran excerpts (moral/ethical content)\n",
    "            \"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u062a\u064f\u0644\u064f\u0648\u0627 \u0627\u0644\u0646\u064e\u0651\u0641\u0652\u0633\u064e \u0627\u0644\u064e\u0651\u062a\u0650\u064a \u062d\u064e\u0631\u064e\u0651\u0645\u064e \u0627\u0644\u0644\u064e\u0651\u0647\u064f \u0625\u0650\u0644\u064e\u0651\u0627 \u0628\u0650\u0627\u0644\u0652\u062d\u064e\u0642\u0650\u0651\",\n",
    "            \"\u0648\u064e\u0628\u0650\u0627\u0644\u0652\u0648\u064e\u0627\u0644\u0650\u062f\u064e\u064a\u0652\u0646\u0650 \u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u064b\u0627\",\n",
    "            \"\u0648\u064e\u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u0650 \u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u064e \u0643\u064e\u0627\u0646\u064e \u0645\u064e\u0633\u0652\u0626\u064f\u0648\u0644\u064b\u0627\",\n",
    "            \"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0644\u064e\u0651\u0647\u064e \u064a\u064e\u0623\u0652\u0645\u064f\u0631\u064f \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u062f\u0652\u0644\u0650 \u0648\u064e\u0627\u0644\u0652\u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u0650\",\n",
    "            \"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0628\u0652\u062e\u064e\u0633\u064f\u0648\u0627 \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u064e \u0623\u064e\u0634\u0652\u064a\u064e\u0627\u0621\u064e\u0647\u064f\u0645\u0652\",\n",
    "            \"\u0648\u064e\u0623\u064e\u0642\u0650\u064a\u0645\u064f\u0648\u0627 \u0627\u0644\u0652\u0648\u064e\u0632\u0652\u0646\u064e \u0628\u0650\u0627\u0644\u0652\u0642\u0650\u0633\u0652\u0637\u0650 \u0648\u064e\u0644\u064e\u0627 \u062a\u064f\u062e\u0652\u0633\u0650\u0631\u064f\u0648\u0627 \u0627\u0644\u0652\u0645\u0650\u064a\u0632\u064e\u0627\u0646\u064e\",\n",
    "            \"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0622\u0645\u064e\u0646\u064f\u0648\u0627 \u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064f\u0642\u064f\u0648\u062f\u0650\",\n",
    "            \"\u0648\u064e\u062a\u064e\u0639\u064e\u0627\u0648\u064e\u0646\u064f\u0648\u0627 \u0639\u064e\u0644\u064e\u0649 \u0627\u0644\u0652\u0628\u0650\u0631\u0650\u0651 \u0648\u064e\u0627\u0644\u062a\u064e\u0651\u0642\u0652\u0648\u064e\u0649\",\n",
    "            # ... more can be added\n",
    "        ]\n",
    "        for i, txt in enumerate(ARABIC_CORPUS):\n",
    "            all_passages.append(\n",
    "                {\n",
    "                    \"id\": f\"arabic_{len(all_passages)}\",\n",
    "                    \"text\": txt,\n",
    "                    \"lang\": \"arabic\",\n",
    "                    \"source\": \"Quran/Hadith\",\n",
    "                    \"period\": \"QURANIC\",\n",
    "                }\n",
    "            )\n",
    "            arabic_count += 1\n",
    "\n",
    "    print(f\"  Arabic: {arabic_count:,}\")\n",
    "\n",
    "    # ===== DEAR ABBY (English) =====\n",
    "    print(\"Loading Dear Abby...\")\n",
    "\n",
    "    english_count = 0\n",
    "    abby_path = Path(\"data/raw/dear_abby.csv\")\n",
    "    print(f\"  Local path exists: {abby_path.exists()}\")\n",
    "\n",
    "    # Check Drive first\n",
    "    drive_abby = f\"{SAVE_DIR}/dear_abby.csv\"\n",
    "    print(f\"  Drive path: {drive_abby}\")\n",
    "    print(f\"  Drive path exists: {os.path.exists(drive_abby)}\")\n",
    "    if not abby_path.exists() and os.path.exists(drive_abby):\n",
    "        os.makedirs(\"data/raw\", exist_ok=True)\n",
    "        shutil.copy(drive_abby, abby_path)\n",
    "        print(\"  Copied from Drive\")\n",
    "\n",
    "    if not abby_path.exists() and REFRESH_DATA_FROM_SOURCE:\n",
    "        try:\n",
    "            import subprocess\n",
    "\n",
    "            subprocess.run([\"pip\", \"install\", \"-q\", \"kaggle\"], check=True)\n",
    "            subprocess.run(\n",
    "                [\n",
    "                    \"kaggle\",\n",
    "                    \"datasets\",\n",
    "                    \"download\",\n",
    "                    \"-d\",\n",
    "                    \"thedevastator/20000-dear-abby-questions\",\n",
    "                    \"-p\",\n",
    "                    \"data/raw\",\n",
    "                    \"-f\",\n",
    "                    \"dear_abby.csv\",\n",
    "                ],\n",
    "                check=True,\n",
    "                timeout=120,\n",
    "            )\n",
    "            print(\"  Downloaded from Kaggle!\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Kaggle download failed: {e}\")\n",
    "\n",
    "    if abby_path.exists():\n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.read_csv(abby_path, nrows=MAX_PER_LANG)\n",
    "        print(f\"  CSV columns: {list(df.columns)}\")\n",
    "        print(f\"  CSV rows: {len(df)}\")\n",
    "        for _, row in df.iterrows():\n",
    "            question = str(row.get(\"question\", \"\"))\n",
    "            answer = str(row.get(\"question_only\", \"\"))\n",
    "            if len(answer) > 50:\n",
    "                all_passages.append(\n",
    "                    {\n",
    "                        \"id\": f\"abby_{len(all_passages)}\",\n",
    "                        \"text\": answer,\n",
    "                        \"lang\": \"english\",\n",
    "                        \"source\": \"Dear Abby\",\n",
    "                        \"period\": \"DEAR_ABBY\",\n",
    "                    }\n",
    "                )\n",
    "                english_count += 1\n",
    "    else:\n",
    "        print(\"  Dear Abby not found\")\n",
    "\n",
    "    print(f\"  Dear Abby: {english_count:,}\")\n",
    "\n",
    "    # ===== WESTERN CLASSICS (Greek/Roman Philosophy) =====\n",
    "    print(\"\\nLoading Western Classics (parallel download)...\")\n",
    "\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "    # Project Gutenberg texts (reliable, plain text)\n",
    "    GUTENBERG_TEXTS = [\n",
    "        # Plato - Ethics & Political Philosophy\n",
    "        (1497, \"Republic\", \"Plato\"),\n",
    "        (1656, \"Apology\", \"Plato\"),\n",
    "        (1657, \"Crito\", \"Plato\"),\n",
    "        (1658, \"Phaedo\", \"Plato\"),\n",
    "        (3794, \"Gorgias\", \"Plato\"),\n",
    "        (1636, \"Symposium\", \"Plato\"),\n",
    "        (1726, \"Meno\", \"Plato\"),\n",
    "        # Aristotle\n",
    "        (8438, \"Nicomachean Ethics\", \"Aristotle\"),\n",
    "        (6762, \"Politics\", \"Aristotle\"),\n",
    "        # Stoics\n",
    "        (2680, \"Meditations\", \"Marcus Aurelius\"),\n",
    "        (10661, \"Enchiridion\", \"Epictetus\"),\n",
    "        (3042, \"Discourses\", \"Epictetus\"),\n",
    "        # Cicero\n",
    "        (14988, \"De Officiis\", \"Cicero\"),\n",
    "    ]\n",
    "\n",
    "    # MIT Classics fallback\n",
    "    MIT_TEXTS = [\n",
    "        (\n",
    "            \"https://classics.mit.edu/Aristotle/nicomachaen.mb.txt\",\n",
    "            \"Nicomachean Ethics\",\n",
    "            \"Aristotle\",\n",
    "        ),\n",
    "        (\"https://classics.mit.edu/Aristotle/politics.mb.txt\", \"Politics\", \"Aristotle\"),\n",
    "        (\"https://classics.mit.edu/Plato/republic.mb.txt\", \"Republic\", \"Plato\"),\n",
    "        (\"https://classics.mit.edu/Plato/laws.mb.txt\", \"Laws\", \"Plato\"),\n",
    "        (\"https://classics.mit.edu/Antoninus/meditations.mb.txt\", \"Meditations\", \"Marcus Aurelius\"),\n",
    "        (\"https://classics.mit.edu/Epictetus/epicench.mb.txt\", \"Enchiridion\", \"Epictetus\"),\n",
    "        (\"https://classics.mit.edu/Cicero/duties.mb.txt\", \"De Officiis\", \"Cicero\"),\n",
    "    ]\n",
    "\n",
    "    western_target = min(MAX_PER_LANG, 15000)\n",
    "\n",
    "    def fetch_gutenberg(item):\n",
    "        \"\"\"Fetch a single Gutenberg text (uses prefetch if available).\"\"\"\n",
    "        gutenberg_id, title, author = item\n",
    "        try:\n",
    "            url = f\"https://www.gutenberg.org/cache/epub/{gutenberg_id}/pg{gutenberg_id}.txt\"\n",
    "            text = get_prefetched(url)\n",
    "            if text:\n",
    "                # Skip Gutenberg header/footer\n",
    "                for marker in [\"*** START OF\", \"***START OF\"]:\n",
    "                    if marker in text:\n",
    "                        text = text.split(marker, 1)[-1]\n",
    "                        break\n",
    "                for marker in [\"*** END OF\", \"***END OF\", \"End of Project Gutenberg\"]:\n",
    "                    if marker in text:\n",
    "                        text = text.split(marker, 1)[0]\n",
    "                        break\n",
    "\n",
    "                paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if len(p.strip()) > 100]\n",
    "                passages = []\n",
    "                for para in paragraphs:\n",
    "                    para = re.sub(r\"\\s+\", \" \", para).strip()\n",
    "                    if 50 < len(para) < 2000:\n",
    "                        passages.append(\n",
    "                            {\n",
    "                                \"text\": para,\n",
    "                                \"source\": f\"{author}: {title}\",\n",
    "                                \"author\": author,\n",
    "                                \"title\": title,\n",
    "                            }\n",
    "                        )\n",
    "                return (title, author, passages)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        return (title, author, [])\n",
    "\n",
    "    def fetch_mit(item):\n",
    "        \"\"\"Fetch a single MIT Classics text (uses prefetch if available).\"\"\"\n",
    "        url, title, author = item\n",
    "        try:\n",
    "            text = get_prefetched(url)\n",
    "            if text:\n",
    "                paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if len(p.strip()) > 100]\n",
    "                passages = []\n",
    "                for para in paragraphs[:500]:\n",
    "                    para = re.sub(r\"\\s+\", \" \", para).strip()\n",
    "                    if 50 < len(para) < 2000:\n",
    "                        passages.append(\n",
    "                            {\n",
    "                                \"text\": para,\n",
    "                                \"source\": f\"{author}: {title}\",\n",
    "                                \"author\": author,\n",
    "                                \"title\": title,\n",
    "                            }\n",
    "                        )\n",
    "                return (title, author, passages)\n",
    "        except:\n",
    "            pass\n",
    "        return (title, author, [])\n",
    "\n",
    "    western_passages = []\n",
    "    loaded_titles = set()\n",
    "\n",
    "    # Parallel fetch from Gutenberg\n",
    "    print(\"  Fetching from Project Gutenberg (parallel)...\")\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        futures = {executor.submit(fetch_gutenberg, item): item for item in GUTENBERG_TEXTS}\n",
    "        for future in as_completed(futures):\n",
    "            title, author, passages = future.result()\n",
    "            if passages and title not in loaded_titles:\n",
    "                western_passages.extend(passages)\n",
    "                loaded_titles.add(title)\n",
    "                print(f\"    {author}: {title} - {len(passages)} passages\")\n",
    "\n",
    "    # Parallel fetch from MIT for any missing\n",
    "    missing_mit = [(url, t, a) for url, t, a in MIT_TEXTS if t not in loaded_titles]\n",
    "    if missing_mit:\n",
    "        print(\"  Fetching missing texts from MIT Classics...\")\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = {executor.submit(fetch_mit, item): item for item in missing_mit}\n",
    "            for future in as_completed(futures):\n",
    "                title, author, passages = future.result()\n",
    "                if passages and title not in loaded_titles:\n",
    "                    western_passages.extend(passages)\n",
    "                    loaded_titles.add(title)\n",
    "                    print(f\"    {author}: {title} - {len(passages)} passages (MIT)\")\n",
    "\n",
    "    # Add to all_passages with proper IDs\n",
    "    western_count = 0\n",
    "    for p in western_passages:\n",
    "        if western_count >= western_target:\n",
    "            break\n",
    "        all_passages.append(\n",
    "            {\n",
    "                \"id\": f\"western_{len(all_passages)}\",\n",
    "                \"text\": p[\"text\"],\n",
    "                \"lang\": \"english\",\n",
    "                \"source\": p[\"source\"],\n",
    "                \"period\": \"WESTERN_CLASSICAL\",\n",
    "                \"time_period\": \"WESTERN_CLASSICAL\",\n",
    "            }\n",
    "        )\n",
    "        western_count += 1\n",
    "\n",
    "    print(f\"  Total Western Classics: {western_count:,}\")\n",
    "    # ===== UNIMORAL: Disabled (gated dataset requires auth) =====\n",
    "    print(\"  Skipping UniMoral (gated HuggingFace dataset)\")\n",
    "\n",
    "    # ===== UN PARALLEL CORPUS (HuggingFace streaming) =====\n",
    "    print(\"\\nLoading UN Corpus from HuggingFace (streaming)...\")\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "\n",
    "        pairs = [(\"ar\", \"en\"), (\"en\", \"zh\")]\n",
    "        un_count = 0\n",
    "        lang_map = {\"ar\": \"arabic\", \"zh\": \"classical_chinese\", \"en\": \"english\"}\n",
    "\n",
    "        for src, tgt in pairs:\n",
    "            if un_count >= MAX_PER_LANG:\n",
    "                break\n",
    "            try:\n",
    "                config = f\"{src}-{tgt}\"\n",
    "                ds = load_dataset(\"Helsinki-NLP/un_pc\", config, split=\"train\", streaming=True)\n",
    "\n",
    "                pair_count = 0\n",
    "                for item in ds:\n",
    "                    if pair_count >= min(MAX_PER_LANG // 4, 5000):\n",
    "                        break\n",
    "\n",
    "                    translation = item.get(\"translation\", {})\n",
    "                    for lang_code in [src, tgt]:\n",
    "                        text = translation.get(lang_code, \"\")\n",
    "                        if len(text) > 30 and lang_code in lang_map:\n",
    "                            all_passages.append(\n",
    "                                {\n",
    "                                    \"id\": f\"un_{len(all_passages)}\",\n",
    "                                    \"text\": text,\n",
    "                                    \"lang\": lang_map[lang_code],\n",
    "                                    \"source\": \"UN Corpus\",\n",
    "                                    \"period\": \"MODERN\",\n",
    "                                }\n",
    "                            )\n",
    "                            pair_count += 1\n",
    "                            un_count += 1\n",
    "\n",
    "                print(f\"  UN {config}: {pair_count:,}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  UN {config} error: {e}\")\n",
    "\n",
    "        print(f\"  UN Corpus total: {un_count:,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  UN Corpus error: {e}\")\n",
    "\n",
    "    # ===== BIBLE PARALLEL CORPUS (GitHub) =====\n",
    "    print(\"\\nLoading Bible Parallel Corpus...\")\n",
    "    try:\n",
    "        base_url = \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles\"\n",
    "        bible_files = [\n",
    "            (\"Hebrew.xml\", \"hebrew\"),\n",
    "            (\"Arabic.xml\", \"arabic\"),\n",
    "            (\"Chinese.xml\", \"classical_chinese\"),\n",
    "        ]\n",
    "\n",
    "        bible_count = 0\n",
    "        for filename, lang in bible_files:\n",
    "            if bible_count >= MAX_PER_LANG * 3:\n",
    "                break\n",
    "            try:\n",
    "                url = f\"{base_url}/{filename}\"\n",
    "                text = get_prefetched(url)\n",
    "                if text:\n",
    "                    verses = re.findall(r\"<seg[^>]*>([^<]+)</seg>\", text)\n",
    "                    file_count = 0\n",
    "                    for verse in verses:\n",
    "                        if file_count >= MAX_PER_LANG:\n",
    "                            break\n",
    "                        verse = verse.strip()\n",
    "                        if len(verse) > 10:\n",
    "                            all_passages.append(\n",
    "                                {\n",
    "                                    \"id\": f\"bible_{len(all_passages)}\",\n",
    "                                    \"text\": verse,\n",
    "                                    \"lang\": lang,\n",
    "                                    \"source\": \"Bible\",\n",
    "                                    \"period\": \"CLASSICAL\",\n",
    "                                }\n",
    "                            )\n",
    "                            file_count += 1\n",
    "                            bible_count += 1\n",
    "                    print(f\"  Bible {lang}: {file_count:,}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Bible {filename} error: {e}\")\n",
    "\n",
    "        print(f\"  Bible total: {bible_count:,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Bible error: {e}\")\n",
    "\n",
    "    # ===== NEW v10.9 CORPORA =====\n",
    "    print(\"\\n--- v10.9 New Corpora (hardcoded) ---\")\n",
    "\n",
    "    # Chinese philosophical traditions\n",
    "    chinese_corpora = [\n",
    "        (BUDDHIST_CHINESE, \"BUDDHIST\", \"Buddhist Chinese\"),\n",
    "        (LEGALIST_CHINESE, \"LEGALIST\", \"Legalist Chinese\"),\n",
    "        (MOHIST_CHINESE, \"MOHIST\", \"Mohist Chinese\"),\n",
    "        (NEO_CONFUCIAN_CHINESE, \"NEO_CONFUCIAN\", \"Neo-Confucian\"),\n",
    "    ]\n",
    "\n",
    "    for corpus, period, label in chinese_corpora:\n",
    "        count = 0\n",
    "        for text_content, source_ref, _ in corpus:\n",
    "            all_passages.append(\n",
    "                {\n",
    "                    \"id\": f\"v109_{label.lower().replace(' ', '_')}_{len(all_passages)}\",\n",
    "                    \"text\": text_content,\n",
    "                    \"lang\": \"classical_chinese\",\n",
    "                    \"source\": source_ref,\n",
    "                    \"period\": period,\n",
    "                }\n",
    "            )\n",
    "            count += 1\n",
    "        print(f\"  {label}: {count}\")\n",
    "\n",
    "    # Arabic/Islamic traditions\n",
    "    arabic_corpora = [\n",
    "        (ISLAMIC_LEGAL_MAXIMS, \"FIQH\", \"Islamic Legal Maxims\"),\n",
    "        (SUFI_ETHICS, \"SUFI\", \"Sufi Ethics\"),\n",
    "        (ARABIC_PHILOSOPHY, \"FALSAFA\", \"Arabic Philosophy\"),\n",
    "    ]\n",
    "\n",
    "    for corpus, period, label in arabic_corpora:\n",
    "        count = 0\n",
    "        for text_content, source_ref, _ in corpus:\n",
    "            all_passages.append(\n",
    "                {\n",
    "                    \"id\": f\"v109_{label.lower().replace(' ', '_')}_{len(all_passages)}\",\n",
    "                    \"text\": text_content,\n",
    "                    \"lang\": \"arabic\",\n",
    "                    \"source\": source_ref,\n",
    "                    \"period\": period,\n",
    "                }\n",
    "            )\n",
    "            count += 1\n",
    "        print(f\"  {label}: {count}\")\n",
    "\n",
    "    # Sanskrit tradition\n",
    "    sanskrit_count = 0\n",
    "    for text_content, source_ref, period_tag in SANSKRIT_DHARMA:\n",
    "        all_passages.append(\n",
    "            {\n",
    "                \"id\": f\"v109_sanskrit_{len(all_passages)}\",\n",
    "                \"text\": text_content,\n",
    "                \"lang\": \"sanskrit\",\n",
    "                \"source\": source_ref,\n",
    "                \"period\": period_tag,\n",
    "            }\n",
    "        )\n",
    "        sanskrit_count += 1\n",
    "    print(f\"  Sanskrit Dharma: {sanskrit_count}\")\n",
    "\n",
    "    # Pali tradition\n",
    "    pali_count = 0\n",
    "    for text_content, source_ref, period_tag in PALI_ETHICS:\n",
    "        all_passages.append(\n",
    "            {\n",
    "                \"id\": f\"v109_pali_{len(all_passages)}\",\n",
    "                \"text\": text_content,\n",
    "                \"lang\": \"pali\",\n",
    "                \"source\": source_ref,\n",
    "                \"period\": period_tag,\n",
    "            }\n",
    "        )\n",
    "        pali_count += 1\n",
    "    print(f\"  Pali Ethics: {pali_count}\")\n",
    "\n",
    "    # Cleanup prefetch executor\n",
    "    print(\"\\nWaiting for any remaining prefetch tasks...\")\n",
    "    prefetch_executor.shutdown(wait=False)\n",
    "\n",
    "    # ===== SUMMARY =====\n",
    "    print(f\"\\nTOTAL: {len(all_passages):,}\")\n",
    "\n",
    "    # Count by language\n",
    "    by_lang = defaultdict(int)\n",
    "    for p in all_passages:\n",
    "        by_lang[p[\"lang\"]] += 1\n",
    "    print(\"\\nBy language:\")\n",
    "    for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {lang}: {cnt:,}\")\n",
    "\n",
    "    # ===== EXTRACT BONDS =====\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXTRACTING BONDS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    def extract_bond(text, language):\n",
    "        \"\"\"Extract bond type with context awareness.\"\"\"\n",
    "        tn = normalize_text(text, language)\n",
    "\n",
    "        for bt, pats in ALL_BOND_PATTERNS.get(language, {}).items():\n",
    "            for p in pats:\n",
    "                match = re.search(p, tn)\n",
    "                if match:\n",
    "                    # Check context around the match\n",
    "                    context, marker_type = detect_context(text, language, match.start())\n",
    "                    confidence = 0.9 if context == \"prescriptive\" else 0.5\n",
    "                    return bt, context, confidence\n",
    "        return None, \"unknown\", 0.5\n",
    "\n",
    "    bonds = []\n",
    "    for p in tqdm(all_passages, desc=\"Extracting bonds\"):\n",
    "        bt, ctx, conf = extract_bond(p[\"text\"], p[\"lang\"])\n",
    "        if bt:\n",
    "            bonds.append(\n",
    "                {\n",
    "                    \"passage_id\": p[\"id\"],\n",
    "                    \"bond_type\": bt,\n",
    "                    \"language\": p[\"lang\"],\n",
    "                    \"time_period\": p[\"period\"],\n",
    "                    \"source\": p[\"source\"],\n",
    "                    \"text\": p[\"text\"][:500],\n",
    "                    \"context\": ctx,\n",
    "                    \"confidence\": conf,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    print(f\"\\nExtracted {len(bonds):,} bonds from {len(all_passages):,} passages\")\n",
    "\n",
    "    # Count by bond type\n",
    "    by_bond = defaultdict(int)\n",
    "    for b in bonds:\n",
    "        by_bond[b[\"bond_type\"]] += 1\n",
    "    print(\"\\nBy bond type:\")\n",
    "    for bt, cnt in sorted(by_bond.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {bt}: {cnt:,}\")\n",
    "\n",
    "    # Count by context\n",
    "    by_ctx = defaultdict(int)\n",
    "    for b in bonds:\n",
    "        by_ctx[b[\"context\"]] += 1\n",
    "    print(\"\\nBy context:\")\n",
    "    for ctx, cnt in sorted(by_ctx.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {ctx}: {cnt:,}\")\n",
    "\n",
    "    # ===== SAVE =====\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAVING DATA\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Save passages\n",
    "    with open(\"data/processed/passages.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for p in all_passages:\n",
    "            # Normalize field names\n",
    "            p_out = {\n",
    "                \"id\": p[\"id\"],\n",
    "                \"text\": p[\"text\"],\n",
    "                \"language\": p[\"lang\"],\n",
    "                \"source\": p[\"source\"],\n",
    "                \"time_period\": p[\"period\"],\n",
    "            }\n",
    "            f.write(json.dumps(p_out, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"  Saved {len(all_passages):,} passages to data/processed/passages.jsonl\")\n",
    "\n",
    "    # Save bonds\n",
    "    with open(\"data/processed/bonds.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for b in bonds:\n",
    "            b_out = {\n",
    "                **b,\n",
    "                \"bond_type\": (\n",
    "                    b[\"bond_type\"].name if hasattr(b[\"bond_type\"], \"name\") else str(b[\"bond_type\"])\n",
    "                ),\n",
    "            }\n",
    "            f.write(json.dumps(b_out, ensure_ascii=False) + chr(10))\n",
    "    print(f\"  Saved {len(bonds):,} bonds to data/processed/bonds.jsonl\")\n",
    "\n",
    "    # Copy to Drive if enabled\n",
    "    if USE_DRIVE_DATA and SAVE_DIR:\n",
    "        try:\n",
    "            os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "            shutil.copy(\"data/processed/passages.jsonl\", f\"{SAVE_DIR}/passages.jsonl\")\n",
    "            shutil.copy(\"data/processed/bonds.jsonl\", f\"{SAVE_DIR}/bonds.jsonl\")\n",
    "            print(f\"  Copied to Drive: {SAVE_DIR}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Drive copy failed: {e}\")\n",
    "\n",
    "    gc.collect()\n",
    "    print(\"\\nDone!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 5. Generate Splits { display-mode: \"form\" }\n",
    "# @markdown Creates train/test splits for cross-lingual experiments\n",
    "# @markdown v10.9: Added confucian_to_buddhist, confucian_to_legalist,\n",
    "# @markdown        all_to_sanskrit, semitic_to_indic, quran_to_fiqh\n",
    "\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATING SPLITS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if splits already exist from Drive\n",
    "# Check if splits are valid (IDs match current passages)\n",
    "splits_valid = False\n",
    "if os.path.exists(\"data/splits/all_splits.json\"):\n",
    "    try:\n",
    "        with open(\"data/splits/all_splits.json\") as f:\n",
    "            cached_splits = json.load(f)\n",
    "        # Get sample of IDs from splits\n",
    "        sample_ids = set()\n",
    "        for split in cached_splits.values():\n",
    "            sample_ids.update(split[\"train_ids\"][:100])\n",
    "            sample_ids.update(split[\"test_ids\"][:100])\n",
    "        # Check if they exist in current passages\n",
    "        passage_ids = set()\n",
    "        with open(\"data/processed/passages.jsonl\") as f:\n",
    "            for line in f:\n",
    "                p = json.loads(line)\n",
    "                passage_ids.add(p[\"id\"])\n",
    "                if len(passage_ids) > 10000:\n",
    "                    break\n",
    "        matches = len(sample_ids & passage_ids)\n",
    "        splits_valid = matches > len(sample_ids) * 0.9  # 90% match\n",
    "        if not splits_valid:\n",
    "            print(f\"Splits invalid: only {matches}/{len(sample_ids)} IDs match current passages\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating splits: {e}\")\n",
    "\n",
    "if splits_valid and not REFRESH_DATA_FROM_SOURCE:\n",
    "    print(\"\\nSplits already loaded from Drive\")\n",
    "    with open(\"data/splits/all_splits.json\") as f:\n",
    "        all_splits = json.load(f)\n",
    "    for name, split in all_splits.items():\n",
    "        print(f\"  {name}: train={split['train_size']:,}, test={split['test_size']:,}\")\n",
    "else:\n",
    "    random.seed(42)\n",
    "\n",
    "    # Read passage metadata\n",
    "    passage_meta = []\n",
    "    with open(\"data/processed/passages.jsonl\", \"r\") as f:\n",
    "        for line in f:\n",
    "            p = json.loads(line)\n",
    "            passage_meta.append(p)\n",
    "\n",
    "    print(f\"Total passages: {len(passage_meta):,}\")\n",
    "\n",
    "    by_lang = defaultdict(list)\n",
    "    by_period = defaultdict(list)\n",
    "    for p in passage_meta:\n",
    "        by_lang[p[\"language\"]].append(p[\"id\"])\n",
    "        by_period[p[\"time_period\"]].append(p[\"id\"])\n",
    "\n",
    "    print(\"\\nBy language:\")\n",
    "    for lang, ids in sorted(by_lang.items(), key=lambda x: -len(x[1])):\n",
    "        print(f\"  {lang}: {len(ids):,}\")\n",
    "\n",
    "    print(\"\\nBy period:\")\n",
    "    for period, ids in sorted(by_period.items(), key=lambda x: -len(x[1])):\n",
    "        print(f\"  {period}: {len(ids):,}\")\n",
    "\n",
    "    all_splits = {}\n",
    "    # ===== SPLIT 1: Hebrew -> Others =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 1: HEBREW -> OTHERS\")\n",
    "    hebrew_ids = by_lang.get(\"hebrew\", [])\n",
    "    other_ids = [p[\"id\"] for p in passage_meta if p[\"language\"] != \"hebrew\"]\n",
    "    random.shuffle(hebrew_ids)\n",
    "    random.shuffle(other_ids)\n",
    "\n",
    "    all_splits[\"hebrew_to_others\"] = {\n",
    "        \"train_ids\": hebrew_ids,\n",
    "        \"test_ids\": other_ids,\n",
    "        \"train_size\": len(hebrew_ids),\n",
    "        \"test_size\": len(other_ids),\n",
    "    }\n",
    "    print(f\"  Train (Hebrew): {len(hebrew_ids):,}\")\n",
    "    print(f\"  Test (Others): {len(other_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 2: Semitic -> Non-Semitic =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 2: SEMITIC -> NON-SEMITIC\")\n",
    "    semitic_ids = by_lang.get(\"hebrew\", []) + by_lang.get(\"aramaic\", []) + by_lang.get(\"arabic\", [])\n",
    "    non_semitic_ids = by_lang.get(\"classical_chinese\", []) + by_lang.get(\"english\", [])\n",
    "    random.shuffle(semitic_ids)\n",
    "    random.shuffle(non_semitic_ids)\n",
    "\n",
    "    all_splits[\"semitic_to_non_semitic\"] = {\n",
    "        \"train_ids\": semitic_ids,\n",
    "        \"test_ids\": non_semitic_ids,\n",
    "        \"train_size\": len(semitic_ids),\n",
    "        \"test_size\": len(non_semitic_ids),\n",
    "    }\n",
    "    print(f\"  Train (Semitic): {len(semitic_ids):,}\")\n",
    "    print(f\"  Test (Non-Semitic): {len(non_semitic_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 3: Ancient -> Modern =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 3: ANCIENT -> MODERN\")\n",
    "    # Define modern periods explicitly, derive ancient dynamically\n",
    "    modern_periods = {\"MODERN\", \"DEAR_ABBY\"}\n",
    "    all_periods = set(by_period.keys())\n",
    "    ancient_periods = all_periods - modern_periods\n",
    "\n",
    "    print(f\"  Ancient periods: {sorted(ancient_periods)}\")\n",
    "    print(f\"  Modern periods: {sorted(modern_periods)}\")\n",
    "\n",
    "    ancient_ids = [p[\"id\"] for p in passage_meta if p[\"time_period\"] in ancient_periods]\n",
    "    modern_ids = [p[\"id\"] for p in passage_meta if p[\"time_period\"] in modern_periods]\n",
    "    random.shuffle(ancient_ids)\n",
    "    random.shuffle(modern_ids)\n",
    "\n",
    "    all_splits[\"ancient_to_modern\"] = {\n",
    "        \"train_ids\": ancient_ids,\n",
    "        \"test_ids\": modern_ids,\n",
    "        \"train_size\": len(ancient_ids),\n",
    "        \"test_size\": len(modern_ids),\n",
    "    }\n",
    "    print(f\"  Train (Ancient): {len(ancient_ids):,}\")\n",
    "    print(f\"  Test (Modern): {len(modern_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 4: Mixed Baseline =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 4: MIXED BASELINE\")\n",
    "    all_ids = [p[\"id\"] for p in passage_meta]\n",
    "    random.shuffle(all_ids)\n",
    "    split_idx = int(0.7 * len(all_ids))\n",
    "\n",
    "    all_splits[\"mixed_baseline\"] = {\n",
    "        \"train_ids\": all_ids[:split_idx],\n",
    "        \"test_ids\": all_ids[split_idx:],\n",
    "        \"train_size\": split_idx,\n",
    "        \"test_size\": len(all_ids) - split_idx,\n",
    "    }\n",
    "    print(f\"  Train: {split_idx:,}\")\n",
    "    print(f\"  Test: {len(all_ids) - split_idx:,}\")\n",
    "\n",
    "    # ===== SPLIT 5: Dear Abby -> Classical Chinese =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 5: DEAR ABBY -> CHINESE\")\n",
    "    abby_ids = [p[\"id\"] for p in passage_meta if p[\"time_period\"] == \"DEAR_ABBY\"]\n",
    "    chinese_ids = [p[\"id\"] for p in passage_meta if p[\"language\"] == \"classical_chinese\"]\n",
    "    random.shuffle(abby_ids)\n",
    "    random.shuffle(chinese_ids)\n",
    "\n",
    "    all_splits[\"abby_to_chinese\"] = {\n",
    "        \"train_ids\": abby_ids,\n",
    "        \"test_ids\": chinese_ids,\n",
    "        \"train_size\": len(abby_ids),\n",
    "        \"test_size\": len(chinese_ids),\n",
    "    }\n",
    "    print(f\"  Train (Dear Abby): {len(abby_ids):,}\")\n",
    "    print(f\"  Test (Chinese): {len(chinese_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 6: Western Classical -> Eastern =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 6: WESTERN CLASSICAL -> EASTERN\")\n",
    "    western_ids = [p[\"id\"] for p in passage_meta if p[\"time_period\"] == \"WESTERN_CLASSICAL\"]\n",
    "    eastern_ids = [\n",
    "        p[\"id\"] for p in passage_meta if p[\"language\"] in (\"classical_chinese\", \"hebrew\")\n",
    "    ]\n",
    "    random.shuffle(western_ids)\n",
    "    random.shuffle(eastern_ids)\n",
    "\n",
    "    all_splits[\"western_to_eastern\"] = {\n",
    "        \"train_ids\": western_ids,\n",
    "        \"test_ids\": eastern_ids,\n",
    "        \"train_size\": len(western_ids),\n",
    "        \"test_size\": len(eastern_ids),\n",
    "    }\n",
    "    print(f\"  Train (Western - Plato, Aristotle, Stoics): {len(western_ids):,}\")\n",
    "    print(f\"  Test (Eastern - Chinese, Hebrew): {len(eastern_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 7: Confucian -> Buddhist (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 7: CONFUCIAN -> BUDDHIST (Chinese intra-tradition)\")\n",
    "    confucian_daoist_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] in (\"CONFUCIAN\", \"DAOIST\") and p[\"language\"] == \"classical_chinese\"\n",
    "    ]\n",
    "    buddhist_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] == \"BUDDHIST\" and p[\"language\"] == \"classical_chinese\"\n",
    "    ]\n",
    "    random.shuffle(confucian_daoist_ids)\n",
    "    random.shuffle(buddhist_ids)\n",
    "\n",
    "    all_splits[\"confucian_to_buddhist\"] = {\n",
    "        \"train_ids\": confucian_daoist_ids,\n",
    "        \"test_ids\": buddhist_ids,\n",
    "        \"train_size\": len(confucian_daoist_ids),\n",
    "        \"test_size\": len(buddhist_ids),\n",
    "        \"description\": \"Test if Chinese performance is tradition-specific\",\n",
    "    }\n",
    "    print(f\"  Train (Confucian+Daoist): {len(confucian_daoist_ids):,}\")\n",
    "    print(f\"  Test (Buddhist): {len(buddhist_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 8: Confucian -> Legalist/Mohist (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 8: CONFUCIAN -> LEGALIST/MOHIST (virtue vs consequentialist)\")\n",
    "    confucian_only_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] == \"CONFUCIAN\" and p[\"language\"] == \"classical_chinese\"\n",
    "    ]\n",
    "    legalist_mohist_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] in (\"LEGALIST\", \"MOHIST\") and p[\"language\"] == \"classical_chinese\"\n",
    "    ]\n",
    "    random.shuffle(confucian_only_ids)\n",
    "    random.shuffle(legalist_mohist_ids)\n",
    "\n",
    "    all_splits[\"confucian_to_legalist\"] = {\n",
    "        \"train_ids\": confucian_only_ids,\n",
    "        \"test_ids\": legalist_mohist_ids,\n",
    "        \"train_size\": len(confucian_only_ids),\n",
    "        \"test_size\": len(legalist_mohist_ids),\n",
    "        \"description\": \"Virtue ethics \u2192 consequentialist/legalist\",\n",
    "    }\n",
    "    print(f\"  Train (Confucian): {len(confucian_only_ids):,}\")\n",
    "    print(f\"  Test (Legalist+Mohist): {len(legalist_mohist_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 9: All -> Sanskrit/Pali (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 9: ALL -> SANSKRIT/PALI (ultimate transfer test)\")\n",
    "    non_indic_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"language\"] in (\"hebrew\", \"aramaic\", \"classical_chinese\", \"arabic\", \"english\")\n",
    "    ]\n",
    "    indic_ids = [p[\"id\"] for p in passage_meta if p[\"language\"] in (\"sanskrit\", \"pali\")]\n",
    "    random.shuffle(non_indic_ids)\n",
    "    random.shuffle(indic_ids)\n",
    "\n",
    "    all_splits[\"all_to_sanskrit\"] = {\n",
    "        \"train_ids\": non_indic_ids,\n",
    "        \"test_ids\": indic_ids,\n",
    "        \"train_size\": len(non_indic_ids),\n",
    "        \"test_size\": len(indic_ids),\n",
    "        \"description\": \"Ultimate transfer test: completely held-out language family\",\n",
    "    }\n",
    "    print(f\"  Train (non-Indic): {len(non_indic_ids):,}\")\n",
    "    print(f\"  Test (Sanskrit+Pali): {len(indic_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 10: Semitic -> Indic (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 10: SEMITIC -> INDIC\")\n",
    "    semitic_only_ids = [\n",
    "        p[\"id\"] for p in passage_meta if p[\"language\"] in (\"hebrew\", \"aramaic\", \"arabic\")\n",
    "    ]\n",
    "    indic_only_ids = [p[\"id\"] for p in passage_meta if p[\"language\"] in (\"sanskrit\", \"pali\")]\n",
    "    random.shuffle(semitic_only_ids)\n",
    "    random.shuffle(indic_only_ids)\n",
    "\n",
    "    all_splits[\"semitic_to_indic\"] = {\n",
    "        \"train_ids\": semitic_only_ids,\n",
    "        \"test_ids\": indic_only_ids,\n",
    "        \"train_size\": len(semitic_only_ids),\n",
    "        \"test_size\": len(indic_only_ids),\n",
    "        \"description\": \"Semitic \u2192 Indo-Aryan transfer\",\n",
    "    }\n",
    "    print(f\"  Train (Semitic): {len(semitic_only_ids):,}\")\n",
    "    print(f\"  Test (Indic): {len(indic_only_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 11: Quran -> Fiqh (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 11: QURAN -> FIQH (religious to legal/philosophical)\")\n",
    "    quranic_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] in (\"QURANIC\", \"HADITH\") and p[\"language\"] == \"arabic\"\n",
    "    ]\n",
    "    fiqh_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] in (\"FIQH\", \"SUFI\", \"FALSAFA\") and p[\"language\"] == \"arabic\"\n",
    "    ]\n",
    "    random.shuffle(quranic_ids)\n",
    "    random.shuffle(fiqh_ids)\n",
    "\n",
    "    all_splits[\"quran_to_fiqh\"] = {\n",
    "        \"train_ids\": quranic_ids,\n",
    "        \"test_ids\": fiqh_ids,\n",
    "        \"train_size\": len(quranic_ids),\n",
    "        \"test_size\": len(fiqh_ids),\n",
    "        \"description\": \"Religious \u2192 legal/philosophical Arabic\",\n",
    "    }\n",
    "    print(f\"  Train (Quranic+Hadith): {len(quranic_ids):,}\")\n",
    "    print(f\"  Test (Fiqh+Sufi+Falsafa): {len(fiqh_ids):,}\")\n",
    "\n",
    "    # Save splits\n",
    "    with open(\"data/splits/all_splits.json\", \"w\") as f:\n",
    "        json.dump(all_splits, f, indent=2)\n",
    "\n",
    "    # Save to Drive\n",
    "    shutil.copy(\"data/splits/all_splits.json\", f\"{SAVE_DIR}/all_splits.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Splits saved to local and Drive\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 6. Model Architecture { display-mode: \"form\" }\n",
    "# @markdown BIP v10.9 model with configurable backbone and adversarial heads\n",
    "# @markdown - Updated: 8 languages, 26 periods\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Backbone: {BACKBONE} ({MODEL_NAME})\")\n",
    "print(f\"Hidden size: {BACKBONE_HIDDEN}\")\n",
    "\n",
    "# Index mappings\n",
    "BOND_TO_IDX = {bt.name: i for i, bt in enumerate(BondType)}\n",
    "IDX_TO_BOND = {i: bt.name for i, bt in enumerate(BondType)}\n",
    "# v10.9: 8 languages (added Sanskrit, Pali, Greek placeholder)\n",
    "LANG_TO_IDX = {\n",
    "    \"hebrew\": 0,\n",
    "    \"aramaic\": 1,\n",
    "    \"classical_chinese\": 2,\n",
    "    \"arabic\": 3,\n",
    "    \"english\": 4,\n",
    "    \"sanskrit\": 5,  # NEW in v10.9\n",
    "    \"pali\": 6,  # NEW in v10.9\n",
    "    \"greek\": 7,  # FUTURE (placeholder)\n",
    "}\n",
    "IDX_TO_LANG = {i: l for l, i in LANG_TO_IDX.items()}\n",
    "\n",
    "# v10.9: 26 periods (expanded Chinese, Arabic, added Sanskrit/Pali traditions)\n",
    "PERIOD_TO_IDX = {\n",
    "    # Semitic traditions\n",
    "    \"BIBLICAL\": 0,\n",
    "    \"TANNAITIC\": 1,\n",
    "    \"AMORAIC\": 2,\n",
    "    \"RISHONIM\": 3,\n",
    "    \"ACHRONIM\": 4,\n",
    "    # Chinese traditions (expanded)\n",
    "    \"CONFUCIAN\": 5,\n",
    "    \"DAOIST\": 6,\n",
    "    \"MOHIST\": 7,  # NEW in v10.9\n",
    "    \"LEGALIST\": 8,  # NEW in v10.9\n",
    "    \"BUDDHIST\": 9,  # NEW in v10.9 (Chinese Buddhism)\n",
    "    \"NEO_CONFUCIAN\": 10,  # NEW in v10.9\n",
    "    # Arabic/Islamic traditions (expanded)\n",
    "    \"QURANIC\": 11,\n",
    "    \"HADITH\": 12,\n",
    "    \"FIQH\": 13,  # NEW in v10.9 (Islamic jurisprudence)\n",
    "    \"SUFI\": 14,  # NEW in v10.9\n",
    "    \"FALSAFA\": 15,  # NEW in v10.9 (Arabic philosophy)\n",
    "    # Sanskrit/Pali traditions (NEW in v10.9)\n",
    "    \"DHARMA\": 16,  # Dharmashastra\n",
    "    \"UPANISHAD\": 17,\n",
    "    \"GITA\": 18,\n",
    "    \"ARTHA\": 19,  # Arthashastra\n",
    "    \"PALI\": 20,  # Pali Canon\n",
    "    # Western traditions\n",
    "    \"WESTERN_CLASSICAL\": 21,\n",
    "    \"MEDIEVAL\": 22,\n",
    "    # Modern\n",
    "    \"DEAR_ABBY\": 23,\n",
    "    \"MODERN\": 24,\n",
    "    \"CLASSICAL\": 25,  # Generic classical (fallback)\n",
    "}  # 26 periods total (0-25)\n",
    "IDX_TO_PERIOD = {i: p for p, i in PERIOD_TO_IDX.items()}\n",
    "HOHFELD_TO_IDX = {hs.name: i for i, hs in enumerate(HohfeldState)}\n",
    "IDX_TO_HOHFELD = {i: hs.name for i, hs in enumerate(HohfeldState)}\n",
    "CONTEXT_TO_IDX = {\"prescriptive\": 0, \"descriptive\": 1, \"unknown\": 2}\n",
    "IDX_TO_CONTEXT = {i: c for c, i in CONTEXT_TO_IDX.items()}\n",
    "\n",
    "\n",
    "def get_confidence_weight(conf):\n",
    "    \"\"\"Map confidence to sample weight. Handles both string ('high'/'medium'/'low') and numeric (0.0-1.0) values.\"\"\"\n",
    "    if isinstance(conf, str):\n",
    "        return {\"high\": 2.0, \"medium\": 1.0, \"low\": 0.5}.get(conf, 1.0)\n",
    "    elif isinstance(conf, (int, float)):\n",
    "        return 2.0 if conf >= 0.8 else 1.0\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "class GradientReversalLayer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.alpha, None\n",
    "\n",
    "\n",
    "class BIPModel(nn.Module):\n",
    "    def __init__(self, model_name=None, hidden_size=None, z_dim=64):\n",
    "        super().__init__()\n",
    "        # Use global config if not specified\n",
    "        model_name = model_name or MODEL_NAME\n",
    "        hidden_size = hidden_size or BACKBONE_HIDDEN\n",
    "\n",
    "        print(f\"  Loading encoder: {model_name}\")\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Get actual hidden size from model config\n",
    "        actual_hidden = self.encoder.config.hidden_size\n",
    "        if actual_hidden != hidden_size:\n",
    "            print(f\"  Note: Using actual hidden size {actual_hidden}\")\n",
    "            hidden_size = actual_hidden\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Projection to z_bond space (scales with backbone size)\n",
    "        proj_hidden = min(512, hidden_size)\n",
    "        self.z_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_size, proj_hidden),\n",
    "            nn.LayerNorm(proj_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(proj_hidden, z_dim),\n",
    "        )\n",
    "\n",
    "        # Task heads\n",
    "        self.bond_head = nn.Linear(z_dim, len(BondType))\n",
    "        self.hohfeld_head = nn.Linear(z_dim, len(HohfeldState))\n",
    "\n",
    "        # Adversarial heads\n",
    "        self.language_head = nn.Linear(z_dim, len(LANG_TO_IDX))\n",
    "        self.period_head = nn.Linear(z_dim, len(PERIOD_TO_IDX))\n",
    "\n",
    "        # Context prediction head (auxiliary task)\n",
    "        self.context_head = nn.Linear(z_dim, len(CONTEXT_TO_IDX))\n",
    "\n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"  Total params: {total_params:,}\")\n",
    "        print(f\"  Trainable: {trainable_params:,}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, adv_lambda=1.0):\n",
    "        enc = self.encoder(input_ids, attention_mask)\n",
    "\n",
    "        # Handle different pooling strategies\n",
    "        if hasattr(enc, \"pooler_output\") and enc.pooler_output is not None:\n",
    "            pooled = enc.pooler_output\n",
    "        else:\n",
    "            pooled = enc.last_hidden_state[:, 0]\n",
    "\n",
    "        z = self.z_proj(pooled)\n",
    "\n",
    "        # Bond prediction (main task)\n",
    "        bond_pred = self.bond_head(z)\n",
    "        hohfeld_pred = self.hohfeld_head(z)\n",
    "\n",
    "        # Adversarial predictions (gradient reversal)\n",
    "        z_rev = GradientReversalLayer.apply(z, adv_lambda)\n",
    "        language_pred = self.language_head(z_rev)\n",
    "        period_pred = self.period_head(z_rev)\n",
    "\n",
    "        return {\n",
    "            \"bond_pred\": bond_pred,\n",
    "            \"hohfeld_pred\": hohfeld_pred,\n",
    "            \"language_pred\": language_pred,\n",
    "            \"period_pred\": period_pred,\n",
    "            \"context_pred\": self.context_head(z),\n",
    "            \"z\": z,\n",
    "        }\n",
    "\n",
    "    def get_bond_embedding(self, input_ids, attention_mask):\n",
    "        \"\"\"Get z_bond embedding for geometric analysis.\"\"\"\n",
    "        enc = self.encoder(input_ids, attention_mask)\n",
    "        if hasattr(enc, \"pooler_output\") and enc.pooler_output is not None:\n",
    "            pooled = enc.pooler_output\n",
    "        else:\n",
    "            pooled = enc.last_hidden_state[:, 0]\n",
    "        return self.z_proj(pooled)\n",
    "\n",
    "\n",
    "# Initialize tokenizer for selected backbone\n",
    "print(f\"\\nLoading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "\n",
    "# Dataset with Hohfeld support\n",
    "class NativeDataset(Dataset):\n",
    "    def __init__(self, ids_set, passages_file, bonds_file, tokenizer, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = []\n",
    "\n",
    "        bonds_by_id = {}\n",
    "        with open(bonds_file) as fb:\n",
    "            for line in fb:\n",
    "                b = json.loads(line)\n",
    "                bonds_by_id[b[\"passage_id\"]] = b\n",
    "\n",
    "        with open(passages_file) as fp:\n",
    "            for line in tqdm(fp, desc=\"Loading\", unit=\"line\"):\n",
    "                p = json.loads(line)\n",
    "                if p[\"id\"] in ids_set and p[\"id\"] in bonds_by_id:\n",
    "                    b = bonds_by_id[p[\"id\"]]\n",
    "                    self.data.append(\n",
    "                        {\n",
    "                            \"text\": p[\"text\"][:1000],\n",
    "                            \"language\": p[\"language\"],\n",
    "                            \"period\": p[\"time_period\"],\n",
    "                            \"bond\": b.get(\"bond_type\") or b.get(\"bonds\", {}).get(\"primary_bond\"),\n",
    "                            \"hohfeld\": None,\n",
    "                            \"context\": b.get(\"context\")\n",
    "                            or b.get(\"bonds\", {}).get(\"context\", \"unknown\"),\n",
    "                            \"confidence\": b.get(\"confidence\")\n",
    "                            or b.get(\"bonds\", {}).get(\"confidence\", \"medium\"),\n",
    "                        }\n",
    "                    )\n",
    "        print(f\"  Loaded {len(self.data):,} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        enc = self.tokenizer(\n",
    "            item[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"bond_label\": BOND_TO_IDX.get(item[\"bond\"], 9),\n",
    "            \"language_label\": LANG_TO_IDX.get(item[\"language\"], 4),\n",
    "            \"period_label\": PERIOD_TO_IDX.get(item[\"period\"], 9),\n",
    "            \"hohfeld_label\": HOHFELD_TO_IDX.get(item[\"hohfeld\"], 0) if item[\"hohfeld\"] else 0,\n",
    "            \"context_label\": CONTEXT_TO_IDX.get(item[\"context\"], 2),\n",
    "            \"sample_weight\": get_confidence_weight(item[\"confidence\"]),\n",
    "            \"language\": item[\"language\"],\n",
    "            \"context\": item[\"context\"],\n",
    "            \"confidence\": item[\"confidence\"],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
    "        \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n",
    "        \"bond_labels\": torch.tensor([x[\"bond_label\"] for x in batch]),\n",
    "        \"language_labels\": torch.tensor([x[\"language_label\"] for x in batch]),\n",
    "        \"period_labels\": torch.tensor([x[\"period_label\"] for x in batch]),\n",
    "        \"hohfeld_labels\": torch.tensor([x[\"hohfeld_label\"] for x in batch]),\n",
    "        \"context_labels\": torch.tensor([x[\"context_label\"] for x in batch]),\n",
    "        \"sample_weights\": torch.tensor([x[\"sample_weight\"] for x in batch], dtype=torch.float),\n",
    "        \"languages\": [x[\"language\"] for x in batch],\n",
    "        \"contexts\": [x[\"context\"] for x in batch],\n",
    "        \"confidences\": [x[\"confidence\"] for x in batch],\n",
    "    }\n",
    "\n",
    "\n",
    "print(f\"\\nArchitecture ready for {BACKBONE}\")\n",
    "print(f\"  Bond classes: {len(BondType)}\")\n",
    "print(f\"  Languages: {len(LANG_TO_IDX)}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 7. Train BIP Model { display-mode: \"form\" }\n",
    "# @markdown Training with tuned adversarial weights and hardware-optimized parameters\n",
    "# @markdown v10.9: Added new splits (confucian_to_buddhist, all_to_sanskrit, etc.)\n",
    "\n",
    "# ===== SUPPRESS DATALOADER MULTIPROCESSING WARNINGS =====\n",
    "# These occur during garbage collection and bypass normal exception handling\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "\n",
    "# Method 1: Filter warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*can only test a child process.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.data\")\n",
    "\n",
    "# Method 2: Suppress logging\n",
    "logging.getLogger(\"torch.utils.data.dataloader\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "# Method 3: Redirect stderr during DataLoader cleanup (most effective)\n",
    "class StderrFilter(io.TextIOWrapper):\n",
    "    \"\"\"Filters out DataLoader multiprocessing cleanup messages from stderr\"\"\"\n",
    "\n",
    "    def __init__(self, original):\n",
    "        self.original = original\n",
    "        self.buffer_lines = []\n",
    "\n",
    "    def write(self, text):\n",
    "        # Filter out the specific error patterns\n",
    "        skip_patterns = [\n",
    "            \"can only test a child process\",\n",
    "            \"_MultiProcessingDataLoaderIter.__del__\",\n",
    "            \"_shutdown_workers\",\n",
    "            \"Exception ignored in:\",\n",
    "            \"w.is_alive()\",\n",
    "        ]\n",
    "        # Buffer multi-line error messages\n",
    "        if any(p in text for p in skip_patterns):\n",
    "            return len(text)  # Pretend we wrote it\n",
    "        # Also skip if it looks like part of a traceback for these errors\n",
    "        if text.strip().startswith(\"^\") and len(text.strip()) < 80:\n",
    "            return len(text)\n",
    "        if text.strip().startswith('File \"/usr') and \"dataloader.py\" in text:\n",
    "            return len(text)\n",
    "        if text.strip() == \"Traceback (most recent call last):\":\n",
    "            self.buffer_lines = [text]\n",
    "            return len(text)\n",
    "        if self.buffer_lines:\n",
    "            self.buffer_lines.append(text)\n",
    "            # Check if this is the DataLoader error traceback\n",
    "            full_msg = \"\".join(self.buffer_lines)\n",
    "            if any(p in full_msg for p in skip_patterns):\n",
    "                self.buffer_lines = []\n",
    "                return len(text)\n",
    "            # After 10 lines, flush if not the target error\n",
    "            if len(self.buffer_lines) > 10:\n",
    "                for line in self.buffer_lines:\n",
    "                    self.original.write(line)\n",
    "                self.buffer_lines = []\n",
    "        return self.original.write(text)\n",
    "\n",
    "    def flush(self):\n",
    "        if self.buffer_lines:\n",
    "            # Flush any remaining buffered content\n",
    "            for line in self.buffer_lines:\n",
    "                self.original.write(line)\n",
    "            self.buffer_lines = []\n",
    "        self.original.flush()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.original, name)\n",
    "\n",
    "\n",
    "# Install the stderr filter\n",
    "_original_stderr = sys.stderr\n",
    "sys.stderr = StderrFilter(_original_stderr)\n",
    "\n",
    "# Method 4: Patch the DataLoader cleanup function directly\n",
    "try:\n",
    "    import torch.utils.data.dataloader as dl_module\n",
    "\n",
    "    _original_del = dl_module._MultiProcessingDataLoaderIter.__del__\n",
    "\n",
    "    def _patched_del(self):\n",
    "        try:\n",
    "            _original_del(self)\n",
    "        except (AssertionError, AttributeError, RuntimeError):\n",
    "            pass  # Silently ignore cleanup errors\n",
    "\n",
    "    dl_module._MultiProcessingDataLoaderIter.__del__ = _patched_del\n",
    "except Exception:\n",
    "    pass  # If patching fails, the stderr filter will still work\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import gc\n",
    "\n",
    "\n",
    "# @markdown **Splits to train:**\n",
    "TRAIN_HEBREW_TO_OTHERS = True  # @param {type:\"boolean\"}\n",
    "TRAIN_SEMITIC_TO_NON_SEMITIC = True  # @param {type:\"boolean\"}\n",
    "TRAIN_ANCIENT_TO_MODERN = True  # @param {type:\"boolean\"}\n",
    "TRAIN_MIXED_BASELINE = True  # @param {type:\"boolean\"}\n",
    "TRAIN_ABBY_TO_CHINESE = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown **v10.9 New Splits:**\n",
    "TRAIN_CONFUCIAN_TO_BUDDHIST = True  # @param {type:\"boolean\"}\n",
    "TRAIN_CONFUCIAN_TO_LEGALIST = True  # @param {type:\"boolean\"}\n",
    "TRAIN_ALL_TO_SANSKRIT = True  # @param {type:\"boolean\"}\n",
    "TRAIN_SEMITIC_TO_INDIC = True  # @param {type:\"boolean\"}\n",
    "TRAIN_QURAN_TO_FIQH = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown **Hyperparameters:**\n",
    "LANG_WEIGHT = 0.1  # @param {type:\"number\"}\n",
    "PERIOD_WEIGHT = 0.066  # @param {type:\"number\"}\n",
    "N_EPOCHS = 10  # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown **Context-Aware Training:**\n",
    "USE_CONFIDENCE_WEIGHTING = True  # @param {type:\"boolean\"}\n",
    "# @markdown Weight prescriptive (high confidence) examples 2x in loss\n",
    "\n",
    "USE_CONTEXT_AUXILIARY = True  # @param {type:\"boolean\"}\n",
    "# @markdown Add context prediction as auxiliary training target\n",
    "\n",
    "CONTEXT_LOSS_WEIGHT = 0.33  # @param {type:\"number\"}\n",
    "# @markdown Weight for context prediction loss\n",
    "\n",
    "STRICT_PRESCRIPTIVE_TEST = False  # @param {type:\"boolean\"}\n",
    "# @markdown Only evaluate on prescriptive examples (reduces test set ~97%!)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING BIP MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSettings:\")\n",
    "print(f\"  Backbone:     {BACKBONE}\")\n",
    "print(f\"  GPU Tier:     {GPU_TIER}\")\n",
    "print(f\"  Batch size:   {BATCH_SIZE}\")\n",
    "print(f\"  Workers:      {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate: {LR:.2e}\")\n",
    "print(f\"  Adv weights:  lang={LANG_WEIGHT}, period={PERIOD_WEIGHT}\")\n",
    "print(\"(0.01 prevents loss explosion while maintaining invariance)\")\n",
    "print(f\"  Confidence weighting: {USE_CONFIDENCE_WEIGHTING}\")\n",
    "print(f\"  Context auxiliary: {USE_CONTEXT_AUXILIARY} (weight={CONTEXT_LOSS_WEIGHT})\")\n",
    "print(f\"  Strict prescriptive test: {STRICT_PRESCRIPTIVE_TEST}\")\n",
    "\n",
    "# tokenizer loaded in Cell 6 based on BACKBONE selection\n",
    "\n",
    "with open(\"data/splits/all_splits.json\") as f:\n",
    "    all_splits = json.load(f)\n",
    "\n",
    "splits_to_train = []\n",
    "if TRAIN_HEBREW_TO_OTHERS:\n",
    "    splits_to_train.append(\"hebrew_to_others\")\n",
    "if TRAIN_SEMITIC_TO_NON_SEMITIC:\n",
    "    splits_to_train.append(\"semitic_to_non_semitic\")\n",
    "if TRAIN_ANCIENT_TO_MODERN:\n",
    "    splits_to_train.append(\"ancient_to_modern\")\n",
    "if TRAIN_MIXED_BASELINE:\n",
    "    splits_to_train.append(\"mixed_baseline\")\n",
    "if TRAIN_ABBY_TO_CHINESE:\n",
    "    splits_to_train.append(\"abby_to_chinese\")\n",
    "# v10.9 new splits\n",
    "if TRAIN_CONFUCIAN_TO_BUDDHIST:\n",
    "    splits_to_train.append(\"confucian_to_buddhist\")\n",
    "if TRAIN_CONFUCIAN_TO_LEGALIST:\n",
    "    splits_to_train.append(\"confucian_to_legalist\")\n",
    "if TRAIN_ALL_TO_SANSKRIT:\n",
    "    splits_to_train.append(\"all_to_sanskrit\")\n",
    "if TRAIN_SEMITIC_TO_INDIC:\n",
    "    splits_to_train.append(\"semitic_to_indic\")\n",
    "if TRAIN_QURAN_TO_FIQH:\n",
    "    splits_to_train.append(\"quran_to_fiqh\")\n",
    "\n",
    "print(f\"\\nTraining {len(splits_to_train)} splits: {splits_to_train}\")\n",
    "\n",
    "all_results = {}\n",
    "MIN_TEST_SIZE = 100  # Lowered to allow smaller test sets like Chinese\n",
    "\n",
    "for split_idx, split_name in enumerate(splits_to_train):\n",
    "    split_start = time.time()\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"[{split_idx+1}/{len(splits_to_train)}] {split_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    split = all_splits[split_name]\n",
    "    print(f\"Train: {split['train_size']:,} | Test: {split['test_size']:,}\")\n",
    "\n",
    "    if split[\"test_size\"] < MIN_TEST_SIZE:\n",
    "        print(f\"WARNING: Test set only {split['test_size']} samples (need {MIN_TEST_SIZE})\")\n",
    "        print(\"Skipping this split - results would be unreliable\")\n",
    "        print(\"To fix: Add more data to the test languages/periods\")\n",
    "        continue\n",
    "\n",
    "    # Create model with OOM recovery\n",
    "    def create_model_with_retry():\n",
    "        \"\"\"Create model, cleaning up GPU memory if OOM occurs.\"\"\"\n",
    "        try:\n",
    "            return BIPModel().to(device)\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(\"  OOM on model creation - cleaning up and retrying...\")\n",
    "            # Clean up any existing model in globals\n",
    "            _g = globals()\n",
    "            for _var in [\"model\", \"analyzer\", \"encoder\"]:\n",
    "                if _var in _g and _g[_var] is not None:\n",
    "                    try:\n",
    "                        if hasattr(_g[_var], \"cpu\"):\n",
    "                            _g[_var].cpu()\n",
    "                        _g[_var] = None\n",
    "                    except:\n",
    "                        pass\n",
    "            # Force cleanup\n",
    "            gc.collect()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            # Retry\n",
    "            return BIPModel().to(device)\n",
    "\n",
    "    model = create_model_with_retry()\n",
    "\n",
    "    train_dataset = NativeDataset(\n",
    "        set(split[\"train_ids\"]),\n",
    "        \"data/processed/passages.jsonl\",\n",
    "        \"data/processed/bonds.jsonl\",\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    test_ids_to_use = split[\"test_ids\"][:MAX_TEST_SAMPLES]\n",
    "\n",
    "    # Optional: strict prescriptive-only test\n",
    "    if STRICT_PRESCRIPTIVE_TEST:\n",
    "        print(\"Filtering to prescriptive examples only...\")\n",
    "        # Load bonds to filter\n",
    "        prescriptive_ids = set()\n",
    "        with open(\"data/processed/bonds.jsonl\") as f:\n",
    "            for line in f:\n",
    "                b = json.loads(line)\n",
    "                if b.get(\"context\") == \"prescriptive\":\n",
    "                    prescriptive_ids.add(b[\"passage_id\"])\n",
    "        test_ids_to_use = [tid for tid in test_ids_to_use if tid in prescriptive_ids]\n",
    "        print(f\"  Filtered to {len(test_ids_to_use):,} prescriptive samples\")\n",
    "\n",
    "    test_dataset = NativeDataset(\n",
    "        set(test_ids_to_use),\n",
    "        \"data/processed/passages.jsonl\",\n",
    "        \"data/processed/bonds.jsonl\",\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"ERROR: No training data!\")\n",
    "        continue\n",
    "\n",
    "    # Use hardware-optimized batch size\n",
    "    actual_batch = min(BATCH_SIZE, max(32, len(train_dataset) // 20))\n",
    "    print(f\"Actual batch size: {actual_batch}\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=actual_batch,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=actual_batch * 2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "    def get_adv_lambda(epoch, warmup=3):\n",
    "        \"\"\"Ramp adversarial strength: 0.1 -> 1.0 over warmup, then hold at 1.0\"\"\"\n",
    "        if epoch <= warmup:\n",
    "            return 0.1 + 0.9 * (epoch / warmup)\n",
    "        return 1.0\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    start_epoch = 1\n",
    "\n",
    "    # Check for existing checkpoint to resume from\n",
    "    checkpoint_path = f\"models/checkpoints/latest_{split_name}.pt\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"  Found checkpoint, resuming...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        best_loss = checkpoint[\"best_loss\"]\n",
    "        print(f\"  Resuming from epoch {start_epoch}, best_loss={best_loss:.4f}\")\n",
    "\n",
    "    for epoch in range(start_epoch, N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            bond_labels = batch[\"bond_labels\"].to(device)\n",
    "            language_labels = batch[\"language_labels\"].to(device)\n",
    "            period_labels = batch[\"period_labels\"].to(device)\n",
    "\n",
    "            adv_lambda = get_adv_lambda(epoch)\n",
    "\n",
    "            # Use new autocast API\n",
    "            with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n",
    "                out = model(input_ids, attention_mask, adv_lambda=adv_lambda)\n",
    "\n",
    "                # Weighted bond loss\n",
    "                if USE_CONFIDENCE_WEIGHTING:\n",
    "                    sample_weights = batch[\"sample_weights\"].to(device)\n",
    "                    loss_bond = F.cross_entropy(out[\"bond_pred\"], bond_labels, reduction=\"none\")\n",
    "                    loss_bond = (loss_bond * sample_weights).mean()\n",
    "                else:\n",
    "                    loss_bond = F.cross_entropy(out[\"bond_pred\"], bond_labels)\n",
    "\n",
    "                # Context auxiliary loss\n",
    "                if USE_CONTEXT_AUXILIARY:\n",
    "                    context_labels = batch[\"context_labels\"].to(device)\n",
    "                    loss_context = F.cross_entropy(out[\"context_pred\"], context_labels)\n",
    "                else:\n",
    "                    loss_context = 0\n",
    "\n",
    "                loss_lang = F.cross_entropy(out[\"language_pred\"], language_labels)\n",
    "                loss_period = F.cross_entropy(out[\"period_pred\"], period_labels)\n",
    "\n",
    "            loss = (\n",
    "                loss_bond\n",
    "                + LANG_WEIGHT * loss_lang\n",
    "                + PERIOD_WEIGHT * loss_period\n",
    "                + CONTEXT_LOSS_WEIGHT * loss_context\n",
    "            )\n",
    "\n",
    "            if USE_AMP and scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "            # Delete intermediate tensors to prevent memory accumulation\n",
    "            del input_ids, attention_mask, bond_labels, language_labels, period_labels\n",
    "            del out, loss, loss_bond, loss_lang, loss_period\n",
    "            if USE_CONFIDENCE_WEIGHTING:\n",
    "                del sample_weights\n",
    "            if USE_CONTEXT_AUXILIARY:\n",
    "                del context_labels, loss_context\n",
    "\n",
    "        avg_loss = total_loss / n_batches\n",
    "\n",
    "        # Aggressive memory cleanup after each epoch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            mem_alloc = torch.cuda.memory_allocated() / 1e9\n",
    "            mem_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "            print(\n",
    "                f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f}) [GPU: {mem_alloc:.1f}GB alloc, {mem_reserved:.1f}GB reserved]\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f})\")\n",
    "\n",
    "        # Save checkpoint every epoch (for crash recovery)\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"loss\": avg_loss,\n",
    "            \"best_loss\": best_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"models/checkpoints/latest_{split_name}.pt\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), f\"models/checkpoints/best_{split_name}.pt\")\n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/best_{split_name}.pt\")\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.load_state_dict(torch.load(f\"models/checkpoints/best_{split_name}.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = {\"bond\": [], \"lang\": []}\n",
    "    all_labels = {\"bond\": [], \"lang\": []}\n",
    "    all_languages = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            out = model(batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), 0)\n",
    "            all_preds[\"bond\"].extend(out[\"bond_pred\"].argmax(-1).cpu().tolist())\n",
    "            all_preds[\"lang\"].extend(out[\"language_pred\"].argmax(-1).cpu().tolist())\n",
    "            all_labels[\"bond\"].extend(batch[\"bond_labels\"].tolist())\n",
    "            all_labels[\"lang\"].extend(batch[\"language_labels\"].tolist())\n",
    "            all_languages.extend(batch[\"languages\"])\n",
    "\n",
    "    bond_f1 = f1_score(all_labels[\"bond\"], all_preds[\"bond\"], average=\"macro\", zero_division=0)\n",
    "    bond_acc = sum(p == l for p, l in zip(all_preds[\"bond\"], all_labels[\"bond\"])) / len(\n",
    "        all_preds[\"bond\"]\n",
    "    )\n",
    "    lang_acc = sum(p == l for p, l in zip(all_preds[\"lang\"], all_labels[\"lang\"])) / len(\n",
    "        all_preds[\"lang\"]\n",
    "    )\n",
    "\n",
    "    # Per-language F1\n",
    "    lang_f1 = {}\n",
    "    for lang in set(all_languages):\n",
    "        mask = [l == lang for l in all_languages]\n",
    "        if sum(mask) > 10:\n",
    "            preds = [p for p, m in zip(all_preds[\"bond\"], mask) if m]\n",
    "            labels = [l for l, m in zip(all_labels[\"bond\"], mask) if m]\n",
    "            lang_f1[lang] = {\n",
    "                \"f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "                \"n\": sum(mask),\n",
    "            }\n",
    "\n",
    "    all_results[split_name] = {\n",
    "        \"bond_f1_macro\": bond_f1,\n",
    "        \"bond_acc\": bond_acc,\n",
    "        \"language_acc\": lang_acc,\n",
    "        \"per_language_f1\": lang_f1,\n",
    "        \"training_time\": time.time() - split_start,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{split_name} RESULTS:\")\n",
    "    print(f\"  Bond F1 (macro): {bond_f1:.3f} ({bond_f1/0.1:.1f}x chance)\")\n",
    "    print(f\"  Bond accuracy:   {bond_acc:.1%}\")\n",
    "    print(f\"  Language acc:    {lang_acc:.1%} (want ~20% = invariant)\")\n",
    "    print(\"  Per-language:\")\n",
    "    for lang, m in sorted(lang_f1.items(), key=lambda x: -x[1][\"n\"]):\n",
    "        print(f\"    {lang:20s}: F1={m['f1']:.3f} (n={m['n']:,})\")\n",
    "\n",
    "    # Context analysis\n",
    "    high_conf = sum(1 for c in test_dataset.data if c[\"confidence\"] == \"high\")\n",
    "    prescriptive = sum(1 for c in test_dataset.data if c[\"context\"] == \"prescriptive\")\n",
    "    print(\n",
    "        f\"  Context: {prescriptive:,}/{len(test_dataset):,} prescriptive ({prescriptive/len(test_dataset)*100:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  High confidence: {high_conf:,}/{len(test_dataset):,} ({high_conf/len(test_dataset)*100:.1f}%)\"\n",
    "    )\n",
    "\n",
    "    # GPU memory usage before cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        mem = torch.cuda.memory_allocated() / 1e9\n",
    "        print(\n",
    "            f\"\\n  GPU memory (before cleanup): {mem:.1f} GB / {VRAM_GB:.1f} GB ({mem/VRAM_GB*100:.0f}%)\"\n",
    "        )\n",
    "\n",
    "    # Aggressive memory cleanup between splits\n",
    "    # Step 1: Move model to CPU to release GPU memory\n",
    "    model.cpu()\n",
    "\n",
    "    # Step 2: Delete all references\n",
    "    del model, train_dataset, test_dataset, train_loader, test_loader, optimizer\n",
    "    if USE_AMP and scaler:\n",
    "        del scaler\n",
    "\n",
    "    # Step 3: Force garbage collection (multiple passes)\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "\n",
    "    # Step 4: Clear CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Step 5: Re-create scaler for next split\n",
    "    if USE_AMP:\n",
    "        scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "    # GPU memory after cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"  GPU memory (after cleanup): {mem_after:.1f} GB (freed {mem - mem_after:.1f} GB)\")\n",
    "        if mem_after > 1.0:\n",
    "            print(f\"  WARNING: {mem_after:.1f} GB still allocated - may cause OOM on next split\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 8. Geometric Analysis & Linear Probe { display-mode: \"form\" }\n",
    "# @markdown v10.9: New geometric analysis module + linear probe test\n",
    "# @markdown Tests latent space structure (axis discovery, role swap analysis)\n",
    "# @markdown Tests if z_bond encodes language/period (should be low = invariant)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "# ===== v10.9: GEOMETRIC ANALYZER CLASS =====\n",
    "class GeometricAnalyzer:\n",
    "    \"\"\"\n",
    "    Probe the latent space geometry to discover moral structure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        inputs = self.tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, max_length=128, padding=\"max_length\"\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        z = self.model.get_bond_embedding(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        return z.cpu().numpy().flatten()\n",
    "\n",
    "    def find_direction(self, positive_texts: List[str], negative_texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Find the direction in z-space that separates two concepts.\n",
    "        E.g., obligation vs permission, harm vs care.\n",
    "        \"\"\"\n",
    "        pos_embs = np.array([self.get_embedding(t) for t in positive_texts])\n",
    "        neg_embs = np.array([self.get_embedding(t) for t in negative_texts])\n",
    "\n",
    "        pos_mean = pos_embs.mean(axis=0)\n",
    "        neg_mean = neg_embs.mean(axis=0)\n",
    "\n",
    "        direction = pos_mean - neg_mean\n",
    "        direction = direction / (np.linalg.norm(direction) + 1e-9)\n",
    "        return direction\n",
    "\n",
    "    def test_direction_transfer(\n",
    "        self, direction: np.ndarray, test_pairs: List[Tuple[str, str]]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Test if a direction generalizes to new examples.\n",
    "        Returns accuracy of direction-based classification.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for pos_text, neg_text in test_pairs:\n",
    "            pos_proj = np.dot(self.get_embedding(pos_text), direction)\n",
    "            neg_proj = np.dot(self.get_embedding(neg_text), direction)\n",
    "            scores.append(1.0 if pos_proj > neg_proj else 0.0)\n",
    "        return np.mean(scores)\n",
    "\n",
    "    def pca_on_pairs(self, concept_pairs: Dict[str, List[Tuple[str, str]]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Run PCA on difference vectors to find dominant axes.\n",
    "\n",
    "        concept_pairs: {\"obligation_permission\": [(obl1, perm1), ...], ...}\n",
    "        \"\"\"\n",
    "        all_diffs = []\n",
    "        labels = []\n",
    "\n",
    "        for concept, pairs in concept_pairs.items():\n",
    "            for pos, neg in pairs:\n",
    "                diff = self.get_embedding(pos) - self.get_embedding(neg)\n",
    "                all_diffs.append(diff)\n",
    "                labels.append(concept)\n",
    "\n",
    "        X = np.array(all_diffs)\n",
    "\n",
    "        pca = PCA(n_components=min(10, len(X)))\n",
    "        pca.fit(X)\n",
    "\n",
    "        return {\n",
    "            \"components\": pca.components_,\n",
    "            \"explained_variance_ratio\": pca.explained_variance_ratio_,\n",
    "            \"labels\": labels,\n",
    "            \"transformed\": pca.transform(X),\n",
    "        }\n",
    "\n",
    "    def role_swap_analysis(self, agent_patient_pairs: List[Tuple[str, str]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Test if swapping agent/patient produces consistent transformation.\n",
    "\n",
    "        agent_patient_pairs: [(\"A harmed B\", \"B harmed A\"), ...]\n",
    "        \"\"\"\n",
    "        transformations = []\n",
    "\n",
    "        for original, swapped in agent_patient_pairs:\n",
    "            orig_emb = self.get_embedding(original)\n",
    "            swap_emb = self.get_embedding(swapped)\n",
    "            transformations.append(swap_emb - orig_emb)\n",
    "\n",
    "        T = np.array(transformations)\n",
    "\n",
    "        # Check consistency: are all transformations similar?\n",
    "        mean_transform = T.mean(axis=0)\n",
    "        cosines = [\n",
    "            np.dot(t, mean_transform) / (np.linalg.norm(t) * np.linalg.norm(mean_transform) + 1e-9)\n",
    "            for t in T\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"mean_transform\": mean_transform,\n",
    "            \"consistency\": np.mean(cosines),\n",
    "            \"consistency_std\": np.std(cosines),\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LINEAR PROBE TEST\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nIf probe accuracy is NEAR CHANCE, representation is INVARIANT\")\n",
    "print(\"(This is what we want for BIP)\")\n",
    "\n",
    "probe_results = {}\n",
    "\n",
    "for split_name in [\"hebrew_to_others\", \"semitic_to_non_semitic\"]:\n",
    "    model_path = f\"{SAVE_DIR}/best_{split_name}.pt\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"\\nSkipping {split_name} - no saved model\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PROBE: {split_name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model = BIPModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    test_ids = set(all_splits[split_name][\"test_ids\"][:5000])\n",
    "    test_dataset = NativeDataset(\n",
    "        test_ids, \"data/processed/passages.jsonl\", \"data/processed/bonds.jsonl\", tokenizer\n",
    "    )\n",
    "\n",
    "    if len(test_dataset) < 50:\n",
    "        print(f\"  Skip - only {len(test_dataset)} samples\")\n",
    "        continue\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "    all_z, all_lang, all_period = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Extract\"):\n",
    "            out = model(batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), 0)\n",
    "            all_z.append(out[\"z\"].cpu().numpy())\n",
    "            all_lang.extend(batch[\"language_labels\"].tolist())\n",
    "            all_period.extend(batch[\"period_labels\"].tolist())\n",
    "\n",
    "    X = np.vstack(all_z)\n",
    "    y_lang = np.array(all_lang)\n",
    "    y_period = np.array(all_period)\n",
    "\n",
    "    scaler_probe = StandardScaler()\n",
    "    X_scaled = scaler_probe.fit_transform(X)\n",
    "\n",
    "    # Train/test split for probes\n",
    "    n = len(X)\n",
    "    idx = np.random.permutation(n)\n",
    "    train_idx, test_idx = idx[: int(0.7 * n)], idx[int(0.7 * n) :]\n",
    "\n",
    "    # Language probe - check for multiple classes\n",
    "    unique_langs = np.unique(y_lang[test_idx])\n",
    "    if len(unique_langs) < 2:\n",
    "        print(f\"  SKIP language probe - only {len(unique_langs)} class\")\n",
    "        lang_acc = 1.0 / max(1, len(np.unique(y_lang)))\n",
    "        lang_chance = lang_acc\n",
    "    else:\n",
    "        lang_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        lang_probe.fit(X_scaled[train_idx], y_lang[train_idx])\n",
    "        lang_acc = (lang_probe.predict(X_scaled[test_idx]) == y_lang[test_idx]).mean()\n",
    "        lang_chance = 1.0 / len(unique_langs)\n",
    "\n",
    "    # Period probe - same check\n",
    "    unique_periods = np.unique(y_period[test_idx])\n",
    "    if len(unique_periods) < 2:\n",
    "        print(f\"  SKIP period probe - only {len(unique_periods)} class\")\n",
    "        period_acc = 1.0 / max(1, len(np.unique(y_period)))\n",
    "        period_chance = period_acc\n",
    "    else:\n",
    "        period_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        period_probe.fit(X_scaled[train_idx], y_period[train_idx])\n",
    "        period_acc = (period_probe.predict(X_scaled[test_idx]) == y_period[test_idx]).mean()\n",
    "        period_chance = 1.0 / len(unique_periods)\n",
    "\n",
    "    lang_status = \"INVARIANT\" if lang_acc < lang_chance + 0.15 else \"NOT invariant\"\n",
    "    period_status = \"INVARIANT\" if period_acc < period_chance + 0.15 else \"NOT invariant\"\n",
    "\n",
    "    probe_results[split_name] = {\n",
    "        \"language_acc\": lang_acc,\n",
    "        \"language_chance\": lang_chance,\n",
    "        \"language_status\": lang_status,\n",
    "        \"period_acc\": period_acc,\n",
    "        \"period_chance\": period_chance,\n",
    "        \"period_status\": period_status,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nRESULTS:\")\n",
    "    print(f\"  Language: {lang_acc:.1%} (chance: {lang_chance:.1%}) -> {lang_status}\")\n",
    "    print(f\"  Period:   {period_acc:.1%} (chance: {period_chance:.1%}) -> {period_status}\")\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Probe tests complete\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===== v10.9: GEOMETRIC ANALYSIS =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GEOMETRIC ANALYSIS (v10.9)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nDiscovering interpretable axes in latent space...\")\n",
    "\n",
    "# Test pairs for axis discovery (cross-lingual)\n",
    "OBLIGATION_PERMISSION_TRAIN = [\n",
    "    # English - training set\n",
    "    (\"You must help the elderly\", \"You may help the elderly\"),\n",
    "    (\"He is required to pay\", \"He is allowed to pay\"),\n",
    "    (\"Parents must protect children\", \"Parents may protect children\"),\n",
    "]\n",
    "\n",
    "OBLIGATION_PERMISSION_TEST = [\n",
    "    # Chinese\n",
    "    (\"\u541b\u5b50\u5fc5\u5b5d\", \"\u541b\u5b50\u53ef\u5b5d\"),  # Gentleman must/may be filial\n",
    "    (\"\u6c11\u5fc5\u5f9e\u6cd5\", \"\u6c11\u53ef\u5f9e\u6cd5\"),  # People must/may follow law\n",
    "    # Arabic\n",
    "    (\"\u064a\u062c\u0628 \u0639\u0644\u064a\u0643 \u0623\u0646 \u062a\u0633\u0627\u0639\u062f\", \"\u064a\u062c\u0648\u0632 \u0644\u0643 \u0623\u0646 \u062a\u0633\u0627\u0639\u062f\"),  # You must/may help\n",
    "    # Hebrew\n",
    "    (\"\u05d7\u05d9\u05d9\u05d1 \u05dc\u05db\u05d1\u05d3\", \"\u05de\u05d5\u05ea\u05e8 \u05dc\u05db\u05d1\u05d3\"),  # Obligated/permitted to honor\n",
    "    # English - held out\n",
    "    (\"She must attend\", \"She may attend\"),\n",
    "]\n",
    "\n",
    "HARM_CARE_PAIRS = [\n",
    "    (\"He injured the child\", \"He protected the child\"),\n",
    "    (\"\u6bba\u4eba\u8005\", \"\u6551\u4eba\u8005\"),  # One who kills / one who saves\n",
    "    (\"\u0638\u0644\u0645 \u0627\u0644\u0636\u0639\u064a\u0641\", \"\u0631\u062d\u0645 \u0627\u0644\u0636\u0639\u064a\u0641\"),  # Oppressed / showed mercy to the weak\n",
    "    (\"She hurt the patient\", \"She healed the patient\"),\n",
    "]\n",
    "\n",
    "ROLE_SWAP_PAIRS = [\n",
    "    (\"The master commands the servant\", \"The servant commands the master\"),\n",
    "    (\"\u541b\u547d\u81e3\", \"\u81e3\u547d\u541b\"),  # Lord commands minister / minister commands lord\n",
    "    (\"\u0627\u0644\u0623\u0628 \u064a\u0623\u0645\u0631 \u0627\u0644\u0627\u0628\u0646\", \"\u0627\u0644\u0627\u0628\u0646 \u064a\u0623\u0645\u0631 \u0627\u0644\u0623\u0628\"),  # Father commands son / son commands father\n",
    "    (\"The parent guides the child\", \"The child guides the parent\"),\n",
    "]\n",
    "\n",
    "geometry_results = {}\n",
    "\n",
    "# Use the best model from mixed_baseline split for geometric analysis\n",
    "model_path = f\"{SAVE_DIR}/best_mixed_baseline.pt\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"\\nLoading model for geometric analysis...\")\n",
    "    model = BIPModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    analyzer = GeometricAnalyzer(model, tokenizer, device)\n",
    "\n",
    "    # 1. Find obligation/permission axis\n",
    "    print(\"\\n--- Obligation/Permission Axis ---\")\n",
    "    obl_texts = [p[0] for p in OBLIGATION_PERMISSION_TRAIN]\n",
    "    perm_texts = [p[1] for p in OBLIGATION_PERMISSION_TRAIN]\n",
    "    obl_perm_axis = analyzer.find_direction(obl_texts, perm_texts)\n",
    "\n",
    "    # Test transfer to other languages\n",
    "    transfer_acc = analyzer.test_direction_transfer(obl_perm_axis, OBLIGATION_PERMISSION_TEST)\n",
    "    print(f\"  Direction found from English training pairs\")\n",
    "    print(f\"  Transfer accuracy to other languages: {transfer_acc:.1%}\")\n",
    "    axis_status = \"STRONG\" if transfer_acc > 0.8 else \"WEAK\" if transfer_acc > 0.5 else \"FAILED\"\n",
    "    print(f\"  Status: {axis_status} deontic axis\")\n",
    "\n",
    "    geometry_results[\"obligation_permission\"] = {\n",
    "        \"transfer_accuracy\": transfer_acc,\n",
    "        \"status\": axis_status,\n",
    "    }\n",
    "\n",
    "    # 2. Find harm/care axis\n",
    "    print(\"\\n--- Harm/Care Axis ---\")\n",
    "    harm_texts = [p[0] for p in HARM_CARE_PAIRS]\n",
    "    care_texts = [p[1] for p in HARM_CARE_PAIRS]\n",
    "    harm_care_axis = analyzer.find_direction(harm_texts, care_texts)\n",
    "\n",
    "    # Check axis orthogonality\n",
    "    axis_correlation = abs(np.dot(obl_perm_axis, harm_care_axis))\n",
    "    print(f\"  Axis found\")\n",
    "    print(f\"  Correlation with obl/perm axis: {axis_correlation:.3f}\")\n",
    "    orthogonal = \"ORTHOGONAL\" if axis_correlation < 0.3 else \"CORRELATED\"\n",
    "    print(f\"  Status: {orthogonal}\")\n",
    "\n",
    "    geometry_results[\"harm_care\"] = {\n",
    "        \"axis_correlation\": axis_correlation,\n",
    "        \"orthogonal\": axis_correlation < 0.3,\n",
    "    }\n",
    "\n",
    "    # 3. Role swap analysis\n",
    "    print(\"\\n--- Role Swap Analysis ---\")\n",
    "    role_analysis = analyzer.role_swap_analysis(ROLE_SWAP_PAIRS)\n",
    "    print(\n",
    "        f\"  Mean consistency: {role_analysis['consistency']:.3f} +/- {role_analysis['consistency_std']:.3f}\"\n",
    "    )\n",
    "    role_status = \"CONSISTENT\" if role_analysis[\"consistency\"] > 0.9 else \"VARIABLE\"\n",
    "    print(f\"  Status: {role_status} agent/patient transformation\")\n",
    "\n",
    "    geometry_results[\"role_swap\"] = {\n",
    "        \"consistency\": role_analysis[\"consistency\"],\n",
    "        \"consistency_std\": role_analysis[\"consistency_std\"],\n",
    "        \"status\": role_status,\n",
    "    }\n",
    "\n",
    "    # 4. PCA on all structural pairs\n",
    "    print(\"\\n--- PCA Analysis ---\")\n",
    "    all_concept_pairs = {\n",
    "        \"obligation_permission\": OBLIGATION_PERMISSION_TRAIN + OBLIGATION_PERMISSION_TEST,\n",
    "        \"harm_care\": HARM_CARE_PAIRS,\n",
    "    }\n",
    "    pca_results = analyzer.pca_on_pairs(all_concept_pairs)\n",
    "\n",
    "    cumsum = np.cumsum(pca_results[\"explained_variance_ratio\"])\n",
    "    n_components_90 = np.argmax(cumsum > 0.9) + 1 if any(cumsum > 0.9) else len(cumsum)\n",
    "\n",
    "    print(f\"  Explained variance ratio: {pca_results['explained_variance_ratio'][:5]}\")\n",
    "    print(f\"  Components for 90% variance: {n_components_90}\")\n",
    "    pca_status = \"LOW-DIM\" if n_components_90 <= 3 else \"HIGH-DIM\"\n",
    "    print(f\"  Status: {pca_status} moral structure\")\n",
    "\n",
    "    geometry_results[\"pca\"] = {\n",
    "        \"explained_variance\": pca_results[\"explained_variance_ratio\"].tolist(),\n",
    "        \"n_components_90pct\": n_components_90,\n",
    "        \"status\": pca_status,\n",
    "    }\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(f\"\\nSkipping geometric analysis - no model at {model_path}\")\n",
    "    geometry_results = {\"error\": \"No model available\"}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Geometric analysis complete\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 9. Fuzz Testing & Final Results { display-mode: \"form\" }\n",
    "# @markdown v10.9: Structural vs Surface fuzz testing + comprehensive summary\n",
    "# @markdown Tests if model responds to moral structure (good) vs surface features (bad)\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FUZZ TESTING (v10.9)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTesting: structural changes should move embeddings,\")\n",
    "print(\"         surface changes should NOT move embeddings.\")\n",
    "\n",
    "\n",
    "# ===== STRUCTURAL FUZZ TEST CLASS =====\n",
    "class StructuralFuzzTest:\n",
    "    \"\"\"\n",
    "    Extended fuzz testing with cross-lingual pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Structural perturbation templates (language-agnostic concepts)\n",
    "    STRUCTURAL_PAIRS = {\n",
    "        \"obligation_to_permission\": [\n",
    "            # English\n",
    "            (\"You must help the elderly\", \"You may help the elderly\"),\n",
    "            (\"He is required to pay\", \"He is allowed to pay\"),\n",
    "            (\"Parents must protect children\", \"Parents may protect children\"),\n",
    "            # Chinese\n",
    "            (\"\u541b\u5b50\u5fc5\u5b5d\", \"\u541b\u5b50\u53ef\u5b5d\"),  # Gentleman must/may be filial\n",
    "            (\"\u6c11\u5fc5\u5f9e\u6cd5\", \"\u6c11\u53ef\u5f9e\u6cd5\"),  # People must/may follow law\n",
    "            # Arabic\n",
    "            (\"\u064a\u062c\u0628 \u0639\u0644\u064a\u0643 \u0623\u0646 \u062a\u0633\u0627\u0639\u062f\", \"\u064a\u062c\u0648\u0632 \u0644\u0643 \u0623\u0646 \u062a\u0633\u0627\u0639\u062f\"),  # You must/may help\n",
    "            # Hebrew\n",
    "            (\"\u05d7\u05d9\u05d9\u05d1 \u05dc\u05db\u05d1\u05d3\", \"\u05de\u05d5\u05ea\u05e8 \u05dc\u05db\u05d1\u05d3\"),  # Obligated/permitted to honor\n",
    "        ],\n",
    "        \"harm_to_care\": [\n",
    "            (\"He injured the child\", \"He protected the child\"),\n",
    "            (\"\u6bba\u4eba\u8005\", \"\u6551\u4eba\u8005\"),  # One who kills / one who saves\n",
    "            (\"\u0638\u0644\u0645 \u0627\u0644\u0636\u0639\u064a\u0641\", \"\u0631\u062d\u0645 \u0627\u0644\u0636\u0639\u064a\u0641\"),  # Oppressed / showed mercy to the weak\n",
    "        ],\n",
    "        \"role_swap\": [\n",
    "            (\"The master commands the servant\", \"The servant commands the master\"),\n",
    "            (\"\u541b\u547d\u81e3\", \"\u81e3\u547d\u541b\"),  # Lord commands minister / minister commands lord\n",
    "            (\"\u0627\u0644\u0623\u0628 \u064a\u0623\u0645\u0631 \u0627\u0644\u0627\u0628\u0646\", \"\u0627\u0644\u0627\u0628\u0646 \u064a\u0623\u0645\u0631 \u0627\u0644\u0623\u0628\"),  # Father commands son / son commands father\n",
    "        ],\n",
    "        \"violation_to_fulfillment\": [\n",
    "            (\"He broke his promise\", \"He kept his promise\"),\n",
    "            (\"\u9055\u7d04\", \"\u5b88\u7d04\"),  # Violate contract / keep contract\n",
    "            (\"\u0646\u0642\u0636 \u0627\u0644\u0639\u0647\u062f\", \"\u0648\u0641\u0649 \u0628\u0627\u0644\u0639\u0647\u062f\"),  # Broke covenant / fulfilled covenant\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Surface perturbation templates (should NOT move embeddings)\n",
    "    SURFACE_PERTURBATIONS = {\n",
    "        \"name_change\": lambda t: t.replace(\"John\", \"Michael\").replace(\"Mary\", \"Lisa\"),\n",
    "        \"irrelevant_detail\": lambda t: t + \" It was Tuesday.\",\n",
    "        \"add_location\": lambda t: t + \" in the city.\",\n",
    "    }\n",
    "\n",
    "    def run_comprehensive_test(self, analyzer) -> dict:\n",
    "        \"\"\"\n",
    "        Run full structural vs surface test battery.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # Test structural perturbations\n",
    "        for perturbation_type, pairs in self.STRUCTURAL_PAIRS.items():\n",
    "            distances = []\n",
    "            for text1, text2 in pairs:\n",
    "                emb1 = analyzer.get_embedding(text1)\n",
    "                emb2 = analyzer.get_embedding(text2)\n",
    "                # Cosine distance\n",
    "                dist = 1 - np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-9)\n",
    "                distances.append(dist)\n",
    "\n",
    "            results[f\"structural_{perturbation_type}\"] = {\n",
    "                \"mean_distance\": np.mean(distances),\n",
    "                \"std\": np.std(distances),\n",
    "                \"n\": len(distances),\n",
    "            }\n",
    "\n",
    "        # Surface perturbations on base sentences\n",
    "        base_sentences = [\n",
    "            \"John borrowed money from Mary and must repay it.\",\n",
    "            \"The doctor has a duty to help patients.\",\n",
    "            \"Parents should protect their children.\",\n",
    "        ]\n",
    "\n",
    "        surface_distances = []\n",
    "        for base in base_sentences:\n",
    "            base_emb = analyzer.get_embedding(base)\n",
    "            for name, perturb_fn in self.SURFACE_PERTURBATIONS.items():\n",
    "                perturbed = perturb_fn(base)\n",
    "                if perturbed != base:\n",
    "                    perturbed_emb = analyzer.get_embedding(perturbed)\n",
    "                    dist = 1 - np.dot(base_emb, perturbed_emb) / (\n",
    "                        np.linalg.norm(base_emb) * np.linalg.norm(perturbed_emb) + 1e-9\n",
    "                    )\n",
    "                    surface_distances.append(dist)\n",
    "\n",
    "        results[\"surface_all\"] = {\n",
    "            \"mean_distance\": np.mean(surface_distances) if surface_distances else 0,\n",
    "            \"std\": np.std(surface_distances) if surface_distances else 0,\n",
    "            \"n\": len(surface_distances),\n",
    "        }\n",
    "\n",
    "        # Statistical comparison\n",
    "        structural_all = []\n",
    "        for k, v in results.items():\n",
    "            if k.startswith(\"structural_\"):\n",
    "                structural_all.extend([v[\"mean_distance\"]] * v[\"n\"])\n",
    "\n",
    "        if structural_all and surface_distances:\n",
    "            t_stat, p_value = stats.ttest_ind(structural_all, surface_distances)\n",
    "        else:\n",
    "            t_stat, p_value = 0, 1.0\n",
    "\n",
    "        results[\"comparison\"] = {\n",
    "            \"structural_mean\": np.mean(structural_all) if structural_all else 0,\n",
    "            \"surface_mean\": np.mean(surface_distances) if surface_distances else 0,\n",
    "            \"ratio\": (\n",
    "                np.mean(structural_all) / (np.mean(surface_distances) + 1e-9)\n",
    "                if structural_all\n",
    "                else 0\n",
    "            ),\n",
    "            \"t_statistic\": t_stat,\n",
    "            \"p_value\": p_value,\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Run fuzz test if model is available\n",
    "fuzz_results = {}\n",
    "model_path = f\"{SAVE_DIR}/best_mixed_baseline.pt\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"\\nLoading model for fuzz testing...\")\n",
    "    model = BIPModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Reuse GeometricAnalyzer from cell 8\n",
    "    analyzer = GeometricAnalyzer(model, tokenizer, device)\n",
    "    fuzz_test = StructuralFuzzTest()\n",
    "\n",
    "    print(\"\\nRunning structural vs surface comparison...\")\n",
    "    fuzz_results = fuzz_test.run_comprehensive_test(analyzer)\n",
    "\n",
    "    print(\"\\n--- Structural Perturbations (should be HIGH) ---\")\n",
    "    for k, v in fuzz_results.items():\n",
    "        if k.startswith(\"structural_\"):\n",
    "            print(f\"  {k}: distance={v['mean_distance']:.4f} +/- {v['std']:.4f} (n={v['n']})\")\n",
    "\n",
    "    print(\"\\n--- Surface Perturbations (should be LOW) ---\")\n",
    "    v = fuzz_results[\"surface_all\"]\n",
    "    print(f\"  surface_all: distance={v['mean_distance']:.4f} +/- {v['std']:.4f} (n={v['n']})\")\n",
    "\n",
    "    print(\"\\n--- Statistical Comparison ---\")\n",
    "    c = fuzz_results[\"comparison\"]\n",
    "    print(f\"  Structural mean: {c['structural_mean']:.4f}\")\n",
    "    print(f\"  Surface mean:    {c['surface_mean']:.4f}\")\n",
    "    print(f\"  Ratio:           {c['ratio']:.2f}x\")\n",
    "    print(f\"  t-statistic:     {c['t_statistic']:.2f}\")\n",
    "    print(f\"  p-value:         {c['p_value']:.4f}\")\n",
    "\n",
    "    # Interpret results\n",
    "    if c[\"ratio\"] > 2.0 and c[\"p_value\"] < 0.05:\n",
    "        fuzz_status = \"EXCELLENT\"\n",
    "        fuzz_msg = \"Model strongly distinguishes structural from surface\"\n",
    "    elif c[\"ratio\"] > 1.5:\n",
    "        fuzz_status = \"GOOD\"\n",
    "        fuzz_msg = \"Model distinguishes structural from surface\"\n",
    "    elif c[\"ratio\"] > 1.0:\n",
    "        fuzz_status = \"MARGINAL\"\n",
    "        fuzz_msg = \"Some structural sensitivity\"\n",
    "    else:\n",
    "        fuzz_status = \"FAILED\"\n",
    "        fuzz_msg = \"Model may be using surface features\"\n",
    "\n",
    "    print(f\"\\n  FUZZ STATUS: {fuzz_status}\")\n",
    "    print(f\"  {fuzz_msg}\")\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(f\"\\nSkipping fuzz test - no model at {model_path}\")\n",
    "    fuzz_status = \"SKIPPED\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL BIP EVALUATION (v10.9)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nHardware: {GPU_TIER} ({VRAM_GB:.0f}GB VRAM, {RAM_GB:.0f}GB RAM)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"CROSS-DOMAIN TRANSFER RESULTS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "successful_splits = []\n",
    "for name, r in all_results.items():\n",
    "    ratio = r[\"bond_f1_macro\"] / 0.1\n",
    "    lang_acc = r[\"language_acc\"]\n",
    "\n",
    "    transfer_ok = ratio > 1.3\n",
    "    invariant_ok = lang_acc < 0.35  # Near chance (20%)\n",
    "\n",
    "    status = \"SUCCESS\" if (transfer_ok and invariant_ok) else \"PARTIAL\" if transfer_ok else \"FAIL\"\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\n",
    "        f\"  Bond F1:     {r['bond_f1_macro']:.3f} ({ratio:.1f}x chance) {'OK' if transfer_ok else 'WEAK'}\"\n",
    "    )\n",
    "    print(f\"  Language:    {lang_acc:.1%} {'INVARIANT' if invariant_ok else 'LEAKING'}\")\n",
    "    print(f\"  -> {status}\")\n",
    "\n",
    "    if transfer_ok and invariant_ok:\n",
    "        successful_splits.append(name)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"VERDICT\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "n_success = len(successful_splits)\n",
    "if n_success >= 3:\n",
    "    verdict = \"STRONGLY_SUPPORTED\"\n",
    "    msg = \"Multiple independent transfer paths demonstrate universal structure\"\n",
    "elif n_success >= 2:\n",
    "    verdict = \"SUPPORTED\"\n",
    "    msg = \"Multiple transfer paths work\"\n",
    "elif n_success >= 1:\n",
    "    verdict = \"PARTIALLY_SUPPORTED\"\n",
    "    msg = \"At least one transfer path works\"\n",
    "elif any(r[\"bond_f1_macro\"] > 0.13 for r in all_results.values()):\n",
    "    verdict = \"WEAK\"\n",
    "    msg = \"Some transfer signal, but not robust\"\n",
    "else:\n",
    "    verdict = \"INCONCLUSIVE\"\n",
    "    msg = \"No clear transfer demonstrated\"\n",
    "\n",
    "print(f\"\\n  Successful transfers: {n_success}/{len(all_results)}\")\n",
    "print(f\"  Splits: {successful_splits if successful_splits else 'None'}\")\n",
    "print(f\"\\n  VERDICT: {verdict}\")\n",
    "print(f\"  {msg}\")\n",
    "\n",
    "# v10.9 specific checks\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"v10.9 SPECIFIC CRITERIA\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check key v10.9 splits\n",
    "v109_checks = {\n",
    "    \"confucian_to_buddhist\": \"Chinese diversity test\",\n",
    "    \"all_to_sanskrit\": \"Sanskrit transfer test\",\n",
    "    \"quran_to_fiqh\": \"Arabic improvement test\",\n",
    "}\n",
    "\n",
    "for split_name, test_name in v109_checks.items():\n",
    "    if split_name in all_results:\n",
    "        r = all_results[split_name]\n",
    "        f1 = r[\"bond_f1_macro\"]\n",
    "        threshold = 0.4 if \"sanskrit\" in split_name else 0.5\n",
    "        status = \"PASS\" if f1 >= threshold else \"FAIL\"\n",
    "        print(f\"  {test_name}: F1={f1:.3f} (need {threshold}) -> {status}\")\n",
    "    else:\n",
    "        print(f\"  {test_name}: NOT RUN\")\n",
    "\n",
    "# Geometry results\n",
    "if \"geometry_results\" in dir() and geometry_results:\n",
    "    print(\"\\n  Geometric Analysis:\")\n",
    "    if \"obligation_permission\" in geometry_results:\n",
    "        acc = geometry_results[\"obligation_permission\"].get(\"transfer_accuracy\", 0)\n",
    "        print(f\"    Deontic axis transfer: {acc:.1%} (need 80%)\")\n",
    "    if \"pca\" in geometry_results:\n",
    "        n_comp = geometry_results[\"pca\"].get(\"n_components_90pct\", 0)\n",
    "        print(f\"    PCA components for 90%: {n_comp} (need \u22643)\")\n",
    "\n",
    "# Fuzz results\n",
    "if fuzz_results and \"comparison\" in fuzz_results:\n",
    "    print(f\"\\n  Fuzz Test: {fuzz_status}\")\n",
    "    print(f\"    Structural/Surface ratio: {fuzz_results['comparison']['ratio']:.2f}x\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    \"version\": \"v10.9\",\n",
    "    \"all_results\": all_results,\n",
    "    \"probe_results\": probe_results if \"probe_results\" in dir() else {},\n",
    "    \"geometry_results\": geometry_results if \"geometry_results\" in dir() else {},\n",
    "    \"fuzz_results\": fuzz_results,\n",
    "    \"successful_splits\": successful_splits,\n",
    "    \"verdict\": verdict,\n",
    "    \"hardware\": {\"gpu\": GPU_TIER, \"vram_gb\": VRAM_GB, \"ram_gb\": RAM_GB},\n",
    "    \"settings\": {\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"max_per_lang\": MAX_PER_LANG,\n",
    "        \"num_workers\": NUM_WORKERS,\n",
    "    },\n",
    "    \"experiment_time\": time.time() - EXPERIMENT_START,\n",
    "}\n",
    "\n",
    "with open(\"results/final_results.json\", \"w\") as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "shutil.copy(\"results/final_results.json\", f\"{SAVE_DIR}/final_results.json\")\n",
    "\n",
    "print(f\"\\nTotal time: {(time.time() - EXPERIMENT_START)/60:.1f} minutes\")\n",
    "print(\"Results saved to Drive!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 10. Download Results { display-mode: \"form\" }\n",
    "# @markdown Download all models and results\n",
    "\n",
    "import zipfile\n",
    "\n",
    "zip_path = \"BIP_v10.8_results.zip\"\n",
    "print(\"Creating download package...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "    # Results\n",
    "    if os.path.exists(\"results/final_results.json\"):\n",
    "        zf.write(\"results/final_results.json\")\n",
    "\n",
    "    # Models (from Drive)\n",
    "    if SAVE_DIR and os.path.exists(SAVE_DIR):\n",
    "        for f in os.listdir(SAVE_DIR):\n",
    "            if f.endswith(\".pt\"):\n",
    "                zf.write(f\"{SAVE_DIR}/{f}\", f\"models/{f}\")\n",
    "\n",
    "    # Config\n",
    "    if os.path.exists(\"data/splits/all_splits.json\"):\n",
    "        zf.write(\"data/splits/all_splits.json\")\n",
    "\n",
    "print(f\"\\nDownload package ready: {zip_path}\")\n",
    "\n",
    "# Download in Colab, or show path otherwise\n",
    "try:\n",
    "    from google.colab import files\n",
    "\n",
    "    files.download(zip_path)\n",
    "except ImportError:\n",
    "    print(f\"Not running in Colab. Results saved to: {os.path.abspath(zip_path)}\")\n"
   ]
  }
 ]
}