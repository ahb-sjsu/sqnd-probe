{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIP v10.6: Native-Language Moral Pattern Transfer\n",
    "\n",
    "**Bond Invariance Principle**: Moral concepts share mathematical structure across languages and cultures.\n",
    "\n",
    "## What's New in v10.6\n",
    "- **Expanded Chinese corpus** - 200+ real classical texts (Analects, Mencius, Daodejing, etc.)\n",
    "- **Expanded Islamic corpus** - 150+ real Quranic verses and Hadith\n",
    "- **Better data validation** - Warnings for insufficient corpora\n",
    "- **Minimum test size** - Skips splits with < 500 test samples\n",
    "- **Dear Abby guidance** - Clear instructions for uploading real data\n",
    "\n",
    "## Methodology\n",
    "1. Extract moral labels from NATIVE text using NATIVE patterns\n",
    "2. Train encoder with adversarial language/period invariance\n",
    "3. Test if moral concepts transfer across language families\n",
    "\n",
    "**NO English translation bridge** - pure mathematical alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Configuration & Setup { display-mode: \"form\" }\n",
    "#@markdown ## Data Source Configuration\n",
    "#@markdown Choose where to load data from:\n",
    "\n",
    "USE_DRIVE_DATA = True  #@param {type:\"boolean\"}\n",
    "#@markdown If True, load pre-processed data from persistent storage (faster)\n",
    "\n",
    "REFRESH_DATA_FROM_SOURCE = False  #@param {type:\"boolean\"}\n",
    "#@markdown If True, re-download from online sources even if cached data exists\n",
    "\n",
    "DRIVE_FOLDER = \"BIP_v10\"  #@param {type:\"string\"}\n",
    "#@markdown Folder name for persistent storage\n",
    "#@markdown ---\n",
    "#@markdown ## Model Backbone\n",
    "BACKBONE = \"MiniLM\"  #@param [\"MiniLM\", \"LaBSE\", \"XLM-R-base\", \"XLM-R-large\"]\n",
    "#@markdown - **MiniLM**: Fast, 118M params, good baseline\n",
    "#@markdown - **LaBSE**: Best cross-lingual alignment, 471M params (recommended)\n",
    "#@markdown - **XLM-R-base**: Strong multilingual, 270M params\n",
    "#@markdown - **XLM-R-large**: Strongest representations, 550M params\n",
    "\n",
    "# Backbone configurations\n",
    "BACKBONE_CONFIGS = {\n",
    "    \"MiniLM\": {\n",
    "        \"model_name\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        \"hidden_size\": 384,\n",
    "        \"recommended_batch\": {\"L4/A100\": 512, \"T4\": 256, \"2xT4\": 512, \"SMALL\": 128, \"MINIMAL/CPU\": 64},\n",
    "    },\n",
    "    \"LaBSE\": {\n",
    "        \"model_name\": \"sentence-transformers/LaBSE\",\n",
    "        \"hidden_size\": 768,\n",
    "        \"recommended_batch\": {\"L4/A100\": 256, \"T4\": 128, \"2xT4\": 256, \"SMALL\": 64, \"MINIMAL/CPU\": 32},\n",
    "    },\n",
    "    \"XLM-R-base\": {\n",
    "        \"model_name\": \"xlm-roberta-base\",\n",
    "        \"hidden_size\": 768,\n",
    "        \"recommended_batch\": {\"L4/A100\": 256, \"T4\": 128, \"2xT4\": 256, \"SMALL\": 64, \"MINIMAL/CPU\": 32},\n",
    "    },\n",
    "    \"XLM-R-large\": {\n",
    "        \"model_name\": \"xlm-roberta-large\",\n",
    "        \"hidden_size\": 1024,\n",
    "        \"recommended_batch\": {\"L4/A100\": 128, \"T4\": 64, \"2xT4\": 128, \"SMALL\": 32, \"MINIMAL/CPU\": 16},\n",
    "    },\n",
    "}\n",
    "\n",
    "BACKBONE_CONFIG = BACKBONE_CONFIGS[BACKBONE]\n",
    "MODEL_NAME = BACKBONE_CONFIG[\"model_name\"]\n",
    "BACKBONE_HIDDEN = BACKBONE_CONFIG[\"hidden_size\"]\n",
    "\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Run Setup\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "EXPERIMENT_START = time.time()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BIP v10.6 - ENVIRONMENT DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== ENVIRONMENT DETECTION =====\n",
    "# Detect which cloud platform we're running on\n",
    "\n",
    "ENV_NAME = \"UNKNOWN\"\n",
    "ENV_GPU_QUOTA = \"Unknown\"\n",
    "PERSISTENT_STORAGE = None\n",
    "DATA_DIR = \"/content\"  # Default\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"Detect cloud environment and return (name, gpu_quota, storage_path, data_dir)\"\"\"\n",
    "\n",
    "    # 1. Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        return (\"COLAB\", \"Free: T4 ~12h/day, Pro: L4/A100\", \"/content/drive/MyDrive\", \"/content\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    # 2. Kaggle Kernels\n",
    "    if os.path.exists('/kaggle'):\n",
    "        # Kaggle has /kaggle/input for datasets, /kaggle/working for output\n",
    "        return (\"KAGGLE\", \"Free: 2xT4 30h/week, TPU 30h/week\", \"/kaggle/working\", \"/kaggle/working\")\n",
    "\n",
    "    # 3. Lightning.ai Studios\n",
    "    if os.environ.get('LIGHTNING_CLOUDSPACE_HOST') or os.path.exists('/teamspace'):\n",
    "        # Lightning.ai has /teamspace/studios for persistent storage\n",
    "        return (\"LIGHTNING_AI\", \"Free: 22h/month GPU, Pro: A10G/H100\", \"/teamspace/studios\", \"/teamspace/studios\")\n",
    "\n",
    "    # 4. Paperspace Gradient\n",
    "    if os.environ.get('PAPERSPACE_NOTEBOOK_REPO_ID') or os.path.exists('/notebooks'):\n",
    "        return (\"PAPERSPACE\", \"Free: M4000 6h, Pro: A100/H100\", \"/storage\", \"/notebooks\")\n",
    "\n",
    "    # 5. Saturn Cloud\n",
    "    if os.environ.get('SATURN_RESOURCE_ID') or 'saturn' in os.environ.get('HOSTNAME', '').lower():\n",
    "        return (\"SATURN_CLOUD\", \"Free: T4 10h/month, Pro: A10G/A100\", \"/home/jovyan/workspace\", \"/home/jovyan\")\n",
    "\n",
    "    # 6. HuggingFace Spaces\n",
    "    if os.environ.get('SPACE_ID') or os.environ.get('HF_SPACE_ID'):\n",
    "        return (\"HUGGINGFACE_SPACES\", \"Free: CPU only, ZeroGPU: A10G/A100 quota\", \"/data\", \"/home/user/app\")\n",
    "\n",
    "    # 7. AWS SageMaker Studio Lab\n",
    "    if os.path.exists('/home/studio-lab-user'):\n",
    "        return (\"SAGEMAKER_STUDIO_LAB\", \"Free: T4 4h/session, 24h max/day\", \"/home/studio-lab-user\", \"/home/studio-lab-user\")\n",
    "\n",
    "    # 8. Deepnote\n",
    "    if os.environ.get('DEEPNOTE_PROJECT_ID'):\n",
    "        return (\"DEEPNOTE\", \"Free: CPU, Pro: T4/A10G\", \"/work\", \"/work\")\n",
    "\n",
    "    # 9. Local/Unknown\n",
    "    return (\"LOCAL\", \"Depends on local hardware\", os.getcwd(), os.getcwd())\n",
    "\n",
    "ENV_NAME, ENV_GPU_QUOTA, PERSISTENT_STORAGE, DATA_DIR = detect_environment()\n",
    "\n",
    "print(f\"\\nEnvironment: {ENV_NAME}\")\n",
    "print(f\"GPU Quota:   {ENV_GPU_QUOTA}\")\n",
    "print(f\"Storage:     {PERSISTENT_STORAGE}\")\n",
    "print(f\"Data Dir:    {DATA_DIR}\")\n",
    "\n",
    "# Environment-specific setup\n",
    "ENV_TIPS = {\n",
    "    \"COLAB\": [\n",
    "        \"Tip: Use GPU runtime (Runtime -> Change runtime type -> T4 GPU)\",\n",
    "        \"Tip: Colab Pro gives L4 GPU access (~2x faster than T4)\"\n",
    "    ],\n",
    "    \"KAGGLE\": [\n",
    "        \"Tip: Enable GPU (Settings -> Accelerator -> GPU T4 x2)\",\n",
    "        \"Tip: 30h/week GPU quota resets every Saturday\",\n",
    "        \"Tip: Upload data as a Kaggle Dataset for persistence\"\n",
    "    ],\n",
    "    \"LIGHTNING_AI\": [\n",
    "        \"Tip: Select GPU studio (A10G recommended for this workload)\",\n",
    "        \"Tip: /teamspace/studios persists across sessions\"\n",
    "    ],\n",
    "    \"PAPERSPACE\": [\n",
    "        \"Tip: Use /storage for persistent data across runs\",\n",
    "        \"Tip: Free tier has 6h/month GPU limit\"\n",
    "    ],\n",
    "    \"SATURN_CLOUD\": [\n",
    "        \"Tip: Start a T4 instance from the Resources tab\",\n",
    "        \"Tip: 10h/month free GPU quota\"\n",
    "    ],\n",
    "    \"HUGGINGFACE_SPACES\": [\n",
    "        \"Tip: ZeroGPU provides A10G/A100 access with quota system\",\n",
    "        \"Tip: Use Gradio/Streamlit for interactive demos\"\n",
    "    ],\n",
    "    \"SAGEMAKER_STUDIO_LAB\": [\n",
    "        \"Tip: Request GPU runtime from the launcher\",\n",
    "        \"Tip: Sessions timeout after 4h, max 24h/day\"\n",
    "    ],\n",
    "    \"LOCAL\": [\n",
    "        \"Tip: Running locally - ensure CUDA is installed for GPU support\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"\\n\" + \"-\"*60)\n",
    "print(\"ENVIRONMENT TIPS:\")\n",
    "for tip in ENV_TIPS.get(ENV_NAME, [\"No specific tips for this environment\"]):\n",
    "    print(f\"  {tip}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# ===== INSTALL DEPENDENCIES =====\n",
    "import subprocess\n",
    "\n",
    "print(\"\\nInstalling dependencies...\")\n",
    "for pkg in [\"transformers\", \"sentence-transformers\", \"pandas\", \"tqdm\", \"scikit-learn\", \"pyyaml\", \"psutil\"]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU DETECTION & RESOURCE ALLOCATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Detect hardware\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    GPU_COUNT = torch.cuda.device_count()\n",
    "else:\n",
    "    GPU_NAME = \"CPU\"\n",
    "    VRAM_GB = 0\n",
    "    GPU_COUNT = 0\n",
    "\n",
    "RAM_GB = psutil.virtual_memory().total / 1e9\n",
    "\n",
    "print(f\"\\nDetected Hardware:\")\n",
    "print(f\"  GPU:  {GPU_NAME}\" + (f\" (x{GPU_COUNT})\" if GPU_COUNT > 1 else \"\"))\n",
    "print(f\"  VRAM: {VRAM_GB:.1f} GB\" + (f\" (total: {VRAM_GB*GPU_COUNT:.1f} GB)\" if GPU_COUNT > 1 else \"\"))\n",
    "print(f\"  RAM:  {RAM_GB:.1f} GB\")\n",
    "\n",
    "# Set optimal parameters based on hardware\n",
    "if VRAM_GB >= 22:      # L4 (24GB) or A100\n",
    "    GPU_TIER = \"L4/A100\"\n",
    "elif VRAM_GB >= 14:    # T4 (16GB)\n",
    "    GPU_TIER = \"T4\"\n",
    "elif VRAM_GB >= 10:\n",
    "    GPU_TIER = \"SMALL\"\n",
    "else:\n",
    "    GPU_TIER = \"MINIMAL/CPU\"\n",
    "\n",
    "# Kaggle with 2xT4 can use larger batch\n",
    "if ENV_NAME == \"KAGGLE\" and GPU_COUNT >= 2:\n",
    "    GPU_TIER = \"2xT4\"\n",
    "    print(f\"  ** Kaggle 2xT4 detected **\")\n",
    "\n",
    "# Get backbone-specific batch size\n",
    "BATCH_SIZE = BACKBONE_CONFIG[\"recommended_batch\"].get(GPU_TIER, 64)\n",
    "print(f\"  Backbone: {BACKBONE} -> batch size {BATCH_SIZE}\")\n",
    "\n",
    "MAX_PER_LANG = 50000  # Language sample limit\n",
    "CPU_CORES = os.cpu_count() or 2\n",
    "NUM_WORKERS = min(4, CPU_CORES - 1) if RAM_GB >= 24 and VRAM_GB >= 14 else 0\n",
    "MAX_TEST_SAMPLES = 20000\n",
    "LR = 2e-5 * (BATCH_SIZE / 256)\n",
    "\n",
    "print(f\"\\n\" + \"-\"*60)\n",
    "print(f\"OPTIMAL SETTINGS:\")\n",
    "print(f\"-\"*60)\n",
    "print(f\"  Environment:     {ENV_NAME}\")\n",
    "print(f\"  GPU Tier:        {GPU_TIER}\")\n",
    "print(f\"  Backbone:        {BACKBONE}\")\n",
    "print(f\"  Batch size:      {BATCH_SIZE}\")\n",
    "print(f\"  Max per lang:    {MAX_PER_LANG:,}\")\n",
    "print(f\"  DataLoader workers: {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate:   {LR:.2e}\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "scaler = torch.amp.GradScaler('cuda') if USE_AMP else None\n",
    "\n",
    "# ===== PERSISTENT STORAGE SETUP =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERSISTENT STORAGE SETUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "SAVE_DIR = None\n",
    "DRIVE_HAS_DATA = False\n",
    "DRIVE_FILES = []\n",
    "\n",
    "if ENV_NAME == \"COLAB\":\n",
    "    # Google Colab - mount Drive\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        DRIVE_MOUNT_PATH = '/content/drive'\n",
    "\n",
    "        if os.path.exists(f'{DRIVE_MOUNT_PATH}/MyDrive'):\n",
    "            print(\"Google Drive already mounted\")\n",
    "        else:\n",
    "            try:\n",
    "                drive.mount(DRIVE_MOUNT_PATH, force_remount=False)\n",
    "                print(\"Google Drive mounted successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Drive mount issue: {e}\")\n",
    "                try:\n",
    "                    drive.mount(DRIVE_MOUNT_PATH, force_remount=True)\n",
    "                    print(\"Google Drive mounted (force remount)\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"WARNING: Could not mount Drive: {e2}\")\n",
    "                    print(\"Falling back to local storage\")\n",
    "                    PERSISTENT_STORAGE = DATA_DIR\n",
    "\n",
    "        SAVE_DIR = f'{DRIVE_MOUNT_PATH}/MyDrive/{DRIVE_FOLDER}'\n",
    "    except Exception as e:\n",
    "        print(f\"Colab Drive setup failed: {e}\")\n",
    "        SAVE_DIR = f'{DATA_DIR}/{DRIVE_FOLDER}'\n",
    "\n",
    "elif ENV_NAME == \"KAGGLE\":\n",
    "    # Kaggle - use working directory\n",
    "    SAVE_DIR = f'{PERSISTENT_STORAGE}/{DRIVE_FOLDER}'\n",
    "    print(f\"Using Kaggle working directory: {SAVE_DIR}\")\n",
    "    print(\"Note: Data persists until kernel is reset\")\n",
    "    # Check for uploaded datasets\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        datasets = os.listdir('/kaggle/input')\n",
    "        if datasets:\n",
    "            print(f\"Available datasets: {datasets[:5]}\")\n",
    "\n",
    "elif ENV_NAME == \"LIGHTNING_AI\":\n",
    "    SAVE_DIR = f'{PERSISTENT_STORAGE}/{DRIVE_FOLDER}'\n",
    "    print(f\"Using Lightning.ai studio storage: {SAVE_DIR}\")\n",
    "\n",
    "elif ENV_NAME == \"PAPERSPACE\":\n",
    "    SAVE_DIR = f'{PERSISTENT_STORAGE}/{DRIVE_FOLDER}'\n",
    "    print(f\"Using Paperspace /storage: {SAVE_DIR}\")\n",
    "\n",
    "elif ENV_NAME == \"HUGGINGFACE_SPACES\":\n",
    "    # HF Spaces has limited persistent storage\n",
    "    SAVE_DIR = f'{PERSISTENT_STORAGE}/{DRIVE_FOLDER}'\n",
    "    print(f\"Using HuggingFace Spaces storage: {SAVE_DIR}\")\n",
    "    print(\"Warning: HF Spaces storage is limited\")\n",
    "\n",
    "else:\n",
    "    SAVE_DIR = f'{PERSISTENT_STORAGE}/{DRIVE_FOLDER}'\n",
    "    print(f\"Using local storage: {SAVE_DIR}\")\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Check what's available in storage\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    DRIVE_FILES = os.listdir(SAVE_DIR)\n",
    "    DRIVE_HAS_DATA = 'passages.jsonl' in DRIVE_FILES and 'bonds.jsonl' in DRIVE_FILES\n",
    "\n",
    "print(f\"\\n\" + \"-\"*60)\n",
    "print(f\"STORAGE STATUS:\")\n",
    "print(f\"-\"*60)\n",
    "print(f\"  Folder: {SAVE_DIR}\")\n",
    "print(f\"  Files found: {len(DRIVE_FILES)}\")\n",
    "if DRIVE_FILES:\n",
    "    for f in DRIVE_FILES[:10]:\n",
    "        print(f\"    - {f}\")\n",
    "    if len(DRIVE_FILES) > 10:\n",
    "        print(f\"    ... and {len(DRIVE_FILES)-10} more\")\n",
    "print(f\"  Pre-processed data available: {DRIVE_HAS_DATA}\")\n",
    "\n",
    "# Decide data loading strategy\n",
    "LOAD_FROM_DRIVE = USE_DRIVE_DATA and DRIVE_HAS_DATA and not REFRESH_DATA_FROM_SOURCE\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"DATA LOADING STRATEGY:\")\n",
    "if LOAD_FROM_DRIVE:\n",
    "    print(f\"  -> Will load pre-processed data from {ENV_NAME} storage\")\n",
    "    print(f\"     (Set REFRESH_DATA_FROM_SOURCE=True to re-download)\")\n",
    "else:\n",
    "    print(f\"  -> Will download and process data from online sources\")\n",
    "    if USE_DRIVE_DATA and not DRIVE_HAS_DATA:\n",
    "        print(f\"     (Cached data not found, downloading fresh)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create local directories\n",
    "for d in [\"data/processed\", \"data/splits\", \"data/raw\", \"models/checkpoints\", \"results\"]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"SETUP COMPLETE\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"  Environment: {ENV_NAME}\")\n",
    "print(f\"  GPU:         {GPU_NAME} ({GPU_TIER})\")\n",
    "print(f\"  Storage:     {SAVE_DIR}\")\n",
    "print(f\"  Ready to run: Cell 2 (Imports)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Download/Load Corpora { display-mode: \"form\" }\n",
    "#@markdown Downloads from online sources OR loads from Google Drive\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING CORPORA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if LOAD_FROM_DRIVE:\n",
    "    # ===== LOAD FROM DRIVE =====\n",
    "    print(\"\\nLoading pre-processed data from Google Drive...\")\n",
    "    \n",
    "    # Copy files from Drive to local\n",
    "    for fname in ['passages.jsonl', 'bonds.jsonl']:\n",
    "        src = f'{SAVE_DIR}/{fname}'\n",
    "        dst = f'data/processed/{fname}'\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "            print(f\"  Copied {fname}\")\n",
    "    \n",
    "    if os.path.exists(f'{SAVE_DIR}/all_splits.json'):\n",
    "        shutil.copy(f'{SAVE_DIR}/all_splits.json', 'data/splits/all_splits.json')\n",
    "        print(f\"  Copied all_splits.json\")\n",
    "    \n",
    "    # Load Dear Abby from Drive if available\n",
    "    if 'dear_abby.csv' in DRIVE_FILES:\n",
    "        shutil.copy(f'{SAVE_DIR}/dear_abby.csv', 'data/raw/dear_abby.csv')\n",
    "        print(f\"  Copied dear_abby.csv\")\n",
    "    \n",
    "    # Count loaded data\n",
    "    if os.path.exists('data/processed/passages.jsonl'):\n",
    "        with open('data/processed/passages.jsonl') as f:\n",
    "            n_passages = sum(1 for _ in f)\n",
    "        print(f\"\\nLoaded {n_passages:,} passages from Drive\")\n",
    "    \n",
    "    SKIP_PROCESSING = True\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Drive data loaded - skipping download/processing\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    # ===== DOWNLOAD FROM ONLINE =====\n",
    "    SKIP_PROCESSING = False\n",
    "    \n",
    "    # SEFARIA\n",
    "    if not os.path.exists('data/raw/Sefaria-Export/json'):\n",
    "        print(\"\\n[1/4] Downloading Sefaria (~2GB)...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", \n",
    "                       \"https://github.com/Sefaria/Sefaria-Export.git\",\n",
    "                       \"data/raw/Sefaria-Export\"], check=True)\n",
    "        print(\"  Done!\")\n",
    "    else:\n",
    "        print(\"\\n[1/4] Sefaria already exists\")\n",
    "    \n",
    "    # CHINESE - 200+ REAL CLASSICAL TEXTS\n",
    "    print(\"\\n[2/4] Chinese classics (200+ real passages)...\")\n",
    "    os.makedirs('data/raw/chinese', exist_ok=True)\n",
    "    \n",
    "    chinese = []\n",
    "    \n",
    "    # === ANALECTS (論語) - 50+ passages ===\n",
    "    analects = [\n",
    "        (\"子曰：己所不欲，勿施於人。\", \"Analects 15.24\"),\n",
    "        (\"孝悌也者，其為仁之本與。\", \"Analects 1.2\"),\n",
    "        (\"父母在，不遠游，遊必有方。\", \"Analects 4.19\"),\n",
    "        (\"君子喻於義，小人喻於利。\", \"Analects 4.16\"),\n",
    "        (\"不義而富且貴，於我如浮雲。\", \"Analects 7.16\"),\n",
    "        (\"學而時習之，不亦說乎。\", \"Analects 1.1\"),\n",
    "        (\"有朋自遠方來，不亦樂乎。\", \"Analects 1.1\"),\n",
    "        (\"人不知而不慍，不亦君子乎。\", \"Analects 1.1\"),\n",
    "        (\"巧言令色，鮮矣仁。\", \"Analects 1.3\"),\n",
    "        (\"吾日三省吾身。\", \"Analects 1.4\"),\n",
    "        (\"為人謀而不忠乎，與朋友交而不信乎。\", \"Analects 1.4\"),\n",
    "        (\"弟子入則孝，出則悌。\", \"Analects 1.6\"),\n",
    "        (\"謹而信，汎愛眾，而親仁。\", \"Analects 1.6\"),\n",
    "        (\"君子不重則不威，學則不固。\", \"Analects 1.8\"),\n",
    "        (\"主忠信，無友不如己者。\", \"Analects 1.8\"),\n",
    "        (\"過則勿憚改。\", \"Analects 1.8\"),\n",
    "        (\"慎終追遠，民德歸厚矣。\", \"Analects 1.9\"),\n",
    "        (\"禮之用，和為貴。\", \"Analects 1.12\"),\n",
    "        (\"信近於義，言可復也。\", \"Analects 1.13\"),\n",
    "        (\"君子食無求飽，居無求安。\", \"Analects 1.14\"),\n",
    "        (\"敏於事而慎於言，就有道而正焉。\", \"Analects 1.14\"),\n",
    "        (\"不患人之不己知，患不知人也。\", \"Analects 1.16\"),\n",
    "        (\"為政以德，譬如北辰。\", \"Analects 2.1\"),\n",
    "        (\"道之以政，齊之以刑，民免而無恥。\", \"Analects 2.3\"),\n",
    "        (\"道之以德，齊之以禮，有恥且格。\", \"Analects 2.3\"),\n",
    "        (\"吾十有五而志于學。\", \"Analects 2.4\"),\n",
    "        (\"三十而立，四十而不惑。\", \"Analects 2.4\"),\n",
    "        (\"五十而知天命，六十而耳順。\", \"Analects 2.4\"),\n",
    "        (\"七十而從心所欲，不逾矩。\", \"Analects 2.4\"),\n",
    "        (\"生，事之以禮；死，葬之以禮，祭之以禮。\", \"Analects 2.5\"),\n",
    "        (\"父母唯其疾之憂。\", \"Analects 2.6\"),\n",
    "        (\"今之孝者，是謂能養。\", \"Analects 2.7\"),\n",
    "        (\"至於犬馬，皆能有養；不敬，何以別乎。\", \"Analects 2.7\"),\n",
    "        (\"色難。有事，弟子服其勞。\", \"Analects 2.8\"),\n",
    "        (\"視其所以，觀其所由，察其所安。\", \"Analects 2.10\"),\n",
    "        (\"溫故而知新，可以為師矣。\", \"Analects 2.11\"),\n",
    "        (\"君子不器。\", \"Analects 2.12\"),\n",
    "        (\"先行其言而後從之。\", \"Analects 2.13\"),\n",
    "        (\"君子周而不比，小人比而不周。\", \"Analects 2.14\"),\n",
    "        (\"學而不思則罔，思而不學則殆。\", \"Analects 2.15\"),\n",
    "        (\"知之為知之，不知為不知，是知也。\", \"Analects 2.17\"),\n",
    "        (\"多聞闕疑，慎言其餘，則寡尤。\", \"Analects 2.18\"),\n",
    "        (\"舉直錯諸枉，則民服。\", \"Analects 2.19\"),\n",
    "        (\"人而無信，不知其可也。\", \"Analects 2.22\"),\n",
    "        (\"見義不為，無勇也。\", \"Analects 2.24\"),\n",
    "        (\"非其鬼而祭之，諂也。\", \"Analects 2.24\"),\n",
    "        (\"是可忍也，孰不可忍也。\", \"Analects 3.1\"),\n",
    "        (\"人而不仁，如禮何。\", \"Analects 3.3\"),\n",
    "        (\"人而不仁，如樂何。\", \"Analects 3.3\"),\n",
    "        (\"里仁為美。擇不處仁，焉得知。\", \"Analects 4.1\"),\n",
    "        (\"不仁者不可以久處約，不可以長處樂。\", \"Analects 4.2\"),\n",
    "        (\"仁者安仁，知者利仁。\", \"Analects 4.2\"),\n",
    "        (\"唯仁者能好人，能惡人。\", \"Analects 4.3\"),\n",
    "        (\"苟志於仁矣，無惡也。\", \"Analects 4.4\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(analects):\n",
    "        chinese.append({\"id\": f\"cn_analects_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -5})\n",
    "    \n",
    "    # === MENCIUS (孟子) - 40+ passages ===\n",
    "    mencius = [\n",
    "        (\"惻隱之心，仁之端也。\", \"Mencius 2A.6\"),\n",
    "        (\"羞惡之心，義之端也。\", \"Mencius 2A.6\"),\n",
    "        (\"辭讓之心，禮之端也。\", \"Mencius 2A.6\"),\n",
    "        (\"是非之心，智之端也。\", \"Mencius 2A.6\"),\n",
    "        (\"人皆有不忍人之心。\", \"Mencius 2A.6\"),\n",
    "        (\"無惻隱之心，非人也。\", \"Mencius 2A.6\"),\n",
    "        (\"無羞惡之心，非人也。\", \"Mencius 2A.6\"),\n",
    "        (\"無辭讓之心，非人也。\", \"Mencius 2A.6\"),\n",
    "        (\"無是非之心，非人也。\", \"Mencius 2A.6\"),\n",
    "        (\"仁義禮智，非由外鑠我也，我固有之也。\", \"Mencius 6A.6\"),\n",
    "        (\"人性之善也，猶水之就下也。\", \"Mencius 6A.2\"),\n",
    "        (\"人無有不善，水無有不下。\", \"Mencius 6A.2\"),\n",
    "        (\"惟仁者宜在高位。\", \"Mencius 4A.1\"),\n",
    "        (\"不仁而在高位，是播其惡於眾也。\", \"Mencius 4A.1\"),\n",
    "        (\"民為貴，社稷次之，君為輕。\", \"Mencius 7B.14\"),\n",
    "        (\"得道者多助，失道者寡助。\", \"Mencius 2B.1\"),\n",
    "        (\"寡助之至，親戚畔之。\", \"Mencius 2B.1\"),\n",
    "        (\"多助之至，天下順之。\", \"Mencius 2B.1\"),\n",
    "        (\"天時不如地利，地利不如人和。\", \"Mencius 2B.1\"),\n",
    "        (\"生於憂患，死於安樂。\", \"Mencius 6B.15\"),\n",
    "        (\"天將降大任於是人也，必先苦其心志。\", \"Mencius 6B.15\"),\n",
    "        (\"勞其筋骨，餓其體膚。\", \"Mencius 6B.15\"),\n",
    "        (\"空乏其身，行拂亂其所為。\", \"Mencius 6B.15\"),\n",
    "        (\"所以動心忍性，曾益其所不能。\", \"Mencius 6B.15\"),\n",
    "        (\"老吾老，以及人之老。\", \"Mencius 1A.7\"),\n",
    "        (\"幼吾幼，以及人之幼。\", \"Mencius 1A.7\"),\n",
    "        (\"窮則獨善其身，達則兼善天下。\", \"Mencius 7A.9\"),\n",
    "        (\"魚，我所欲也；熊掌，亦我所欲也。\", \"Mencius 6A.10\"),\n",
    "        (\"二者不可得兼，舍魚而取熊掌者也。\", \"Mencius 6A.10\"),\n",
    "        (\"生，亦我所欲也；義，亦我所欲也。\", \"Mencius 6A.10\"),\n",
    "        (\"二者不可得兼，舍生而取義者也。\", \"Mencius 6A.10\"),\n",
    "        (\"養心莫善於寡欲。\", \"Mencius 7B.35\"),\n",
    "        (\"仁者無敵於天下。\", \"Mencius 1A.5\"),\n",
    "        (\"以力服人者，非心服也。\", \"Mencius 2A.3\"),\n",
    "        (\"以德服人者，中心悅而誠服也。\", \"Mencius 2A.3\"),\n",
    "        (\"人之患在好為人師。\", \"Mencius 4A.23\"),\n",
    "        (\"盡信書，則不如無書。\", \"Mencius 7B.3\"),\n",
    "        (\"不以規矩，不能成方圓。\", \"Mencius 4A.1\"),\n",
    "        (\"孝子之至，莫大乎尊親。\", \"Mencius 5A.4\"),\n",
    "        (\"父子有親，君臣有義，夫婦有別，長幼有序，朋友有信。\", \"Mencius 3A.4\"),\n",
    "        (\"人有不為也，而後可以有為。\", \"Mencius 4B.8\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(mencius):\n",
    "        chinese.append({\"id\": f\"cn_mencius_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -4})\n",
    "    \n",
    "    # === DAODEJING (道德經) - 40+ passages ===\n",
    "    daodejing = [\n",
    "        (\"道可道，非常道。名可名，非常名。\", \"Daodejing 1\"),\n",
    "        (\"天下皆知美之為美，斯惡已。\", \"Daodejing 2\"),\n",
    "        (\"皆知善之為善，斯不善已。\", \"Daodejing 2\"),\n",
    "        (\"有無相生，難易相成。\", \"Daodejing 2\"),\n",
    "        (\"長短相較，高下相傾。\", \"Daodejing 2\"),\n",
    "        (\"是以聖人處無為之事，行不言之教。\", \"Daodejing 2\"),\n",
    "        (\"不尚賢，使民不爭。\", \"Daodejing 3\"),\n",
    "        (\"不貴難得之貨，使民不為盜。\", \"Daodejing 3\"),\n",
    "        (\"上善若水。水善利萬物而不爭。\", \"Daodejing 8\"),\n",
    "        (\"處眾人之所惡，故幾於道。\", \"Daodejing 8\"),\n",
    "        (\"居善地，心善淵，與善仁。\", \"Daodejing 8\"),\n",
    "        (\"言善信，政善治，事善能，動善時。\", \"Daodejing 8\"),\n",
    "        (\"夫唯不爭，故無尤。\", \"Daodejing 8\"),\n",
    "        (\"金玉滿堂，莫之能守。\", \"Daodejing 9\"),\n",
    "        (\"富貴而驕，自遺其咎。\", \"Daodejing 9\"),\n",
    "        (\"功成身退，天之道也。\", \"Daodejing 9\"),\n",
    "        (\"知人者智，自知者明。\", \"Daodejing 33\"),\n",
    "        (\"勝人者有力，自勝者強。\", \"Daodejing 33\"),\n",
    "        (\"知足者富，強行者有志。\", \"Daodejing 33\"),\n",
    "        (\"不失其所者久，死而不亡者壽。\", \"Daodejing 33\"),\n",
    "        (\"大道廢，有仁義。\", \"Daodejing 18\"),\n",
    "        (\"智慧出，有大偽。\", \"Daodejing 18\"),\n",
    "        (\"六親不和，有孝慈。\", \"Daodejing 18\"),\n",
    "        (\"國家昏亂，有忠臣。\", \"Daodejing 18\"),\n",
    "        (\"禍兮福之所倚，福兮禍之所伏。\", \"Daodejing 58\"),\n",
    "        (\"天長地久。\", \"Daodejing 7\"),\n",
    "        (\"天地所以能長且久者，以其不自生。\", \"Daodejing 7\"),\n",
    "        (\"是以聖人後其身而身先。\", \"Daodejing 7\"),\n",
    "        (\"外其身而身存。\", \"Daodejing 7\"),\n",
    "        (\"非以其無私耶，故能成其私。\", \"Daodejing 7\"),\n",
    "        (\"柔弱勝剛強。\", \"Daodejing 36\"),\n",
    "        (\"大方無隅，大器晚成。\", \"Daodejing 41\"),\n",
    "        (\"大音希聲，大象無形。\", \"Daodejing 41\"),\n",
    "        (\"道生一，一生二，二生三，三生萬物。\", \"Daodejing 42\"),\n",
    "        (\"天下萬物生於有，有生於無。\", \"Daodejing 40\"),\n",
    "        (\"千里之行，始於足下。\", \"Daodejing 64\"),\n",
    "        (\"合抱之木，生於毫末。\", \"Daodejing 64\"),\n",
    "        (\"九層之臺，起於累土。\", \"Daodejing 64\"),\n",
    "        (\"民不畏死，奈何以死懼之。\", \"Daodejing 74\"),\n",
    "        (\"信言不美，美言不信。\", \"Daodejing 81\"),\n",
    "        (\"善者不辯，辯者不善。\", \"Daodejing 81\"),\n",
    "        (\"知者不博，博者不知。\", \"Daodejing 81\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(daodejing):\n",
    "        chinese.append({\"id\": f\"cn_daodejing_{i}\", \"text\": text, \"source\": source, \"period\": \"DAOIST\", \"century\": -4})\n",
    "    \n",
    "    # === GREAT LEARNING (大學) - 20+ passages ===\n",
    "    daxue = [\n",
    "        (\"大學之道，在明明德，在親民，在止於至善。\", \"Great Learning 1\"),\n",
    "        (\"知止而後有定，定而後能靜。\", \"Great Learning 1\"),\n",
    "        (\"靜而後能安，安而後能慮，慮而後能得。\", \"Great Learning 1\"),\n",
    "        (\"物有本末，事有終始。\", \"Great Learning 1\"),\n",
    "        (\"知所先後，則近道矣。\", \"Great Learning 1\"),\n",
    "        (\"古之欲明明德於天下者，先治其國。\", \"Great Learning 1\"),\n",
    "        (\"欲治其國者，先齊其家。\", \"Great Learning 1\"),\n",
    "        (\"欲齊其家者，先修其身。\", \"Great Learning 1\"),\n",
    "        (\"欲修其身者，先正其心。\", \"Great Learning 1\"),\n",
    "        (\"欲正其心者，先誠其意。\", \"Great Learning 1\"),\n",
    "        (\"欲誠其意者，先致其知。\", \"Great Learning 1\"),\n",
    "        (\"致知在格物。\", \"Great Learning 1\"),\n",
    "        (\"物格而後知至，知至而後意誠。\", \"Great Learning 1\"),\n",
    "        (\"意誠而後心正，心正而後身修。\", \"Great Learning 1\"),\n",
    "        (\"身修而後家齊，家齊而後國治。\", \"Great Learning 1\"),\n",
    "        (\"國治而後天下平。\", \"Great Learning 1\"),\n",
    "        (\"自天子以至於庶人，壹是皆以修身為本。\", \"Great Learning 1\"),\n",
    "        (\"其本亂而末治者否矣。\", \"Great Learning 1\"),\n",
    "        (\"所謂誠其意者，毋自欺也。\", \"Great Learning 6\"),\n",
    "        (\"如惡惡臭，如好好色，此之謂自謙。\", \"Great Learning 6\"),\n",
    "        (\"故君子必慎其獨也。\", \"Great Learning 6\"),\n",
    "        (\"富潤屋，德潤身，心廣體胖。\", \"Great Learning 6\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(daxue):\n",
    "        chinese.append({\"id\": f\"cn_daxue_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -5})\n",
    "    \n",
    "    # === DOCTRINE OF THE MEAN (中庸) - 20+ passages ===\n",
    "    zhongyong = [\n",
    "        (\"天命之謂性，率性之謂道，修道之謂教。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"道也者，不可須臾離也；可離，非道也。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"是故君子戒慎乎其所不睹，恐懼乎其所不聞。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"莫見乎隱，莫顯乎微，故君子慎其獨也。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"喜怒哀樂之未發，謂之中。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"發而皆中節，謂之和。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"中也者，天下之大本也。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"和也者，天下之達道也。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"致中和，天地位焉，萬物育焉。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"君子中庸，小人反中庸。\", \"Doctrine of the Mean 2\"),\n",
    "        (\"君子之中庸也，君子而時中。\", \"Doctrine of the Mean 2\"),\n",
    "        (\"小人之反中庸也，小人而無忌憚也。\", \"Doctrine of the Mean 2\"),\n",
    "        (\"中庸其至矣乎！民鮮能久矣。\", \"Doctrine of the Mean 3\"),\n",
    "        (\"道之不行也，我知之矣：知者過之，愚者不及也。\", \"Doctrine of the Mean 4\"),\n",
    "        (\"道之不明也，我知之矣：賢者過之，不肖者不及也。\", \"Doctrine of the Mean 4\"),\n",
    "        (\"人莫不飲食也，鮮能知味也。\", \"Doctrine of the Mean 4\"),\n",
    "        (\"誠者，天之道也。誠之者，人之道也。\", \"Doctrine of the Mean 20\"),\n",
    "        (\"誠者，不勉而中，不思而得，從容中道，聖人也。\", \"Doctrine of the Mean 20\"),\n",
    "        (\"誠之者，擇善而固執之者也。\", \"Doctrine of the Mean 20\"),\n",
    "        (\"博學之，審問之，慎思之，明辨之，篤行之。\", \"Doctrine of the Mean 20\"),\n",
    "        (\"人一能之，己百之；人十能之，己千之。\", \"Doctrine of the Mean 20\"),\n",
    "        (\"果能此道矣，雖愚必明，雖柔必強。\", \"Doctrine of the Mean 20\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(zhongyong):\n",
    "        chinese.append({\"id\": f\"cn_zhongyong_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -5})\n",
    "    \n",
    "    # === BOOK OF RITES (禮記) - 30+ passages ===\n",
    "    liji = [\n",
    "        (\"禮尚往來。往而不來，非禮也；來而不往，亦非禮也。\", \"Book of Rites - Quli\"),\n",
    "        (\"敖不可長，欲不可從，志不可滿，樂不可極。\", \"Book of Rites - Quli\"),\n",
    "        (\"臨財毋茍得，臨難毋茍免。\", \"Book of Rites - Quli\"),\n",
    "        (\"夫禮者，自卑而尊人。\", \"Book of Rites - Quli\"),\n",
    "        (\"雖負販者，必有尊也，而況富貴乎。\", \"Book of Rites - Quli\"),\n",
    "        (\"富貴而知好禮，則不驕不淫。\", \"Book of Rites - Quli\"),\n",
    "        (\"貧賤而知好禮，則志不懾。\", \"Book of Rites - Quli\"),\n",
    "        (\"大道之行也，天下為公。\", \"Book of Rites - Liyun\"),\n",
    "        (\"選賢與能，講信修睦。\", \"Book of Rites - Liyun\"),\n",
    "        (\"故人不獨親其親，不獨子其子。\", \"Book of Rites - Liyun\"),\n",
    "        (\"使老有所終，壯有所用，幼有所長。\", \"Book of Rites - Liyun\"),\n",
    "        (\"矜寡孤獨廢疾者皆有所養。\", \"Book of Rites - Liyun\"),\n",
    "        (\"男有分，女有歸。\", \"Book of Rites - Liyun\"),\n",
    "        (\"貨惡其棄於地也，不必藏於己。\", \"Book of Rites - Liyun\"),\n",
    "        (\"力惡其不出於身也，不必為己。\", \"Book of Rites - Liyun\"),\n",
    "        (\"是故謀閉而不興，盜竊亂賊而不作。\", \"Book of Rites - Liyun\"),\n",
    "        (\"故外戶而不閉，是謂大同。\", \"Book of Rites - Liyun\"),\n",
    "        (\"玉不琢，不成器；人不學，不知道。\", \"Book of Rites - Xueji\"),\n",
    "        (\"是故學然後知不足，教然後知困。\", \"Book of Rites - Xueji\"),\n",
    "        (\"知不足，然後能自反也。\", \"Book of Rites - Xueji\"),\n",
    "        (\"知困，然後能自強也。\", \"Book of Rites - Xueji\"),\n",
    "        (\"故曰：教學相長也。\", \"Book of Rites - Xueji\"),\n",
    "        (\"凡學之道，嚴師為難。\", \"Book of Rites - Xueji\"),\n",
    "        (\"師嚴然後道尊，道尊然後民知敬學。\", \"Book of Rites - Xueji\"),\n",
    "        (\"善歌者使人繼其聲，善教者使人繼其志。\", \"Book of Rites - Xueji\"),\n",
    "        (\"記問之學，不足以為人師。\", \"Book of Rites - Xueji\"),\n",
    "        (\"必也其聽語乎，力不能問，然後語之。\", \"Book of Rites - Xueji\"),\n",
    "        (\"語之而不知，雖舍之可也。\", \"Book of Rites - Xueji\"),\n",
    "        (\"博學而不窮，篤行而不倦。\", \"Book of Rites - Ruxing\"),\n",
    "        (\"君子之於學也，藏焉，修焉，息焉，游焉。\", \"Book of Rites - Xueji\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(liji):\n",
    "        chinese.append({\"id\": f\"cn_liji_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -3})\n",
    "    \n",
    "    with open('data/raw/chinese/chinese_native.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(chinese, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"  Created {len(chinese)} Chinese passages\")\n",
    "    \n",
    "    # ISLAMIC - 150+ REAL PASSAGES\n",
    "    print(\"\\n[3/4] Islamic texts (150+ real passages)...\")\n",
    "    os.makedirs('data/raw/islamic', exist_ok=True)\n",
    "    \n",
    "    islamic = []\n",
    "    \n",
    "    # === QURANIC VERSES (40+) ===\n",
    "    quran = [\n",
    "        (\"وَلَا تَقْتُلُوا النَّفْسَ الَّتِي حَرَّمَ اللَّهُ إِلَّا بِالْحَقِّ\", \"Quran 6:151\"),\n",
    "        (\"وَبِالْوَالِدَيْنِ إِحْسَانًا\", \"Quran 17:23\"),\n",
    "        (\"إِمَّا يَبْلُغَنَّ عِندَكَ الْكِبَرَ أَحَدُهُمَا أَوْ كِلَاهُمَا فَلَا تَقُل لَّهُمَا أُفٍّ\", \"Quran 17:23\"),\n",
    "        (\"وَلَا تَنْهَرْهُمَا وَقُل لَّهُمَا قَوْلًا كَرِيمًا\", \"Quran 17:23\"),\n",
    "        (\"وَاخْفِضْ لَهُمَا جَنَاحَ الذُّلِّ مِنَ الرَّحْمَةِ\", \"Quran 17:24\"),\n",
    "        (\"وَقُل رَّبِّ ارْحَمْهُمَا كَمَا رَبَّيَانِي صَغِيرًا\", \"Quran 17:24\"),\n",
    "        (\"وَآتِ ذَا الْقُرْبَىٰ حَقَّهُ وَالْمِسْكِينَ وَابْنَ السَّبِيلِ\", \"Quran 17:26\"),\n",
    "        (\"وَلَا تُبَذِّرْ تَبْذِيرًا\", \"Quran 17:26\"),\n",
    "        (\"إِنَّ الْمُبَذِّرِينَ كَانُوا إِخْوَانَ الشَّيَاطِينِ\", \"Quran 17:27\"),\n",
    "        (\"وَلَا تَجْعَلْ يَدَكَ مَغْلُولَةً إِلَىٰ عُنُقِكَ وَلَا تَبْسُطْهَا كُلَّ الْبَسْطِ\", \"Quran 17:29\"),\n",
    "        (\"وَلَا تَقْرَبُوا الزِّنَا ۖ إِنَّهُ كَانَ فَاحِشَةً وَسَاءَ سَبِيلًا\", \"Quran 17:32\"),\n",
    "        (\"وَلَا تَقْتُلُوا أَوْلَادَكُمْ خَشْيَةَ إِمْلَاقٍ\", \"Quran 17:31\"),\n",
    "        (\"وَلَا تَقْرَبُوا مَالَ الْيَتِيمِ إِلَّا بِالَّتِي هِيَ أَحْسَنُ\", \"Quran 17:34\"),\n",
    "        (\"وَأَوْفُوا بِالْعَهْدِ ۖ إِنَّ الْعَهْدَ كَانَ مَسْئُولًا\", \"Quran 17:34\"),\n",
    "        (\"وَأَوْفُوا الْكَيْلَ إِذَا كِلْتُمْ وَزِنُوا بِالْقِسْطَاسِ الْمُسْتَقِيمِ\", \"Quran 17:35\"),\n",
    "        (\"وَلَا تَقْفُ مَا لَيْسَ لَكَ بِهِ عِلْمٌ\", \"Quran 17:36\"),\n",
    "        (\"إِنَّ السَّمْعَ وَالْبَصَرَ وَالْفُؤَادَ كُلُّ أُولَٰئِكَ كَانَ عَنْهُ مَسْئُولًا\", \"Quran 17:36\"),\n",
    "        (\"وَلَا تَمْشِ فِي الْأَرْضِ مَرَحًا\", \"Quran 17:37\"),\n",
    "        (\"إِنَّ اللَّهَ يَأْمُرُ بِالْعَدْلِ وَالْإِحْسَانِ وَإِيتَاءِ ذِي الْقُرْبَىٰ\", \"Quran 16:90\"),\n",
    "        (\"وَيَنْهَىٰ عَنِ الْفَحْشَاءِ وَالْمُنكَرِ وَالْبَغْيِ\", \"Quran 16:90\"),\n",
    "        (\"يَا أَيُّهَا الَّذِينَ آمَنُوا كُونُوا قَوَّامِينَ بِالْقِسْطِ\", \"Quran 4:135\"),\n",
    "        (\"شُهَدَاءَ لِلَّهِ وَلَوْ عَلَىٰ أَنفُسِكُمْ أَوِ الْوَالِدَيْنِ وَالْأَقْرَبِينَ\", \"Quran 4:135\"),\n",
    "        (\"وَإِذَا حَكَمْتُم بَيْنَ النَّاسِ أَن تَحْكُمُوا بِالْعَدْلِ\", \"Quran 4:58\"),\n",
    "        (\"يَا أَيُّهَا الَّذِينَ آمَنُوا أَوْفُوا بِالْعُقُودِ\", \"Quran 5:1\"),\n",
    "        (\"وَتَعَاوَنُوا عَلَى الْبِرِّ وَالتَّقْوَىٰ ۖ وَلَا تَعَاوَنُوا عَلَى الْإِثْمِ وَالْعُدْوَانِ\", \"Quran 5:2\"),\n",
    "        (\"مَن قَتَلَ نَفْسًا بِغَيْرِ نَفْسٍ أَوْ فَسَادٍ فِي الْأَرْضِ فَكَأَنَّمَا قَتَلَ النَّاسَ جَمِيعًا\", \"Quran 5:32\"),\n",
    "        (\"وَمَنْ أَحْيَاهَا فَكَأَنَّمَا أَحْيَا النَّاسَ جَمِيعًا\", \"Quran 5:32\"),\n",
    "        (\"وَلَا يَجْرِمَنَّكُمْ شَنَآنُ قَوْمٍ عَلَىٰ أَلَّا تَعْدِلُوا\", \"Quran 5:8\"),\n",
    "        (\"اعْدِلُوا هُوَ أَقْرَبُ لِلتَّقْوَىٰ\", \"Quran 5:8\"),\n",
    "        (\"لَّيْسَ الْبِرَّ أَن تُوَلُّوا وُجُوهَكُمْ قِبَلَ الْمَشْرِقِ وَالْمَغْرِبِ\", \"Quran 2:177\"),\n",
    "        (\"وَلَٰكِنَّ الْبِرَّ مَنْ آمَنَ بِاللَّهِ وَالْيَوْمِ الْآخِرِ\", \"Quran 2:177\"),\n",
    "        (\"وَآتَى الْمَالَ عَلَىٰ حُبِّهِ ذَوِي الْقُرْبَىٰ وَالْيَتَامَىٰ وَالْمَسَاكِينَ\", \"Quran 2:177\"),\n",
    "        (\"وَابْنَ السَّبِيلِ وَالسَّائِلِينَ وَفِي الرِّقَابِ\", \"Quran 2:177\"),\n",
    "        (\"وَأَقَامَ الصَّلَاةَ وَآتَى الزَّكَاةَ\", \"Quran 2:177\"),\n",
    "        (\"وَالْمُوفُونَ بِعَهْدِهِمْ إِذَا عَاهَدُوا\", \"Quran 2:177\"),\n",
    "        (\"وَالصَّابِرِينَ فِي الْبَأْسَاءِ وَالضَّرَّاءِ وَحِينَ الْبَأْسِ\", \"Quran 2:177\"),\n",
    "        (\"خُذِ الْعَفْوَ وَأْمُرْ بِالْعُرْفِ وَأَعْرِضْ عَنِ الْجَاهِلِينَ\", \"Quran 7:199\"),\n",
    "        (\"وَالْكَاظِمِينَ الْغَيْظَ وَالْعَافِينَ عَنِ النَّاسِ\", \"Quran 3:134\"),\n",
    "        (\"وَاللَّهُ يُحِبُّ الْمُحْسِنِينَ\", \"Quran 3:134\"),\n",
    "        (\"ادْفَعْ بِالَّتِي هِيَ أَحْسَنُ فَإِذَا الَّذِي بَيْنَكَ وَبَيْنَهُ عَدَاوَةٌ كَأَنَّهُ وَلِيٌّ حَمِيمٌ\", \"Quran 41:34\"),\n",
    "        (\"وَمَا يُلَقَّاهَا إِلَّا الَّذِينَ صَبَرُوا وَمَا يُلَقَّاهَا إِلَّا ذُو حَظٍّ عَظِيمٍ\", \"Quran 41:35\"),\n",
    "        (\"إِنَّ اللَّهَ يَأْمُرُكُمْ أَن تُؤَدُّوا الْأَمَانَاتِ إِلَىٰ أَهْلِهَا\", \"Quran 4:58\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(quran):\n",
    "        islamic.append({\"id\": f\"quran_{i}\", \"text\": text, \"source\": source, \"period\": \"QURANIC\", \"century\": 7})\n",
    "    \n",
    "    # === HADITH (110+) ===\n",
    "    hadith = [\n",
    "        (\"لا ضرر ولا ضرار\", \"Hadith - Ibn Majah\"),\n",
    "        (\"إنما الأعمال بالنيات وإنما لكل امرئ ما نوى\", \"Hadith - Bukhari 1\"),\n",
    "        (\"المسلم من سلم المسلمون من لسانه ويده\", \"Hadith - Bukhari 10\"),\n",
    "        (\"لا يؤمن أحدكم حتى يحب لأخيه ما يحب لنفسه\", \"Hadith - Bukhari 13\"),\n",
    "        (\"من كان يؤمن بالله واليوم الآخر فليقل خيرا أو ليصمت\", \"Hadith - Bukhari 6018\"),\n",
    "        (\"من كان يؤمن بالله واليوم الآخر فليكرم ضيفه\", \"Hadith - Bukhari 6019\"),\n",
    "        (\"من كان يؤمن بالله واليوم الآخر فليصل رحمه\", \"Hadith - Bukhari 6138\"),\n",
    "        (\"ارحموا من في الأرض يرحمكم من في السماء\", \"Hadith - Tirmidhi 1924\"),\n",
    "        (\"الراحمون يرحمهم الرحمن\", \"Hadith - Abu Dawud 4941\"),\n",
    "        (\"ليس منا من لم يرحم صغيرنا ويوقر كبيرنا\", \"Hadith - Tirmidhi 1919\"),\n",
    "        (\"خيركم خيركم لأهله وأنا خيركم لأهلي\", \"Hadith - Tirmidhi 3895\"),\n",
    "        (\"اتق الله حيثما كنت وأتبع السيئة الحسنة تمحها\", \"Hadith - Tirmidhi 1987\"),\n",
    "        (\"وخالق الناس بخلق حسن\", \"Hadith - Tirmidhi 1987\"),\n",
    "        (\"أكمل المؤمنين إيمانا أحسنهم خلقا\", \"Hadith - Abu Dawud 4682\"),\n",
    "        (\"إن من أحبكم إلي وأقربكم مني مجلسا يوم القيامة أحاسنكم أخلاقا\", \"Hadith - Tirmidhi 2018\"),\n",
    "        (\"ما من شيء أثقل في ميزان المؤمن يوم القيامة من حسن الخلق\", \"Hadith - Tirmidhi 2002\"),\n",
    "        (\"البر حسن الخلق والإثم ما حاك في صدرك وكرهت أن يطلع عليه الناس\", \"Hadith - Muslim 2553\"),\n",
    "        (\"الحياء من الإيمان\", \"Hadith - Bukhari 24\"),\n",
    "        (\"الحياء لا يأتي إلا بخير\", \"Hadith - Bukhari 6117\"),\n",
    "        (\"إن الله رفيق يحب الرفق في الأمر كله\", \"Hadith - Bukhari 6927\"),\n",
    "        (\"ما كان الرفق في شيء إلا زانه وما نزع من شيء إلا شانه\", \"Hadith - Muslim 2594\"),\n",
    "        (\"من يحرم الرفق يحرم الخير كله\", \"Hadith - Muslim 2592\"),\n",
    "        (\"أد الأمانة إلى من ائتمنك ولا تخن من خانك\", \"Hadith - Abu Dawud 3535\"),\n",
    "        (\"آية المنافق ثلاث إذا حدث كذب وإذا وعد أخلف وإذا اؤتمن خان\", \"Hadith - Bukhari 33\"),\n",
    "        (\"الصدق يهدي إلى البر والبر يهدي إلى الجنة\", \"Hadith - Bukhari 6094\"),\n",
    "        (\"وإن الكذب يهدي إلى الفجور والفجور يهدي إلى النار\", \"Hadith - Bukhari 6094\"),\n",
    "        (\"عليكم بالصدق فإن الصدق يهدي إلى البر\", \"Hadith - Muslim 2607\"),\n",
    "        (\"إياكم والكذب فإن الكذب يهدي إلى الفجور\", \"Hadith - Muslim 2607\"),\n",
    "        (\"من غشنا فليس منا\", \"Hadith - Muslim 101\"),\n",
    "        (\"كلكم راع وكلكم مسؤول عن رعيته\", \"Hadith - Bukhari 893\"),\n",
    "        (\"الإمام راع ومسؤول عن رعيته\", \"Hadith - Bukhari 893\"),\n",
    "        (\"والرجل راع في أهله ومسؤول عن رعيته\", \"Hadith - Bukhari 893\"),\n",
    "        (\"والمرأة راعية في بيت زوجها ومسؤولة عن رعيتها\", \"Hadith - Bukhari 893\"),\n",
    "        (\"انصر أخاك ظالما أو مظلوما\", \"Hadith - Bukhari 2444\"),\n",
    "        (\"تنصره إذا كان مظلوما أفرأيت إذا كان ظالما كيف تنصره قال تحجزه أو تمنعه من الظلم فإن ذلك نصره\", \"Hadith - Bukhari 2444\"),\n",
    "        (\"المؤمن للمؤمن كالبنيان يشد بعضه بعضا\", \"Hadith - Bukhari 481\"),\n",
    "        (\"مثل المؤمنين في توادهم وتراحمهم وتعاطفهم مثل الجسد الواحد\", \"Hadith - Muslim 2586\"),\n",
    "        (\"إذا اشتكى منه عضو تداعى له سائر الجسد بالسهر والحمى\", \"Hadith - Muslim 2586\"),\n",
    "        (\"المسلم أخو المسلم لا يظلمه ولا يسلمه\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"من كان في حاجة أخيه كان الله في حاجته\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"ومن فرج عن مسلم كربة فرج الله عنه كربة من كربات يوم القيامة\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"ومن ستر مسلما ستره الله يوم القيامة\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"لا تحاسدوا ولا تناجشوا ولا تباغضوا ولا تدابروا\", \"Hadith - Muslim 2564\"),\n",
    "        (\"ولا يبع بعضكم على بيع بعض وكونوا عباد الله إخوانا\", \"Hadith - Muslim 2564\"),\n",
    "        (\"بحسب امرئ من الشر أن يحقر أخاه المسلم\", \"Hadith - Muslim 2564\"),\n",
    "        (\"كل المسلم على المسلم حرام دمه وماله وعرضه\", \"Hadith - Muslim 2564\"),\n",
    "        (\"إياكم والظن فإن الظن أكذب الحديث\", \"Hadith - Bukhari 6064\"),\n",
    "        (\"ولا تجسسوا ولا تحسسوا ولا تنافسوا\", \"Hadith - Bukhari 6064\"),\n",
    "        (\"الظلم ظلمات يوم القيامة\", \"Hadith - Bukhari 2447\"),\n",
    "        (\"اتقوا الظلم فإن الظلم ظلمات يوم القيامة\", \"Hadith - Muslim 2578\"),\n",
    "        (\"واتقوا الشح فإن الشح أهلك من كان قبلكم\", \"Hadith - Muslim 2578\"),\n",
    "        (\"أفضل الجهاد كلمة عدل عند سلطان جائر\", \"Hadith - Abu Dawud 4344\"),\n",
    "        (\"سيد الشهداء حمزة بن عبد المطلب ورجل قام إلى إمام جائر فأمره ونهاه فقتله\", \"Hadith - Hakim 4884\"),\n",
    "        (\"إذا رأيت أمتي تهاب أن تقول للظالم يا ظالم فقد تودع منهم\", \"Hadith - Ahmad 6521\"),\n",
    "        (\"من رأى منكم منكرا فليغيره بيده\", \"Hadith - Muslim 49\"),\n",
    "        (\"فإن لم يستطع فبلسانه فإن لم يستطع فبقلبه وذلك أضعف الإيمان\", \"Hadith - Muslim 49\"),\n",
    "        (\"أحب الناس إلى الله أنفعهم للناس\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"وأحب الأعمال إلى الله سرور تدخله على مسلم\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"أو تكشف عنه كربة أو تقضي عنه دينا أو تطرد عنه جوعا\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"ولأن أمشي مع أخي في حاجة أحب إلي من أن أعتكف في هذا المسجد شهرا\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"الدين النصيحة قلنا لمن قال لله ولكتابه ولرسوله ولأئمة المسلمين وعامتهم\", \"Hadith - Muslim 55\"),\n",
    "        (\"ما نقصت صدقة من مال\", \"Hadith - Muslim 2588\"),\n",
    "        (\"وما زاد الله عبدا بعفو إلا عزا\", \"Hadith - Muslim 2588\"),\n",
    "        (\"وما تواضع أحد لله إلا رفعه الله\", \"Hadith - Muslim 2588\"),\n",
    "        (\"اليد العليا خير من اليد السفلى\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"وابدأ بمن تعول\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"وخير الصدقة ما كان عن ظهر غنى\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"من استطاع منكم الباءة فليتزوج\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"فإنه أغض للبصر وأحصن للفرج\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"ومن لم يستطع فعليه بالصوم فإنه له وجاء\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"استوصوا بالنساء خيرا\", \"Hadith - Bukhari 3331\"),\n",
    "        (\"خذوا عني خذوا عني قد جعل الله لهن سبيلا البكر بالبكر جلد مائة ونفي سنة\", \"Hadith - Muslim 1690\"),\n",
    "        (\"لا يفرك مؤمن مؤمنة إن كره منها خلقا رضي منها آخر\", \"Hadith - Muslim 1469\"),\n",
    "        (\"أكمل المؤمنين إيمانا أحسنهم خلقا وخياركم خياركم لنسائهم\", \"Hadith - Tirmidhi 1162\"),\n",
    "        (\"ما أكرمهن إلا كريم وما أهانهن إلا لئيم\", \"Hadith - Ibn Asakir\"),\n",
    "        (\"اللهم إني أحرج حق الضعيفين اليتيم والمرأة\", \"Hadith - Ahmad 9664\"),\n",
    "        (\"ألا أخبركم بخياركم قالوا بلى قال خياركم أحاسنكم أخلاقا\", \"Hadith - Bukhari 6035\"),\n",
    "        (\"إنكم لن تسعوا الناس بأموالكم فليسعهم منكم بسط الوجه وحسن الخلق\", \"Hadith - Hakim 422\"),\n",
    "        (\"تبسمك في وجه أخيك صدقة\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"وأمرك بالمعروف ونهيك عن المنكر صدقة\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"وإرشادك الرجل في أرض الضلال لك صدقة\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"وإماطتك الأذى والشوك والعظم عن الطريق لك صدقة\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"وإفراغك من دلوك في دلو أخيك لك صدقة\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"الكلمة الطيبة صدقة\", \"Hadith - Bukhari 2989\"),\n",
    "        (\"وكل خطوة تمشيها إلى الصلاة صدقة\", \"Hadith - Bukhari 2989\"),\n",
    "        (\"من دل على خير فله مثل أجر فاعله\", \"Hadith - Muslim 1893\"),\n",
    "        (\"ليس الشديد بالصرعة إنما الشديد الذي يملك نفسه عند الغضب\", \"Hadith - Bukhari 6114\"),\n",
    "        (\"لا تغضب فردد مرارا قال لا تغضب\", \"Hadith - Bukhari 6116\"),\n",
    "        (\"إن الغضب من الشيطان وإن الشيطان خلق من النار\", \"Hadith - Abu Dawud 4784\"),\n",
    "        (\"وإنما تطفأ النار بالماء فإذا غضب أحدكم فليتوضأ\", \"Hadith - Abu Dawud 4784\"),\n",
    "        (\"لا يحل لمسلم أن يهجر أخاه فوق ثلاث ليال\", \"Hadith - Bukhari 6077\"),\n",
    "        (\"يلتقيان فيعرض هذا ويعرض هذا وخيرهما الذي يبدأ بالسلام\", \"Hadith - Bukhari 6077\"),\n",
    "        (\"أفشوا السلام بينكم\", \"Hadith - Muslim 54\"),\n",
    "        (\"والذي نفسي بيده لا تدخلوا الجنة حتى تؤمنوا\", \"Hadith - Muslim 54\"),\n",
    "        (\"ولا تؤمنوا حتى تحابوا أولا أدلكم على شيء إذا فعلتموه تحاببتم أفشوا السلام بينكم\", \"Hadith - Muslim 54\"),\n",
    "        (\"طعام الاثنين كافي الثلاثة وطعام الثلاثة كافي الأربعة\", \"Hadith - Bukhari 5392\"),\n",
    "        (\"ما ملأ آدمي وعاء شرا من بطن\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"بحسب ابن آدم أكلات يقمن صلبه\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"فإن كان لا محالة فثلث لطعامه وثلث لشرابه وثلث لنفسه\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"إن الله كتب الإحسان على كل شيء\", \"Hadith - Muslim 1955\"),\n",
    "        (\"فإذا قتلتم فأحسنوا القتلة وإذا ذبحتم فأحسنوا الذبح\", \"Hadith - Muslim 1955\"),\n",
    "        (\"وليحد أحدكم شفرته وليرح ذبيحته\", \"Hadith - Muslim 1955\"),\n",
    "        (\"عذبت امرأة في هرة سجنتها حتى ماتت\", \"Hadith - Bukhari 3318\"),\n",
    "        (\"فلا هي أطعمتها ولا سقتها إذ حبستها ولا هي تركتها تأكل من خشاش الأرض\", \"Hadith - Bukhari 3318\"),\n",
    "        (\"بينما رجل يمشي بطريق اشتد عليه العطش فوجد بئرا فنزل فيها فشرب\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"ثم خرج فإذا كلب يلهث يأكل الثرى من العطش\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"فقال لقد بلغ هذا الكلب من العطش مثل الذي كان بلغ مني\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"فنزل البئر فملأ خفه ماء ثم أمسكه بفيه حتى رقي فسقى الكلب\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"فشكر الله له فغفر له\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"في كل كبد رطبة أجر\", \"Hadith - Bukhari 2466\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(hadith):\n",
    "        islamic.append({\"id\": f\"hadith_{i}\", \"text\": text, \"source\": source, \"period\": \"HADITH\", \"century\": 9})\n",
    "    \n",
    "    with open('data/raw/islamic/islamic_native.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(islamic, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"  Created {len(islamic)} Islamic passages\")\n",
    "    \n",
    "    # DEAR ABBY\n",
    "    print(\"\\n[4/4] Dear Abby...\")\n",
    "    abby_count = 0\n",
    "    if not os.path.exists('data/raw/dear_abby.csv') or os.path.getsize('data/raw/dear_abby.csv') < 10000:\n",
    "        # Check if in Drive\n",
    "        if 'dear_abby.csv' in DRIVE_FILES:\n",
    "            shutil.copy(f'{SAVE_DIR}/dear_abby.csv', 'data/raw/dear_abby.csv')\n",
    "            print(\"  Loaded from Drive\")\n",
    "        else:\n",
    "            try:\n",
    "                subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", \n",
    "                               \"thedevastator/20000-dear-abby-questions\", \n",
    "                               \"-p\", \"data/raw/\", \"--unzip\"], check=True, timeout=120)\n",
    "                print(\"  Downloaded from Kaggle\")\n",
    "            except:\n",
    "                print(\"  Kaggle failed - creating minimal fallback\")\n",
    "                fallback = [{\"question_only\": f\"Dear Abby, I have a problem {i}\", \"year\": 1990+i%30} for i in range(100)]\n",
    "                pd.DataFrame(fallback).to_csv('data/raw/dear_abby.csv', index=False)\n",
    "    else:\n",
    "        print(\"  Already exists\")\n",
    "    \n",
    "    # Count Dear Abby samples\n",
    "    try:\n",
    "        df = pd.read_csv('data/raw/dear_abby.csv')\n",
    "        abby_count = len([1 for _, row in df.iterrows() if str(row.get('question_only', '')) != 'nan' and 50 <= len(str(row.get('question_only', ''))) <= 2000])\n",
    "    except:\n",
    "        abby_count = 0\n",
    "    \n",
    "    # Warning for insufficient Dear Abby data\n",
    "    if abby_count < 1000:\n",
    "        print(\"\\n\" + \"!\"*60)\n",
    "        print(\"CRITICAL: Dear Abby corpus is too small!\")\n",
    "        print(\"The semitic_to_non_semitic split WILL FAIL without this data.\")\n",
    "        print(\"\\nTo fix:\")\n",
    "        print(\"1. Download from: kaggle.com/datasets/thedevastator/20000-dear-abby-questions\")\n",
    "        print(\"2. Upload dear_abby.csv to your Google Drive BIP_v10 folder\")\n",
    "        print(\"3. Set REFRESH_DATA_FROM_SOURCE = True and rerun\")\n",
    "        print(\"!\"*60 + \"\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Downloads complete\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. Patterns + Normalization { display-mode: \"form\" }\n",
    "#@markdown Complete native patterns for moral concepts in 5 languages\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from enum import Enum, auto\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEXT NORMALIZATION & PATTERNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== TEXT NORMALIZATION =====\n",
    "def normalize_hebrew(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'[\\u0591-\\u05C7]', '', text)  # Remove nikud\n",
    "    for final, regular in [('\\u05da','\\u05db'), ('\\u05dd','\\u05de'), ('\\u05df','\\u05e0'), ('\\u05e3','\\u05e4'), ('\\u05e5','\\u05e6')]:\n",
    "        text = text.replace(final, regular)\n",
    "    return text\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'[\\u064B-\\u065F]', '', text)  # Remove tashkeel\n",
    "    text = text.replace('\\u0640', '')  # Remove tatweel\n",
    "    for v in ['\\u0623', '\\u0625', '\\u0622', '\\u0671']:\n",
    "        text = text.replace(v, '\\u0627')\n",
    "    text = text.replace('\\u0629', '\\u0647').replace('\\u0649', '\\u064a')\n",
    "    return text\n",
    "\n",
    "def normalize_text(text, language):\n",
    "    if language in ['hebrew', 'aramaic']:\n",
    "        return normalize_hebrew(text)\n",
    "    elif language == 'arabic':\n",
    "        return normalize_arabic(text)\n",
    "    elif language == 'classical_chinese':\n",
    "        return unicodedata.normalize('NFKC', text)\n",
    "    else:\n",
    "        return unicodedata.normalize('NFKC', text.lower())\n",
    "\n",
    "# ===== BOND AND HOHFELD TYPES =====\n",
    "class BondType(Enum):\n",
    "    HARM_PREVENTION = auto()\n",
    "    RECIPROCITY = auto()\n",
    "    AUTONOMY = auto()\n",
    "    PROPERTY = auto()\n",
    "    FAMILY = auto()\n",
    "    AUTHORITY = auto()\n",
    "    CARE = auto()\n",
    "    FAIRNESS = auto()\n",
    "    CONTRACT = auto()\n",
    "    NONE = auto()\n",
    "\n",
    "class HohfeldState(Enum):\n",
    "    OBLIGATION = auto()\n",
    "    RIGHT = auto()\n",
    "    LIBERTY = auto()\n",
    "    NO_RIGHT = auto()\n",
    "\n",
    "# ===== COMPLETE BOND PATTERNS =====\n",
    "ALL_BOND_PATTERNS = {\n",
    "    'hebrew': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u05d4\\u05e8\\u05d2', r'\\u05e8\\u05e6\\u05d7', r'\\u05e0\\u05d6\\u05e7', r'\\u05d4\\u05db\\u05d4', r'\\u05d4\\u05e6\\u05d9\\u05dc', r'\\u05e9\\u05de\\u05e8', r'\\u05e4\\u05e7\\u05d5\\u05d7.\\u05e0\\u05e4\\u05e9'],\n",
    "        BondType.RECIPROCITY: [r'\\u05d2\\u05de\\u05d5\\u05dc', r'\\u05d4\\u05e9\\u05d9\\u05d1', r'\\u05e4\\u05e8\\u05e2', r'\\u05e0\\u05ea\\u05df.*\\u05e7\\u05d1\\u05dc', r'\\u05de\\u05d3\\u05d4.\\u05db\\u05e0\\u05d2\\u05d3'],\n",
    "        BondType.AUTONOMY: [r'\\u05d1\\u05d7\\u05e8', r'\\u05e8\\u05e6\\u05d5\\u05df', r'\\u05d7\\u05e4\\u05e9', r'\\u05e2\\u05e6\\u05de'],\n",
    "        BondType.PROPERTY: [r'\\u05e7\\u05e0\\u05d4', r'\\u05de\\u05db\\u05e8', r'\\u05d2\\u05d6\\u05dc', r'\\u05d2\\u05e0\\u05d1', r'\\u05de\\u05de\\u05d5\\u05df', r'\\u05e0\\u05db\\u05e1', r'\\u05d9\\u05e8\\u05e9'],\n",
    "        BondType.FAMILY: [r'\\u05d0\\u05d1', r'\\u05d0\\u05de', r'\\u05d1\\u05e0', r'\\u05db\\u05d1\\u05d3.*\\u05d0\\u05d1', r'\\u05db\\u05d1\\u05d3.*\\u05d0\\u05de', r'\\u05de\\u05e9\\u05e4\\u05d7\\u05d4', r'\\u05d0\\u05d7', r'\\u05d0\\u05d7\\u05d5\\u05ea'],\n",
    "        BondType.AUTHORITY: [r'\\u05de\\u05dc\\u05db', r'\\u05e9\\u05d5\\u05e4\\u05d8', r'\\u05e6\\u05d5\\u05d4', r'\\u05ea\\u05d5\\u05e8\\u05d4', r'\\u05de\\u05e6\\u05d5\\u05d4', r'\\u05d3\\u05d9\\u05df', r'\\u05d7\\u05e7'],\n",
    "        BondType.CARE: [r'\\u05d7\\u05e1\\u05d3', r'\\u05e8\\u05d7\\u05de', r'\\u05e2\\u05d6\\u05e8', r'\\u05ea\\u05de\\u05db', r'\\u05e6\\u05d3\\u05e7\\u05d4'],\n",
    "        BondType.FAIRNESS: [r'\\u05e6\\u05d3\\u05e7', r'\\u05de\\u05e9\\u05e4\\u05d8', r'\\u05d9\\u05e9\\u05e8', r'\\u05e9\\u05d5\\u05d4'],\n",
    "        BondType.CONTRACT: [r'\\u05d1\\u05e8\\u05d9\\u05ea', r'\\u05e0\\u05d3\\u05e8', r'\\u05e9\\u05d1\\u05d5\\u05e2', r'\\u05d4\\u05ea\\u05d7\\u05d9\\u05d1', r'\\u05e2\\u05e8\\u05d1'],\n",
    "    },\n",
    "    'aramaic': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u05e7\\u05d8\\u05dc', r'\\u05e0\\u05d6\\u05e7', r'\\u05d7\\u05d1\\u05dc', r'\\u05e9\\u05d6\\u05d9\\u05d1', r'\\u05e4\\u05e6\\u05d9'],\n",
    "        BondType.RECIPROCITY: [r'\\u05e4\\u05e8\\u05e2', r'\\u05e9\\u05dc\\u05de', r'\\u05d0\\u05d2\\u05e8'],\n",
    "        BondType.AUTONOMY: [r'\\u05e6\\u05d1\\u05d9', r'\\u05e8\\u05e2\\u05d5'],\n",
    "        BondType.PROPERTY: [r'\\u05d6\\u05d1\\u05e0', r'\\u05e7\\u05e0\\u05d4', r'\\u05d2\\u05d6\\u05dc', r'\\u05de\\u05de\\u05d5\\u05e0\\u05d0', r'\\u05e0\\u05db\\u05e1\\u05d9'],\n",
    "        BondType.FAMILY: [r'\\u05d0\\u05d1\\u05d0', r'\\u05d0\\u05de\\u05d0', r'\\u05d1\\u05e8\\u05d0', r'\\u05d1\\u05e8\\u05ea\\u05d0', r'\\u05d9\\u05e7\\u05e8', r'\\u05d0\\u05d7\\u05d0'],\n",
    "        BondType.AUTHORITY: [r'\\u05de\\u05dc\\u05db\\u05d0', r'\\u05d3\\u05d9\\u05e0\\u05d0', r'\\u05d3\\u05d9\\u05d9\\u05e0\\u05d0', r'\\u05e4\\u05e7\\u05d5\\u05d3\\u05d0', r'\\u05d0\\u05d5\\u05e8\\u05d9\\u05ea'],\n",
    "        BondType.CARE: [r'\\u05d7\\u05e1\\u05d3', r'\\u05e8\\u05d7\\u05de', r'\\u05e1\\u05e2\\u05d3'],\n",
    "        BondType.FAIRNESS: [r'\\u05d3\\u05d9\\u05e0\\u05d0', r'\\u05e7\\u05e9\\u05d5\\u05d8', r'\\u05ea\\u05e8\\u05d9\\u05e6'],\n",
    "        BondType.CONTRACT: [r'\\u05e7\\u05d9\\u05de\\u05d0', r'\\u05e9\\u05d1\\u05d5\\u05e2\\u05d4', r'\\u05e0\\u05d3\\u05e8\\u05d0', r'\\u05e2\\u05e8\\u05d1\\u05d0'],\n",
    "    },\n",
    "    'classical_chinese': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u6bba', r'\\u5bb3', r'\\u50b7', r'\\u6551', r'\\u8b77', r'\\u885b', r'\\u66b4'],\n",
    "        BondType.RECIPROCITY: [r'\\u5831', r'\\u9084', r'\\u511f', r'\\u8ced', r'\\u7b54'],\n",
    "        BondType.AUTONOMY: [r'\\u81ea', r'\\u7531', r'\\u4efb', r'\\u610f', r'\\u5fd7'],\n",
    "        BondType.PROPERTY: [r'\\u8ca1', r'\\u7269', r'\\u7522', r'\\u76dc', r'\\u7aca', r'\\u8ce3', r'\\u8cb7'],\n",
    "        BondType.FAMILY: [r'\\u5b5d', r'\\u7236', r'\\u6bcd', r'\\u89aa', r'\\u5b50', r'\\u5f1f', r'\\u5144', r'\\u5bb6'],\n",
    "        BondType.AUTHORITY: [r'\\u541b', r'\\u81e3', r'\\u738b', r'\\u547d', r'\\u4ee4', r'\\u6cd5', r'\\u6cbb'],\n",
    "        BondType.CARE: [r'\\u4ec1', r'\\u611b', r'\\u6148', r'\\u60e0', r'\\u6069', r'\\u6190'],\n",
    "        BondType.FAIRNESS: [r'\\u7fa9', r'\\u6b63', r'\\u516c', r'\\u5e73', r'\\u5747'],\n",
    "        BondType.CONTRACT: [r'\\u7d04', r'\\u76df', r'\\u8a93', r'\\u8afe', r'\\u4fe1'],\n",
    "    },\n",
    "    'arabic': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u0642\\u062a\\u0644', r'\\u0636\\u0631\\u0631', r'\\u0627\\u0630[\\u064a\\u0649]', r'\\u0638\\u0644\\u0645', r'\\u0627\\u0646\\u0642\\u0630', r'\\u062d\\u0641\\u0638', r'\\u0627\\u0645\\u0627\\u0646'],\n",
    "        BondType.RECIPROCITY: [r'\\u062c\\u0632\\u0627', r'\\u0631\\u062f', r'\\u0642\\u0635\\u0627\\u0635', r'\\u0645\\u062b\\u0644', r'\\u0639\\u0648\\u0636'],\n",
    "        BondType.AUTONOMY: [r'\\u062d\\u0631', r'\\u0627\\u0631\\u0627\\u062f\\u0629', r'\\u0627\\u062e\\u062a\\u064a\\u0627\\u0631', r'\\u0645\\u0634\\u064a\\u0626'],\n",
    "        BondType.PROPERTY: [r'\\u0645\\u0627\\u0644', r'\\u0645\\u0644\\u0643', r'\\u0633\\u0631\\u0642', r'\\u0628\\u064a\\u0639', r'\\u0634\\u0631\\u0627', r'\\u0645\\u064a\\u0631\\u0627\\u062b', r'\\u063a\\u0635\\u0628'],\n",
    "        BondType.FAMILY: [r'\\u0648\\u0627\\u0644\\u062f', r'\\u0627\\u0628\\u0648', r'\\u0627\\u0645', r'\\u0627\\u0628\\u0646', r'\\u0628\\u0646\\u062a', r'\\u0627\\u0647\\u0644', r'\\u0642\\u0631\\u0628[\\u064a\\u0649]', r'\\u0631\\u062d\\u0645'],\n",
    "        BondType.AUTHORITY: [r'\\u0637\\u0627\\u0639', r'\\u0627\\u0645\\u0631', r'\\u062d\\u0643\\u0645', r'\\u0633\\u0644\\u0637\\u0627\\u0646', r'\\u062e\\u0644\\u064a\\u0641', r'\\u0627\\u0645\\u0627\\u0645', r'\\u0634\\u0631\\u064a\\u0639'],\n",
    "        BondType.CARE: [r'\\u0631\\u062d\\u0645', r'\\u0627\\u062d\\u0633\\u0627\\u0646', r'\\u0639\\u0637\\u0641', r'\\u0635\\u062f\\u0642', r'\\u0632\\u0643\\u0627'],\n",
    "        BondType.FAIRNESS: [r'\\u0639\\u062f\\u0644', r'\\u0642\\u0633\\u0637', r'\\u062d\\u0642', r'\\u0627\\u0646\\u0635\\u0627\\u0641', r'\\u0633\\u0648[\\u064a\\u0649]'],\n",
    "        BondType.CONTRACT: [r'\\u0639\\u0647\\u062f', r'\\u0639\\u0642\\u062f', r'\\u0646\\u0630\\u0631', r'\\u064a\\u0645\\u064a\\u0646', r'\\u0648\\u0641\\u0627', r'\\u0627\\u0645\\u0627\\u0646'],\n",
    "    },\n",
    "    'english': {\n",
    "        BondType.HARM_PREVENTION: [r'\\bkill', r'\\bmurder', r'\\bharm', r'\\bhurt', r'\\bsave', r'\\bprotect', r'\\bviolence'],\n",
    "        BondType.RECIPROCITY: [r'\\breturn', r'\\brepay', r'\\bexchange', r'\\bgive.*back', r'\\breciproc'],\n",
    "        BondType.AUTONOMY: [r'\\bfree', r'\\bchoice', r'\\bchoose', r'\\bconsent', r'\\bautonomy', r'\\bright to'],\n",
    "        BondType.PROPERTY: [r'\\bsteal', r'\\btheft', r'\\bown', r'\\bproperty', r'\\bbelong', r'\\binherit'],\n",
    "        BondType.FAMILY: [r'\\bfather', r'\\bmother', r'\\bparent', r'\\bchild', r'\\bfamily', r'\\bhonor.*parent'],\n",
    "        BondType.AUTHORITY: [r'\\bobey', r'\\bcommand', r'\\bauthority', r'\\blaw', r'\\brule', r'\\bgovern'],\n",
    "        BondType.CARE: [r'\\bcare', r'\\bhelp', r'\\bkind', r'\\bcompassion', r'\\bcharity', r'\\bmercy'],\n",
    "        BondType.FAIRNESS: [r'\\bfair', r'\\bjust', r'\\bequal', r'\\bequity', r'\\bright\\b'],\n",
    "        BondType.CONTRACT: [r'\\bpromise', r'\\bcontract', r'\\bagreem', r'\\bvow', r'\\boath', r'\\bcommit'],\n",
    "    },\n",
    "}\n",
    "\n",
    "# ===== COMPLETE HOHFELD PATTERNS =====\n",
    "ALL_HOHFELD_PATTERNS = {\n",
    "    'hebrew': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u05d7\\u05d9\\u05d9\\u05d1', r'\\u05e6\\u05e8\\u05d9\\u05db', r'\\u05de\\u05d5\\u05db\\u05e8\\u05d7', r'\\u05de\\u05e6\\u05d5\\u05d5\\u05d4'],\n",
    "        HohfeldState.RIGHT: [r'\\u05d6\\u05db\\u05d5\\u05ea', r'\\u05e8\\u05e9\\u05d0\\u05d9', r'\\u05d6\\u05db\\u05d0\\u05d9', r'\\u05de\\u05d2\\u05d9\\u05e2'],\n",
    "        HohfeldState.LIBERTY: [r'\\u05de\\u05d5\\u05ea\\u05e8', r'\\u05e8\\u05e9\\u05d5\\u05ea', r'\\u05e4\\u05d8\\u05d5\\u05e8', r'\\u05d9\\u05db\\u05d5\\u05dc'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u05d0\\u05e1\\u05d5\\u05e8', r'\\u05d0\\u05d9\\u05e0\\u05d5 \\u05e8\\u05e9\\u05d0\\u05d9', r'\\u05d0\\u05d9\\u05df.*\\u05d6\\u05db\\u05d5\\u05ea'],\n",
    "    },\n",
    "    'aramaic': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u05d7\\u05d9\\u05d9\\u05d1', r'\\u05de\\u05d7\\u05d5\\u05d9\\u05d1', r'\\u05d1\\u05e2\\u05d9'],\n",
    "        HohfeldState.RIGHT: [r'\\u05d6\\u05db\\u05d5\\u05ea', r'\\u05e8\\u05e9\\u05d0\\u05d9', r'\\u05d6\\u05db\\u05d9'],\n",
    "        HohfeldState.LIBERTY: [r'\\u05e9\\u05e8\\u05d9', r'\\u05de\\u05d5\\u05ea\\u05e8', r'\\u05e4\\u05d8\\u05d5\\u05e8'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u05d0\\u05e1\\u05d5\\u05e8', r'\\u05dc\\u05d0.*\\u05e8\\u05e9\\u05d0\\u05d9'],\n",
    "    },\n",
    "    'classical_chinese': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u5fc5', r'\\u9808', r'\\u7576', r'\\u61c9', r'\\u5b9c'],\n",
    "        HohfeldState.RIGHT: [r'\\u53ef', r'\\u5f97', r'\\u6b0a', r'\\u5b9c'],\n",
    "        HohfeldState.LIBERTY: [r'\\u8a31', r'\\u4efb', r'\\u807d', r'\\u514d'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u4e0d\\u53ef', r'\\u52ff', r'\\u7981', r'\\u83ab', r'\\u975e'],\n",
    "    },\n",
    "    'arabic': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u064a\\u062c\\u0628', r'\\u0648\\u0627\\u062c\\u0628', r'\\u0641\\u0631\\u0636', r'\\u0644\\u0627\\u0632\\u0645', r'\\u0648\\u062c\\u0648\\u0628'],\n",
    "        HohfeldState.RIGHT: [r'\\u062d\\u0642', r'\\u064a\\u062d\\u0642', r'\\u062c\\u0627\\u0626\\u0632', r'\\u064a\\u062c\\u0648\\u0632'],\n",
    "        HohfeldState.LIBERTY: [r'\\u0645\\u0628\\u0627\\u062d', r'\\u062d\\u0644\\u0627\\u0644', r'\\u062c\\u0627\\u0626\\u0632', r'\\u0627\\u0628\\u0627\\u062d'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u062d\\u0631\\u0627\\u0645', r'\\u0645\\u062d\\u0631\\u0645', r'\\u0645\\u0645\\u0646\\u0648\\u0639', r'\\u0644\\u0627 \\u064a\\u062c\\u0648\\u0632', r'\\u0646\\u0647[\\u064a\\u0649]'],\n",
    "    },\n",
    "    'english': {\n",
    "        HohfeldState.OBLIGATION: [r'\\bmust\\b', r'\\bshall\\b', r'\\bobligat', r'\\bduty', r'\\brequir'],\n",
    "        HohfeldState.RIGHT: [r'\\bright\\b', r'\\bentitle', r'\\bdeserve', r'\\bclaim'],\n",
    "        HohfeldState.LIBERTY: [r'\\bmay\\b', r'\\bpermit', r'\\ballow', r'\\bfree to'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\bforbid', r'\\bprohibit', r'\\bmust not', r'\\bshall not'],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ===== CONTEXT MARKERS FOR GRAMMAR-AWARE EXTRACTION =====\n",
    "# These help distinguish \"thou shalt not kill\" from \"he killed\"\n",
    "CONTEXT_MARKERS = {\n",
    "    'hebrew': {\n",
    "        'negation': [r'לא', r'אל', r'אין', r'בלי', r'אינ'],\n",
    "        'obligation': [r'חייב', r'צריך', r'מוכרח', r'צווה'],\n",
    "        'prohibition': [r'אסור', r'אל.*ת'],\n",
    "        'permission': [r'מותר', r'רשאי', r'פטור'],\n",
    "    },\n",
    "    'aramaic': {\n",
    "        'negation': [r'לא', r'לית', r'לאו'],\n",
    "        'obligation': [r'חייב', r'בעי'],\n",
    "        'prohibition': [r'אסור'],\n",
    "        'permission': [r'שרי', r'מותר'],\n",
    "    },\n",
    "    'classical_chinese': {\n",
    "        'negation': [r'不', r'非', r'無', r'未', r'毋'],\n",
    "        'obligation': [r'必', r'當', r'須', r'應', r'宜'],\n",
    "        'prohibition': [r'勿', r'禁', r'莫', r'不可'],\n",
    "        'permission': [r'可', r'得', r'許'],\n",
    "    },\n",
    "    'arabic': {\n",
    "        'negation': [r'لا', r'ما', r'ليس', r'لم', r'غير'],\n",
    "        'obligation': [r'يجب', r'واجب', r'فرض', r'عليه'],\n",
    "        'prohibition': [r'حرام', r'محرم', r'لا يجوز', r'نهى'],\n",
    "        'permission': [r'حلال', r'مباح', r'جائز'],\n",
    "    },\n",
    "    'english': {\n",
    "        'negation': [r'\bnot\b', r'\bno\b', r'\bnever\b', r'\bneither\b', r\"n't\b\"],\n",
    "        'obligation': [r'\bmust\b', r'\bshall\b', r'\bshould\b', r'\bought\b', r'\brequired\b'],\n",
    "        'prohibition': [r'\bforbid', r'\bprohibit', r'\bmust not\b', r'\bshall not\b', r\"\bdon't\b\"],\n",
    "        'permission': [r'\bmay\b', r'\bcan\b', r'\ballowed\b', r'\bpermit'],\n",
    "    },\n",
    "}\n",
    "\n",
    "def detect_context(text, language, match_pos, window=30):\n",
    "    \"\"\"\n",
    "    Detect grammatical context around a pattern match.\n",
    "    Returns: ('prescriptive'/'descriptive'/'unknown', marker_type or None)\n",
    "    \"\"\"\n",
    "    markers = CONTEXT_MARKERS.get(language, {})\n",
    "    if not markers:\n",
    "        return 'unknown', None\n",
    "    \n",
    "    start = max(0, match_pos - window)\n",
    "    end = min(len(text), match_pos + window)\n",
    "    window_text = text[start:end]\n",
    "    \n",
    "    # Check for deontic markers (prescriptive = moral statement)\n",
    "    for marker_type in ['prohibition', 'obligation', 'permission']:\n",
    "        for pattern in markers.get(marker_type, []):\n",
    "            if re.search(pattern, window_text):\n",
    "                return 'prescriptive', marker_type\n",
    "    \n",
    "    # Check for simple negation (may be descriptive)\n",
    "    for pattern in markers.get('negation', []):\n",
    "        if re.search(pattern, window_text):\n",
    "            return 'descriptive', 'negated'\n",
    "    \n",
    "    return 'descriptive', None\n",
    "\n",
    "print(\"\\nContext markers defined for grammar-aware extraction\")\n",
    "print(\"  Detects: negation, obligation, prohibition, permission\")\n",
    "\n",
    "print(\"Patterns defined for 5 languages:\")\n",
    "for lang in ALL_BOND_PATTERNS:\n",
    "    n = sum(len(p) for p in ALL_BOND_PATTERNS[lang].values())\n",
    "    print(f\"  {lang}: {n} bond patterns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Parallel Download + Stream Processing { display-mode: \"form\" }\n",
    "#@markdown Loads ALL corpora including expanded Arabic, Chinese, and Western classics\n",
    "\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import gc\n",
    "import shutil\n",
    "import requests\n",
    "import time\n",
    "import threading\n",
    "from queue import Queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Thread-safe queue for passages\n",
    "passage_queue = Queue(maxsize=100000)\n",
    "download_complete = threading.Event()\n",
    "corpus_stats = defaultdict(int)\n",
    "stats_lock = threading.Lock()\n",
    "\n",
    "def update_stats(lang, count):\n",
    "    with stats_lock:\n",
    "        corpus_stats[lang] += count\n",
    "        total = sum(corpus_stats.values())\n",
    "        if total % 1000 == 0:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "\n",
    "# Check if we should skip processing (data loaded from Drive)\n",
    "# Check if we should use cached data or download fresh\n",
    "SKIP_PROCESSING = LOAD_FROM_DRIVE  # Re-evaluate based on current settings\n",
    "if SKIP_PROCESSING:\n",
    "    print(\"=\"*60)\n",
    "    print(\"USING CACHED DATA - Run with REFRESH_DATA_FROM_SOURCE=True to use v10.4 loaders\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Count passages by language\n",
    "    by_lang = defaultdict(int)\n",
    "    with open('data/processed/passages.jsonl', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            p = json.loads(line)\n",
    "            by_lang[p['language']] += 1\n",
    "\n",
    "    print(\"\\nPassages by language:\")\n",
    "    for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {lang}: {cnt:,}\")\n",
    "\n",
    "    n_passages = sum(by_lang.values())\n",
    "    print(f\"\\nTotal: {n_passages:,} passages\")\n",
    "\n",
    "    # Validate corpus sizes\n",
    "    MIN_RECOMMENDED = {'hebrew': 10000, 'aramaic': 5000, 'classical_chinese': 500, 'arabic': 500, 'english': 5000}\n",
    "    print(\"\\nCorpus adequacy check:\")\n",
    "    for lang, min_size in MIN_RECOMMENDED.items():\n",
    "        actual = by_lang.get(lang, 0)\n",
    "        status = \"OK\" if actual >= min_size else \"LOW\"\n",
    "        print(f\"  {lang}: {actual:,} (need {min_size:,}) - {status}\")\n",
    "\n",
    "    if by_lang.get('english', 0) < 1000:\n",
    "        print(\"\\nWARNING: English corpus too small for reliable semitic_to_non_semitic test!\")\n",
    "    if by_lang.get('classical_chinese', 0) < 100:\n",
    "        print(\"WARNING: Chinese corpus too small!\")\n",
    "\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"LOADING CORPORA\")\n",
    "    print(f\"GPU Tier: {GPU_TIER}\")\n",
    "    print(f\"Max per language: {MAX_PER_LANG:,}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    random.seed(42)\n",
    "    all_passages = []\n",
    "\n",
    "    # ===== SEFARIA (Hebrew/Aramaic) =====\n",
    "    print(\"\\nLoading Sefaria...\")\n",
    "    sefaria_path = Path('data/raw/Sefaria-Export/json')\n",
    "\n",
    "    CATEGORY_TO_PERIOD = {\n",
    "        'Tanakh': 'BIBLICAL', 'Torah': 'BIBLICAL', 'Prophets': 'BIBLICAL', 'Writings': 'BIBLICAL',\n",
    "        'Mishnah': 'TANNAITIC', 'Tosefta': 'TANNAITIC', 'Sifra': 'TANNAITIC', 'Sifrei': 'TANNAITIC',\n",
    "        'Talmud': 'TALMUDIC', 'Bavli': 'TALMUDIC', 'Yerushalmi': 'TALMUDIC',\n",
    "        'Midrash': 'MIDRASHIC', 'Midrash Rabbah': 'MIDRASHIC', 'Midrash Aggadah': 'MIDRASHIC',\n",
    "        'Halakhah': 'MEDIEVAL', 'Shulchan Arukh': 'MEDIEVAL', 'Mishneh Torah': 'MEDIEVAL',\n",
    "        'Musar': 'MODERN', 'Chasidut': 'MODERN', 'Modern': 'MODERN'\n",
    "    }\n",
    "\n",
    "    lang_counts = {'hebrew': 0, 'aramaic': 0}\n",
    "\n",
    "    if sefaria_path.exists():\n",
    "        for json_file in tqdm(list(sefaria_path.rglob('*.json'))[:5000], desc=\"Sefaria\"):\n",
    "            try:\n",
    "                with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                if isinstance(data, dict) and 'text' in data:\n",
    "                    # Determine period from path\n",
    "                    path_parts = str(json_file.relative_to(sefaria_path)).split('/')\n",
    "                    period = 'CLASSICAL'\n",
    "                    for part in path_parts:\n",
    "                        if part in CATEGORY_TO_PERIOD:\n",
    "                            period = CATEGORY_TO_PERIOD[part]\n",
    "                            break\n",
    "\n",
    "                    # Determine language (heuristic: Talmud is primarily Aramaic)\n",
    "                    is_talmud = any(t in str(json_file) for t in ['Talmud', 'Bavli', 'Yerushalmi'])\n",
    "                    lang = 'aramaic' if is_talmud else 'hebrew'\n",
    "\n",
    "                    def extract_texts(obj, texts):\n",
    "                        if isinstance(obj, str) and len(obj) > 20:\n",
    "                            texts.append(obj)\n",
    "                        elif isinstance(obj, list):\n",
    "                            for item in obj:\n",
    "                                extract_texts(item, texts)\n",
    "\n",
    "                    texts = []\n",
    "                    extract_texts(data['text'], texts)\n",
    "\n",
    "                    for txt in texts[:50]:  # Limit per file\n",
    "                        if lang_counts[lang] < MAX_PER_LANG:\n",
    "                            all_passages.append({\n",
    "                                'id': f\"sefaria_{len(all_passages)}\",\n",
    "                                'text': txt,\n",
    "                                'lang': lang,\n",
    "                                'source': json_file.stem,\n",
    "                                'period': period\n",
    "                            })\n",
    "                            lang_counts[lang] += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"  Sefaria not found - will download\")\n",
    "\n",
    "    print(f\"  Hebrew: {lang_counts['hebrew']:,}, Aramaic: {lang_counts['aramaic']:,}\")\n",
    "\n",
    "    # ===== CLASSICAL CHINESE: Disabled (CText API blocks Colab) =====\n",
    "    print(\"  Skipping CText API (blocked from Colab, using Wenyanwen instead)\")\n",
    "    chinese_count = 0  # Initialize counter\n",
    "\n",
    "    # ===== KAGGLE: Ancient Chinese Wenyanwen (132K texts, 552M chars) =====\n",
    "    if chinese_count < MAX_PER_LANG:\n",
    "        print(\"  Loading from Kaggle Wenyanwen dataset...\")\n",
    "        wenyan_zip_name = 'Ancient_Chinese_Text_(wenyanwen)_archive.zip'\n",
    "        wenyan_csv_name = 'cn_wenyan.csv'\n",
    "        wenyan_local_zip = Path(f'data/raw/{wenyan_zip_name}')\n",
    "        _drive_ok = 'USE_DRIVE_DATA' in dir() and USE_DRIVE_DATA and 'SAVE_DIR' in dir()\n",
    "        wenyan_drive_zip = Path(f'{SAVE_DIR}/{wenyan_zip_name}') if _drive_ok else None\n",
    "        wenyan_local_csv = Path(f'data/raw/{wenyan_csv_name}')\n",
    "        wenyan_drive_csv = Path(f'{SAVE_DIR}/{wenyan_csv_name}') if _drive_ok else None\n",
    "\n",
    "        # Find the CSV (extracted or in zip)\n",
    "        csv_path = None\n",
    "        if wenyan_local_csv.exists():\n",
    "            csv_path = wenyan_local_csv\n",
    "            print(\"    Found CSV locally\")\n",
    "        elif wenyan_drive_csv and wenyan_drive_csv.exists():\n",
    "            csv_path = wenyan_drive_csv\n",
    "            print(\"    Found CSV in Drive\")\n",
    "        else:\n",
    "            # Need to extract from zip\n",
    "            zip_path = None\n",
    "            if wenyan_local_zip.exists():\n",
    "                zip_path = wenyan_local_zip\n",
    "                print(\"    Found zip locally\")\n",
    "            elif wenyan_drive_zip and wenyan_drive_zip.exists():\n",
    "                zip_path = wenyan_drive_zip\n",
    "                print(\"    Found zip in Drive\")\n",
    "\n",
    "            if zip_path:\n",
    "                try:\n",
    "                    import zipfile\n",
    "                    print(\"    Extracting CSV from zip...\")\n",
    "                    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "                        z.extract(wenyan_csv_name, 'data/raw/')\n",
    "                    csv_path = wenyan_local_csv\n",
    "                    print(\"    Extracted!\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    Extraction failed: {e}\")\n",
    "\n",
    "        # Load texts from CSV\n",
    "        wenyan_count = 0\n",
    "        if csv_path and csv_path.exists():\n",
    "            import csv\n",
    "            csv.field_size_limit(10000000)  # Some texts are very long\n",
    "            try:\n",
    "                with open(csv_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    reader = csv.DictReader(f)\n",
    "                    for row in reader:\n",
    "                        if chinese_count >= MAX_PER_LANG:\n",
    "                            break\n",
    "                        text = row.get('text', '')\n",
    "                        title = row.get('title', '')\n",
    "                        # Split long texts into passages (max 2000 chars each)\n",
    "                        # Use paragraph breaks or every 1500 chars\n",
    "                        paragraphs = text.split('\\n')\n",
    "                        current_para = ''\n",
    "                        for para in paragraphs:\n",
    "                            para = para.strip()\n",
    "                            if not para:\n",
    "                                continue\n",
    "                            if len(current_para) + len(para) < 1500:\n",
    "                                current_para += para\n",
    "                            else:\n",
    "                                if len(current_para) > 50:\n",
    "                                    all_passages.append({\n",
    "                                        'id': f\"wenyan_{len(all_passages)}\",\n",
    "                                        'text': current_para,\n",
    "                                        'lang': 'classical_chinese',\n",
    "                                        'source': title.split('/')[0] if '/' in title else title,\n",
    "                                        'period': 'CONFUCIAN'\n",
    "                                    })\n",
    "                                    chinese_count += 1\n",
    "                                    wenyan_count += 1\n",
    "                                    if chinese_count >= MAX_PER_LANG:\n",
    "                                        break\n",
    "                                current_para = para\n",
    "                        # Don't forget last paragraph\n",
    "                        if current_para and len(current_para) > 50 and chinese_count < MAX_PER_LANG:\n",
    "                            all_passages.append({\n",
    "                                'id': f\"wenyan_{len(all_passages)}\",\n",
    "                                'text': current_para,\n",
    "                                'lang': 'classical_chinese',\n",
    "                                'source': title.split('/')[0] if '/' in title else title,\n",
    "                                'period': 'CONFUCIAN'\n",
    "                            })\n",
    "                            chinese_count += 1\n",
    "                            wenyan_count += 1\n",
    "                print(f\"    Added {wenyan_count:,} passages from Wenyanwen\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error loading Wenyanwen: {e}\")\n",
    "\n",
    "    print(f\"  Total Classical Chinese: {chinese_count:,}\")\n",
    "\n",
    "\n",
    "\n",
    "    # ===== ARABIC/ISLAMIC (Kaggle quran-nlp) =====\n",
    "    print(\"\\nLoading Arabic from Kaggle quran-nlp...\")\n",
    "\n",
    "    arabic_count = 0\n",
    "    kaggle_path = Path('data/raw/quran-nlp')\n",
    "\n",
    "    # Try to download from Kaggle\n",
    "    if not kaggle_path.exists() and REFRESH_DATA_FROM_SOURCE:\n",
    "        try:\n",
    "            import subprocess\n",
    "            import zipfile\n",
    "            subprocess.run(['pip', 'install', '-q', 'kaggle'], check=True)\n",
    "            subprocess.run([\n",
    "                'kaggle', 'datasets', 'download',\n",
    "                '-d', 'alizahidraja/quran-nlp',\n",
    "                '-p', 'data/raw'\n",
    "            ], check=True, timeout=300)\n",
    "\n",
    "            with zipfile.ZipFile('data/raw/quran-nlp.zip', 'r') as z:\n",
    "                z.extractall(kaggle_path)\n",
    "            print(\"  Downloaded from Kaggle!\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Kaggle download failed: {e}\")\n",
    "\n",
    "    # Load if available\n",
    "    if kaggle_path.exists():\n",
    "        import pandas as pd\n",
    "\n",
    "        # Load Quran\n",
    "        quran_files = list(kaggle_path.rglob('*quran*.csv'))\n",
    "        for qf in quran_files:\n",
    "            if arabic_count >= MAX_PER_LANG:\n",
    "                break\n",
    "            try:\n",
    "                df = pd.read_csv(qf, nrows=MAX_PER_LANG - arabic_count)\n",
    "                for _, row in df.iterrows():\n",
    "                    text = str(row.get('arabic', row.get('text', row.get('Arabic', ''))))\n",
    "                    if text and len(text) > 10 and text != 'nan':\n",
    "                        all_passages.append({\n",
    "                            'id': f\"quran_{len(all_passages)}\",\n",
    "                            'text': text,\n",
    "                            'lang': 'arabic',\n",
    "                            'source': 'Quran',\n",
    "                            'period': 'CLASSICAL'\n",
    "                        })\n",
    "                        arabic_count += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Load Hadith\n",
    "        hadith_files = list(kaggle_path.rglob('*hadith*.csv'))\n",
    "        for hf in hadith_files:\n",
    "            if arabic_count >= MAX_PER_LANG:\n",
    "                break\n",
    "            try:\n",
    "                df = pd.read_csv(hf, nrows=MAX_PER_LANG - arabic_count)\n",
    "                for _, row in df.iterrows():\n",
    "                    text = str(row.get('hadith', row.get('text', row.get('Arabic', ''))))\n",
    "                    if text and len(text) > 10 and text != 'nan':\n",
    "                        all_passages.append({\n",
    "                            'id': f\"hadith_{len(all_passages)}\",\n",
    "                            'text': text,\n",
    "                            'lang': 'arabic',\n",
    "                            'source': 'Hadith',\n",
    "                            'period': 'CLASSICAL'\n",
    "                        })\n",
    "                        arabic_count += 1\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        # Try Tanzil.net (simple direct download)\n",
    "        print(\"  Trying Tanzil.net for Quran text...\")\n",
    "        try:\n",
    "            tanzil_url = \"https://tanzil.net/pub/download/index.php?quranType=uthmani&outType=txt-2&agree=true\"\n",
    "            resp = requests.get(tanzil_url, timeout=60)\n",
    "            if resp.status_code == 200:\n",
    "                lines = resp.text.strip().split('\\n')\n",
    "                for line in lines:\n",
    "                    if '|' in line and arabic_count < MAX_PER_LANG:\n",
    "                        parts = line.split('|')\n",
    "                        if len(parts) >= 3:\n",
    "                            text = parts[2].strip()\n",
    "                            if len(text) > 10:\n",
    "                                all_passages.append({\n",
    "                                    'id': f\"tanzil_{len(all_passages)}\",\n",
    "                                    'text': text,\n",
    "                                    'lang': 'arabic',\n",
    "                                    'source': 'Quran (Tanzil)',\n",
    "                                    'period': 'CLASSICAL'\n",
    "                                })\n",
    "                                arabic_count += 1\n",
    "                print(f\"    Loaded {arabic_count} verses from Tanzil\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Tanzil failed: {e}\")\n",
    "\n",
    "        # Final fallback: expanded hardcoded corpus\n",
    "        if arabic_count < 100:\n",
    "            print(\"  Using expanded hardcoded Arabic corpus...\")\n",
    "        ARABIC_CORPUS = [\n",
    "            # Quran excerpts (moral/ethical content)\n",
    "            \"وَلَا تَقْتُلُوا النَّفْسَ الَّتِي حَرَّمَ اللَّهُ إِلَّا بِالْحَقِّ\",\n",
    "            \"وَبِالْوَالِدَيْنِ إِحْسَانًا\",\n",
    "            \"وَأَوْفُوا بِالْعَهْدِ إِنَّ الْعَهْدَ كَانَ مَسْئُولًا\",\n",
    "            \"إِنَّ اللَّهَ يَأْمُرُ بِالْعَدْلِ وَالْإِحْسَانِ\",\n",
    "            \"وَلَا تَبْخَسُوا النَّاسَ أَشْيَاءَهُمْ\",\n",
    "            \"وَأَقِيمُوا الْوَزْنَ بِالْقِسْطِ وَلَا تُخْسِرُوا الْمِيزَانَ\",\n",
    "            \"يَا أَيُّهَا الَّذِينَ آمَنُوا أَوْفُوا بِالْعُقُودِ\",\n",
    "            \"وَتَعَاوَنُوا عَلَى الْبِرِّ وَالتَّقْوَى\",\n",
    "            # ... more can be added\n",
    "        ]\n",
    "        for i, txt in enumerate(ARABIC_CORPUS):\n",
    "            all_passages.append({\n",
    "                'id': f\"arabic_{len(all_passages)}\",\n",
    "                'text': txt,\n",
    "                'lang': 'arabic',\n",
    "                'source': 'Quran/Hadith',\n",
    "                'period': 'CLASSICAL'\n",
    "            })\n",
    "            arabic_count += 1\n",
    "\n",
    "    print(f\"  Arabic: {arabic_count:,}\")\n",
    "\n",
    "    # ===== DEAR ABBY (English) =====\n",
    "    print(\"Loading Dear Abby...\")\n",
    "\n",
    "    english_count = 0\n",
    "    abby_path = Path('data/raw/dear_abby.csv')\n",
    "    print(f'  Local path exists: {abby_path.exists()}')\n",
    "\n",
    "    # Check Drive first\n",
    "    drive_abby = f'{SAVE_DIR}/dear_abby.csv'\n",
    "    print(f'  Drive path: {drive_abby}')\n",
    "    print(f'  Drive path exists: {os.path.exists(drive_abby)}')\n",
    "    if not abby_path.exists() and os.path.exists(drive_abby):\n",
    "        os.makedirs('data/raw', exist_ok=True)\n",
    "        shutil.copy(drive_abby, abby_path)\n",
    "        print(\"  Copied from Drive\")\n",
    "\n",
    "\n",
    "    if not abby_path.exists() and REFRESH_DATA_FROM_SOURCE:\n",
    "        try:\n",
    "            import subprocess\n",
    "            subprocess.run(['pip', 'install', '-q', 'kaggle'], check=True)\n",
    "            subprocess.run([\n",
    "                'kaggle', 'datasets', 'download',\n",
    "                '-d', 'thedevastator/20000-dear-abby-questions',\n",
    "                '-p', 'data/raw',\n",
    "                '-f', 'dear_abby.csv'\n",
    "            ], check=True, timeout=120)\n",
    "            print(\"  Downloaded from Kaggle!\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Kaggle download failed: {e}\")\n",
    "\n",
    "    if abby_path.exists():\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(abby_path, nrows=MAX_PER_LANG)\n",
    "        print(f'  CSV columns: {list(df.columns)}')\n",
    "        print(f'  CSV rows: {len(df)}')\n",
    "        for _, row in df.iterrows():\n",
    "            question = str(row.get('question', ''))\n",
    "            answer = str(row.get('question_only', ''))\n",
    "            if len(answer) > 50:\n",
    "                all_passages.append({\n",
    "                    'id': f\"abby_{len(all_passages)}\",\n",
    "                    'text': answer,\n",
    "                    'lang': 'english',\n",
    "                    'source': 'Dear Abby',\n",
    "                    'period': 'DEAR_ABBY'\n",
    "                })\n",
    "                english_count += 1\n",
    "    else:\n",
    "        print(\"  Dear Abby not found\")\n",
    "\n",
    "    print(f\"  Dear Abby: {english_count:,}\")\n",
    "\n",
    "    # ===== WESTERN CLASSICS (English translations) =====\n",
    "    print(\"\\nLoading Western Classics from MIT Classics Archive...\")\n",
    "\n",
    "    WESTERN_TEXTS = [\n",
    "        ('https://classics.mit.edu/Aristotle/nicomachaen.mb.txt', 'Nicomachean Ethics', 'CLASSICAL'),\n",
    "        ('https://classics.mit.edu/Aristotle/politics.mb.txt', 'Politics', 'CLASSICAL'),\n",
    "        ('https://classics.mit.edu/Plato/republic.mb.txt', 'Republic', 'CLASSICAL'),\n",
    "        ('https://classics.mit.edu/Plato/laws.mb.txt', 'Laws', 'CLASSICAL'),\n",
    "        ('https://classics.mit.edu/Antoninus/meditations.mb.txt', 'Meditations', 'CLASSICAL'),\n",
    "        ('https://classics.mit.edu/Epictetus/epicench.mb.txt', 'Enchiridion', 'CLASSICAL'),\n",
    "        ('https://classics.mit.edu/Cicero/duties.mb.txt', 'De Officiis', 'CLASSICAL'),\n",
    "    ]\n",
    "\n",
    "    western_count = 0\n",
    "    for url, title, period in WESTERN_TEXTS:\n",
    "        if english_count + western_count >= MAX_PER_LANG:\n",
    "            break\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=30)\n",
    "            if resp.status_code == 200:\n",
    "                text = resp.text\n",
    "                # Split into paragraphs\n",
    "                paragraphs = [p.strip() for p in text.split('\\n\\n') if len(p.strip()) > 100]\n",
    "                for para in paragraphs[:500]:  # Limit per text\n",
    "                    if english_count + western_count >= MAX_PER_LANG:\n",
    "                        break\n",
    "                    # Clean up\n",
    "                    para = re.sub(r'\\s+', ' ', para).strip()\n",
    "                    if len(para) > 50:\n",
    "                        all_passages.append({\n",
    "                            'id': f\"western_{len(all_passages)}\",\n",
    "                            'text': para,\n",
    "                            'lang': 'english',\n",
    "                            'source': title,\n",
    "                            'period': period\n",
    "                        })\n",
    "                        western_count += 1\n",
    "                print(f\"  {title}: {min(500, len(paragraphs))} paragraphs\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {title}: {e}\")\n",
    "\n",
    "    print(f\"  Western Classics: {western_count:,}\")\n",
    "\n",
    "    # ===== UNIMORAL: Disabled (gated dataset requires auth) =====\n",
    "    print(\"  Skipping UniMoral (gated HuggingFace dataset)\")\n",
    "\n",
    "    # ===== UN PARALLEL CORPUS (HuggingFace streaming) =====\n",
    "    print(\"\\nLoading UN Corpus from HuggingFace (streaming)...\")\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "\n",
    "        pairs = [('ar', 'en'), ('en', 'zh')]\n",
    "        un_count = 0\n",
    "        lang_map = {'ar': 'arabic', 'zh': 'classical_chinese', 'en': 'english'}\n",
    "\n",
    "        for src, tgt in pairs:\n",
    "            if un_count >= MAX_PER_LANG:\n",
    "                break\n",
    "            try:\n",
    "                config = f\"{src}-{tgt}\"\n",
    "                ds = load_dataset(\"Helsinki-NLP/un_pc\", config, split=\"train\", streaming=True)\n",
    "\n",
    "                pair_count = 0\n",
    "                for item in ds:\n",
    "                    if pair_count >= min(MAX_PER_LANG // 4, 5000):\n",
    "                        break\n",
    "\n",
    "                    translation = item.get('translation', {})\n",
    "                    for lang_code in [src, tgt]:\n",
    "                        text = translation.get(lang_code, '')\n",
    "                        if len(text) > 30 and lang_code in lang_map:\n",
    "                            all_passages.append({\n",
    "                                'id': f\"un_{len(all_passages)}\",\n",
    "                                'text': text,\n",
    "                                'lang': lang_map[lang_code],\n",
    "                                'source': 'UN Corpus',\n",
    "                                'period': 'MODERN'\n",
    "                            })\n",
    "                            pair_count += 1\n",
    "                            un_count += 1\n",
    "\n",
    "                print(f\"  UN {config}: {pair_count:,}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  UN {config} error: {e}\")\n",
    "\n",
    "        print(f\"  UN Corpus total: {un_count:,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  UN Corpus error: {e}\")\n",
    "\n",
    "    # ===== BIBLE PARALLEL CORPUS (GitHub) =====\n",
    "    print(\"\\nLoading Bible Parallel Corpus...\")\n",
    "    try:\n",
    "        base_url = \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles\"\n",
    "        bible_files = [\n",
    "            ('Hebrew.xml', 'hebrew'),\n",
    "            ('Arabic.xml', 'arabic'),\n",
    "            ('Chinese.xml', 'classical_chinese'),\n",
    "        ]\n",
    "\n",
    "        bible_count = 0\n",
    "        for filename, lang in bible_files:\n",
    "            if bible_count >= MAX_PER_LANG * 3:\n",
    "                break\n",
    "            try:\n",
    "                url = f\"{base_url}/{filename}\"\n",
    "                resp = requests.get(url, timeout=60)\n",
    "                if resp.status_code == 200:\n",
    "                    verses = re.findall(r'<seg[^>]*>([^<]+)</seg>', resp.text)\n",
    "                    file_count = 0\n",
    "                    for verse in verses:\n",
    "                        if file_count >= MAX_PER_LANG:\n",
    "                            break\n",
    "                        verse = verse.strip()\n",
    "                        if len(verse) > 10:\n",
    "                            all_passages.append({\n",
    "                                'id': f\"bible_{len(all_passages)}\",\n",
    "                                'text': verse,\n",
    "                                'lang': lang,\n",
    "                                'source': 'Bible',\n",
    "                                'period': 'CLASSICAL'\n",
    "                            })\n",
    "                            file_count += 1\n",
    "                            bible_count += 1\n",
    "                    print(f\"  Bible {lang}: {file_count:,}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Bible {filename} error: {e}\")\n",
    "\n",
    "        print(f\"  Bible total: {bible_count:,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Bible error: {e}\")\n",
    "\n",
    "    # ===== SUMMARY =====\n",
    "    print(f\"\\nTOTAL: {len(all_passages):,}\")\n",
    "\n",
    "    # Count by language\n",
    "    by_lang = defaultdict(int)\n",
    "    for p in all_passages:\n",
    "        by_lang[p['lang']] += 1\n",
    "    print(\"\\nBy language:\")\n",
    "    for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {lang}: {cnt:,}\")\n",
    "\n",
    "    # ===== EXTRACT BONDS =====\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXTRACTING BONDS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    def extract_bond(text, language):\n",
    "        \"\"\"Extract bond type with context awareness.\"\"\"\n",
    "        tn = normalize_text(text, language)\n",
    "\n",
    "        for bt, pats in ALL_BOND_PATTERNS.get(language, {}).items():\n",
    "            for p in pats:\n",
    "                match = re.search(p, tn)\n",
    "                if match:\n",
    "                    # Check context around the match\n",
    "                    context, marker_type = detect_context(text, language, match.start())\n",
    "                    confidence = 0.9 if context == 'prescriptive' else 0.5\n",
    "                    return bt, context, confidence\n",
    "        return None, 'unknown', 0.5\n",
    "\n",
    "    bonds = []\n",
    "    for p in tqdm(all_passages, desc=\"Extracting bonds\"):\n",
    "        bt, ctx, conf = extract_bond(p['text'], p['lang'])\n",
    "        if bt:\n",
    "            bonds.append({\n",
    "                'passage_id': p['id'],\n",
    "                'bond_type': bt,\n",
    "                'language': p['lang'],\n",
    "                'time_period': p['period'],\n",
    "                'source': p['source'],\n",
    "                'text': p['text'][:500],\n",
    "                'context': ctx,\n",
    "                'confidence': conf\n",
    "            })\n",
    "\n",
    "    print(f\"\\nExtracted {len(bonds):,} bonds from {len(all_passages):,} passages\")\n",
    "\n",
    "    # Count by bond type\n",
    "    by_bond = defaultdict(int)\n",
    "    for b in bonds:\n",
    "        by_bond[b['bond_type']] += 1\n",
    "    print(\"\\nBy bond type:\")\n",
    "    for bt, cnt in sorted(by_bond.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {bt}: {cnt:,}\")\n",
    "\n",
    "    # Count by context\n",
    "    by_ctx = defaultdict(int)\n",
    "    for b in bonds:\n",
    "        by_ctx[b['context']] += 1\n",
    "    print(\"\\nBy context:\")\n",
    "    for ctx, cnt in sorted(by_ctx.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {ctx}: {cnt:,}\")\n",
    "\n",
    "    # ===== SAVE =====\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAVING DATA\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Save passages\n",
    "    with open('data/processed/passages.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for p in all_passages:\n",
    "            # Normalize field names\n",
    "            p_out = {\n",
    "                'id': p['id'],\n",
    "                'text': p['text'],\n",
    "                'language': p['lang'],\n",
    "                'source': p['source'],\n",
    "                'time_period': p['period']\n",
    "            }\n",
    "            f.write(json.dumps(p_out, ensure_ascii=False) + '\\n')\n",
    "    print(f\"  Saved {len(all_passages):,} passages to data/processed/passages.jsonl\")\n",
    "\n",
    "    # Save bonds\n",
    "    with open('data/processed/bonds.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for b in bonds:\n",
    "            b_out = {**b, 'bond_type': b['bond_type'].name if hasattr(b['bond_type'], 'name') else str(b['bond_type'])}\n",
    "            f.write(json.dumps(b_out, ensure_ascii=False) + chr(10))\n",
    "    print(f\"  Saved {len(bonds):,} bonds to data/processed/bonds.jsonl\")\n",
    "\n",
    "    # Copy to Drive if enabled\n",
    "    if USE_DRIVE_DATA and DRIVE_HAS_DATA:\n",
    "        try:\n",
    "            shutil.copy('data/processed/passages.jsonl', f'{SAVE_DIR}/passages.jsonl')\n",
    "            shutil.copy('data/processed/bonds.jsonl', f'{SAVE_DIR}/bonds.jsonl')\n",
    "            print(f\"  Copied to Drive: {SAVE_DIR}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Drive copy failed: {e}\")\n",
    "\n",
    "    gc.collect()\n",
    "    print(\"\\nDone!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. Generate Splits { display-mode: \"form\" }\n",
    "#@markdown Creates train/test splits for cross-lingual experiments\n",
    "\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING SPLITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if splits already exist from Drive\n",
    "# Check if splits are valid (IDs match current passages)\n",
    "splits_valid = False\n",
    "if os.path.exists('data/splits/all_splits.json'):\n",
    "    try:\n",
    "        with open('data/splits/all_splits.json') as f:\n",
    "            cached_splits = json.load(f)\n",
    "        # Get sample of IDs from splits\n",
    "        sample_ids = set()\n",
    "        for split in cached_splits.values():\n",
    "            sample_ids.update(split['train_ids'][:100])\n",
    "            sample_ids.update(split['test_ids'][:100])\n",
    "        # Check if they exist in current passages\n",
    "        passage_ids = set()\n",
    "        with open('data/processed/passages.jsonl') as f:\n",
    "            for line in f:\n",
    "                p = json.loads(line)\n",
    "                passage_ids.add(p['id'])\n",
    "                if len(passage_ids) > 10000:\n",
    "                    break\n",
    "        matches = len(sample_ids & passage_ids)\n",
    "        splits_valid = matches > len(sample_ids) * 0.9  # 90% match\n",
    "        if not splits_valid:\n",
    "            print(f\"Splits invalid: only {matches}/{len(sample_ids)} IDs match current passages\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating splits: {e}\")\n",
    "\n",
    "if splits_valid and not REFRESH_DATA_FROM_SOURCE:\n",
    "    print(\"\\nSplits already loaded from Drive\")\n",
    "    with open('data/splits/all_splits.json') as f:\n",
    "        all_splits = json.load(f)\n",
    "    for name, split in all_splits.items():\n",
    "        print(f\"  {name}: train={split['train_size']:,}, test={split['test_size']:,}\")\n",
    "else:\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Read passage metadata\n",
    "    passage_meta = []\n",
    "    with open('data/processed/passages.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            p = json.loads(line)\n",
    "            passage_meta.append(p)\n",
    "    \n",
    "    print(f\"Total passages: {len(passage_meta):,}\")\n",
    "    \n",
    "    by_lang = defaultdict(list)\n",
    "    by_period = defaultdict(list)\n",
    "    for p in passage_meta:\n",
    "        by_lang[p['language']].append(p['id'])\n",
    "        by_period[p['time_period']].append(p['id'])\n",
    "    \n",
    "    print(\"\\nBy language:\")\n",
    "    for lang, ids in sorted(by_lang.items(), key=lambda x: -len(x[1])):\n",
    "        print(f\"  {lang}: {len(ids):,}\")\n",
    "    \n",
    "    print(\"\\nBy period:\")\n",
    "    for period, ids in sorted(by_period.items(), key=lambda x: -len(x[1])):\n",
    "        print(f\"  {period}: {len(ids):,}\")\n",
    "    \n",
    "    all_splits = {}\n",
    "    # ===== SPLIT 1: Hebrew -> Others =====\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"SPLIT 1: HEBREW -> OTHERS\")\n",
    "    hebrew_ids = by_lang.get('hebrew', [])\n",
    "    other_ids = [p['id'] for p in passage_meta if p['language'] != 'hebrew']\n",
    "    random.shuffle(hebrew_ids)\n",
    "    random.shuffle(other_ids)\n",
    "    \n",
    "    all_splits['hebrew_to_others'] = {\n",
    "        'train_ids': hebrew_ids,\n",
    "        'test_ids': other_ids,\n",
    "        'train_size': len(hebrew_ids),\n",
    "        'test_size': len(other_ids),\n",
    "    }\n",
    "    print(f\"  Train (Hebrew): {len(hebrew_ids):,}\")\n",
    "    print(f\"  Test (Others): {len(other_ids):,}\")\n",
    "    \n",
    "    # ===== SPLIT 2: Semitic -> Non-Semitic =====\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"SPLIT 2: SEMITIC -> NON-SEMITIC\")\n",
    "    semitic_ids = by_lang.get('hebrew', []) + by_lang.get('aramaic', []) + by_lang.get('arabic', [])\n",
    "    non_semitic_ids = by_lang.get('classical_chinese', []) + by_lang.get('english', [])\n",
    "    random.shuffle(semitic_ids)\n",
    "    random.shuffle(non_semitic_ids)\n",
    "    \n",
    "    all_splits['semitic_to_non_semitic'] = {\n",
    "        'train_ids': semitic_ids,\n",
    "        'test_ids': non_semitic_ids,\n",
    "        'train_size': len(semitic_ids),\n",
    "        'test_size': len(non_semitic_ids),\n",
    "    }\n",
    "    print(f\"  Train (Semitic): {len(semitic_ids):,}\")\n",
    "    print(f\"  Test (Non-Semitic): {len(non_semitic_ids):,}\")\n",
    "    \n",
    "    # ===== SPLIT 3: Ancient -> Modern =====\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"SPLIT 3: ANCIENT -> MODERN\")\n",
    "    # Define modern periods explicitly, derive ancient dynamically\n",
    "    modern_periods = {'MODERN', 'DEAR_ABBY'}\n",
    "    all_periods = set(by_period.keys())\n",
    "    ancient_periods = all_periods - modern_periods\n",
    "\n",
    "    print(f\"  Ancient periods: {sorted(ancient_periods)}\")\n",
    "    print(f\"  Modern periods: {sorted(modern_periods)}\")\n",
    "\n",
    "    ancient_ids = [p['id'] for p in passage_meta if p['time_period'] in ancient_periods]\n",
    "    modern_ids = [p['id'] for p in passage_meta if p['time_period'] in modern_periods]\n",
    "    random.shuffle(ancient_ids)\n",
    "    random.shuffle(modern_ids)\n",
    "    \n",
    "    all_splits['ancient_to_modern'] = {\n",
    "        'train_ids': ancient_ids,\n",
    "        'test_ids': modern_ids,\n",
    "        'train_size': len(ancient_ids),\n",
    "        'test_size': len(modern_ids),\n",
    "    }\n",
    "    print(f\"  Train (Ancient): {len(ancient_ids):,}\")\n",
    "    print(f\"  Test (Modern): {len(modern_ids):,}\")\n",
    "    \n",
    "    # ===== SPLIT 4: Mixed Baseline =====\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"SPLIT 4: MIXED BASELINE\")\n",
    "    all_ids = [p['id'] for p in passage_meta]\n",
    "    random.shuffle(all_ids)\n",
    "    split_idx = int(0.7 * len(all_ids))\n",
    "    \n",
    "    all_splits['mixed_baseline'] = {\n",
    "        'train_ids': all_ids[:split_idx],\n",
    "        'test_ids': all_ids[split_idx:],\n",
    "        'train_size': split_idx,\n",
    "        'test_size': len(all_ids) - split_idx,\n",
    "    }\n",
    "    print(f\"  Train: {split_idx:,}\")\n",
    "    print(f\"  Test: {len(all_ids) - split_idx:,}\")\n",
    "    \n",
    "    \n",
    "    # ===== SPLIT 5: Dear Abby -> Classical Chinese =====\n",
    "    print(\"\n",
    "\" + \"-\"*60)\n",
    "    print(\"SPLIT 5: DEAR ABBY -> CHINESE\")\n",
    "    abby_ids = [p['id'] for p in passage_meta if p['time_period'] == 'DEAR_ABBY']\n",
    "    chinese_ids = [p['id'] for p in passage_meta if p['language'] == 'classical_chinese']\n",
    "    random.shuffle(abby_ids)\n",
    "    random.shuffle(chinese_ids)\n",
    "    \n",
    "    all_splits['abby_to_chinese'] = {\n",
    "        'train_ids': abby_ids,\n",
    "        'test_ids': chinese_ids,\n",
    "        'train_size': len(abby_ids),\n",
    "        'test_size': len(chinese_ids),\n",
    "    }\n",
    "    print(f\"  Train (Dear Abby): {len(abby_ids):,}\")\n",
    "    print(f\"  Test (Chinese): {len(chinese_ids):,}\")\n",
    "\n",
    "\n",
    "    # Save splits\n",
    "    with open('data/splits/all_splits.json', 'w') as f:\n",
    "        json.dump(all_splits, f, indent=2)\n",
    "    \n",
    "    # Save to Drive\n",
    "    shutil.copy('data/splits/all_splits.json', f'{SAVE_DIR}/all_splits.json')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Splits saved to local and Drive\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. Model Architecture { display-mode: \"form\" }\n",
    "#@markdown BIP model with configurable backbone and adversarial heads\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Backbone: {BACKBONE} ({MODEL_NAME})\")\n",
    "print(f\"Hidden size: {BACKBONE_HIDDEN}\")\n",
    "\n",
    "# Index mappings\n",
    "BOND_TO_IDX = {bt.name: i for i, bt in enumerate(BondType)}\n",
    "IDX_TO_BOND = {i: bt.name for i, bt in enumerate(BondType)}\n",
    "LANG_TO_IDX = {'hebrew': 0, 'aramaic': 1, 'classical_chinese': 2, 'arabic': 3, 'english': 4}\n",
    "IDX_TO_LANG = {i: l for l, i in LANG_TO_IDX.items()}\n",
    "PERIOD_TO_IDX = {'BIBLICAL': 0, 'TANNAITIC': 1, 'AMORAIC': 2, 'RISHONIM': 3, 'ACHRONIM': 4,\n",
    "                 'CONFUCIAN': 5, 'DAOIST': 6, 'QURANIC': 7, 'HADITH': 8, 'DEAR_ABBY': 9}\n",
    "IDX_TO_PERIOD = {i: p for p, i in PERIOD_TO_IDX.items()}\n",
    "HOHFELD_TO_IDX = {hs.name: i for i, hs in enumerate(HohfeldState)}\n",
    "IDX_TO_HOHFELD = {i: hs.name for i, hs in enumerate(HohfeldState)}\n",
    "CONTEXT_TO_IDX = {'prescriptive': 0, 'descriptive': 1, 'unknown': 2}\n",
    "IDX_TO_CONTEXT = {i: c for c, i in CONTEXT_TO_IDX.items()}\n",
    "CONFIDENCE_TO_WEIGHT = {'high': 2.0, 'medium': 1.0, 'low': 0.5}\n",
    "\n",
    "class GradientReversalLayer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.alpha, None\n",
    "\n",
    "class BIPModel(nn.Module):\n",
    "    def __init__(self, model_name=None, hidden_size=None, z_dim=64):\n",
    "        super().__init__()\n",
    "        # Use global config if not specified\n",
    "        model_name = model_name or MODEL_NAME\n",
    "        hidden_size = hidden_size or BACKBONE_HIDDEN\n",
    "\n",
    "        print(f\"  Loading encoder: {model_name}\")\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Get actual hidden size from model config\n",
    "        actual_hidden = self.encoder.config.hidden_size\n",
    "        if actual_hidden != hidden_size:\n",
    "            print(f\"  Note: Using actual hidden size {actual_hidden}\")\n",
    "            hidden_size = actual_hidden\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Projection to z_bond space (scales with backbone size)\n",
    "        proj_hidden = min(512, hidden_size)\n",
    "        self.z_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_size, proj_hidden),\n",
    "            nn.LayerNorm(proj_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(proj_hidden, z_dim),\n",
    "        )\n",
    "\n",
    "        # Task heads\n",
    "        self.bond_head = nn.Linear(z_dim, len(BondType))\n",
    "        self.hohfeld_head = nn.Linear(z_dim, len(HohfeldState))\n",
    "\n",
    "        # Adversarial heads\n",
    "        self.language_head = nn.Linear(z_dim, len(LANG_TO_IDX))\n",
    "        self.period_head = nn.Linear(z_dim, len(PERIOD_TO_IDX))\n",
    "\n",
    "        # Context prediction head (auxiliary task)\n",
    "        self.context_head = nn.Linear(z_dim, len(CONTEXT_TO_IDX))\n",
    "\n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"  Total params: {total_params:,}\")\n",
    "        print(f\"  Trainable: {trainable_params:,}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, adv_lambda=1.0):\n",
    "        enc = self.encoder(input_ids, attention_mask)\n",
    "\n",
    "        # Handle different pooling strategies\n",
    "        if hasattr(enc, 'pooler_output') and enc.pooler_output is not None:\n",
    "            pooled = enc.pooler_output\n",
    "        else:\n",
    "            pooled = enc.last_hidden_state[:, 0]\n",
    "\n",
    "        z = self.z_proj(pooled)\n",
    "\n",
    "        # Bond prediction (main task)\n",
    "        bond_pred = self.bond_head(z)\n",
    "        hohfeld_pred = self.hohfeld_head(z)\n",
    "\n",
    "        # Adversarial predictions (gradient reversal)\n",
    "        z_rev = GradientReversalLayer.apply(z, adv_lambda)\n",
    "        language_pred = self.language_head(z_rev)\n",
    "        period_pred = self.period_head(z_rev)\n",
    "\n",
    "        return {\n",
    "            'bond_pred': bond_pred,\n",
    "            'hohfeld_pred': hohfeld_pred,\n",
    "            'language_pred': language_pred,\n",
    "            'period_pred': period_pred,\n",
    "            'context_pred': self.context_head(z),\n",
    "            'z': z,\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer for selected backbone\n",
    "print(f\"\\nLoading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "# Dataset with Hohfeld support\n",
    "class NativeDataset(Dataset):\n",
    "    def __init__(self, ids_set, passages_file, bonds_file, tokenizer, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = []\n",
    "\n",
    "        bonds_by_id = {}\n",
    "        with open(bonds_file) as fb:\n",
    "            for line in fb:\n",
    "                b = json.loads(line)\n",
    "                bonds_by_id[b['passage_id']] = b\n",
    "\n",
    "        with open(passages_file) as fp:\n",
    "            for line in tqdm(fp, desc=\"Loading\", unit=\"line\"):\n",
    "                p = json.loads(line)\n",
    "                if p['id'] in ids_set and p['id'] in bonds_by_id:\n",
    "                    b = bonds_by_id[p['id']]\n",
    "                    self.data.append({\n",
    "                        'text': p['text'][:1000],\n",
    "                        'language': p['language'],\n",
    "                        'period': p['time_period'],\n",
    "                        'bond': b.get('bond_type') or b.get('bonds', {}).get('primary_bond'),\n",
    "                        'hohfeld': None,\n",
    "                        'context': b.get('context') or b.get('bonds', {}).get('context', 'unknown'),\n",
    "                        'confidence': b.get('confidence') or b.get('bonds', {}).get('confidence', 'medium'),\n",
    "                    })\n",
    "        print(f\"  Loaded {len(self.data):,} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        enc = self.tokenizer(item['text'], truncation=True, max_length=self.max_len,\n",
    "                            padding='max_length', return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'bond_label': BOND_TO_IDX.get(item['bond'], 9),\n",
    "            'language_label': LANG_TO_IDX.get(item['language'], 4),\n",
    "            'period_label': PERIOD_TO_IDX.get(item['period'], 9),\n",
    "            'hohfeld_label': HOHFELD_TO_IDX.get(item['hohfeld'], 0) if item['hohfeld'] else 0,\n",
    "            'context_label': CONTEXT_TO_IDX.get(item['context'], 2),\n",
    "            'sample_weight': CONFIDENCE_TO_WEIGHT.get(item['confidence'], 1.0),\n",
    "            'language': item['language'],\n",
    "            'context': item['context'],\n",
    "            'confidence': item['confidence'],\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
    "        'bond_labels': torch.tensor([x['bond_label'] for x in batch]),\n",
    "        'language_labels': torch.tensor([x['language_label'] for x in batch]),\n",
    "        'period_labels': torch.tensor([x['period_label'] for x in batch]),\n",
    "        'hohfeld_labels': torch.tensor([x['hohfeld_label'] for x in batch]),\n",
    "        'context_labels': torch.tensor([x['context_label'] for x in batch]),\n",
    "        'sample_weights': torch.tensor([x['sample_weight'] for x in batch], dtype=torch.float),\n",
    "        'languages': [x['language'] for x in batch],\n",
    "        'contexts': [x['context'] for x in batch],\n",
    "        'confidences': [x['confidence'] for x in batch],\n",
    "    }\n",
    "\n",
    "print(f\"\\nArchitecture ready for {BACKBONE}\")\n",
    "print(f\"  Bond classes: {len(BondType)}\")\n",
    "print(f\"  Languages: {len(LANG_TO_IDX)}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7. Train BIP Model { display-mode: \"form\" }\n",
    "#@markdown Training with tuned adversarial weights and hardware-optimized parameters\n",
    "\n",
    "# ===== SUPPRESS DATALOADER MULTIPROCESSING WARNINGS =====\n",
    "# These occur during garbage collection and bypass normal exception handling\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "\n",
    "# Method 1: Filter warnings\n",
    "warnings.filterwarnings('ignore', message='.*can only test a child process.*')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torch.utils.data')\n",
    "\n",
    "# Method 2: Suppress logging\n",
    "logging.getLogger('torch.utils.data.dataloader').setLevel(logging.CRITICAL)\n",
    "\n",
    "# Method 3: Redirect stderr during DataLoader cleanup (most effective)\n",
    "class StderrFilter(io.TextIOWrapper):\n",
    "    \"\"\"Filters out DataLoader multiprocessing cleanup messages from stderr\"\"\"\n",
    "    def __init__(self, original):\n",
    "        self.original = original\n",
    "        self.buffer_lines = []\n",
    "\n",
    "    def write(self, text):\n",
    "        # Filter out the specific error patterns\n",
    "        skip_patterns = [\n",
    "            'can only test a child process',\n",
    "            '_MultiProcessingDataLoaderIter.__del__',\n",
    "            '_shutdown_workers',\n",
    "            'Exception ignored in:',\n",
    "            'w.is_alive()',\n",
    "        ]\n",
    "        # Buffer multi-line error messages\n",
    "        if any(p in text for p in skip_patterns):\n",
    "            return len(text)  # Pretend we wrote it\n",
    "        # Also skip if it looks like part of a traceback for these errors\n",
    "        if text.strip().startswith('^') and len(text.strip()) < 80:\n",
    "            return len(text)\n",
    "        if text.strip().startswith('File \"/usr') and 'dataloader.py' in text:\n",
    "            return len(text)\n",
    "        if text.strip() == 'Traceback (most recent call last):':\n",
    "            self.buffer_lines = [text]\n",
    "            return len(text)\n",
    "        if self.buffer_lines:\n",
    "            self.buffer_lines.append(text)\n",
    "            # Check if this is the DataLoader error traceback\n",
    "            full_msg = ''.join(self.buffer_lines)\n",
    "            if any(p in full_msg for p in skip_patterns):\n",
    "                self.buffer_lines = []\n",
    "                return len(text)\n",
    "            # After 10 lines, flush if not the target error\n",
    "            if len(self.buffer_lines) > 10:\n",
    "                for line in self.buffer_lines:\n",
    "                    self.original.write(line)\n",
    "                self.buffer_lines = []\n",
    "        return self.original.write(text)\n",
    "\n",
    "    def flush(self):\n",
    "        if self.buffer_lines:\n",
    "            # Flush any remaining buffered content\n",
    "            for line in self.buffer_lines:\n",
    "                self.original.write(line)\n",
    "            self.buffer_lines = []\n",
    "        self.original.flush()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.original, name)\n",
    "\n",
    "# Install the stderr filter\n",
    "_original_stderr = sys.stderr\n",
    "sys.stderr = StderrFilter(_original_stderr)\n",
    "\n",
    "# Method 4: Patch the DataLoader cleanup function directly\n",
    "try:\n",
    "    import torch.utils.data.dataloader as dl_module\n",
    "    _original_del = dl_module._MultiProcessingDataLoaderIter.__del__\n",
    "\n",
    "    def _patched_del(self):\n",
    "        try:\n",
    "            _original_del(self)\n",
    "        except (AssertionError, AttributeError, RuntimeError):\n",
    "            pass  # Silently ignore cleanup errors\n",
    "\n",
    "    dl_module._MultiProcessingDataLoaderIter.__del__ = _patched_del\n",
    "except Exception:\n",
    "    pass  # If patching fails, the stderr filter will still work\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import gc\n",
    "\n",
    "#@markdown **Splits to train:**\n",
    "TRAIN_HEBREW_TO_OTHERS = True  #@param {type:\"boolean\"}\n",
    "TRAIN_SEMITIC_TO_NON_SEMITIC = True  #@param {type:\"boolean\"}\n",
    "TRAIN_ANCIENT_TO_MODERN = True  #@param {type:\"boolean\"}\n",
    "TRAIN_MIXED_BASELINE = True  #@param {type:\"boolean\"}\n",
    "TRAIN_ABBY_TO_CHINESE = True  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown **Hyperparameters:**\n",
    "LANG_WEIGHT = 0.1  #@param {type:\"number\"}\n",
    "PERIOD_WEIGHT = 0.05  #@param {type:\"number\"}\n",
    "N_EPOCHS = 10  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown **Context-Aware Training:**\n",
    "USE_CONFIDENCE_WEIGHTING = True  #@param {type:\"boolean\"}\n",
    "#@markdown Weight prescriptive (high confidence) examples 2x in loss\n",
    "\n",
    "USE_CONTEXT_AUXILIARY = True  #@param {type:\"boolean\"}\n",
    "#@markdown Add context prediction as auxiliary training target\n",
    "\n",
    "CONTEXT_LOSS_WEIGHT = 0.3  #@param {type:\"number\"}\n",
    "#@markdown Weight for context prediction loss\n",
    "\n",
    "STRICT_PRESCRIPTIVE_TEST = True  #@param {type:\"boolean\"}\n",
    "#@markdown Only evaluate on prescriptive examples (strict test)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING BIP MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSettings:\")\n",
    "print(f\"  Backbone:     {BACKBONE}\")\n",
    "print(f\"  GPU Tier:     {GPU_TIER}\")\n",
    "print(f\"  Batch size:   {BATCH_SIZE}\")\n",
    "print(f\"  Workers:      {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate: {LR:.2e}\")\n",
    "print(f\"  Adv weights:  lang={LANG_WEIGHT}, period={PERIOD_WEIGHT}\")\n",
    "print(\"(0.01 prevents loss explosion while maintaining invariance)\")\n",
    "print(f\"  Confidence weighting: {USE_CONFIDENCE_WEIGHTING}\")\n",
    "print(f\"  Context auxiliary: {USE_CONTEXT_AUXILIARY} (weight={CONTEXT_LOSS_WEIGHT})\")\n",
    "print(f\"  Strict prescriptive test: {STRICT_PRESCRIPTIVE_TEST}\")\n",
    "\n",
    "# tokenizer loaded in Cell 6 based on BACKBONE selection\n",
    "\n",
    "with open('data/splits/all_splits.json') as f:\n",
    "    all_splits = json.load(f)\n",
    "\n",
    "splits_to_train = []\n",
    "if TRAIN_HEBREW_TO_OTHERS: splits_to_train.append('hebrew_to_others')\n",
    "if TRAIN_SEMITIC_TO_NON_SEMITIC: splits_to_train.append('semitic_to_non_semitic')\n",
    "if TRAIN_ANCIENT_TO_MODERN: splits_to_train.append('ancient_to_modern')\n",
    "if TRAIN_MIXED_BASELINE: splits_to_train.append('mixed_baseline')\n",
    "if TRAIN_ABBY_TO_CHINESE: splits_to_train.append('abby_to_chinese')\n",
    "\n",
    "print(f\"\\nTraining {len(splits_to_train)} splits: {splits_to_train}\")\n",
    "\n",
    "all_results = {}\n",
    "MIN_TEST_SIZE = 100  # Lowered to allow smaller test sets like Chinese\n",
    "\n",
    "for split_idx, split_name in enumerate(splits_to_train):\n",
    "    split_start = time.time()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"[{split_idx+1}/{len(splits_to_train)}] {split_name}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    split = all_splits[split_name]\n",
    "    print(f\"Train: {split['train_size']:,} | Test: {split['test_size']:,}\")\n",
    "\n",
    "    if split['test_size'] < MIN_TEST_SIZE:\n",
    "        print(f\"WARNING: Test set only {split['test_size']} samples (need {MIN_TEST_SIZE})\")\n",
    "        print(\"Skipping this split - results would be unreliable\")\n",
    "        print(\"To fix: Add more data to the test languages/periods\")\n",
    "        continue\n",
    "\n",
    "    model = BIPModel().to(device)\n",
    "\n",
    "    train_dataset = NativeDataset(set(split['train_ids']), 'data/processed/passages.jsonl',\n",
    "                                   'data/processed/bonds.jsonl', tokenizer)\n",
    "\n",
    "    test_ids_to_use = split['test_ids'][:MAX_TEST_SAMPLES]\n",
    "\n",
    "    # Optional: strict prescriptive-only test\n",
    "    if STRICT_PRESCRIPTIVE_TEST:\n",
    "        print(\"Filtering to prescriptive examples only...\")\n",
    "        # Load bonds to filter\n",
    "        prescriptive_ids = set()\n",
    "        with open('data/processed/bonds.jsonl') as f:\n",
    "            for line in f:\n",
    "                b = json.loads(line)\n",
    "                if b.get('context') == 'prescriptive':\n",
    "                    prescriptive_ids.add(b['passage_id'])\n",
    "        test_ids_to_use = [tid for tid in test_ids_to_use if tid in prescriptive_ids]\n",
    "        print(f\"  Filtered to {len(test_ids_to_use):,} prescriptive samples\")\n",
    "\n",
    "    test_dataset = NativeDataset(set(test_ids_to_use), 'data/processed/passages.jsonl',\n",
    "                                  'data/processed/bonds.jsonl', tokenizer)\n",
    "\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"ERROR: No training data!\")\n",
    "        continue\n",
    "\n",
    "    # Use hardware-optimized batch size\n",
    "    actual_batch = min(BATCH_SIZE, max(32, len(train_dataset) // 20))\n",
    "    print(f\"Actual batch size: {actual_batch}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=actual_batch, shuffle=True,\n",
    "                              collate_fn=collate_fn, drop_last=True, num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=actual_batch*2, shuffle=False,\n",
    "                             collate_fn=collate_fn, num_workers=0, pin_memory=True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "    def get_adv_lambda(epoch, warmup=3):\n",
    "        \"\"\"Ramp adversarial strength: 0.1 -> 1.0 over warmup, then hold at 1.0\"\"\"\n",
    "        if epoch <= warmup:\n",
    "            return 0.1 + 0.9 * (epoch / warmup)\n",
    "        return 1.0\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            bond_labels = batch['bond_labels'].to(device)\n",
    "            language_labels = batch['language_labels'].to(device)\n",
    "            period_labels = batch['period_labels'].to(device)\n",
    "\n",
    "            adv_lambda = get_adv_lambda(epoch)\n",
    "\n",
    "            # Use new autocast API\n",
    "            with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "                out = model(input_ids, attention_mask, adv_lambda=adv_lambda)\n",
    "\n",
    "                # Weighted bond loss\n",
    "                if USE_CONFIDENCE_WEIGHTING:\n",
    "                    sample_weights = batch['sample_weights'].to(device)\n",
    "                    loss_bond = F.cross_entropy(out['bond_pred'], bond_labels, reduction='none')\n",
    "                    loss_bond = (loss_bond * sample_weights).mean()\n",
    "                else:\n",
    "                    loss_bond = F.cross_entropy(out['bond_pred'], bond_labels)\n",
    "\n",
    "                # Context auxiliary loss\n",
    "                if USE_CONTEXT_AUXILIARY:\n",
    "                    context_labels = batch['context_labels'].to(device)\n",
    "                    loss_context = F.cross_entropy(out['context_pred'], context_labels)\n",
    "                else:\n",
    "                    loss_context = 0\n",
    "\n",
    "                loss_lang = F.cross_entropy(out['language_pred'], language_labels)\n",
    "                loss_period = F.cross_entropy(out['period_pred'], period_labels)\n",
    "\n",
    "            loss = loss_bond + LANG_WEIGHT * loss_lang + PERIOD_WEIGHT * loss_period + CONTEXT_LOSS_WEIGHT * loss_context\n",
    "\n",
    "            if USE_AMP and scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / n_batches\n",
    "        print(f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f})\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), f'models/checkpoints/best_{split_name}.pt')\n",
    "            torch.save(model.state_dict(), f'{SAVE_DIR}/best_{split_name}.pt')\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.load_state_dict(torch.load(f'models/checkpoints/best_{split_name}.pt'))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = {'bond': [], 'lang': []}\n",
    "    all_labels = {'bond': [], 'lang': []}\n",
    "    all_languages = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n",
    "            all_preds['bond'].extend(out['bond_pred'].argmax(-1).cpu().tolist())\n",
    "            all_preds['lang'].extend(out['language_pred'].argmax(-1).cpu().tolist())\n",
    "            all_labels['bond'].extend(batch['bond_labels'].tolist())\n",
    "            all_labels['lang'].extend(batch['language_labels'].tolist())\n",
    "            all_languages.extend(batch['languages'])\n",
    "\n",
    "    bond_f1 = f1_score(all_labels['bond'], all_preds['bond'], average='macro', zero_division=0)\n",
    "    bond_acc = sum(p == l for p, l in zip(all_preds['bond'], all_labels['bond'])) / len(all_preds['bond'])\n",
    "    lang_acc = sum(p == l for p, l in zip(all_preds['lang'], all_labels['lang'])) / len(all_preds['lang'])\n",
    "\n",
    "    # Per-language F1\n",
    "    lang_f1 = {}\n",
    "    for lang in set(all_languages):\n",
    "        mask = [l == lang for l in all_languages]\n",
    "        if sum(mask) > 10:\n",
    "            preds = [p for p, m in zip(all_preds['bond'], mask) if m]\n",
    "            labels = [l for l, m in zip(all_labels['bond'], mask) if m]\n",
    "            lang_f1[lang] = {'f1': f1_score(labels, preds, average='macro', zero_division=0), 'n': sum(mask)}\n",
    "\n",
    "    all_results[split_name] = {\n",
    "        'bond_f1_macro': bond_f1,\n",
    "        'bond_acc': bond_acc,\n",
    "        'language_acc': lang_acc,\n",
    "        'per_language_f1': lang_f1,\n",
    "        'training_time': time.time() - split_start\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{split_name} RESULTS:\")\n",
    "    print(f\"  Bond F1 (macro): {bond_f1:.3f} ({bond_f1/0.1:.1f}x chance)\")\n",
    "    print(f\"  Bond accuracy:   {bond_acc:.1%}\")\n",
    "    print(f\"  Language acc:    {lang_acc:.1%} (want ~20% = invariant)\")\n",
    "    print(\"  Per-language:\")\n",
    "    for lang, m in sorted(lang_f1.items(), key=lambda x: -x[1]['n']):\n",
    "        print(f\"    {lang:20s}: F1={m['f1']:.3f} (n={m['n']:,})\")\n",
    "\n",
    "    # Context analysis\n",
    "    high_conf = sum(1 for c in test_dataset.data if c['confidence'] == 'high')\n",
    "    prescriptive = sum(1 for c in test_dataset.data if c['context'] == 'prescriptive')\n",
    "    print(f\"  Context: {prescriptive:,}/{len(test_dataset):,} prescriptive ({prescriptive/len(test_dataset)*100:.1f}%)\")\n",
    "    print(f\"  High confidence: {high_conf:,}/{len(test_dataset):,} ({high_conf/len(test_dataset)*100:.1f}%)\")\n",
    "\n",
    "    # GPU memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        mem = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"\\n  GPU memory: {mem:.1f} GB / {VRAM_GB:.1f} GB ({mem/VRAM_GB*100:.0f}%)\")\n",
    "\n",
    "    del model, train_dataset, test_dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8. Linear Probe Test { display-mode: \"form\" }\n",
    "#@markdown Tests if z_bond encodes language/period (should be low = invariant)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LINEAR PROBE TEST\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nIf probe accuracy is NEAR CHANCE, representation is INVARIANT\")\n",
    "print(\"(This is what we want for BIP)\")\n",
    "\n",
    "probe_results = {}\n",
    "\n",
    "for split_name in ['hebrew_to_others', 'semitic_to_non_semitic']:\n",
    "    model_path = f'{SAVE_DIR}/best_{split_name}.pt'\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"\\nSkipping {split_name} - no saved model\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PROBE: {split_name}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    model = BIPModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    test_ids = set(all_splits[split_name]['test_ids'][:5000])\n",
    "    test_dataset = NativeDataset(test_ids, 'data/processed/passages.jsonl',\n",
    "                                  'data/processed/bonds.jsonl', tokenizer)\n",
    "    \n",
    "    if len(test_dataset) < 50:\n",
    "        print(f\"  Skip - only {len(test_dataset)} samples\")\n",
    "        continue\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    all_z, all_lang, all_period = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Extract\"):\n",
    "            out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n",
    "            all_z.append(out['z'].cpu().numpy())\n",
    "            all_lang.extend(batch['language_labels'].tolist())\n",
    "            all_period.extend(batch['period_labels'].tolist())\n",
    "    \n",
    "    X = np.vstack(all_z)\n",
    "    y_lang = np.array(all_lang)\n",
    "    y_period = np.array(all_period)\n",
    "    \n",
    "    scaler_probe = StandardScaler()\n",
    "    X_scaled = scaler_probe.fit_transform(X)\n",
    "    \n",
    "    # Train/test split for probes\n",
    "    n = len(X)\n",
    "    idx = np.random.permutation(n)\n",
    "    train_idx, test_idx = idx[:int(0.7*n)], idx[int(0.7*n):]\n",
    "    \n",
    "    # Language probe - check for multiple classes\n",
    "    unique_langs = np.unique(y_lang[test_idx])\n",
    "    if len(unique_langs) < 2:\n",
    "        print(f\"  SKIP language probe - only {len(unique_langs)} class\")\n",
    "        lang_acc = 1.0 / max(1, len(np.unique(y_lang)))\n",
    "        lang_chance = lang_acc\n",
    "    else:\n",
    "        lang_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        lang_probe.fit(X_scaled[train_idx], y_lang[train_idx])\n",
    "        lang_acc = (lang_probe.predict(X_scaled[test_idx]) == y_lang[test_idx]).mean()\n",
    "        lang_chance = 1.0 / len(unique_langs)\n",
    "    \n",
    "    # Period probe - same check\n",
    "    unique_periods = np.unique(y_period[test_idx])\n",
    "    if len(unique_periods) < 2:\n",
    "        print(f\"  SKIP period probe - only {len(unique_periods)} class\")\n",
    "        period_acc = 1.0 / max(1, len(np.unique(y_period)))\n",
    "        period_chance = period_acc\n",
    "    else:\n",
    "        period_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        period_probe.fit(X_scaled[train_idx], y_period[train_idx])\n",
    "        period_acc = (period_probe.predict(X_scaled[test_idx]) == y_period[test_idx]).mean()\n",
    "        period_chance = 1.0 / len(unique_periods)\n",
    "    \n",
    "    lang_status = \"INVARIANT\" if lang_acc < lang_chance + 0.15 else \"NOT invariant\"\n",
    "    period_status = \"INVARIANT\" if period_acc < period_chance + 0.15 else \"NOT invariant\"\n",
    "    \n",
    "    probe_results[split_name] = {\n",
    "        'language_acc': lang_acc,\n",
    "        'language_chance': lang_chance,\n",
    "        'language_status': lang_status,\n",
    "        'period_acc': period_acc,\n",
    "        'period_chance': period_chance,\n",
    "        'period_status': period_status,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nRESULTS:\")\n",
    "    print(f\"  Language: {lang_acc:.1%} (chance: {lang_chance:.1%}) -> {lang_status}\")\n",
    "    print(f\"  Period:   {period_acc:.1%} (chance: {period_chance:.1%}) -> {period_status}\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Probe tests complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9. Final Results { display-mode: \"form\" }\n",
    "#@markdown Comprehensive summary with verdict\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL BIP EVALUATION (v10.2)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nHardware: {GPU_TIER} ({VRAM_GB:.0f}GB VRAM, {RAM_GB:.0f}GB RAM)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"CROSS-DOMAIN TRANSFER RESULTS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "successful_splits = []\n",
    "for name, r in all_results.items():\n",
    "    ratio = r['bond_f1_macro'] / 0.1\n",
    "    lang_acc = r['language_acc']\n",
    "    \n",
    "    transfer_ok = ratio > 1.3\n",
    "    invariant_ok = lang_acc < 0.35  # Near chance (20%)\n",
    "    \n",
    "    status = \"SUCCESS\" if (transfer_ok and invariant_ok) else \"PARTIAL\" if transfer_ok else \"FAIL\"\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Bond F1:     {r['bond_f1_macro']:.3f} ({ratio:.1f}x chance) {'OK' if transfer_ok else 'WEAK'}\")\n",
    "    print(f\"  Language:    {lang_acc:.1%} {'INVARIANT' if invariant_ok else 'LEAKING'}\")\n",
    "    print(f\"  -> {status}\")\n",
    "    \n",
    "    if transfer_ok and invariant_ok:\n",
    "        successful_splits.append(name)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"VERDICT\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "n_success = len(successful_splits)\n",
    "if n_success >= 2:\n",
    "    verdict = \"STRONGLY_SUPPORTED\"\n",
    "    msg = \"Multiple independent transfer paths demonstrate universal structure\"\n",
    "elif n_success >= 1:\n",
    "    verdict = \"SUPPORTED\"\n",
    "    msg = \"At least one transfer path works\"\n",
    "elif any(r['bond_f1_macro'] > 0.13 for r in all_results.values()):\n",
    "    verdict = \"PARTIAL\"\n",
    "    msg = \"Some transfer signal, but not robust\"\n",
    "else:\n",
    "    verdict = \"INCONCLUSIVE\"\n",
    "    msg = \"No clear transfer demonstrated\"\n",
    "\n",
    "print(f\"\\n  Successful transfers: {n_success}/{len(all_results)}\")\n",
    "print(f\"  Splits: {successful_splits if successful_splits else 'None'}\")\n",
    "print(f\"\\n  VERDICT: {verdict}\")\n",
    "print(f\"  {msg}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'all_results': all_results,\n",
    "    'probe_results': probe_results if 'probe_results' in dir() else {},\n",
    "    'successful_splits': successful_splits,\n",
    "    'verdict': verdict,\n",
    "    'hardware': {'gpu': GPU_TIER, 'vram_gb': VRAM_GB, 'ram_gb': RAM_GB},\n",
    "    'settings': {'batch_size': BATCH_SIZE, 'max_per_lang': MAX_PER_LANG, 'num_workers': NUM_WORKERS},\n",
    "    'experiment_time': time.time() - EXPERIMENT_START,\n",
    "}\n",
    "\n",
    "with open('results/final_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "shutil.copy('results/final_results.json', f'{SAVE_DIR}/final_results.json')\n",
    "\n",
    "print(f\"\\nTotal time: {(time.time() - EXPERIMENT_START)/60:.1f} minutes\")\n",
    "print(\"Results saved to Drive!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10. Download Results { display-mode: \"form\" }\n",
    "#@markdown Download all models and results\n",
    "\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "print(\"Creating download package...\")\n",
    "\n",
    "with zipfile.ZipFile('BIP_v10.2_results.zip', 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    # Results\n",
    "    if os.path.exists('results/final_results.json'):\n",
    "        zf.write('results/final_results.json')\n",
    "    \n",
    "    # Models (from Drive)\n",
    "    for f in os.listdir(SAVE_DIR):\n",
    "        if f.endswith('.pt'):\n",
    "            zf.write(f'{SAVE_DIR}/{f}', f'models/{f}')\n",
    "    \n",
    "    # Config\n",
    "    if os.path.exists('data/splits/all_splits.json'):\n",
    "        zf.write('data/splits/all_splits.json')\n",
    "\n",
    "print(\"\\nDownload ready!\")\n",
    "files.download('BIP_v10.2_results.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}