{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# BIP Cross-Cultural Universal Morality Experiment\n\n**Testing the Bond Invariance Principle across 2000+ years AND across languages (Hebrew + Chinese + Arabic \u2192 English)**\n\nThis experiment tests whether moral cognition has invariant structure by:\n1. Training on ORIGINAL HEBREW texts (Sefaria corpus, ~500 BCE - 1800 CE)\n2. Testing transfer to modern ENGLISH advice columns (Dear Abby, 1956-2020)\n\n**Hypothesis**: If BIP holds, bond-level features should transfer across 2000 years with no accuracy drop.\n\n---\n\n## Setup Instructions\n1. **Runtime -> Change runtime type -> GPU (T4) or TPU (v5e)**\n2. Run cells in order - each shows progress in real-time\n3. Expected runtime: ~1-2 hours (TPU) or ~2-4 hours (GPU)\n\n**Supported Accelerators:**\n- NVIDIA GPU (T4, V100, A100)\n- Google TPU (v2, v3, v4, v5e)\n- CPU (slow, not recommended)\n\n---"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 1. Setup and Install Dependencies { display-mode: \"form\" }\n#@markdown Installs packages and detects GPU/TPU. Memory-optimized for Colab.\n\nprint(\"=\" * 60)\nprint(\"BIP TEMPORAL INVARIANCE EXPERIMENT\")\nprint(\"=\" * 60)\nprint()\n\n# Progress tracker\nTASKS = [\n    \"Install dependencies\",\n    \"Clone Sefaria corpus (~8GB)\",\n    \"Clone sqnd-probe repo (Dear Abby data)\",\n    \"Preprocess corpora\",\n    \"Extract bond structures\",\n    \"Generate train/test splits\",\n    \"Train BIP model\",\n    \"Evaluate results\"\n]\ntask_status = {task: \"pending\" for task in TASKS}\n\ndef print_progress():\n    print()\n    print(\"-\" * 50)\n    print(\"EXPERIMENT PROGRESS:\")\n    print(\"-\" * 50)\n    for task in TASKS:\n        status = task_status[task]\n        if status == \"done\":\n            mark = \"[X]\"\n        elif status == \"running\":\n            mark = \"[>]\"\n        else:\n            mark = \"[ ]\"\n        print(f\"  {mark} {task}\")\n    print(\"-\" * 50)\n    print(flush=True)\n\ndef mark_task(task, status):\n    task_status[task] = status\n    print_progress()\n\nprint_progress()\n\nmark_task(\"Install dependencies\", \"running\")\n\nimport os\nimport subprocess\nimport sys\n\n# Install dependencies - MINIMAL set to save memory\nprint(\"Installing minimal dependencies...\")\ndeps = [\n    \"transformers\",\n    \"torch\", \n    \"sentence-transformers\",\n    \"pandas\",\n    \"tqdm\",\n    \"psutil\"\n]\n\nfor dep in deps:\n    print(f\"  Installing {dep}...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", dep])\n\nprint()\n\n# Detect accelerator - WITHOUT importing tensorflow\nUSE_TPU = False\nTPU_TYPE = None\n\n# Check for TPU\nif 'COLAB_TPU_ADDR' in os.environ:\n    USE_TPU = True\n    TPU_TYPE = \"TPU (Colab)\"\n    print(\"TPU detected!\")\n\n# Check for GPU\nimport torch\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    ACCELERATOR = f\"GPU: {gpu_name} ({gpu_mem:.1f}GB)\"\n    device = torch.device(\"cuda\")\nelif USE_TPU:\n    ACCELERATOR = TPU_TYPE\n    import torch_xla.core.xla_model as xm\n    device = xm.xla_device()\nelse:\n    ACCELERATOR = \"CPU (slow!)\"\n    device = torch.device(\"cpu\")\n\nprint(f\"Accelerator: {ACCELERATOR}\")\nprint(f\"Device: {device}\")\n\n# Memory status\nimport psutil\nmem = psutil.virtual_memory()\nprint(f\"System RAM: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n\nif torch.cuda.is_available():\n    print(f\"GPU RAM: {torch.cuda.memory_allocated()/1e9:.1f}/{torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n\n# Enable mixed precision for 2-3x speedup\nif torch.cuda.is_available():\n    print()\n    print(\"Enabling mixed precision (FP16) for faster training...\")\n    from torch.cuda.amp import autocast, GradScaler\n    USE_AMP = True\n    scaler = GradScaler()\nelse:\n    USE_AMP = False\n    scaler = None\n\n# torch.compile for PyTorch 2.0+ (10-30% speedup)\nTORCH_COMPILE = False\nif hasattr(torch, 'compile') and torch.cuda.is_available():\n    print(\"PyTorch 2.0+ detected - torch.compile available\")\n    TORCH_COMPILE = False  # Disabled - overhead > benefit for short runs\n\n\n\n# ============================================================\n# GOOGLE DRIVE - SAVE RESULTS EVEN IF SESSION DIES\n# ============================================================\nprint()\nprint(\"=\" * 60)\nprint(\"MOUNTING GOOGLE DRIVE FOR PERSISTENT STORAGE\")\nprint(\"=\" * 60)\nfrom google.colab import drive\ndrive.mount('/content/drive')\nSAVE_DIR = '/content/drive/MyDrive/BIP_results'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# Create local directories\nos.makedirs(\"data/processed\", exist_ok=True)\nos.makedirs(\"data/splits\", exist_ok=True)\nos.makedirs(\"models/checkpoints\", exist_ok=True)\nos.makedirs(\"results\", exist_ok=True)\nprint(f\"Results will be saved to: {SAVE_DIR}\")\nprint(\"If session crashes, your data survives.\")\nprint()\n\nmark_task(\"Install dependencies\", \"done\")\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 2. Download Sefaria Corpus (~8GB) { display-mode: \"form\" }\n",
        "#@markdown Downloads the complete Sefaria corpus with real-time git progress.\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "mark_task(\"Clone Sefaria corpus (~8GB)\", \"running\")\n",
        "\n",
        "sefaria_path = 'data/raw/Sefaria-Export'\n",
        "\n",
        "if not os.path.exists(sefaria_path) or not os.path.exists(f\"{sefaria_path}/json\"):\n",
        "    print(\"=\"*60)\n",
        "    print(\"CLONING SEFARIA CORPUS\")\n",
        "    print(\"=\"*60)\n",
        "    print()\n",
        "    print(\"This downloads ~3.5GB and takes 5-15 minutes.\")\n",
        "    print(\"Git's native progress will display below:\")\n",
        "    print(\"-\"*60)\n",
        "    print(flush=True)\n",
        "    \n",
        "    # Use subprocess.Popen for real-time output streaming\n",
        "    process = subprocess.Popen(\n",
        "        ['git', 'clone', '--depth', '1', '--progress',\n",
        "         'https://github.com/Sefaria/Sefaria-Export.git', sefaria_path],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,  # Git writes progress to stderr\n",
        "        text=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    \n",
        "    # Stream output in real-time\n",
        "    for line in process.stdout:\n",
        "        print(line, end='', flush=True)\n",
        "    \n",
        "    process.wait()\n",
        "    \n",
        "    print(\"-\"*60)\n",
        "    if process.returncode == 0:\n",
        "        print(\"\\nSefaria clone COMPLETE!\")\n",
        "    else:\n",
        "        print(f\"\\nERROR: Git clone failed with code {process.returncode}\")\n",
        "        print(\"Try running this cell again, or check your internet connection.\")\n",
        "else:\n",
        "    print(\"Sefaria already exists, skipping download.\")\n",
        "\n",
        "# Verify and count files\n",
        "print()\n",
        "print(\"Verifying download...\")\n",
        "!du -sh {sefaria_path} 2>/dev/null || echo \"Directory not found\"\n",
        "json_count = !find {sefaria_path}/json -name \"*.json\" 2>/dev/null | wc -l\n",
        "print(f\"Sefaria JSON files found: {json_count[0]}\")\n",
        "\n",
        "mark_task(\"Clone Sefaria corpus (~8GB)\", \"done\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 3. Download Dear Abby Dataset { display-mode: \"form\" }\n",
        "#@markdown Downloads the Dear Abby advice column dataset (68,330 entries).\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "mark_task(\"Clone sqnd-probe repo (Dear Abby data)\", \"running\")\n",
        "\n",
        "sqnd_path = 'sqnd-probe-data'\n",
        "if not os.path.exists(sqnd_path):\n",
        "    print(\"Cloning sqnd-probe repo...\")\n",
        "    process = subprocess.Popen(\n",
        "        ['git', 'clone', '--depth', '1', '--progress',\n",
        "         'https://github.com/ahb-sjsu/sqnd-probe.git', sqnd_path],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    for line in process.stdout:\n",
        "        print(line, end='', flush=True)\n",
        "    process.wait()\n",
        "else:\n",
        "    print(\"Repo already cloned.\")\n",
        "\n",
        "# Copy Dear Abby data\n",
        "dear_abby_source = Path('sqnd-probe-data/dear_abby_data/raw_da_qs.csv')\n",
        "dear_abby_path = Path('data/raw/dear_abby.csv')\n",
        "\n",
        "if dear_abby_source.exists():\n",
        "    !cp \"{dear_abby_source}\" \"{dear_abby_path}\"\n",
        "    print(f\"\\nCopied Dear Abby data\")\n",
        "elif not dear_abby_path.exists():\n",
        "    raise FileNotFoundError(\"Dear Abby dataset not found!\")\n",
        "\n",
        "# Verify\n",
        "df_check = pd.read_csv(dear_abby_path)\n",
        "print(f\"\\n\" + \"=\" * 50)\n",
        "print(f\"Dear Abby dataset: {len(df_check):,} entries\")\n",
        "print(f\"Columns: {list(df_check.columns)}\")\n",
        "print(f\"Year range: {df_check['year'].min():.0f} - {df_check['year'].max():.0f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "mark_task(\"Clone sqnd-probe repo (Dear Abby data)\", \"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 4. Define Data Classes and Loaders { display-mode: \"form\" }\n",
        "#@markdown Defines enums, dataclasses, and corpus loaders.\n",
        "\n",
        "import json\n",
        "import hashlib\n",
        "import re\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import List, Dict\n",
        "from enum import Enum\n",
        "from collections import defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"Defining data structures...\")\n",
        "\n",
        "class TimePeriod(Enum):\n",
        "    BIBLICAL = 0        # ~1000-500 BCE\n",
        "    SECOND_TEMPLE = 1   # ~500 BCE - 70 CE\n",
        "    TANNAITIC = 2       # ~70-200 CE\n",
        "    AMORAIC = 3         # ~200-500 CE\n",
        "    GEONIC = 4          # ~600-1000 CE\n",
        "    RISHONIM = 5        # ~1000-1500 CE\n",
        "    ACHRONIM = 6        # ~1500-1800 CE\n",
        "    MODERN_HEBREW = 7   # ~1800-present\n",
        "    DEAR_ABBY = 8       # 1956-2020\n",
        "\n",
        "class BondType(Enum):\n",
        "    HARM_PREVENTION = 0\n",
        "    RECIPROCITY = 1\n",
        "    AUTONOMY = 2\n",
        "    PROPERTY = 3\n",
        "    FAMILY = 4\n",
        "    AUTHORITY = 5\n",
        "    EMERGENCY = 6\n",
        "    CONTRACT = 7\n",
        "    CARE = 8\n",
        "    FAIRNESS = 9\n",
        "\n",
        "class HohfeldianState(Enum):\n",
        "    RIGHT = 0\n",
        "    OBLIGATION = 1\n",
        "    LIBERTY = 2\n",
        "    NO_RIGHT = 3\n",
        "\n",
        "@dataclass\n",
        "class Passage:\n",
        "    id: str\n",
        "    text_original: str\n",
        "    text_english: str\n",
        "    time_period: str\n",
        "    century: int\n",
        "    source: str\n",
        "    source_type: str\n",
        "    category: str\n",
        "    language: str = \"hebrew\"\n",
        "    word_count: int = 0\n",
        "    has_dispute: bool = False\n",
        "    consensus_tier: str = \"unknown\"\n",
        "    bond_types: List[str] = field(default_factory=list)\n",
        "    \n",
        "    def to_dict(self):\n",
        "        return asdict(self)\n",
        "\n",
        "CATEGORY_TO_PERIOD = {\n",
        "    'Tanakh': TimePeriod.BIBLICAL,\n",
        "    'Torah': TimePeriod.BIBLICAL,\n",
        "    'Mishnah': TimePeriod.TANNAITIC,\n",
        "    'Tosefta': TimePeriod.TANNAITIC,\n",
        "    'Talmud': TimePeriod.AMORAIC,\n",
        "    'Bavli': TimePeriod.AMORAIC,\n",
        "    'Midrash': TimePeriod.AMORAIC,\n",
        "    'Halakhah': TimePeriod.RISHONIM,\n",
        "    'Chasidut': TimePeriod.ACHRONIM,\n",
        "}\n",
        "\n",
        "PERIOD_TO_CENTURY = {\n",
        "    TimePeriod.BIBLICAL: -6,\n",
        "    TimePeriod.SECOND_TEMPLE: -2,\n",
        "    TimePeriod.TANNAITIC: 2,\n",
        "    TimePeriod.AMORAIC: 4,\n",
        "    TimePeriod.GEONIC: 8,\n",
        "    TimePeriod.RISHONIM: 12,\n",
        "    TimePeriod.ACHRONIM: 17,\n",
        "    TimePeriod.MODERN_HEBREW: 20,\n",
        "}\n",
        "\n",
        "def load_sefaria(base_path: str, max_passages: int = None) -> List[Passage]:\n",
        "    \"\"\"Load Sefaria corpus with progress bar.\"\"\"\n",
        "    passages = []\n",
        "    json_path = Path(base_path) / \"json\"\n",
        "    \n",
        "    if not json_path.exists():\n",
        "        print(f\"Warning: {json_path} not found\")\n",
        "        return []\n",
        "    \n",
        "    json_files = list(json_path.rglob(\"*.json\"))\n",
        "    print(f\"Processing {len(json_files):,} JSON files...\")\n",
        "    \n",
        "    for json_file in tqdm(json_files[:max_passages] if max_passages else json_files,\n",
        "                          desc=\"Loading Sefaria\", unit=\"file\"):\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "        except:\n",
        "            continue\n",
        "        \n",
        "        rel_path = json_file.relative_to(json_path)\n",
        "        category = str(rel_path.parts[0]) if rel_path.parts else \"unknown\"\n",
        "        time_period = CATEGORY_TO_PERIOD.get(category, TimePeriod.AMORAIC)\n",
        "        century = PERIOD_TO_CENTURY.get(time_period, 0)\n",
        "        \n",
        "        if isinstance(data, dict):\n",
        "            hebrew = data.get('he', data.get('text', []))\n",
        "            english = data.get('text', data.get('en', []))\n",
        "            \n",
        "            def flatten(h, e, ref=\"\"):\n",
        "                if isinstance(h, str) and isinstance(e, str):\n",
        "                    h_clean = re.sub(r'<[^>]+>', '', h).strip()\n",
        "                    e_clean = re.sub(r'<[^>]+>', '', e).strip()\n",
        "                    if 50 <= len(e_clean) <= 2000:\n",
        "                        pid = hashlib.md5(f\"{json_file.stem}:{ref}:{h_clean[:50]}\".encode()).hexdigest()[:12]\n",
        "                        return [Passage(\n",
        "                            id=f\"sefaria_{pid}\",\n",
        "                            text_original=h_clean,\n",
        "                            text_english=e_clean,\n",
        "                            time_period=time_period.name,\n",
        "                            century=century,\n",
        "                            source=f\"{json_file.stem} {ref}\".strip(),\n",
        "                            source_type=\"sefaria\",\n",
        "                            category=category,\n",
        "                            language=\"hebrew\",\n",
        "                            word_count=len(e_clean.split())\n",
        "                        )]\n",
        "                    return []\n",
        "                elif isinstance(h, list) and isinstance(e, list):\n",
        "                    result = []\n",
        "                    for i, (hh, ee) in enumerate(zip(h, e)):\n",
        "                        result.extend(flatten(hh, ee, f\"{ref}.{i+1}\" if ref else str(i+1)))\n",
        "                    return result\n",
        "                return []\n",
        "            \n",
        "            passages.extend(flatten(hebrew, english))\n",
        "    \n",
        "    return passages\n",
        "\n",
        "def load_dear_abby(path: str, max_passages: int = None) -> List[Passage]:\n",
        "    \"\"\"Load Dear Abby corpus with progress bar.\"\"\"\n",
        "    passages = []\n",
        "    df = pd.read_csv(path)\n",
        "    \n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading Dear Abby\", unit=\"row\"):\n",
        "        question = str(row.get('question_only', ''))\n",
        "        if not question or question == 'nan' or len(question) < 50 or len(question) > 2000:\n",
        "            continue\n",
        "        \n",
        "        year = int(row.get('year', 1990))\n",
        "        pid = hashlib.md5(f\"abby:{idx}:{question[:50]}\".encode()).hexdigest()[:12]\n",
        "        \n",
        "        passages.append(Passage(\n",
        "            id=f\"abby_{pid}\",\n",
        "            text_original=question,\n",
        "            text_english=question,\n",
        "            time_period=TimePeriod.DEAR_ABBY.name,\n",
        "            century=20 if year < 2000 else 21,\n",
        "            source=f\"Dear Abby {year}\",\n",
        "            source_type=\"dear_abby\",\n",
        "            category=\"general\",\n",
        "            language=\"english\",\n",
        "            word_count=len(question.split())\n",
        "        ))\n",
        "        \n",
        "        if max_passages and len(passages) >= max_passages:\n",
        "            break\n",
        "    \n",
        "    return passages\n",
        "\n",
        "print(\"Data structures defined!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 5. Load and Preprocess Corpora { display-mode: \"form\" }\n",
        "#@markdown Loads both corpora. Set MAX_SEFARIA_PASSAGES to limit memory usage.\n",
        "\n",
        "#@markdown **Memory Management:**\n",
        "MAX_SEFARIA_PASSAGES = 200000  #@param {type:\"integer\"}\n",
        "#@markdown **FAST MODE:** 200K passages. Set to 500K+ for full run.  #@param {type:\"integer\"}\n",
        "#@markdown Set to 0 for unlimited. Recommended: 500000 for Colab (12GB RAM)\n",
        "\n",
        "import gc\n",
        "\n",
        "mark_task(\"Preprocess corpora\", \"running\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING CORPORA\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "if MAX_SEFARIA_PASSAGES > 0:\n",
        "    print(f\"*** MEMORY MODE: Limited to {MAX_SEFARIA_PASSAGES:,} Sefaria passages ***\")\n",
        "    print()\n",
        "\n",
        "# Load Sefaria with optional limit\n",
        "limit = MAX_SEFARIA_PASSAGES if MAX_SEFARIA_PASSAGES > 0 else None\n",
        "sefaria_passages = load_sefaria(\"data/raw/Sefaria-Export\", max_passages=limit)\n",
        "print(f\"\\nSefaria passages loaded: {len(sefaria_passages):,}\")\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# Load Dear Abby\n",
        "print()\n",
        "abby_passages = load_dear_abby(\"data/raw/dear_abby.csv\")\n",
        "print(f\"\\nDear Abby passages loaded: {len(abby_passages):,}\")\n",
        "\n",
        "# Combine\n",
        "all_passages = sefaria_passages + abby_passages\n",
        "\n",
        "# Clear individual lists to save memory\n",
        "del sefaria_passages\n",
        "del abby_passages\n",
        "gc.collect()\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(f\"TOTAL PASSAGES: {len(all_passages):,}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Statistics\n",
        "by_period = defaultdict(int)\n",
        "by_source = defaultdict(int)\n",
        "for p in all_passages:\n",
        "    by_period[p.time_period] += 1\n",
        "    by_source[p.source_type] += 1\n",
        "\n",
        "print(\"\\nBy source:\")\n",
        "for source, count in sorted(by_source.items()):\n",
        "    print(f\"  {source}: {count:,}\")\n",
        "\n",
        "print(\"\\nBy time period:\")\n",
        "for period, count in sorted(by_period.items()):\n",
        "    pct = count / len(all_passages) * 100\n",
        "    bar = '#' * int(pct / 2)\n",
        "    print(f\"  {period:20s}: {count:6,} ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "# Memory status\n",
        "import psutil\n",
        "mem = psutil.virtual_memory()\n",
        "print(f\"\\nMemory: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n",
        "\n",
        "mark_task(\"Preprocess corpora\", \"done\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title 5a. Load Chinese Classics (Chinese Text Project) { display-mode: \"form\" }\n#@markdown Downloads Confucian and Daoist classics in original Classical Chinese.\n#@markdown Source: ctext.org (free API, academic use)\n\nimport requests\nimport time\nimport json\nimport os\nfrom tqdm import tqdm\n\nprint(\"=\" * 60)\nprint(\"LOADING CHINESE CLASSICS\")\nprint(\"=\" * 60)\nprint()\nprint(\"Source: Chinese Text Project (ctext.org)\")\nprint(\"Period: ~500 BCE - 200 CE\")\nprint(\"Language: Classical Chinese (\u6587\u8a00\u6587)\")\nprint()\n\n# CText API base\nCTEXT_API = \"https://api.ctext.org\"\n\n# Key Confucian/Daoist/Legalist texts with moral content\nCHINESE_TEXTS = {\n    # Confucian Four Books\n    \"analects\": {\"title\": \"\u8ad6\u8a9e Analects\", \"period\": \"CONFUCIAN\", \"century\": -5},\n    \"mengzi\": {\"title\": \"\u5b5f\u5b50 Mencius\", \"period\": \"CONFUCIAN\", \"century\": -4},\n    \"daxue\": {\"title\": \"\u5927\u5b78 Great Learning\", \"period\": \"CONFUCIAN\", \"century\": -5},\n    \"zhongyong\": {\"title\": \"\u4e2d\u5eb8 Doctrine of the Mean\", \"period\": \"CONFUCIAN\", \"century\": -5},\n    \n    # Daoist\n    \"dao-de-jing\": {\"title\": \"\u9053\u5fb7\u7d93 Dao De Jing\", \"period\": \"DAOIST\", \"century\": -6},\n    \"zhuangzi\": {\"title\": \"\u838a\u5b50 Zhuangzi\", \"period\": \"DAOIST\", \"century\": -4},\n    \n    # Legalist/Other\n    \"xunzi\": {\"title\": \"\u8340\u5b50 Xunzi\", \"period\": \"CONFUCIAN\", \"century\": -3},\n    \"mozi\": {\"title\": \"\u58a8\u5b50 Mozi\", \"period\": \"MOHIST\", \"century\": -5},\n}\n\ndef fetch_ctext_book(textid, max_chapters=50):\n    \"\"\"Fetch a book from Chinese Text Project API.\"\"\"\n    passages = []\n    \n    try:\n        # Get table of contents\n        toc_url = f\"{CTEXT_API}/gettoc?urn=ctp:{textid}\"\n        resp = requests.get(toc_url, timeout=30)\n        \n        if resp.status_code != 200:\n            print(f\"  Warning: Could not fetch TOC for {textid}\")\n            return passages\n        \n        toc = resp.json()\n        \n        if \"children\" not in toc:\n            print(f\"  Warning: No chapters in {textid}\")\n            return passages\n        \n        chapters = toc[\"children\"][:max_chapters]\n        \n        for chapter in chapters:\n            chapter_urn = chapter.get(\"urn\", \"\")\n            if not chapter_urn:\n                continue\n            \n            # Get chapter text\n            text_url = f\"{CTEXT_API}/gettext?urn={chapter_urn}\"\n            \n            try:\n                resp = requests.get(text_url, timeout=30)\n                if resp.status_code != 200:\n                    continue\n                \n                data = resp.json()\n                \n                # Extract passages (each paragraph)\n                if \"text\" in data:\n                    for i, para in enumerate(data[\"text\"]):\n                        zh_text = para.get(\"text\", \"\")\n                        en_text = para.get(\"translation\", \"\")  # May not always have translation\n                        \n                        if zh_text and len(zh_text) > 10:\n                            passages.append({\n                                \"text_original\": zh_text,\n                                \"text_english\": en_text if en_text else \"\",\n                                \"source_ref\": f\"{chapter_urn}:{i}\",\n                            })\n                \n                time.sleep(0.2)  # Rate limiting\n                \n            except Exception as e:\n                continue\n        \n    except Exception as e:\n        print(f\"  Error fetching {textid}: {e}\")\n    \n    return passages\n\n# Alternative: Download pre-packaged texts if API fails\ndef download_chinese_fallback():\n    \"\"\"Download Chinese texts from backup source.\"\"\"\n    passages = []\n    \n    # Analects (public domain, widely available)\n    analects_url = \"https://raw.githubusercontent.com/cjdd3b/chinese-texts/master/analects.json\"\n    \n    try:\n        resp = requests.get(analects_url, timeout=30)\n        if resp.status_code == 200:\n            data = resp.json()\n            for item in data:\n                if \"chinese\" in item and len(item[\"chinese\"]) > 10:\n                    passages.append({\n                        \"text_original\": item[\"chinese\"],\n                        \"text_english\": item.get(\"english\", \"\"),\n                        \"source_ref\": item.get(\"ref\", \"analects\"),\n                    })\n    except:\n        pass\n    \n    return passages\n\n# Load Chinese texts\nchinese_passages = []\nos.makedirs(\"data/raw/chinese\", exist_ok=True)\n\n# Check for cached data first\ncache_file = \"data/raw/chinese/all_passages.jsonl\"\nif os.path.exists(cache_file):\n    print(\"Loading from cache...\")\n    with open(cache_file, 'r') as f:\n        for line in f:\n            chinese_passages.append(json.loads(line))\n    print(f\"Loaded {len(chinese_passages):,} cached passages\")\nelse:\n    print(\"Fetching from Chinese Text Project API...\")\n    print(\"(This may take a few minutes)\")\n    print()\n    \n    for textid, info in tqdm(CHINESE_TEXTS.items(), desc=\"Books\"):\n        print(f\"\\n  {info['title']}...\")\n        book_passages = fetch_ctext_book(textid)\n        \n        for p in book_passages:\n            p[\"source\"] = \"ctext\"\n            p[\"source_type\"] = \"chinese_classic\"\n            p[\"category\"] = info[\"title\"]\n            p[\"time_period\"] = info[\"period\"]\n            p[\"century\"] = info[\"century\"]\n            p[\"language\"] = \"chinese\"\n        \n        chinese_passages.extend(book_passages)\n        print(f\"    Got {len(book_passages)} passages\")\n    \n    # Fallback if API didn't work well\n    if len(chinese_passages) < 1000:\n        print(\"\\nAPI limited, trying fallback sources...\")\n        fallback = download_chinese_fallback()\n        for p in fallback:\n            p[\"source\"] = \"ctext_fallback\"\n            p[\"source_type\"] = \"chinese_classic\"\n            p[\"category\"] = \"Analects\"\n            p[\"time_period\"] = \"CONFUCIAN\"\n            p[\"century\"] = -5\n            p[\"language\"] = \"chinese\"\n        chinese_passages.extend(fallback)\n    \n    # Cache results\n    if chinese_passages:\n        with open(cache_file, 'w') as f:\n            for p in chinese_passages:\n                f.write(json.dumps(p, ensure_ascii=False) + '\\n')\n        print(f\"\\nCached {len(chinese_passages):,} passages\")\n\nprint()\nprint(\"=\" * 60)\nprint(f\"CHINESE CLASSICS LOADED: {len(chinese_passages):,} passages\")\nprint(\"=\" * 60)\n\n# Show sample\nif chinese_passages:\n    sample = chinese_passages[0]\n    print(f\"\\nSample passage:\")\n    print(f\"  Chinese: {sample['text_original'][:100]}...\")\n    if sample.get('text_english'):\n        print(f\"  English: {sample['text_english'][:100]}...\")\n    print(f\"  Period: {sample['time_period']}\")\n    print(f\"  Century: {sample['century']} CE\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title 5b. Load Arabic Classics (Quran + Hadith) { display-mode: \"form\" }\n#@markdown Downloads Quran and Hadith collections in original Arabic.\n#@markdown Sources: quran.com API, sunnah.com\n\nimport requests\nimport time\nimport json\nimport os\nfrom tqdm import tqdm\n\nprint(\"=\" * 60)\nprint(\"LOADING ARABIC CLASSICS\")\nprint(\"=\" * 60)\nprint()\nprint(\"Sources: Quran API, Sunnah.com (Hadith)\")\nprint(\"Period: 600 CE - 900 CE\")\nprint(\"Language: Classical Arabic (\u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u0627\u0644\u0641\u0635\u062d\u0649)\")\nprint()\n\n# Quran API (free, no auth needed)\nQURAN_API = \"https://api.quran.com/api/v4\"\n\n# Sunnah.com API for Hadith\nSUNNAH_API = \"https://api.sunnah.com/v1\"\n\ndef fetch_quran():\n    \"\"\"Fetch all Quran verses with translations.\"\"\"\n    passages = []\n    \n    print(\"Fetching Quran (114 surahs)...\")\n    \n    for surah in tqdm(range(1, 115), desc=\"Surahs\"):\n        try:\n            # Get Arabic text\n            url = f\"{QURAN_API}/verses/by_chapter/{surah}?language=en&words=false&translations=131&fields=text_uthmani\"\n            resp = requests.get(url, timeout=30)\n            \n            if resp.status_code != 200:\n                continue\n            \n            data = resp.json()\n            verses = data.get(\"verses\", [])\n            \n            for verse in verses:\n                arabic = verse.get(\"text_uthmani\", \"\")\n                \n                # Get translation if available\n                translations = verse.get(\"translations\", [])\n                english = translations[0][\"text\"] if translations else \"\"\n                \n                # Clean HTML tags from translation\n                import re\n                english = re.sub(r'<[^>]+>', '', english)\n                \n                if arabic and len(arabic) > 10:\n                    passages.append({\n                        \"text_original\": arabic,\n                        \"text_english\": english,\n                        \"source_ref\": f\"quran:{surah}:{verse.get('verse_number', 0)}\",\n                        \"source\": \"quran\",\n                        \"source_type\": \"quran\",\n                        \"category\": f\"Surah {surah}\",\n                        \"time_period\": \"QURANIC\",\n                        \"century\": 7,\n                        \"language\": \"arabic\",\n                    })\n            \n            time.sleep(0.1)  # Rate limiting\n            \n        except Exception as e:\n            continue\n    \n    return passages\n\ndef fetch_hadith_collection(collection, max_hadiths=2000):\n    \"\"\"Fetch hadiths from a collection.\"\"\"\n    passages = []\n    \n    # Sunnah.com requires API key, try alternative source\n    # Use hadithapi.com (free tier)\n    HADITH_API = \"https://hadithapi.com/api\"\n    \n    collections_map = {\n        \"bukhari\": \"bukhari\",\n        \"muslim\": \"muslim\", \n        \"abudawud\": \"abudawud\",\n        \"tirmidhi\": \"tirmidhi\",\n    }\n    \n    if collection not in collections_map:\n        return passages\n    \n    try:\n        # This API may have rate limits\n        url = f\"{HADITH_API}/{collections_map[collection]}?apiKey=$2y$10$HwOv6dXfZMRHxxxxxxxxxxxx\"\n        \n        # Alternative: use pre-compiled hadith datasets\n        # Many are available on GitHub/Kaggle\n        pass\n        \n    except:\n        pass\n    \n    return passages\n\ndef download_arabic_fallback():\n    \"\"\"Download Arabic texts from backup sources.\"\"\"\n    passages = []\n    \n    # Try to get Quran from alternative source\n    quran_url = \"https://raw.githubusercontent.com/risan/quran-json/main/quran.json\"\n    \n    try:\n        print(\"  Trying fallback Quran source...\")\n        resp = requests.get(quran_url, timeout=60)\n        if resp.status_code == 200:\n            data = resp.json()\n            for surah in data:\n                surah_num = surah.get(\"id\", 0)\n                for verse in surah.get(\"verses\", []):\n                    arabic = verse.get(\"text\", \"\")\n                    if arabic and len(arabic) > 10:\n                        passages.append({\n                            \"text_original\": arabic,\n                            \"text_english\": \"\",  # This source may not have translations\n                            \"source_ref\": f\"quran:{surah_num}:{verse.get('id', 0)}\",\n                            \"source\": \"quran\",\n                            \"source_type\": \"quran\",\n                            \"category\": f\"Surah {surah_num}\",\n                            \"time_period\": \"QURANIC\",\n                            \"century\": 7,\n                            \"language\": \"arabic\",\n                        })\n            print(f\"    Got {len(passages)} verses\")\n    except Exception as e:\n        print(f\"    Fallback failed: {e}\")\n    \n    # Try Hadith from tanzil.net or similar\n    # These are often pre-compiled\n    \n    return passages\n\ndef create_arabic_from_embedded():\n    \"\"\"Create Arabic dataset from embedded essential texts.\"\"\"\n    passages = []\n    \n    # Core Quranic verses with strong moral content (embedded for reliability)\n    # These are the most frequently cited verses on ethics/morality\n    CORE_QURAN = [\n        # Justice\n        (\"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0622\u0645\u064e\u0646\u064f\u0648\u0627 \u0643\u064f\u0648\u0646\u064f\u0648\u0627 \u0642\u064e\u0648\u064e\u0651\u0627\u0645\u0650\u064a\u0646\u064e \u0628\u0650\u0627\u0644\u0652\u0642\u0650\u0633\u0652\u0637\u0650 \u0634\u064f\u0647\u064e\u062f\u064e\u0627\u0621\u064e \u0644\u0650\u0644\u064e\u0651\u0647\u0650\", \n         \"O you who believe, be persistently standing firm in justice, witnesses for Allah\", \"4:135\"),\n        (\"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0644\u064e\u0651\u0647\u064e \u064a\u064e\u0623\u0652\u0645\u064f\u0631\u064f \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u062f\u0652\u0644\u0650 \u0648\u064e\u0627\u0644\u0652\u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u0650\", \n         \"Indeed, Allah orders justice and good conduct\", \"16:90\"),\n        \n        # Kindness to parents\n        (\"\u0648\u064e\u0642\u064e\u0636\u064e\u0649\u0670 \u0631\u064e\u0628\u064f\u0651\u0643\u064e \u0623\u064e\u0644\u064e\u0651\u0627 \u062a\u064e\u0639\u0652\u0628\u064f\u062f\u064f\u0648\u0627 \u0625\u0650\u0644\u064e\u0651\u0627 \u0625\u0650\u064a\u064e\u0651\u0627\u0647\u064f \u0648\u064e\u0628\u0650\u0627\u0644\u0652\u0648\u064e\u0627\u0644\u0650\u062f\u064e\u064a\u0652\u0646\u0650 \u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u064b\u0627\",\n         \"Your Lord has decreed that you worship none but Him, and be good to your parents\", \"17:23\"),\n        \n        # No compulsion\n        (\"\u0644\u064e\u0627 \u0625\u0650\u0643\u0652\u0631\u064e\u0627\u0647\u064e \u0641\u0650\u064a \u0627\u0644\u062f\u0650\u0651\u064a\u0646\u0650\",\n         \"There is no compulsion in religion\", \"2:256\"),\n        \n        # Honoring contracts\n        (\"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0622\u0645\u064e\u0646\u064f\u0648\u0627 \u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064f\u0642\u064f\u0648\u062f\u0650\",\n         \"O you who believe, fulfill your contracts\", \"5:1\"),\n        \n        # Speak truth\n        (\"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0622\u0645\u064e\u0646\u064f\u0648\u0627 \u0627\u062a\u064e\u0651\u0642\u064f\u0648\u0627 \u0627\u0644\u0644\u064e\u0651\u0647\u064e \u0648\u064e\u0642\u064f\u0648\u0644\u064f\u0648\u0627 \u0642\u064e\u0648\u0652\u0644\u064b\u0627 \u0633\u064e\u062f\u0650\u064a\u062f\u064b\u0627\",\n         \"O you who believe, fear Allah and speak words of truth\", \"33:70\"),\n        \n        # Charity\n        (\"\u0648\u064e\u0622\u062a\u064f\u0648\u0627 \u062d\u064e\u0642\u064e\u0651\u0647\u064f \u064a\u064e\u0648\u0652\u0645\u064e \u062d\u064e\u0635\u064e\u0627\u062f\u0650\u0647\u0650\",\n         \"And give its due on the day of harvest\", \"6:141\"),\n        \n        # Patience\n        (\"\u0648\u064e\u0627\u0635\u0652\u0628\u0650\u0631\u0652 \u0648\u064e\u0645\u064e\u0627 \u0635\u064e\u0628\u0652\u0631\u064f\u0643\u064e \u0625\u0650\u0644\u064e\u0651\u0627 \u0628\u0650\u0627\u0644\u0644\u064e\u0651\u0647\u0650\",\n         \"Be patient, for your patience is only through Allah\", \"16:127\"),\n        \n        # Forgiveness\n        (\"\u0648\u064e\u0644\u0652\u064a\u064e\u0639\u0652\u0641\u064f\u0648\u0627 \u0648\u064e\u0644\u0652\u064a\u064e\u0635\u0652\u0641\u064e\u062d\u064f\u0648\u0627 \u0623\u064e\u0644\u064e\u0627 \u062a\u064f\u062d\u0650\u0628\u064f\u0651\u0648\u0646\u064e \u0623\u064e\u0646\u0652 \u064a\u064e\u063a\u0652\u0641\u0650\u0631\u064e \u0627\u0644\u0644\u064e\u0651\u0647\u064f \u0644\u064e\u0643\u064f\u0645\u0652\",\n         \"Let them pardon and overlook. Would you not like Allah to forgive you?\", \"24:22\"),\n        \n        # Prohibition of murder\n        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u062a\u064f\u0644\u064f\u0648\u0627 \u0627\u0644\u0646\u064e\u0651\u0641\u0652\u0633\u064e \u0627\u0644\u064e\u0651\u062a\u0650\u064a \u062d\u064e\u0631\u064e\u0651\u0645\u064e \u0627\u0644\u0644\u064e\u0651\u0647\u064f \u0625\u0650\u0644\u064e\u0651\u0627 \u0628\u0650\u0627\u0644\u0652\u062d\u064e\u0642\u0650\u0651\",\n         \"Do not kill the soul which Allah has forbidden except by right\", \"6:151\"),\n        \n        # Care for orphans\n        (\"\u0648\u064e\u064a\u064e\u0633\u0652\u0623\u064e\u0644\u064f\u0648\u0646\u064e\u0643\u064e \u0639\u064e\u0646\u0650 \u0627\u0644\u0652\u064a\u064e\u062a\u064e\u0627\u0645\u064e\u0649\u0670 \u0642\u064f\u0644\u0652 \u0625\u0650\u0635\u0652\u0644\u064e\u0627\u062d\u064c \u0644\u064e\u0647\u064f\u0645\u0652 \u062e\u064e\u064a\u0652\u0631\u064c\",\n         \"They ask you about orphans. Say: Improvement for them is best\", \"2:220\"),\n        \n        # Honesty in trade\n        (\"\u0648\u064e\u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0627\u0644\u0652\u0643\u064e\u064a\u0652\u0644\u064e \u0648\u064e\u0627\u0644\u0652\u0645\u0650\u064a\u0632\u064e\u0627\u0646\u064e \u0628\u0650\u0627\u0644\u0652\u0642\u0650\u0633\u0652\u0637\u0650\",\n         \"Give full measure and weight in justice\", \"6:152\"),\n        \n        # Mutual consultation\n        (\"\u0648\u064e\u0623\u064e\u0645\u0652\u0631\u064f\u0647\u064f\u0645\u0652 \u0634\u064f\u0648\u0631\u064e\u0649\u0670 \u0628\u064e\u064a\u0652\u0646\u064e\u0647\u064f\u0645\u0652\",\n         \"Their affair is consultation among themselves\", \"42:38\"),\n        \n        # Avoid suspicion\n        (\"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0622\u0645\u064e\u0646\u064f\u0648\u0627 \u0627\u062c\u0652\u062a\u064e\u0646\u0650\u0628\u064f\u0648\u0627 \u0643\u064e\u062b\u0650\u064a\u0631\u064b\u0627 \u0645\u0650\u0646\u064e \u0627\u0644\u0638\u064e\u0651\u0646\u0650\u0651\",\n         \"O you who believe, avoid much suspicion\", \"49:12\"),\n        \n        # Equality\n        (\"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u064f \u0625\u0650\u0646\u064e\u0651\u0627 \u062e\u064e\u0644\u064e\u0642\u0652\u0646\u064e\u0627\u0643\u064f\u0645\u0652 \u0645\u0650\u0646\u0652 \u0630\u064e\u0643\u064e\u0631\u064d \u0648\u064e\u0623\u064f\u0646\u0652\u062b\u064e\u0649\u0670 \u0648\u064e\u062c\u064e\u0639\u064e\u0644\u0652\u0646\u064e\u0627\u0643\u064f\u0645\u0652 \u0634\u064f\u0639\u064f\u0648\u0628\u064b\u0627 \u0648\u064e\u0642\u064e\u0628\u064e\u0627\u0626\u0650\u0644\u064e \u0644\u0650\u062a\u064e\u0639\u064e\u0627\u0631\u064e\u0641\u064f\u0648\u0627\",\n         \"O mankind, We created you from male and female and made you peoples and tribes that you may know one another\", \"49:13\"),\n    ]\n    \n    for arabic, english, ref in CORE_QURAN:\n        passages.append({\n            \"text_original\": arabic,\n            \"text_english\": english,\n            \"source_ref\": f\"quran:{ref}\",\n            \"source\": \"quran\",\n            \"source_type\": \"quran\",\n            \"category\": \"Core Ethics\",\n            \"time_period\": \"QURANIC\",\n            \"century\": 7,\n            \"language\": \"arabic\",\n        })\n    \n    # Core Hadith on ethics\n    CORE_HADITH = [\n        # Golden Rule\n        (\"\u0644\u0627 \u064a\u064f\u0624\u0652\u0645\u0650\u0646\u064f \u0623\u064e\u062d\u064e\u062f\u064f\u0643\u064f\u0645\u0652 \u062d\u064e\u062a\u064e\u0651\u0649 \u064a\u064f\u062d\u0650\u0628\u064e\u0651 \u0644\u0650\u0623\u064e\u062e\u0650\u064a\u0647\u0650 \u0645\u064e\u0627 \u064a\u064f\u062d\u0650\u0628\u064f\u0651 \u0644\u0650\u0646\u064e\u0641\u0652\u0633\u0650\u0647\u0650\",\n         \"None of you truly believes until he loves for his brother what he loves for himself\", \"bukhari:13\"),\n        \n        # Kindness\n        (\"\u0627\u0644\u0631\u064e\u0651\u0627\u062d\u0650\u0645\u064f\u0648\u0646\u064e \u064a\u064e\u0631\u0652\u062d\u064e\u0645\u064f\u0647\u064f\u0645\u064f \u0627\u0644\u0631\u064e\u0651\u062d\u0652\u0645\u064e\u0646\u064f \u0627\u0631\u0652\u062d\u064e\u0645\u064f\u0648\u0627 \u0645\u064e\u0646\u0652 \u0641\u0650\u064a \u0627\u0644\u0652\u0623\u064e\u0631\u0652\u0636\u0650 \u064a\u064e\u0631\u0652\u062d\u064e\u0645\u0652\u0643\u064f\u0645\u0652 \u0645\u064e\u0646\u0652 \u0641\u0650\u064a \u0627\u0644\u0633\u064e\u0651\u0645\u064e\u0627\u0621\u0650\",\n         \"The merciful are shown mercy by the Most Merciful. Show mercy to those on earth, and the One above will show mercy to you\", \"tirmidhi:1924\"),\n        \n        # Truth\n        (\"\u0639\u064e\u0644\u064e\u064a\u0652\u0643\u064f\u0645\u0652 \u0628\u0650\u0627\u0644\u0635\u0650\u0651\u062f\u0652\u0642\u0650 \u0641\u064e\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0635\u0650\u0651\u062f\u0652\u0642\u064e \u064a\u064e\u0647\u0652\u062f\u0650\u064a \u0625\u0650\u0644\u064e\u0649 \u0627\u0644\u0652\u0628\u0650\u0631\u0650\u0651\",\n         \"You must be truthful, for truthfulness leads to righteousness\", \"bukhari:6094\"),\n        \n        # Remove harm\n        (\"\u0644\u064e\u0627 \u0636\u064e\u0631\u064e\u0631\u064e \u0648\u064e\u0644\u064e\u0627 \u0636\u0650\u0631\u064e\u0627\u0631\u064e\",\n         \"There should be no harm and no reciprocal harm\", \"ibn_majah:2341\"),\n        \n        # Good character\n        (\"\u0625\u0650\u0646\u064e\u0651\u0645\u064e\u0627 \u0628\u064f\u0639\u0650\u062b\u0652\u062a\u064f \u0644\u0650\u0623\u064f\u062a\u064e\u0645\u0650\u0651\u0645\u064e \u0645\u064e\u0643\u064e\u0627\u0631\u0650\u0645\u064e \u0627\u0644\u0652\u0623\u064e\u062e\u0652\u0644\u064e\u0627\u0642\u0650\",\n         \"I was sent to perfect good character\", \"malik:muwatta\"),\n        \n        # Helping others\n        (\"\u0627\u0644\u0644\u064e\u0651\u0647\u064f \u0641\u0650\u064a \u0639\u064e\u0648\u0652\u0646\u0650 \u0627\u0644\u0652\u0639\u064e\u0628\u0652\u062f\u0650 \u0645\u064e\u0627 \u0643\u064e\u0627\u0646\u064e \u0627\u0644\u0652\u0639\u064e\u0628\u0652\u062f\u064f \u0641\u0650\u064a \u0639\u064e\u0648\u0652\u0646\u0650 \u0623\u064e\u062e\u0650\u064a\u0647\u0650\",\n         \"Allah helps the servant as long as the servant helps his brother\", \"muslim:2699\"),\n        \n        # Trust\n        (\"\u0623\u064e\u062f\u0650\u0651 \u0627\u0644\u0652\u0623\u064e\u0645\u064e\u0627\u0646\u064e\u0629\u064e \u0625\u0650\u0644\u064e\u0649 \u0645\u064e\u0646\u0650 \u0627\u0626\u0652\u062a\u064e\u0645\u064e\u0646\u064e\u0643\u064e\",\n         \"Render trusts to those who entrusted you\", \"tirmidhi:1264\"),\n        \n        # Avoiding anger\n        (\"\u0644\u064e\u064a\u0652\u0633\u064e \u0627\u0644\u0634\u064e\u0651\u062f\u0650\u064a\u062f\u064f \u0628\u0650\u0627\u0644\u0635\u064f\u0651\u0631\u064e\u0639\u064e\u0629\u0650 \u0625\u0650\u0646\u064e\u0651\u0645\u064e\u0627 \u0627\u0644\u0634\u064e\u0651\u062f\u0650\u064a\u062f\u064f \u0627\u0644\u064e\u0651\u0630\u0650\u064a \u064a\u064e\u0645\u0652\u0644\u0650\u0643\u064f \u0646\u064e\u0641\u0652\u0633\u064e\u0647\u064f \u0639\u0650\u0646\u0652\u062f\u064e \u0627\u0644\u0652\u063a\u064e\u0636\u064e\u0628\u0650\",\n         \"The strong man is not the one who can wrestle, but the one who controls himself when angry\", \"bukhari:6114\"),\n        \n        # Feeding the hungry\n        (\"\u0623\u064e\u0637\u0652\u0639\u0650\u0645\u064f\u0648\u0627 \u0627\u0644\u0652\u062c\u064e\u0627\u0626\u0650\u0639\u064e \u0648\u064e\u0639\u064f\u0648\u062f\u064f\u0648\u0627 \u0627\u0644\u0652\u0645\u064e\u0631\u0650\u064a\u0636\u064e \u0648\u064e\u0641\u064f\u0643\u064f\u0651\u0648\u0627 \u0627\u0644\u0652\u0639\u064e\u0627\u0646\u0650\u064a\u064e\",\n         \"Feed the hungry, visit the sick, and free the captive\", \"bukhari:5649\"),\n        \n        # Neighbor rights\n        (\"\u0645\u064e\u0627 \u0632\u064e\u0627\u0644\u064e \u062c\u0650\u0628\u0652\u0631\u0650\u064a\u0644\u064f \u064a\u064f\u0648\u0635\u0650\u064a\u0646\u0650\u064a \u0628\u0650\u0627\u0644\u0652\u062c\u064e\u0627\u0631\u0650 \u062d\u064e\u062a\u064e\u0651\u0649 \u0638\u064e\u0646\u064e\u0646\u0652\u062a\u064f \u0623\u064e\u0646\u064e\u0651\u0647\u064f \u0633\u064e\u064a\u064f\u0648\u064e\u0631\u0650\u0651\u062b\u064f\u0647\u064f\",\n         \"Gabriel kept recommending the neighbor to me until I thought he would make him an heir\", \"bukhari:6015\"),\n    ]\n    \n    for arabic, english, ref in CORE_HADITH:\n        passages.append({\n            \"text_original\": arabic,\n            \"text_english\": english,\n            \"source_ref\": f\"hadith:{ref}\",\n            \"source\": \"hadith\",\n            \"source_type\": \"hadith\",\n            \"category\": \"Core Ethics\",\n            \"time_period\": \"HADITH\",\n            \"century\": 8,\n            \"language\": \"arabic\",\n        })\n    \n    return passages\n\n# Load Arabic texts\narabic_passages = []\nos.makedirs(\"data/raw/arabic\", exist_ok=True)\n\ncache_file = \"data/raw/arabic/all_passages.jsonl\"\nif os.path.exists(cache_file):\n    print(\"Loading from cache...\")\n    with open(cache_file, 'r') as f:\n        for line in f:\n            arabic_passages.append(json.loads(line))\n    print(f\"Loaded {len(arabic_passages):,} cached passages\")\nelse:\n    # Try API first\n    print(\"Fetching from Quran API...\")\n    arabic_passages = fetch_quran()\n    \n    # If API limited, use fallback\n    if len(arabic_passages) < 1000:\n        print(\"\\nAPI limited, trying fallback...\")\n        arabic_passages.extend(download_arabic_fallback())\n    \n    # Always add embedded core texts (guaranteed to work)\n    print(\"\\nAdding embedded core ethical texts...\")\n    arabic_passages.extend(create_arabic_from_embedded())\n    \n    # Cache\n    if arabic_passages:\n        with open(cache_file, 'w') as f:\n            for p in arabic_passages:\n                f.write(json.dumps(p, ensure_ascii=False) + '\\n')\n        print(f\"Cached {len(arabic_passages):,} passages\")\n\nprint()\nprint(\"=\" * 60)\nprint(f\"ARABIC CLASSICS LOADED: {len(arabic_passages):,} passages\")\nprint(\"=\" * 60)\n\n# Show sample\nif arabic_passages:\n    sample = arabic_passages[0]\n    print(f\"\\nSample passage:\")\n    print(f\"  Arabic: {sample['text_original'][:100]}...\")\n    if sample.get('text_english'):\n        print(f\"  English: {sample['text_english'][:100]}...\")\n    print(f\"  Period: {sample['time_period']}\")\n    print(f\"  Century: {sample['century']} CE\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "#@title 5c. Merge All Corpora (Hebrew + Chinese + Arabic) { display-mode: \"form\" }\n#@markdown Combines all ancient moral texts into unified dataset.\n#@markdown Each passage retains its original language.\n\nimport json\nimport os\nfrom collections import Counter\nfrom tqdm import tqdm\n\nprint(\"=\" * 60)\nprint(\"MERGING CROSS-CULTURAL CORPUS\")\nprint(\"=\" * 60)\nprint()\n\n# Aggregate all passages\nall_ancient_passages = []\n\n# 1. Hebrew (Sefaria) - load from disk if not in memory\nif 'passages' not in dir() or not passages:\n    print(\"Loading Hebrew passages from disk...\")\n    passages = []\n    with open(\"data/processed/passages.jsonl\", 'r') as f:\n        for line in f:\n            passages.append(json.loads(line))\n\nprint(f\"Hebrew (Sefaria):  {len(passages):,} passages\")\nfor p in passages:\n    if hasattr(p, 'to_dict'):\n        p_dict = p.to_dict()\n    elif isinstance(p, dict):\n        p_dict = p\n    else:\n        continue\n    p_dict['language'] = 'hebrew'\n    p_dict['corpus'] = 'sefaria'\n    all_ancient_passages.append(p_dict)\n\n# 2. Chinese classics\nif 'chinese_passages' in dir() and chinese_passages:\n    print(f\"Chinese (CText):   {len(chinese_passages):,} passages\")\n    for p in chinese_passages:\n        p['corpus'] = 'ctext'\n        all_ancient_passages.append(p)\nelse:\n    print(\"Chinese: Not loaded (run cell 5a)\")\n\n# 3. Arabic (Quran + Hadith)\nif 'arabic_passages' in dir() and arabic_passages:\n    print(f\"Arabic (Quran):    {len(arabic_passages):,} passages\")\n    for p in arabic_passages:\n        p['corpus'] = 'quran_hadith'\n        all_ancient_passages.append(p)\nelse:\n    print(\"Arabic: Not loaded (run cell 5b)\")\n\nprint()\nprint(f\"TOTAL ANCIENT: {len(all_ancient_passages):,} passages\")\nprint()\n\n# Language distribution\nlang_counts = Counter(p.get('language', 'unknown') for p in all_ancient_passages)\nprint(\"Language Distribution:\")\nfor lang, count in sorted(lang_counts.items(), key=lambda x: -x[1]):\n    pct = count / len(all_ancient_passages) * 100\n    print(f\"  {lang:10s}: {count:>10,} ({pct:5.1f}%)\")\n\nprint()\n\n# Time period distribution\nperiod_counts = Counter(p.get('time_period', 'unknown') for p in all_ancient_passages)\nprint(\"Time Period Distribution:\")\nfor period, count in sorted(period_counts.items(), key=lambda x: -x[1])[:15]:\n    pct = count / len(all_ancient_passages) * 100\n    print(f\"  {period:15s}: {count:>10,} ({pct:5.1f}%)\")\n\nprint()\n\n# Update time period mapping to include all cultures\nUNIFIED_TIME_PERIODS = {\n    # Hebrew\n    'BIBLICAL': 0,\n    'SECOND_TEMPLE': 1,\n    'TANNAITIC': 2,\n    'AMORAIC': 3,\n    'GEONIC': 4,\n    'RISHONIM': 5,\n    'ACHRONIM': 6,\n    'MODERN_HEBREW': 7,\n    \n    # Chinese\n    'CONFUCIAN': 8,\n    'DAOIST': 9,\n    'MOHIST': 10,\n    \n    # Arabic\n    'QURANIC': 11,\n    'HADITH': 12,\n    \n    # Modern English\n    'DEAR_ABBY': 13,\n}\n\nprint(f\"Unified time periods: {len(UNIFIED_TIME_PERIODS)}\")\n\n# Save merged corpus\nprint()\nprint(\"Saving merged corpus...\")\n\nos.makedirs(\"data/processed\", exist_ok=True)\n\nwith open(\"data/processed/all_passages_multilingual.jsonl\", 'w') as f:\n    for i, p in enumerate(all_ancient_passages):\n        p['unified_id'] = f\"ancient_{i:07d}\"\n        f.write(json.dumps(p, ensure_ascii=False) + '\\n')\n\nprint(f\"Saved to data/processed/all_passages_multilingual.jsonl\")\n\n# Summary statistics\nprint()\nprint(\"=\" * 60)\nprint(\"CROSS-CULTURAL CORPUS READY\")\nprint(\"=\" * 60)\nprint()\nprint(f\"  Languages: {len(lang_counts)}\")\nprint(f\"  Hebrew passages:  {lang_counts.get('hebrew', 0):,}\")\nprint(f\"  Chinese passages: {lang_counts.get('chinese', 0):,}\")\nprint(f\"  Arabic passages:  {lang_counts.get('arabic', 0):,}\")\nprint(f\"  Time periods: {len(period_counts)}\")\nprint(f\"  Total: {len(all_ancient_passages):,}\")\nprint()\nprint(\"The multilingual encoder will map all languages\")\nprint(\"into a shared semantic space for training.\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 6. Extract Bond Structures { display-mode: \"form\" }\n",
        "#@markdown Extracts moral bond structures. Streams to disk to save memory.\n",
        "\n",
        "import gc\n",
        "\n",
        "mark_task(\"Extract bond structures\", \"running\")\n",
        "\n",
        "RELATION_PATTERNS = {\n",
        "    BondType.HARM_PREVENTION: [r'\\b(kill|murder|harm|hurt|save|rescue|protect|danger)\\b'],\n",
        "    BondType.RECIPROCITY: [r'\\b(return|repay|owe|debt|mutual|exchange)\\b'],\n",
        "    BondType.AUTONOMY: [r'\\b(choose|decision|consent|agree|force|coerce|right)\\b'],\n",
        "    BondType.PROPERTY: [r'\\b(property|own|steal|theft|buy|sell|land)\\b'],\n",
        "    BondType.FAMILY: [r'\\b(honor|parent|marry|divorce|inherit|family)\\b'],\n",
        "    BondType.AUTHORITY: [r'\\b(obey|command|law|judge|rule|teach)\\b'],\n",
        "    BondType.CARE: [r'\\b(care|help|assist|feed|clothe|visit)\\b'],\n",
        "    BondType.FAIRNESS: [r'\\b(fair|just|equal|deserve|bias)\\b'],\n",
        "}\n",
        "\n",
        "HOHFELD_PATTERNS = {\n",
        "    HohfeldianState.OBLIGATION: [r'\\b(must|shall|duty|require|should)\\b'],\n",
        "    HohfeldianState.RIGHT: [r'\\b(right to|entitled|deserve)\\b'],\n",
        "    HohfeldianState.LIBERTY: [r'\\b(may|can|permitted|allowed)\\b'],\n",
        "}\n",
        "\n",
        "def extract_bond_structure(passage: Passage) -> Dict:\n",
        "    \"\"\"Extract bond structure from passage.\"\"\"\n",
        "    text = passage.text_english.lower()\n",
        "    \n",
        "    relations = []\n",
        "    for rel_type, patterns in RELATION_PATTERNS.items():\n",
        "        for pattern in patterns:\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                relations.append(rel_type.name)\n",
        "                break\n",
        "    \n",
        "    if not relations:\n",
        "        relations = ['CARE']\n",
        "    \n",
        "    hohfeld = None\n",
        "    for state, patterns in HOHFELD_PATTERNS.items():\n",
        "        for pattern in patterns:\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                hohfeld = state.name\n",
        "                break\n",
        "        if hohfeld:\n",
        "            break\n",
        "    \n",
        "    signature = \"|\".join(sorted(set(relations)))\n",
        "    \n",
        "    return {\n",
        "        'bonds': [{'relation': r} for r in relations],\n",
        "        'primary_relation': relations[0],\n",
        "        'hohfeld_state': hohfeld,\n",
        "        'signature': signature\n",
        "    }\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXTRACTING & SAVING (STREAMING)\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(\"Writing directly to disk to conserve memory...\")\n",
        "print()\n",
        "\n",
        "bond_counts = defaultdict(int)\n",
        "\n",
        "# Stream directly to files - don't accumulate in memory\n",
        "with open(\"data/processed/passages.jsonl\", 'w') as f_pass, \\\n",
        "     open(\"data/processed/bond_structures.jsonl\", 'w') as f_bond:\n",
        "    \n",
        "    for passage in tqdm(all_passages, desc=\"Processing\", unit=\"passage\"):\n",
        "        # Extract bonds\n",
        "        bond_struct = extract_bond_structure(passage)\n",
        "        passage.bond_types = [b['relation'] for b in bond_struct['bonds']]\n",
        "        \n",
        "        # Count for stats\n",
        "        for bond in bond_struct['bonds']:\n",
        "            bond_counts[bond['relation']] += 1\n",
        "        \n",
        "        # Write immediately (don't accumulate)\n",
        "        f_pass.write(json.dumps(passage.to_dict()) + '\\n')\n",
        "        f_bond.write(json.dumps({\n",
        "            'passage_id': passage.id,\n",
        "            'bond_structure': bond_struct\n",
        "        }) + '\\n')\n",
        "\n",
        "# Clear passages from memory - we've saved them to disk\n",
        "n_passages = len(all_passages)\n",
        "del all_passages\n",
        "gc.collect()\n",
        "\n",
        "print()\n",
        "print(f\"Saved {n_passages:,} passages to disk\")\n",
        "print(\"Cleared passages from memory\")\n",
        "\n",
        "# Memory status\n",
        "import psutil\n",
        "mem = psutil.virtual_memory()\n",
        "print(f\"Memory: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n",
        "\n",
        "print()\n",
        "print(\"Bond type distribution:\")\n",
        "for bond_type, count in sorted(bond_counts.items(), key=lambda x: -x[1]):\n",
        "    pct = count / sum(bond_counts.values()) * 100\n",
        "    bar = '#' * int(pct)\n",
        "    print(f\"  {bond_type:20s}: {count:6,} ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "mark_task(\"Extract bond structures\", \"done\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 7. Generate Train/Test Splits { display-mode: \"form\" }\n#@markdown Creates splits from saved files. Memory efficient - reads only IDs.\n\nimport random\nimport gc\nrandom.seed(42)\n\nmark_task(\"Generate train/test splits\", \"running\")\n\nprint(\"=\" * 60)\nprint(\"GENERATING SPLITS (MEMORY EFFICIENT)\")\nprint(\"=\" * 60)\nprint()\n\n# Read only IDs and time periods from disk - don't load full passages\nprint(\"Reading passage metadata from disk...\")\npassage_info = []  # List of (id, time_period) tuples - minimal memory\n\nwith open(\"data/processed/passages.jsonl\", 'r') as f:\n    for line in tqdm(f, desc=\"Reading IDs\", unit=\"line\"):\n        p = json.loads(line)\n        passage_info.append((p['id'], p['time_period']))\n\nprint(f\"Loaded {len(passage_info):,} passage IDs\")\n\n# Define time periods\ntrain_periods = {'BIBLICAL', 'SECOND_TEMPLE', 'TANNAITIC', 'AMORAIC', 'GEONIC', 'RISHONIM'}\nvalid_periods = {'ACHRONIM'}\ntest_periods = {'MODERN_HEBREW', 'DEAR_ABBY'}\n\nprint()\nprint(\"Filtering by time period...\")\nancient_ids = [(pid, tp) for pid, tp in passage_info if tp in train_periods]\nearly_modern_ids = [(pid, tp) for pid, tp in passage_info if tp in valid_periods]\nmodern_ids = [(pid, tp) for pid, tp in passage_info if tp in test_periods]\n\nprint(f\"  Ancient/Medieval: {len(ancient_ids):,}\")\nprint(f\"  Early Modern:     {len(early_modern_ids):,}\")\nprint(f\"  Modern:           {len(modern_ids):,}\")\n\n# Shuffle\nrandom.shuffle(ancient_ids)\nrandom.shuffle(early_modern_ids)\nrandom.shuffle(modern_ids)\n\n# ============================================================\n# SPLIT A: ANCIENT -> MODERN\n# ============================================================\nprint()\nprint(\"-\" * 60)\nprint(\"SPLIT A: Train ANCIENT, Test MODERN\")\nprint(\"-\" * 60)\n\ntemporal_A = {\n    'name': 'ancient_to_modern',\n    'direction': 'A->M',\n    'train_ids': [pid for pid, _ in ancient_ids],\n    'valid_ids': [pid for pid, _ in early_modern_ids],\n    'test_ids': [pid for pid, _ in modern_ids],\n    'train_size': len(ancient_ids),\n    'valid_size': len(early_modern_ids),\n    'test_size': len(modern_ids)\n}\nprint(f\"  Train: {temporal_A['train_size']:,}\")\nprint(f\"  Valid: {temporal_A['valid_size']:,}\")\nprint(f\"  Test:  {temporal_A['test_size']:,}\")\n\n# ============================================================\n# SPLIT B: MODERN -> ANCIENT\n# ============================================================\nprint()\nprint(\"-\" * 60)\nprint(\"SPLIT B: Train MODERN, Test ANCIENT\")\nprint(\"-\" * 60)\n\nn_modern = len(modern_ids)\nancient_test = ancient_ids[n_modern:n_modern*2] if len(ancient_ids) >= n_modern*2 else ancient_ids[n_modern:]\n\ntemporal_B = {\n    'name': 'modern_to_ancient',\n    'direction': 'M->A',\n    'train_ids': [pid for pid, _ in modern_ids],\n    'valid_ids': [pid for pid, _ in early_modern_ids[:len(early_modern_ids)//2]],\n    'test_ids': [pid for pid, _ in ancient_test],\n    'train_size': len(modern_ids),\n    'valid_size': len(early_modern_ids) // 2,\n    'test_size': len(ancient_test)\n}\nprint(f\"  Train: {temporal_B['train_size']:,}\")\nprint(f\"  Valid: {temporal_B['valid_size']:,}\")\nprint(f\"  Test:  {temporal_B['test_size']:,}\")\n\n# ============================================================\n# SPLIT C: MIXED CONTROL\n# ============================================================\nprint()\nprint(\"-\" * 60)\nprint(\"SPLIT C: MIXED (Control)\")\nprint(\"-\" * 60)\n\nall_ids = ancient_ids + modern_ids\nrandom.shuffle(all_ids)\nn = len(all_ids)\nn_train = int(0.7 * n)\nn_valid = int(0.15 * n)\n\ntemporal_C = {\n    'name': 'mixed_control',\n    'direction': 'MIXED',\n    'train_ids': [pid for pid, _ in all_ids[:n_train]],\n    'valid_ids': [pid for pid, _ in all_ids[n_train:n_train+n_valid]],\n    'test_ids': [pid for pid, _ in all_ids[n_train+n_valid:]],\n    'train_size': n_train,\n    'valid_size': n_valid,\n    'test_size': n - n_train - n_valid\n}\nprint(f\"  Train: {temporal_C['train_size']:,}\")\nprint(f\"  Valid: {temporal_C['valid_size']:,}\")\nprint(f\"  Test:  {temporal_C['test_size']:,}\")\n\n# Clear temporary data\ndel passage_info, ancient_ids, early_modern_ids, modern_ids, all_ids\ngc.collect()\n\n# Save\nprint()\nprint(\"Saving splits...\")\nsplits = {\n    'ancient_to_modern': temporal_A,\n    'modern_to_ancient': temporal_B,\n    'mixed_control': temporal_C\n}\n\nwith open(\"data/splits/all_splits.json\", 'w') as f:\n    json.dump(splits, f, indent=2)\n\n# Memory status\nimport psutil\nmem = psutil.virtual_memory()\nprint(f\"Memory: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n\nprint()\nprint(\"SPLITS SAVED:\")\nprint(\"  - ancient_to_modern (A->M)\")\nprint(\"  - modern_to_ancient (M->A)\")  \nprint(\"  - mixed_control\")\n\n\n\n\n\n# ============================================================\n# DISTRIBUTION CHECK - Catch problems early\n# ============================================================\nprint()\nprint(\"=\" * 60)\nprint(\"LABEL DISTRIBUTION CHECK\")\nprint(\"=\" * 60)\n\n# Count Hohfeld labels\nhohfeld_counts = {}\ntime_counts = {}\nwith open(\"data/processed/bond_structures.jsonl\", 'r') as fb, \\\n     open(\"data/processed/passages.jsonl\", 'r') as fp:\n    for b_line, p_line in zip(fb, fp):\n        b = json.loads(b_line)\n        p = json.loads(p_line)\n        h = b['bond_structure'].get('hohfeld_state', None)\n        t = p['time_period']\n        hohfeld_counts[h] = hohfeld_counts.get(h, 0) + 1\n        time_counts[t] = time_counts.get(t, 0) + 1\n\nprint()\nprint(\"Hohfeld distribution:\")\ntotal_h = sum(hohfeld_counts.values())\nfor h, c in sorted(hohfeld_counts.items(), key=lambda x: -x[1]):\n    pct = 100 * c / total_h\n    bar = \"#\" * int(pct / 2)\n    print(f\"  {str(h):15s}: {c:>8,} ({pct:5.1f}%) {bar}\")\n\nprint()\nprint(\"Time period distribution:\")\ntotal_t = sum(time_counts.values())\nfor t, c in sorted(time_counts.items(), key=lambda x: -x[1]):\n    pct = 100 * c / total_t\n    bar = \"#\" * int(pct / 2)\n    print(f\"  {t:15s}: {c:>8,} ({pct:5.1f}%) {bar}\")\n\n# Compute actual chance baselines\nN_HOHFELD_CLASSES = len([h for h in hohfeld_counts if h is not None]) + 1  # +1 for None\nN_TIME_CLASSES = len(time_counts)\nCHANCE_HOHFELD = 1.0 / N_HOHFELD_CLASSES\nCHANCE_TIME = 1.0 / N_TIME_CLASSES\nprint()\nprint(f\"Chance baseline - Hohfeld: {CHANCE_HOHFELD:.1%} ({N_HOHFELD_CLASSES} classes)\")\nprint(f\"Chance baseline - Time:    {CHANCE_TIME:.1%} ({N_TIME_CLASSES} classes)\")\n\n# Save baselines for later\nbaselines = {\n    'hohfeld_counts': {str(k): v for k, v in hohfeld_counts.items()},\n    'time_counts': time_counts,\n    'chance_hohfeld': CHANCE_HOHFELD,\n    'chance_time': CHANCE_TIME,\n    'n_hohfeld_classes': N_HOHFELD_CLASSES,\n    'n_time_classes': N_TIME_CLASSES\n}\nwith open(\"data/splits/baselines.json\", 'w') as f:\n    json.dump(baselines, f, indent=2)\n\n# Warn if severe imbalance\nmost_common_hohfeld = max(hohfeld_counts.values()) / total_h\nif most_common_hohfeld > 0.7:\n    print()\n    print(f\"WARNING: Hohfeld labels severely imbalanced! Most common = {most_common_hohfeld:.1%}\")\n    print(\"         Model may just predict majority class.\")\n\n# ============================================================\n# SAVE PREPROCESSING TO DRIVE\n# ============================================================\nprint()\nprint(\"=\" * 60)\nprint(\"SAVING PREPROCESSED DATA TO GOOGLE DRIVE\")\nprint(\"=\" * 60)\nimport shutil\nshutil.copytree(\"data/processed\", f\"{SAVE_DIR}/processed\", dirs_exist_ok=True)\nshutil.copytree(\"data/splits\", f\"{SAVE_DIR}/splits\", dirs_exist_ok=True)\nprint(f\"Saved to {SAVE_DIR}\")\nprint(\"If session dies, run: !cp -r {SAVE_DIR}/* data/\")\nprint(\"Then skip to Cell 8.\")\nprint()\n\nmark_task(\"Generate train/test splits\", \"done\")\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 8. Define BIP Model Architecture { display-mode: \"form\" }\n#@markdown Defines the model. Clears memory first to avoid OOM.\n\nimport gc\nimport psutil\n\n# CRITICAL: Clear memory before loading model\nprint(\"Clearing memory before model load...\")\ngc.collect()\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nmem = psutil.virtual_memory()\nprint(f\"Memory before model: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\nprint()\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nimport gc\n\nprint(\"=\" * 60)\nprint(\"DEFINING MODEL ARCHITECTURE\")\nprint(\"=\" * 60)\nprint()\nprint(\"*** CROSS-CULTURAL MODE ***\")\nprint(\"Encoder: paraphrase-multilingual-MiniLM-L12-v2\")\nprint(\"  - Trained on 50+ languages including Hebrew and English\")\nprint(\"  - Maps both languages into shared embedding space\")\nprint(\"  - Sefaria passages: ORIGINAL HEBREW\")\nprint(\"  - Dear Abby passages: ENGLISH\")\nprint()\nprint(\"This is the STRONG test: Does Hebrew moral structure\")\nprint(\"transfer to English with no translation intermediary?\")\nprint()\nprint(\"=\" * 60)\nprint()\nprint(\"=\" * 60)\nprint()\n\nclass GradientReversal(torch.autograd.Function):\n    \"\"\"Gradient reversal layer for adversarial training.\"\"\"\n    @staticmethod\n    def forward(ctx, x, lambda_):\n        ctx.lambda_ = lambda_\n        return x.clone()\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        return -ctx.lambda_ * grad_output, None\n\ndef gradient_reversal(x, lambda_=1.0):\n    return GradientReversal.apply(x, lambda_)\n\nclass BIPEncoder(nn.Module):\n    \"\"\"Sentence encoder using pretrained transformer.\"\"\"\n    def __init__(self, model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", d_model=384):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        self.d_model = d_model\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        hidden = outputs.last_hidden_state\n        mask = attention_mask.unsqueeze(-1).float()\n        pooled = (hidden * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n        return pooled\n\nclass BIPModel(nn.Module):\n    \"\"\"Bond Invariance Principle Model with adversarial disentanglement.\"\"\"\n    def __init__(self, d_model=384, d_bond=64, d_label=32, n_periods=14, n_hohfeld=4):\n        super().__init__()\n        \n        self.encoder = BIPEncoder()\n        \n        self.bond_proj = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, d_bond)\n        )\n        \n        self.label_proj = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, d_label)\n        )\n        \n        self.time_classifier_bond = nn.Linear(d_bond, n_periods)\n        self.time_classifier_label = nn.Linear(d_label, n_periods)\n        self.hohfeld_classifier = nn.Linear(d_bond, n_hohfeld)\n    \n    def forward(self, input_ids, attention_mask, adversarial_lambda=1.0):\n        h = self.encoder(input_ids, attention_mask)\n        \n        z_bond = self.bond_proj(h)\n        z_label = self.label_proj(h)\n        \n        z_bond_adv = gradient_reversal(z_bond, adversarial_lambda)\n        time_pred_bond = self.time_classifier_bond(z_bond_adv)\n        time_pred_label = self.time_classifier_label(z_label)\n        hohfeld_pred = self.hohfeld_classifier(z_bond)\n        \n        return {\n            'z_bond': z_bond,\n            'z_label': z_label,\n            'time_pred_bond': time_pred_bond,\n            'time_pred_label': time_pred_label,\n            'hohfeld_pred': hohfeld_pred\n        }\n\n# Time period mapping\nTIME_PERIOD_TO_IDX = {\n    'BIBLICAL': 0, 'SECOND_TEMPLE': 1, 'TANNAITIC': 2, 'AMORAIC': 3,\n    'GEONIC': 4, 'RISHONIM': 5, 'ACHRONIM': 6, 'MODERN_HEBREW': 7,\n    # Chinese\n    'CONFUCIAN': 8, 'DAOIST': 9, 'MOHIST': 10,\n    # Arabic  \n    'QURANIC': 11, 'HADITH': 12,\n    # Modern\n    'DEAR_ABBY': 13\n}\n\nHOHFELD_TO_IDX = {\n    'OBLIGATION': 0, 'RIGHT': 1, 'LIBERTY': 2, None: 3\n}\n\nclass MoralDataset(Dataset):\n    \"\"\"\n    MEMORY-EFFICIENT Dataset that reads from disk on demand.\n    Does NOT load all data into memory at once.\n    \"\"\"\n    def __init__(self, passage_ids: set, passages_file: str, bonds_file: str, tokenizer, max_len=64):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.passage_ids = passage_ids\n        \n        # Build index: passage_id -> (file_offset, line_length) for passages file\n        # This allows us to seek directly to the line we need\n        print(f\"  Indexing {len(passage_ids):,} passages...\")\n        \n        self.data = []  # Store minimal data: (text, time_period, hohfeld_state)\n        \n        # Load only the passages we need\n        with open(passages_file, 'r') as f_pass, open(bonds_file, 'r') as f_bond:\n            for p_line, b_line in tqdm(zip(f_pass, f_bond), desc=\"  Loading subset\", unit=\"line\", total=None):\n                p = json.loads(p_line)\n                if p['id'] in passage_ids:\n                    b = json.loads(b_line)\n                    self.data.append({\n                        'text': (p.get('text_original', '') if p.get('language') in ['hebrew', 'chinese', 'arabic'] else p.get('text_english', ''))[:1000],  # Use native script for ancient (Hebrew/Chinese/Arabic), English for modern\n                        'time_period': p['time_period'],\n                        'hohfeld': b['bond_structure']['hohfeld_state']\n                    })\n        \n        print(f\"  Loaded {len(self.data):,} samples\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        encoding = self.tokenizer(\n            item['text'],\n            truncation=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'time_label': TIME_PERIOD_TO_IDX.get(item['time_period'], 8),\n            'hohfeld_label': HOHFELD_TO_IDX.get(item['hohfeld'], 3)\n        }\n\ndef collate_fn(batch):\n    return {\n        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n        'time_labels': torch.tensor([x['time_label'] for x in batch]),\n        'hohfeld_labels': torch.tensor([x['hohfeld_label'] for x in batch])\n    }\n\n# Memory cleanup\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(\"Model architecture defined!\")\nprint()\n\n# Memory status\nimport psutil\nmem = psutil.virtual_memory()\nprint(f\"Memory: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 9. Train BIP Model - BIDIRECTIONAL { display-mode: \"form\" }\n#@markdown Trains on BOTH directions for stronger invariance testing.\n\nimport gc\nimport psutil\n\nmark_task(\"Train BIP model\", \"running\")\n\n# Memory cleanup before training\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nmem = psutil.virtual_memory()\nprint(f\"Memory at start: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB\")\n\nprint(\"=\" * 60)\nprint(\"BIDIRECTIONAL BIP TRAINING\")\nprint(\"=\" * 60)\nprint()\nprint(f\"Accelerator: {ACCELERATOR}\")\nprint(f\"Device: {device}\")\nprint()\n\n# Load tokenizer once\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n\n# Store results for both directions\n# Memory cleanup between directions built into the loop\nall_results = {}\n\nfor split_name in ['ancient_to_modern', 'modern_to_ancient']:\n    print()\n    print(\"=\" * 60)\n    print(f\"DIRECTION {split_name}: {'Ancient \u2192 Modern' if split_name == 'ancient_to_modern' else 'Modern \u2192 Ancient'}\")\n    print(\"=\" * 60)\n    print()\n    \n    # Load appropriate split\n    # Keys match what Cell 7 saved\n    with open(\"data/splits/all_splits.json\", 'r') as f:\n        splits = json.load(f)\n    split = splits[split_name]\n    \n    print(f\"Train: {split['train_size']:,}\")\n    print(f\"Valid: {split['valid_size']:,}\")\n    print(f\"Test:  {split['test_size']:,}\")\n    print()\n    \n    # Create fresh model for each direction\n    print(\"Creating fresh model...\")\n    model = BIPModel().to(device)\n    \n    # Compile model for speed (PyTorch 2.0+)\n    if TORCH_COMPILE:\n        print(\"Compiling model with torch.compile...\")\n        model = torch.compile(model, mode=\"reduce-overhead\")\n    \n    if split_name == 'ancient_to_modern':\n        print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Create datasets\n    print(\"Creating datasets...\")\n    train_dataset = MoralDataset(\n        set(split['train_ids']),\n        \"data/processed/passages.jsonl\",\n        \"data/processed/bond_structures.jsonl\",\n        tokenizer\n    )\n    valid_dataset = MoralDataset(\n        set(split['valid_ids']),\n        \"data/processed/passages.jsonl\",\n        \"data/processed/bond_structures.jsonl\",\n        tokenizer\n    )\n    test_dataset = MoralDataset(\n        set(split['test_ids']),\n        \"data/processed/passages.jsonl\",\n        \"data/processed/bond_structures.jsonl\",\n        tokenizer\n    )\n    \n    print(f\"Train samples: {len(train_dataset):,}\")\n    print(f\"Valid samples: {len(valid_dataset):,}\")\n    print(f\"Test samples:  {len(test_dataset):,}\")\n    print()\n    \n    if len(train_dataset) == 0:\n        print(\"ERROR: No training data!\")\n        continue\n    \n    # Adjust batch size based on dataset size\n    batch_size = 256 if split_name == 'ancient_to_modern' else min(32, len(train_dataset) // 10)\n    batch_size = max(32, batch_size)  # Minimum batch size\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                              collate_fn=collate_fn, drop_last=True, num_workers=4, pin_memory=True, prefetch_factor=4, persistent_workers=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size*2, shuffle=False,\n                              collate_fn=collate_fn, num_workers=4, pin_memory=True, prefetch_factor=4, persistent_workers=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False,\n                             collate_fn=collate_fn, num_workers=4, pin_memory=True, prefetch_factor=4, persistent_workers=True)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    \n    # Fewer epochs for direction B (smaller dataset)\n    n_epochs = 3  # Fast mode if direction == 'A' else 15\n    best_valid_loss = float('inf')\n    patience = 3  # Early stopping patience\n    patience_counter = 0\n    \n    print(f\"Training for {n_epochs} epochs (batch_size={batch_size})...\")\n    print()\n    \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        total_loss = 0\n        n_batches = 0\n        \n        pbar = tqdm(train_loader, desc=f\"[{split_name}] Epoch {epoch}/{n_epochs}\", unit=\"batch\")\n        for batch in pbar:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            time_labels = batch['time_labels'].to(device)\n            hohfeld_labels = batch['hohfeld_labels'].to(device)\n            \n            # Mixed precision forward pass\n            with torch.cuda.amp.autocast(enabled=USE_AMP):\n                outputs = model(input_ids, attention_mask, adversarial_lambda=1.0)\n                \n                # Losses\n                # STANDARD DANN: Cross-entropy on time classifier + GRL handles adversarial\n                # DO NOT use entropy maximization - it double-reverses!\n                loss_time_bond = F.cross_entropy(outputs['time_pred_bond'], time_labels)\n                loss_time_label = F.cross_entropy(outputs['time_pred_label'], time_labels)\n                loss_hohfeld = F.cross_entropy(outputs['hohfeld_pred'], hohfeld_labels)\n            \n            loss = loss_hohfeld + loss_time_label + loss_time_bond\n            \n            optimizer.zero_grad()\n            if USE_AMP and scaler is not None:\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n            \n            if USE_TPU:\n                xm.optimizer_step(optimizer)\n                xm.mark_step()\n            # GPU step handled in mixed precision block above\n            \n            total_loss += loss.item()\n            n_batches += 1\n            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n        \n        avg_train_loss = total_loss / n_batches\n        \n        # Validation\n        model.eval()\n        valid_loss = 0\n        valid_batches = 0\n        time_correct = 0\n        time_total = 0\n        hohfeld_correct = 0\n        hohfeld_total = 0\n        \n        with torch.no_grad():\n            for batch in valid_loader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                time_labels = batch['time_labels'].to(device)\n                hohfeld_labels = batch['hohfeld_labels'].to(device)\n                \n                outputs = model(input_ids, attention_mask, adversarial_lambda=0)\n                loss = F.cross_entropy(outputs['hohfeld_pred'], hohfeld_labels)\n                valid_loss += loss.item()\n                valid_batches += 1\n                \n                time_preds = outputs['time_pred_bond'].argmax(dim=-1)\n                time_correct += (time_preds == time_labels).sum().item()\n                time_total += len(time_labels)\n                \n                hohfeld_preds = outputs['hohfeld_pred'].argmax(dim=-1)\n                hohfeld_correct += (hohfeld_preds == hohfeld_labels).sum().item()\n                hohfeld_total += len(hohfeld_labels)\n                \n                if USE_TPU:\n                    xm.mark_step()\n        \n        avg_valid_loss = valid_loss / valid_batches if valid_batches > 0 else 0\n        time_acc = time_correct / time_total if time_total > 0 else 0\n        hohfeld_acc_val = hohfeld_correct / hohfeld_total if hohfeld_total > 0 else 0\n        \n        print(f\"[{split_name}] Epoch {epoch}: Loss={avg_train_loss:.4f}/{avg_valid_loss:.4f}, Hohfeld={hohfeld_acc_val:.1%}, TimeAcc={time_acc:.1%}\")\n        \n        if avg_valid_loss < best_valid_loss:\n            best_valid_loss = avg_valid_loss\n            model_path = f\"models/checkpoints/best_model_{split_name}.pt\"\n            if USE_TPU:\n                xm.save(model.state_dict(), model_path)\n            else:\n                torch.save(model.state_dict(), model_path)\n            print(f\"  -> Saved best model for {split_name}!\")\n            # Backup to Drive\n            import shutil\n            shutil.copy(model_path, f\"{SAVE_DIR}/best_model_{split_name}.pt\")\n            print(f\"  -> Backed up to Google Drive\")\n            patience_counter = 0  # Reset patience\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"  Early stopping at epoch {epoch} (no improvement for {patience} epochs)\")\n                break\n    \n    # Evaluate on test set\n    print()\n    print(f\"Evaluating {split_name} on test set...\")\n    \n    model.load_state_dict(torch.load(f\"models/checkpoints/best_model_{split_name}.pt\", map_location='cpu'))\n    model = model.to(device)\n    model.eval()\n    \n    all_time_preds = []\n    all_time_labels = []\n    all_hohfeld_preds = []\n    all_hohfeld_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=f\"[{split_name}] Testing\", unit=\"batch\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            outputs = model(input_ids, attention_mask, adversarial_lambda=0)\n            \n            all_time_preds.extend(outputs['time_pred_bond'].argmax(dim=-1).cpu().tolist())\n            all_time_labels.extend(batch['time_labels'].tolist())\n            all_hohfeld_preds.extend(outputs['hohfeld_pred'].argmax(dim=-1).cpu().tolist())\n            all_hohfeld_labels.extend(batch['hohfeld_labels'].tolist())\n            \n            if USE_TPU:\n                xm.mark_step()\n    \n    # Calculate metrics\n    time_acc = sum(p == l for p, l in zip(all_time_preds, all_time_labels)) / len(all_time_preds)\n    hohfeld_acc = sum(p == l for p, l in zip(all_hohfeld_preds, all_hohfeld_labels)) / len(all_hohfeld_preds)\n    \n    all_results[split_name] = {\n        'time_acc': time_acc,\n        'hohfeld_acc': hohfeld_acc,\n        'train_size': split['train_size'],\n        'test_size': split['test_size']\n    }\n    \n    print()\n    print(f\"{split_name.upper()} RESULTS:\")\n    print(f\"  Time prediction from z_bond: {time_acc:.1%} (chance ~11%)\")\n    print(f\"  Hohfeld classification:      {hohfeld_acc:.1%} (chance 25%)\")\n\nprint()\nprint(\"=\" * 60)\nprint(\"TRAINING COMPLETE - BOTH DIRECTIONS\")\nprint(\"=\" * 60)\n\nmark_task(\"Train BIP model\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 10. Evaluate Bidirectional Results { display-mode: \"form\" }\n#@markdown Compares results from BOTH directions to assess true invariance.\n\nimport gc\nimport psutil\n\nmark_task(\"Evaluate results\", \"running\")\n\nfrom collections import Counter\ntry:\n    from sklearn.metrics import confusion_matrix, classification_report\n    HAS_SKLEARN = True\nexcept ImportError:\n    HAS_SKLEARN = False\n    print(\"sklearn not available - skipping confusion matrices\")\n\nmem = psutil.virtual_memory()\nprint(f\"Memory at eval start: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB\")\n\nprint(\"=\" * 60)\nprint(\"BIDIRECTIONAL BIP RESULTS\")\nprint(\"=\" * 60)\nprint()\n\nchance_time = 1/9  # 9 time periods\nchance_hohfeld = 1/4  # 4 Hohfeld states\n\nprint(\"DIRECTION A: Ancient \u2192 Modern\")\nprint(\"-\" * 40)\nprint(f\"  Trained on:    {all_results.get('ancient_to_modern', {})['train_size']:,} ancient passages\")\nprint(f\"  Tested on:     {all_results.get('ancient_to_modern', {})['test_size']:,} modern passages\")\nprint(f\"  Time acc:      {all_results.get('ancient_to_modern', {})['time_acc']:.1%} (chance: {chance_time:.1%})\")\nprint(f\"  Hohfeld acc:   {all_results.get('ancient_to_modern', {})['hohfeld_acc']:.1%} (chance: {chance_hohfeld:.1%})\")\nprint()\n\nA_time_near_chance = abs(all_results.get('ancient_to_modern', {})['time_acc'] - chance_time) < 0.05\nA_hohfeld_good = all_results.get('ancient_to_modern', {})['hohfeld_acc'] > 0.35\n\nprint(f\"  Time invariant?    {'YES \u2713' if A_time_near_chance else 'NO \u2717'}\")\nprint(f\"  Moral structure?   {'YES \u2713' if A_hohfeld_good else 'WEAK'}\")\nprint()\n\nprint(\"DIRECTION B: Modern \u2192 Ancient\")\nprint(\"-\" * 40)\nprint(f\"  Trained on:    {all_results.get('modern_to_ancient', {})['train_size']:,} modern passages\")\nprint(f\"  Tested on:     {all_results.get('modern_to_ancient', {})['test_size']:,} ancient passages\")\nprint(f\"  Time acc:      {all_results.get('modern_to_ancient', {})['time_acc']:.1%} (chance: {chance_time:.1%})\")\nprint(f\"  Hohfeld acc:   {all_results.get('modern_to_ancient', {})['hohfeld_acc']:.1%} (chance: {chance_hohfeld:.1%})\")\nprint()\n\nB_time_near_chance = abs(all_results.get('modern_to_ancient', {})['time_acc'] - chance_time) < 0.05\nB_hohfeld_good = all_results.get('modern_to_ancient', {})['hohfeld_acc'] > 0.35\n\nprint(f\"  Time invariant?    {'YES \u2713' if B_time_near_chance else 'NO \u2717'}\")\nprint(f\"  Moral structure?   {'YES \u2713' if B_hohfeld_good else 'WEAK'}\")\nprint()\n\nprint(\"=\" * 60)\nprint(\"BIDIRECTIONAL INVARIANCE TEST\")\nprint(\"=\" * 60)\nprint()\n\nif A_time_near_chance and B_time_near_chance and A_hohfeld_good and B_hohfeld_good:\n    print(\"\"\"\n    \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n    \u2551                                                          \u2551\n    \u2551     BIDIRECTIONAL BIP: STRONGLY SUPPORTED                \u2551\n    \u2551                                                          \u2551\n    \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n    \u2551                                                          \u2551\n    \u2551  \u2713 Ancient \u2192 Modern: Structure transfers                 \u2551\n    \u2551  \u2713 Modern \u2192 Ancient: Structure transfers                 \u2551\n    \u2551  \u2713 BOTH directions show time-invariant moral geometry    \u2551\n    \u2551                                                          \u2551\n    \u2551  This is STRONG evidence for universal moral structure.  \u2551\n    \u2551                                                          \u2551\n    \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \"\"\")\n    bip_result = \"STRONGLY_SUPPORTED\"\nelif A_time_near_chance and A_hohfeld_good:\n    print(\"\"\"\n    \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n    \u2551                                                          \u2551\n    \u2551     BIP: SUPPORTED (Direction A only)                    \u2551\n    \u2551                                                          \u2551\n    \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n    \u2551                                                          \u2551\n    \u2551  \u2713 Ancient \u2192 Modern: Structure transfers                 \u2551\n    \u2551  ? Modern \u2192 Ancient: Weaker or inconclusive              \u2551\n    \u2551                                                          \u2551\n    \u2551  Possible explanations:                                  \u2551\n    \u2551  - Ancient corpus richer/more diverse                    \u2551\n    \u2551  - Sample size imbalance                                 \u2551\n    \u2551  - Asymmetric structure (still interesting)              \u2551\n    \u2551                                                          \u2551\n    \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \"\"\")\n    bip_result = \"SUPPORTED_UNIDIRECTIONAL\"\nelif B_time_near_chance and B_hohfeld_good:\n    print(\"\"\"\n    \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n    \u2551                                                          \u2551\n    \u2551     BIP: SUPPORTED (Direction B only)                    \u2551\n    \u2551                                                          \u2551\n    \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n    \u2551                                                          \u2551\n    \u2551  ? Ancient \u2192 Modern: Weaker or inconclusive              \u2551\n    \u2551  \u2713 Modern \u2192 Ancient: Structure transfers                 \u2551\n    \u2551                                                          \u2551\n    \u2551  Unexpected result - needs investigation.                \u2551\n    \u2551                                                          \u2551\n    \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \"\"\")\n    bip_result = \"SUPPORTED_REVERSE_ONLY\"\nelse:\n    print(\"\"\"\n    \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n    \u2551                                                          \u2551\n    \u2551     BIP: INCONCLUSIVE                                    \u2551\n    \u2551                                                          \u2551\n    \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n    \u2551                                                          \u2551\n    \u2551  Neither direction shows clear invariance.               \u2551\n    \u2551                                                          \u2551\n    \u2551  Possible issues:                                        \u2551\n    \u2551  - Need more training epochs                             \u2551\n    \u2551  - Need better bond extraction                           \u2551\n    \u2551  - BIP may not hold (null result)                        \u2551\n    \u2551                                                          \u2551\n    \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \"\"\")\n    bip_result = \"INCONCLUSIVE\"\n\n# Load baselines\ntry:\n    with open(\"data/splits/baselines.json\", 'r') as f:\n        baselines = json.load(f)\n    chance_time = baselines['chance_time']\n    chance_hohfeld = baselines['chance_hohfeld']\nexcept:\n    pass  # Use defaults from above\n\n# Save detailed results including predictions\ndetailed_results = {\n    'ancient_to_modern': all_results.get('ancient_to_modern', {}),\n    'modern_to_ancient': all_results.get('modern_to_ancient', {}),\n}\n\n# Save to Drive for post-mortem\nwith open(f\"{SAVE_DIR}/detailed_results.json\", 'w') as f:\n    json.dump(detailed_results, f, indent=2)\nprint(f\"Detailed results saved to {SAVE_DIR}/detailed_results.json\")\n\n# Save results\nresults_summary = {\n    'ancient_to_modern': all_results.get('ancient_to_modern', {}),\n    'modern_to_ancient': all_results.get('modern_to_ancient', {}),\n    'A_time_invariant': A_time_near_chance,\n    'A_moral_structure': A_hohfeld_good,\n    'B_time_invariant': B_time_near_chance,\n    'B_moral_structure': B_hohfeld_good,\n    'bip_result': bip_result,\n    'chance_time': chance_time,\n    'chance_hohfeld': chance_hohfeld\n}\n\nwith open('results/bidirectional_results.json', 'w') as f:\n    json.dump(results_summary, f, indent=2)\n\nprint()\nprint(\"Results saved to results/bidirectional_results.json\")\n\nmark_task(\"Evaluate results\", \"done\")\n\nprint()\nprint(\"=\" * 60)\nprint(\"EXPERIMENT COMPLETE\")\nprint(\"=\" * 60)\nprint_progress()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Save Results\n",
        "\n",
        "Run the cell below to download your trained model and results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 11. Download Results (Optional) { display-mode: \"form\" }\n",
        "#@markdown Creates a zip file with model checkpoint and metrics.\n",
        "\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create results directory\n",
        "!mkdir -p results\n",
        "!cp models/checkpoints/best_model.pt results/\n",
        "!cp data/splits/all_splits.json results/\n",
        "\n",
        "# Save metrics\n",
        "if len(train_dataset) > 0:\n",
        "    metrics = {\n",
        "        'accelerator': ACCELERATOR,\n",
        "        'time_acc_from_bond': time_acc,\n",
        "        'hohfeld_acc': hohfeld_acc,\n",
        "        'chance_level': chance_level,\n",
        "        'time_invariant': time_invariant,\n",
        "        'moral_structure': moral_structure,\n",
        "        'bip_supported': time_invariant and moral_structure\n",
        "    }\n",
        "    with open('results/metrics.json', 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "# Zip\n",
        "shutil.make_archive('bip_results', 'zip', 'results')\n",
        "print(\"Results saved to bip_results.zip\")\n",
        "print()\n",
        "print(\"Contents:\")\n",
        "!ls -la results/\n",
        "\n",
        "# Download\n",
        "files.download('bip_results.zip')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}