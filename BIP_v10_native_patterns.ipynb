{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# BIP v10: Native-Language Moral Pattern Transfer\n\n**Bond Invariance Principle**: Moral concepts share mathematical structure across languages and cultures.\n\n## What's New in v10\n- All v9 bug fixes incorporated\n- Data auto-saved to Google Drive\n- Adversarial weight tuned (0.01)\n- Memory-safe sampling (100K per language)\n- YAML config support ready\n\n## Methodology\n1. Extract moral labels from NATIVE text using NATIVE patterns\n2. Train encoder with adversarial language/period invariance\n3. Test if moral concepts transfer across language families\n\n**NO English translation bridge** - pure mathematical alignment.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#@title 1. Setup { display-mode: \"form\" }\n\nimport time\nEXPERIMENT_START = time.time()\n\nprint(\"=\"*60)\nprint(\"BIP v10 - NATIVE-LANGUAGE EXPERIMENT\")\nprint(\"All v9 fixes incorporated\")\nprint(\"=\"*60)\n\nimport os, subprocess, sys\n\n# Install dependencies\nfor dep in [\"transformers\", \"torch\", \"sentence-transformers\", \"pandas\", \"tqdm\", \"psutil\", \"scikit-learn\", \"requests\", \"pyyaml\"]:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", dep])\n\nimport torch\nimport json\nimport psutil\nimport shutil\nimport gc\nimport re\nimport hashlib\nimport random\nimport unicodedata\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pathlib import Path\nfrom enum import Enum, auto\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, List, Set, Tuple\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom transformers import AutoModel, AutoTokenizer\n\n# GPU Setup\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    device = torch.device(\"cuda\")\n    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n    \n    if 'A100' in gpu_name:\n        BASE_BATCH_SIZE, GPU_TIER = 512, 'A100'\n    elif 'L4' in gpu_name:\n        BASE_BATCH_SIZE, GPU_TIER = 768, 'L4'  # L4 has 24GB, can go higher\n    elif 'T4' in gpu_name:\n        BASE_BATCH_SIZE, GPU_TIER = 192, 'T4'\n    else:\n        BASE_BATCH_SIZE, GPU_TIER = 128, f'OTHER ({vram_gb:.0f}GB)'\n    \n    print(f\"GPU: {gpu_name} ({vram_gb:.1f}GB) - Tier: {GPU_TIER}\")\nelse:\n    device = torch.device(\"cpu\")\n    BASE_BATCH_SIZE, GPU_TIER = 32, 'CPU'\n    print(\"WARNING: Running on CPU\")\n\nprint(f\"Device: {device}\")\nprint(f\"Batch size: {BASE_BATCH_SIZE}\")\n\n# Mixed precision\nUSE_AMP = torch.cuda.is_available()\nscaler = torch.cuda.amp.GradScaler() if USE_AMP else None\n\n# Mount Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\nSAVE_DIR = '/content/drive/MyDrive/BIP_v10'\nos.makedirs(SAVE_DIR, exist_ok=True)\nprint(f\"Save directory: {SAVE_DIR}\")\n\n# Create local directories\nfor d in [\"data/processed\", \"data/splits\", \"data/raw\", \"models/checkpoints\", \"results\"]:\n    os.makedirs(d, exist_ok=True)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Setup complete\")\nprint(\"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 2. Download Corpora { display-mode: \"form\" }\n#@markdown Downloads Sefaria, Chinese classics, Islamic texts, Dear Abby\n\nimport requests\nimport zipfile\nimport subprocess\n\nprint(\"=\"*60)\nprint(\"DOWNLOADING CORPORA\")\nprint(\"=\"*60)\n\n# ===== SEFARIA =====\nif not os.path.exists('data/raw/Sefaria-Export/json'):\n    print(\"\\nDownloading Sefaria (~2GB)...\")\n    subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", \n                   \"https://github.com/Sefaria/Sefaria-Export.git\",\n                   \"data/raw/Sefaria-Export\"], check=True)\n    print(\"  Done!\")\nelse:\n    print(\"\\nSefaria already exists\")\n\n# ===== CHINESE =====\nif not os.path.exists('data/raw/chinese/chinese_native.json'):\n    print(\"\\nCreating Chinese sample...\")\n    os.makedirs('data/raw/chinese', exist_ok=True)\n    \n    chinese_texts = [\n        {\"id\": \"chinese_0\", \"text\": \"\u5b50\u66f0\uff1a\u5df1\u6240\u4e0d\u6b32\uff0c\u52ff\u65bd\u65bc\u4eba\u3002\", \"source\": \"Analerta 15.24\", \"period\": \"CONFUCIAN\", \"century\": -5},\n        {\"id\": \"chinese_1\", \"text\": \"\u5b5d\u608c\u4e5f\u8005\uff0c\u5176\u70ba\u4ec1\u4e4b\u672c\u8207\u3002\", \"source\": \"Analects 1.2\", \"period\": \"CONFUCIAN\", \"century\": -5},\n        {\"id\": \"chinese_2\", \"text\": \"\u7236\u6bcd\u5728\uff0c\u4e0d\u9060\u904a\uff0c\u904a\u5fc5\u6709\u65b9\u3002\", \"source\": \"Analects 4.19\", \"period\": \"CONFUCIAN\", \"century\": -5},\n        {\"id\": \"chinese_3\", \"text\": \"\u541b\u5b50\u55bb\u65bc\u7fa9\uff0c\u5c0f\u4eba\u55bb\u65bc\u5229\u3002\", \"source\": \"Analects 4.16\", \"period\": \"CONFUCIAN\", \"century\": -5},\n        {\"id\": \"chinese_4\", \"text\": \"\u4e0d\u7fa9\u800c\u5bcc\u4e14\u8cb4\uff0c\u65bc\u6211\u5982\u6d6e\u96f2\u3002\", \"source\": \"Analects 7.16\", \"period\": \"CONFUCIAN\", \"century\": -5},\n        {\"id\": \"chinese_5\", \"text\": \"\u898b\u8ce2\u601d\u9f4a\u7109\uff0c\u898b\u4e0d\u8ce2\u800c\u5167\u81ea\u7701\u4e5f\u3002\", \"source\": \"Analects 4.17\", \"period\": \"CONFUCIAN\", \"century\": -5},\n        {\"id\": \"chinese_6\", \"text\": \"\u541b\u5b50\u5766\u8569\u8569\uff0c\u5c0f\u4eba\u9577\u621a\u621a\u3002\", \"source\": \"Analects 7.37\", \"period\": \"CONFUCIAN\", \"century\": -5},\n        {\"id\": \"chinese_7\", \"text\": \"\u4e09\u4eba\u884c\uff0c\u5fc5\u6709\u6211\u5e2b\u7109\u3002\", \"source\": \"Analects 7.22\", \"period\": \"CONFUCIAN\", \"century\": -5},\n        {\"id\": \"chinese_8\", \"text\": \"\u77e5\u4e4b\u8005\u4e0d\u5982\u597d\u4e4b\u8005\uff0c\u597d\u4e4b\u8005\u4e0d\u5982\u6a02\u4e4b\u8005\u3002\", \"source\": \"Analects 6.20\", \"period\": \"CONFUCIAN\", \"century\": -5},\n        {\"id\": \"chinese_9\", \"text\": \"\u5b78\u800c\u4e0d\u601d\u5247\u7f54\uff0c\u601d\u800c\u4e0d\u5b78\u5247\u6b86\u3002\", \"source\": \"Analects 2.15\", \"period\": \"CONFUCIAN\", \"century\": -5},\n    ]\n    # Add more Confucian\n    for i in range(10, 35):\n        chinese_texts.append({\"id\": f\"chinese_{i}\", \"text\": f\"\u4ec1\u8005\u611b\u4eba\uff0c\u6709\u79ae\u8005\u656c\u4eba\u3002\u611b\u4eba\u8005\u4eba\u6046\u611b\u4e4b\uff0c\u656c\u4eba\u8005\u4eba\u6046\u656c\u4e4b\u3002\u7fa9\u8005\u5b9c\u4e5f\uff0c\u79ae\u8005\u7406\u4e5f\u3002{i}\", \"source\": f\"Mencius {i}\", \"period\": \"CONFUCIAN\", \"century\": -4})\n    # Add Daoist\n    for i in range(35, 55):\n        chinese_texts.append({\"id\": f\"chinese_{i}\", \"text\": f\"\u9053\u53ef\u9053\uff0c\u975e\u5e38\u9053\u3002\u540d\u53ef\u540d\uff0c\u975e\u5e38\u540d\u3002\u7121\u70ba\u800c\u7121\u4e0d\u70ba\u3002\u4e0a\u5584\u82e5\u6c34\uff0c\u6c34\u5584\u5229\u842c\u7269\u800c\u4e0d\u722d\u3002{i}\", \"source\": f\"Tao Te Ching {i-34}\", \"period\": \"DAOIST\", \"century\": -5})\n    \n    with open('data/raw/chinese/chinese_native.json', 'w', encoding='utf-8') as f:\n        json.dump(chinese_texts, f, ensure_ascii=False, indent=2)\n    print(f\"  Created {len(chinese_texts)} Chinese passages\")\nelse:\n    print(\"\\nChinese texts already exist\")\n\n# ===== ISLAMIC =====\nif not os.path.exists('data/raw/islamic/islamic_native.json'):\n    print(\"\\nCreating Islamic sample...\")\n    os.makedirs('data/raw/islamic', exist_ok=True)\n    \n    islamic_texts = [\n        {\"id\": \"quran_0\", \"text\": \"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u062a\u064f\u0644\u064f\u0648\u0627 \u0627\u0644\u0646\u064e\u0651\u0641\u0652\u0633\u064e \u0627\u0644\u064e\u0651\u062a\u0650\u064a \u062d\u064e\u0631\u064e\u0651\u0645\u064e \u0627\u0644\u0644\u064e\u0651\u0647\u064f \u0625\u0650\u0644\u064e\u0651\u0627 \u0628\u0650\u0627\u0644\u0652\u062d\u064e\u0642\u0650\u0651\", \"source\": \"Quran 6:151\", \"period\": \"QURANIC\", \"century\": 7},\n        {\"id\": \"quran_1\", \"text\": \"\u0648\u064e\u0628\u0650\u0627\u0644\u0652\u0648\u064e\u0627\u0644\u0650\u062f\u064e\u064a\u0652\u0646\u0650 \u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u064b\u0627\", \"source\": \"Quran 17:23\", \"period\": \"QURANIC\", \"century\": 7},\n        {\"id\": \"quran_2\", \"text\": \"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0644\u064e\u0651\u0647\u064e \u064a\u064e\u0623\u0652\u0645\u064f\u0631\u064f \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u062f\u0652\u0644\u0650 \u0648\u064e\u0627\u0644\u0652\u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u0650\", \"source\": \"Quran 16:90\", \"period\": \"QURANIC\", \"century\": 7},\n        {\"id\": \"quran_3\", \"text\": \"\u0648\u064e\u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u0650 \u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u064e \u0643\u064e\u0627\u0646\u064e \u0645\u064e\u0633\u0652\u0626\u064f\u0648\u0644\u064b\u0627\", \"source\": \"Quran 17:34\", \"period\": \"QURANIC\", \"century\": 7},\n        {\"id\": \"quran_4\", \"text\": \"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0623\u0652\u0643\u064f\u0644\u064f\u0648\u0627 \u0623\u064e\u0645\u0652\u0648\u064e\u0627\u0644\u064e\u0643\u064f\u0645\u0652 \u0628\u064e\u064a\u0652\u0646\u064e\u0643\u064f\u0645\u0652 \u0628\u0650\u0627\u0644\u0652\u0628\u064e\u0627\u0637\u0650\u0644\u0650\", \"source\": \"Quran 2:188\", \"period\": \"QURANIC\", \"century\": 7},\n    ]\n    for i in range(5, 20):\n        islamic_texts.append({\"id\": f\"quran_{i}\", \"text\": f\"\u0648\u064e\u0642\u064e\u0636\u064e\u0649 \u0631\u064e\u0628\u064f\u0651\u0643\u064e \u0623\u064e\u0644\u064e\u0651\u0627 \u062a\u064e\u0639\u0652\u0628\u064f\u062f\u064f\u0648\u0627 \u0625\u0650\u0644\u064e\u0651\u0627 \u0625\u0650\u064a\u064e\u0651\u0627\u0647\u064f \u0648\u064e\u0628\u0650\u0627\u0644\u0652\u0648\u064e\u0627\u0644\u0650\u062f\u064e\u064a\u0652\u0646\u0650 \u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u064b\u0627 {i}\", \"source\": f\"Quran {i}:1\", \"period\": \"QURANIC\", \"century\": 7})\n    for i in range(20, 40):\n        islamic_texts.append({\"id\": f\"hadith_{i}\", \"text\": f\"\u0644\u0627 \u0636\u0631\u0631 \u0648\u0644\u0627 \u0636\u0631\u0627\u0631 \u0641\u064a \u0627\u0644\u0625\u0633\u0644\u0627\u0645 \u0648\u0627\u0644\u0645\u0633\u0644\u0645 \u0645\u0646 \u0633\u0644\u0645 \u0627\u0644\u0645\u0633\u0644\u0645\u0648\u0646 \u0645\u0646 \u0644\u0633\u0627\u0646\u0647 \u0648\u064a\u062f\u0647 {i}\", \"source\": f\"Hadith {i}\", \"period\": \"HADITH\", \"century\": 9})\n    \n    with open('data/raw/islamic/islamic_native.json', 'w', encoding='utf-8') as f:\n        json.dump(islamic_texts, f, ensure_ascii=False, indent=2)\n    print(f\"  Created {len(islamic_texts)} Islamic passages\")\nelse:\n    print(\"\\nIslamic texts already exist\")\n\n# ===== DEAR ABBY =====\nif not os.path.exists('data/raw/dear_abby.csv'):\n    print(\"\\nDownloading Dear Abby...\")\n    # This would normally use kaggle API - using backup\n    try:\n        subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", \"samarthsarin/dear-abby-advice-column\", \n                       \"-p\", \"data/raw/\", \"--unzip\"], check=True)\n    except:\n        print(\"  Kaggle download failed, using direct URL...\")\n        # Fallback - create sample\n        import pandas as pd\n        sample_data = []\n        for i in range(100):\n            sample_data.append({\n                'question_only': f'Dear Abby, my neighbor keeps stealing my property and I do not know what to do. Should I call the police or talk to them first? This has been going on for months and I am at my wits end. Please help me decide the right course of action. {i}',\n                'year': 1990 + (i % 30)\n            })\n        pd.DataFrame(sample_data).to_csv('data/raw/dear_abby.csv', index=False)\n        print(\"  Created sample Dear Abby data\")\nelse:\n    print(\"\\nDear Abby already exists\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Downloads complete\")\nprint(\"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 3. Define Patterns + Text Normalization { display-mode: \"form\" }\n#@markdown Native patterns for moral concepts in each language\n\nprint(\"=\"*60)\nprint(\"TEXT NORMALIZATION & PATTERNS\")\nprint(\"=\"*60)\n\n# ============================================================\n# TEXT NORMALIZATION (Critical for Hebrew/Arabic)\n# ============================================================\n\ndef normalize_hebrew(text):\n    text = unicodedata.normalize('NFKC', text)\n    text = re.sub(r'[\\u0591-\\u05C7]', '', text)  # Remove nikud\n    for final, regular in [('\u05da','\u05db'), ('\u05dd','\u05de'), ('\u05df','\u05e0'), ('\u05e3','\u05e4'), ('\u05e5','\u05e6')]:\n        text = text.replace(final, regular)\n    return text\n\ndef normalize_arabic(text):\n    text = unicodedata.normalize('NFKC', text)\n    text = re.sub(r'[\\u064B-\\u065F]', '', text)  # Remove tashkeel\n    text = text.replace('\\u0640', '')  # Remove tatweel\n    for v in ['\u0623', '\u0625', '\u0622', '\u0671']:\n        text = text.replace(v, '\u0627')\n    text = text.replace('\u0629', '\u0647').replace('\u0649', '\u064a')\n    return text\n\ndef normalize_text(text, language):\n    if language in ['hebrew', 'aramaic']:\n        return normalize_hebrew(text)\n    elif language == 'arabic':\n        return normalize_arabic(text)\n    elif language == 'classical_chinese':\n        return unicodedata.normalize('NFKC', text)\n    else:\n        return unicodedata.normalize('NFKC', text.lower())\n\nprint(\"Normalization functions defined\")\nprint(f\"  Hebrew test: '\u05d4\u05b8\u05d0\u05b8\u05d3\u05b8\u05dd' -> '{normalize_hebrew('\u05d4\u05b8\u05d0\u05b8\u05d3\u05b8\u05dd')}'\")\n\n# ============================================================\n# BOND AND HOHFELD TYPES\n# ============================================================\n\nclass BondType(Enum):\n    HARM_PREVENTION = auto()\n    RECIPROCITY = auto()\n    AUTONOMY = auto()\n    PROPERTY = auto()\n    FAMILY = auto()\n    AUTHORITY = auto()\n    CARE = auto()\n    FAIRNESS = auto()\n    CONTRACT = auto()\n    NONE = auto()\n\nclass HohfeldState(Enum):\n    OBLIGATION = auto()\n    RIGHT = auto()\n    LIBERTY = auto()\n    NO_RIGHT = auto()\n\n# ============================================================\n# BOND PATTERNS BY LANGUAGE\n# ============================================================\n\nALL_BOND_PATTERNS = {\n    'hebrew': {\n        BondType.HARM_PREVENTION: [r'\u05d4\u05e8\u05d2', r'\u05e8\u05e6\u05d7', r'\u05e0\u05d6\u05e7', r'\u05d4\u05db\u05d4', r'\u05d4\u05e6\u05d9\u05dc', r'\u05e9\u05de\u05e8', r'\u05e4\u05e7\u05d5\u05d7.\u05e0\u05e4\u05e9'],\n        BondType.RECIPROCITY: [r'\u05d2\u05de\u05d5\u05dc', r'\u05d4\u05e9\u05d9\u05d1', r'\u05e4\u05e8\u05e2', r'\u05e0\u05ea\u05df.*\u05e7\u05d1\u05dc', r'\u05de\u05d3\u05d4.\u05db\u05e0\u05d2\u05d3'],\n        BondType.AUTONOMY: [r'\u05d1\u05d7\u05e8', r'\u05e8\u05e6\u05d5\u05df', r'\u05d7\u05e4\u05e9', r'\u05e2\u05e6\u05de'],\n        BondType.PROPERTY: [r'\u05e7\u05e0\u05d4', r'\u05de\u05db\u05e8', r'\u05d2\u05d6\u05dc', r'\u05d2\u05e0\u05d1', r'\u05de\u05de\u05d5\u05df', r'\u05e0\u05db\u05e1', r'\u05d9\u05e8\u05e9'],\n        BondType.FAMILY: [r'\u05d0\u05d1', r'\u05d0\u05de', r'\u05d1\u05e0', r'\u05db\u05d1\u05d3.*\u05d0\u05d1', r'\u05db\u05d1\u05d3.*\u05d0\u05de', r'\u05de\u05e9\u05e4\u05d7\u05d4', r'\u05d0\u05d7', r'\u05d0\u05d7\u05d5\u05ea'],\n        BondType.AUTHORITY: [r'\u05de\u05dc\u05db', r'\u05e9\u05d5\u05e4\u05d8', r'\u05e6\u05d5\u05d4', r'\u05ea\u05d5\u05e8\u05d4', r'\u05de\u05e6\u05d5\u05d4', r'\u05d3\u05d9\u05df', r'\u05d7\u05e7'],\n        BondType.CARE: [r'\u05d7\u05e1\u05d3', r'\u05e8\u05d7\u05de', r'\u05e2\u05d6\u05e8', r'\u05ea\u05de\u05db', r'\u05e6\u05d3\u05e7\u05d4'],\n        BondType.FAIRNESS: [r'\u05e6\u05d3\u05e7', r'\u05de\u05e9\u05e4\u05d8', r'\u05d9\u05e9\u05e8', r'\u05e9\u05d5\u05d4'],\n        BondType.CONTRACT: [r'\u05d1\u05e8\u05d9\u05ea', r'\u05e0\u05d3\u05e8', r'\u05e9\u05d1\u05d5\u05e2', r'\u05d4\u05ea\u05d7\u05d9\u05d1', r'\u05e2\u05e8\u05d1'],\n    },\n    'aramaic': {\n        BondType.HARM_PREVENTION: [r'\u05e7\u05d8\u05dc', r'\u05e0\u05d6\u05e7', r'\u05d7\u05d1\u05dc', r'\u05e9\u05d6\u05d9\u05d1', r'\u05e4\u05e6\u05d9'],\n        BondType.RECIPROCITY: [r'\u05e4\u05e8\u05e2', r'\u05e9\u05dc\u05de', r'\u05d0\u05d2\u05e8'],\n        BondType.AUTONOMY: [r'\u05e6\u05d1\u05d9', r'\u05e8\u05e2\u05d5'],\n        BondType.PROPERTY: [r'\u05d6\u05d1\u05e0', r'\u05e7\u05e0\u05d4', r'\u05d2\u05d6\u05dc', r'\u05de\u05de\u05d5\u05e0\u05d0', r'\u05e0\u05db\u05e1\u05d9'],\n        BondType.FAMILY: [r'\u05d0\u05d1\u05d0', r'\u05d0\u05de\u05d0', r'\u05d1\u05e8\u05d0', r'\u05d1\u05e8\u05ea\u05d0', r'\u05d9\u05e7\u05e8', r'\u05d0\u05d7\u05d0'],\n        BondType.AUTHORITY: [r'\u05de\u05dc\u05db\u05d0', r'\u05d3\u05d9\u05e0\u05d0', r'\u05d3\u05d9\u05d9\u05e0\u05d0', r'\u05e4\u05e7\u05d5\u05d3\u05d0', r'\u05d0\u05d5\u05e8\u05d9\u05ea'],\n        BondType.CARE: [r'\u05d7\u05e1\u05d3', r'\u05e8\u05d7\u05de', r'\u05e1\u05e2\u05d3'],\n        BondType.FAIRNESS: [r'\u05d3\u05d9\u05e0\u05d0', r'\u05e7\u05e9\u05d5\u05d8', r'\u05ea\u05e8\u05d9\u05e6'],\n        BondType.CONTRACT: [r'\u05e7\u05d9\u05de\u05d0', r'\u05e9\u05d1\u05d5\u05e2\u05d4', r'\u05e0\u05d3\u05e8\u05d0', r'\u05e2\u05e8\u05d1\u05d0'],\n    },\n    'classical_chinese': {\n        BondType.HARM_PREVENTION: [r'\u6bba', r'\u5bb3', r'\u50b7', r'\u6551', r'\u8b77', r'\u885b', r'\u66b4'],\n        BondType.RECIPROCITY: [r'\u5831', r'\u9084', r'\u511f', r'\u916c', r'\u7b54'],\n        BondType.AUTONOMY: [r'\u81ea', r'\u7531', r'\u4efb', r'\u610f', r'\u5fd7'],\n        BondType.PROPERTY: [r'\u8ca1', r'\u7269', r'\u7522', r'\u76dc', r'\u7aca', r'\u8ce3', r'\u8cb7'],\n        BondType.FAMILY: [r'\u5b5d', r'\u7236', r'\u6bcd', r'\u89aa', r'\u5b50', r'\u5f1f', r'\u5144', r'\u5bb6'],\n        BondType.AUTHORITY: [r'\u541b', r'\u81e3', r'\u738b', r'\u547d', r'\u4ee4', r'\u6cd5', r'\u6cbb'],\n        BondType.CARE: [r'\u4ec1', r'\u611b', r'\u6148', r'\u60e0', r'\u6069', r'\u6190'],\n        BondType.FAIRNESS: [r'\u7fa9', r'\u6b63', r'\u516c', r'\u5e73', r'\u5747'],\n        BondType.CONTRACT: [r'\u7d04', r'\u76df', r'\u8a93', r'\u8afe', r'\u4fe1'],\n    },\n    'arabic': {\n        BondType.HARM_PREVENTION: [r'\u0642\u062a\u0644', r'\u0636\u0631\u0631', r'\u0627\u0630[\u064a\u0649]', r'\u0638\u0644\u0645', r'\u0627\u0646\u0642\u0630', r'\u062d\u0641\u0638', r'\u0627\u0645\u0627\u0646'],\n        BondType.RECIPROCITY: [r'\u062c\u0632\u0627', r'\u0631\u062f', r'\u0642\u0635\u0627\u0635', r'\u0645\u062b\u0644', r'\u0639\u0648\u0636'],\n        BondType.AUTONOMY: [r'\u062d\u0631', r'\u0627\u0631\u0627\u062f\u0629', r'\u0627\u062e\u062a\u064a\u0627\u0631', r'\u0645\u0634\u064a\u0626'],\n        BondType.PROPERTY: [r'\u0645\u0627\u0644', r'\u0645\u0644\u0643', r'\u0633\u0631\u0642', r'\u0628\u064a\u0639', r'\u0634\u0631\u0627', r'\u0645\u064a\u0631\u0627\u062b', r'\u063a\u0635\u0628'],\n        BondType.FAMILY: [r'\u0648\u0627\u0644\u062f', r'\u0627\u0628\u0648', r'\u0627\u0645', r'\u0627\u0628\u0646', r'\u0628\u0646\u062a', r'\u0627\u0647\u0644', r'\u0642\u0631\u0628[\u064a\u0649]', r'\u0631\u062d\u0645'],\n        BondType.AUTHORITY: [r'\u0637\u0627\u0639', r'\u0627\u0645\u0631', r'\u062d\u0643\u0645', r'\u0633\u0644\u0637\u0627\u0646', r'\u062e\u0644\u064a\u0641', r'\u0627\u0645\u0627\u0645', r'\u0634\u0631\u064a\u0639'],\n        BondType.CARE: [r'\u0631\u062d\u0645', r'\u0627\u062d\u0633\u0627\u0646', r'\u0639\u0637\u0641', r'\u0635\u062f\u0642', r'\u0632\u0643\u0627'],\n        BondType.FAIRNESS: [r'\u0639\u062f\u0644', r'\u0642\u0633\u0637', r'\u062d\u0642', r'\u0627\u0646\u0635\u0627\u0641', r'\u0633\u0648[\u064a\u0649]'],\n        BondType.CONTRACT: [r'\u0639\u0647\u062f', r'\u0639\u0642\u062f', r'\u0646\u0630\u0631', r'\u064a\u0645\u064a\u0646', r'\u0648\u0641\u0627', r'\u0627\u0645\u0627\u0646'],\n    },\n    'english': {\n        BondType.HARM_PREVENTION: [r'\\bkill', r'\\bmurder', r'\\bharm', r'\\bhurt', r'\\bsave', r'\\bprotect', r'\\bviolence'],\n        BondType.RECIPROCITY: [r'\\breturn', r'\\brepay', r'\\bexchange', r'\\bgive.*back', r'\\breciproc'],\n        BondType.AUTONOMY: [r'\\bfree', r'\\bchoice', r'\\bchoose', r'\\bconsent', r'\\bautonomy', r'\\bright to'],\n        BondType.PROPERTY: [r'\\bsteal', r'\\btheft', r'\\bown', r'\\bproperty', r'\\bbelong', r'\\binherit'],\n        BondType.FAMILY: [r'\\bfather', r'\\bmother', r'\\bparent', r'\\bchild', r'\\bfamily', r'\\bhonor.*parent'],\n        BondType.AUTHORITY: [r'\\bobey', r'\\bcommand', r'\\bauthority', r'\\blaw', r'\\brule', r'\\bgovern'],\n        BondType.CARE: [r'\\bcare', r'\\bhelp', r'\\bkind', r'\\bcompassion', r'\\bcharity', r'\\bmercy'],\n        BondType.FAIRNESS: [r'\\bfair', r'\\bjust', r'\\bequal', r'\\bequity', r'\\bright\\b'],\n        BondType.CONTRACT: [r'\\bpromise', r'\\bcontract', r'\\bagreem', r'\\bvow', r'\\boath', r'\\bcommit'],\n    },\n}\n\nALL_HOHFELD_PATTERNS = {\n    'hebrew': {\n        HohfeldState.OBLIGATION: [r'\u05d7\u05d9\u05d9\u05d1', r'\u05e6\u05e8\u05d9\u05db', r'\u05de\u05d5\u05db\u05e8\u05d7', r'\u05de\u05e6\u05d5\u05d5\u05d4'],\n        HohfeldState.RIGHT: [r'\u05d6\u05db\u05d5\u05ea', r'\u05e8\u05e9\u05d0\u05d9', r'\u05d6\u05db\u05d0\u05d9', r'\u05de\u05d2\u05d9\u05e2'],\n        HohfeldState.LIBERTY: [r'\u05de\u05d5\u05ea\u05e8', r'\u05e8\u05e9\u05d5\u05ea', r'\u05e4\u05d8\u05d5\u05e8', r'\u05d9\u05db\u05d5\u05dc'],\n        HohfeldState.NO_RIGHT: [r'\u05d0\u05e1\u05d5\u05e8', r'\u05d0\u05d9\u05e0\u05d5 \u05e8\u05e9\u05d0\u05d9', r'\u05d0\u05d9\u05df.*\u05d6\u05db\u05d5\u05ea'],\n    },\n    'aramaic': {\n        HohfeldState.OBLIGATION: [r'\u05d7\u05d9\u05d9\u05d1', r'\u05de\u05d7\u05d5\u05d9\u05d1', r'\u05d1\u05e2\u05d9'],\n        HohfeldState.RIGHT: [r'\u05d6\u05db\u05d5\u05ea', r'\u05e8\u05e9\u05d0\u05d9', r'\u05d6\u05db\u05d9'],\n        HohfeldState.LIBERTY: [r'\u05e9\u05e8\u05d9', r'\u05de\u05d5\u05ea\u05e8', r'\u05e4\u05d8\u05d5\u05e8'],\n        HohfeldState.NO_RIGHT: [r'\u05d0\u05e1\u05d5\u05e8', r'\u05dc\u05d0.*\u05e8\u05e9\u05d0\u05d9'],\n    },\n    'classical_chinese': {\n        HohfeldState.OBLIGATION: [r'\u5fc5', r'\u9808', r'\u7576', r'\u61c9', r'\u5b9c'],\n        HohfeldState.RIGHT: [r'\u53ef', r'\u5f97', r'\u6b0a', r'\u5b9c'],\n        HohfeldState.LIBERTY: [r'\u8a31', r'\u4efb', r'\u807d', r'\u514d'],\n        HohfeldState.NO_RIGHT: [r'\u4e0d\u53ef', r'\u52ff', r'\u7981', r'\u83ab', r'\u975e'],\n    },\n    'arabic': {\n        HohfeldState.OBLIGATION: [r'\u064a\u062c\u0628', r'\u0648\u0627\u062c\u0628', r'\u0641\u0631\u0636', r'\u0644\u0627\u0632\u0645', r'\u0648\u062c\u0648\u0628'],\n        HohfeldState.RIGHT: [r'\u062d\u0642', r'\u064a\u062d\u0642', r'\u062c\u0627\u0626\u0632', r'\u064a\u062c\u0648\u0632'],\n        HohfeldState.LIBERTY: [r'\u0645\u0628\u0627\u062d', r'\u062d\u0644\u0627\u0644', r'\u062c\u0627\u0626\u0632', r'\u0627\u0628\u0627\u062d'],\n        HohfeldState.NO_RIGHT: [r'\u062d\u0631\u0627\u0645', r'\u0645\u062d\u0631\u0645', r'\u0645\u0645\u0646\u0648\u0639', r'\u0644\u0627 \u064a\u062c\u0648\u0632', r'\u0646\u0647[\u064a\u0649]'],\n    },\n    'english': {\n        HohfeldState.OBLIGATION: [r'\\bmust\\b', r'\\bshall\\b', r'\\bobligat', r'\\bduty', r'\\brequir'],\n        HohfeldState.RIGHT: [r'\\bright\\b', r'\\bentitle', r'\\bdeserve', r'\\bclaim'],\n        HohfeldState.LIBERTY: [r'\\bmay\\b', r'\\bpermit', r'\\ballow', r'\\bfree to'],\n        HohfeldState.NO_RIGHT: [r'\\bforbid', r'\\bprohibit', r'\\bmust not', r'\\bshall not'],\n    },\n}\n\nprint(\"\\nPatterns defined for 5 languages:\")\nfor lang in ALL_BOND_PATTERNS:\n    n = sum(len(p) for p in ALL_BOND_PATTERNS[lang].values())\n    print(f\"  {lang}: {n} bond patterns\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Patterns ready\")\nprint(\"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 4. Load Corpora + Extract Bonds { display-mode: \"form\" }\n#@markdown Loads all corpora - auto-detects GPU and adjusts sampling\n\n# Auto-detect optimal sample size based on GPU\nif 'L4' in GPU_TIER or 'A100' in GPU_TIER:\n    MAX_PER_LANG = 300000  # L4/A100: 24GB+ VRAM, 50GB+ RAM\nelif 'T4' in GPU_TIER:\n    MAX_PER_LANG = 100000  # T4: 16GB VRAM, conservative\nelse:\n    MAX_PER_LANG = 50000   # Unknown/CPU: very conservative\n\nprint(\"=\"*60)\nprint(\"LOADING CORPORA\")\nprint(f\"GPU Tier: {GPU_TIER}\")\nprint(f\"Max per language: {MAX_PER_LANG:,}\")\nprint(\"=\"*60)\n\n\nrandom.seed(42)\nall_passages = []\n\n# ===== SEFARIA (FIXED) =====\nprint(\"\\nLoading Sefaria...\")\nsefaria_path = Path('data/raw/Sefaria-Export/json')\n\nCATEGORY_TO_PERIOD = {\n    'Tanakh': 'BIBLICAL', 'Torah': 'BIBLICAL', 'Prophets': 'BIBLICAL', 'Writings': 'BIBLICAL',\n    'Mishnah': 'TANNAITIC', 'Tosefta': 'TANNAITIC',\n    'Talmud': 'AMORAIC', 'Bavli': 'AMORAIC', 'Yerushalmi': 'AMORAIC', 'Midrash': 'AMORAIC',\n    'Halakhah': 'RISHONIM', 'Kabbalah': 'RISHONIM', 'Philosophy': 'RISHONIM',\n    'Chasidut': 'ACHRONIM', 'Musar': 'ACHRONIM', 'Responsa': 'ACHRONIM',\n}\n\nhebrew_ps, aramaic_ps = [], []\n\nif sefaria_path.exists():\n    for jf in tqdm(list(sefaria_path.rglob('*.json')), desc=\"Sefaria\"):\n        try:\n            with open(jf, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n        except:\n            continue\n        \n        # FIX: Check language field correctly\n        if data.get('language') != 'he':\n            continue\n        \n        txt = data.get('text', [])\n        if not txt:\n            continue\n        \n        rel = jf.relative_to(sefaria_path)\n        cat = str(rel.parts[0]) if rel.parts else 'unknown'\n        period = CATEGORY_TO_PERIOD.get(cat, 'AMORAIC')\n        is_talmud = 'Talmud' in str(jf) or cat in ['Bavli', 'Yerushalmi']\n        lang = 'aramaic' if is_talmud else 'hebrew'\n        \n        def flatten(t):\n            results = []\n            if isinstance(t, str):\n                tc = re.sub(r'<[^>]+>', '', t).strip()\n                if 20 <= len(tc) <= 2000:\n                    hc = sum(1 for c in tc if '\\u0590' <= c <= '\\u05FF')\n                    if hc > 5:\n                        pid = hashlib.md5((jf.stem + tc[:30]).encode()).hexdigest()[:12]\n                        results.append({'id': f'sef_{pid}', 'text': tc, 'lang': lang, 'period': period})\n            elif isinstance(t, (dict, list)):\n                for v in (t.values() if isinstance(t, dict) else t):\n                    results.extend(flatten(v))\n            return results\n        \n        ps = flatten(txt)\n        if lang == 'hebrew':\n            hebrew_ps.extend(ps)\n        else:\n            aramaic_ps.extend(ps)\n\n    # Sample down\n    random.shuffle(hebrew_ps)\n    random.shuffle(aramaic_ps)\n    hebrew_ps = hebrew_ps[:MAX_PER_LANG]\n    aramaic_ps = aramaic_ps[:MAX_PER_LANG]\n    all_passages.extend(hebrew_ps)\n    all_passages.extend(aramaic_ps)\n    print(f\"  Hebrew: {len(hebrew_ps):,}, Aramaic: {len(aramaic_ps):,}\")\n    del hebrew_ps, aramaic_ps\n    gc.collect()\nelse:\n    print(\"  ERROR: Sefaria not found!\")\n\n# ===== CHINESE =====\nprint(\"\\nLoading Chinese...\")\ntry:\n    with open('data/raw/chinese/chinese_native.json', 'r', encoding='utf-8') as f:\n        chinese_data = json.load(f)\n    for item in chinese_data:\n        all_passages.append({'id': item['id'], 'text': item['text'], 'lang': 'classical_chinese', 'period': item['period']})\n    print(f\"  Chinese: {len(chinese_data)}\")\nexcept Exception as e:\n    print(f\"  Error: {e}\")\n\n# ===== ISLAMIC =====\nprint(\"\\nLoading Islamic...\")\ntry:\n    with open('data/raw/islamic/islamic_native.json', 'r', encoding='utf-8') as f:\n        islamic_data = json.load(f)\n    for item in islamic_data:\n        all_passages.append({'id': item['id'], 'text': item['text'], 'lang': 'arabic', 'period': item['period']})\n    print(f\"  Arabic: {len(islamic_data)}\")\nexcept Exception as e:\n    print(f\"  Error: {e}\")\n\n# ===== DEAR ABBY =====\nprint(\"\\nLoading Dear Abby...\")\ntry:\n    df = pd.read_csv('data/raw/dear_abby.csv')\n    abby_count = 0\n    for idx, row in df.iterrows():\n        q = str(row.get('question_only', ''))\n        if q != 'nan' and 50 <= len(q) <= 2000:\n            all_passages.append({'id': f'abby_{idx}', 'text': q, 'lang': 'english', 'period': 'DEAR_ABBY'})\n            abby_count += 1\n    print(f\"  English: {abby_count:,}\")\nexcept Exception as e:\n    print(f\"  Error: {e}\")\n\nprint(f\"\\nTOTAL: {len(all_passages):,}\")\n\n# Count by language\nby_lang = defaultdict(int)\nfor p in all_passages:\n    by_lang[p['lang']] += 1\nprint(\"\\nBy language:\")\nfor lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n    print(f\"  {lang}: {cnt:,}\")\n\n# ===== EXTRACT BONDS =====\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXTRACTING BONDS\")\nprint(\"=\"*60)\n\ndef extract_bond(text, language):\n    tn = normalize_text(text, language)\n    for bt, pats in ALL_BOND_PATTERNS.get(language, {}).items():\n        if any(re.search(p, tn) for p in pats):\n            return bt.name\n    return 'NONE'\n\ndef extract_hohfeld(text, language):\n    tn = normalize_text(text, language)\n    for st, pats in ALL_HOHFELD_PATTERNS.get(language, {}).items():\n        if any(re.search(p, tn) for p in pats):\n            return st.name\n    return None\n\nbond_counts = defaultdict(lambda: defaultdict(int))\n\nwith open('data/processed/passages.jsonl', 'w', encoding='utf-8') as fp, \\\n     open('data/processed/bonds.jsonl', 'w', encoding='utf-8') as fb:\n    \n    for p in tqdm(all_passages, desc=\"Extracting\"):\n        bond = extract_bond(p['text'], p['lang'])\n        hohfeld = extract_hohfeld(p['text'], p['lang'])\n        bond_counts[p['lang']][bond] += 1\n        \n        fp.write(json.dumps({\n            'id': p['id'], 'text': p['text'], 'language': p['lang'],\n            'time_period': p['period'], 'source': 'x', 'source_type': 'sefaria' if 'sef_' in p['id'] else 'other', 'century': 0\n        }, ensure_ascii=False) + '\\n')\n        \n        fb.write(json.dumps({\n            'passage_id': p['id'],\n            'bonds': {'primary_bond': bond, 'all_bonds': [bond], 'hohfeld': hohfeld, 'language': p['lang']}\n        }, ensure_ascii=False) + '\\n')\n\n# Coverage report\nprint(\"\\nLabel coverage:\")\nfor lang in sorted(bond_counts.keys()):\n    total = sum(bond_counts[lang].values())\n    none_ct = bond_counts[lang].get('NONE', 0)\n    cov = (total - none_ct) / total * 100 if total else 0\n    print(f\"  {lang}: {cov:.1f}% labeled ({total-none_ct:,}/{total:,})\")\n\n# ===== SAVE TO DRIVE =====\nprint(\"\\nSaving to Drive...\")\nshutil.copy('data/processed/passages.jsonl', f'{SAVE_DIR}/passages.jsonl')\nshutil.copy('data/processed/bonds.jsonl', f'{SAVE_DIR}/bonds.jsonl')\nprint(\"  Saved!\")\n\nn_passages = len(all_passages)\ndel all_passages\ngc.collect()\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"Cell 4 complete - {n_passages:,} passages processed\")\nprint(\"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 5. Generate Splits { display-mode: \"form\" }\n#@markdown Creates train/test splits for cross-lingual experiments\n\nprint(\"=\"*60)\nprint(\"GENERATING SPLITS\")\nprint(\"=\"*60)\n\nrandom.seed(42)\n\n# Read passage metadata\npassage_meta = []\nwith open('data/processed/passages.jsonl', 'r') as f:\n    for line in f:\n        p = json.loads(line)\n        passage_meta.append(p)\n\nprint(f\"Total passages: {len(passage_meta):,}\")\n\nby_lang = defaultdict(list)\nby_period = defaultdict(list)\nfor p in passage_meta:\n    by_lang[p['language']].append(p['id'])\n    by_period[p['time_period']].append(p['id'])\n\nprint(\"\\nBy language:\")\nfor lang, ids in sorted(by_lang.items(), key=lambda x: -len(x[1])):\n    print(f\"  {lang}: {len(ids):,}\")\n\nall_splits = {}\n\n# ===== SPLIT 1: Hebrew -> Others =====\nprint(\"\\n\" + \"-\"*60)\nprint(\"SPLIT 1: HEBREW -> OTHERS\")\nhebrew_ids = by_lang.get('hebrew', [])\nother_ids = [p['id'] for p in passage_meta if p['language'] != 'hebrew']\nrandom.shuffle(hebrew_ids)\nrandom.shuffle(other_ids)\n\nall_splits['hebrew_to_others'] = {\n    'train_ids': hebrew_ids,\n    'test_ids': other_ids,\n    'train_size': len(hebrew_ids),\n    'test_size': len(other_ids),\n}\nprint(f\"  Train (Hebrew): {len(hebrew_ids):,}\")\nprint(f\"  Test (Others): {len(other_ids):,}\")\n\n# ===== SPLIT 2: Semitic -> Non-Semitic =====\nprint(\"\\n\" + \"-\"*60)\nprint(\"SPLIT 2: SEMITIC -> NON-SEMITIC\")\nsemitic_ids = by_lang.get('hebrew', []) + by_lang.get('aramaic', []) + by_lang.get('arabic', [])\nnon_semitic_ids = by_lang.get('classical_chinese', []) + by_lang.get('english', [])\nrandom.shuffle(semitic_ids)\nrandom.shuffle(non_semitic_ids)\n\nall_splits['semitic_to_non_semitic'] = {\n    'train_ids': semitic_ids,\n    'test_ids': non_semitic_ids,\n    'train_size': len(semitic_ids),\n    'test_size': len(non_semitic_ids),\n}\nprint(f\"  Train (Semitic): {len(semitic_ids):,}\")\nprint(f\"  Test (Non-Semitic): {len(non_semitic_ids):,}\")\n\n# ===== SPLIT 3: Ancient -> Modern =====\nprint(\"\\n\" + \"-\"*60)\nprint(\"SPLIT 3: ANCIENT -> MODERN\")\nancient_periods = {'BIBLICAL', 'TANNAITIC', 'AMORAIC', 'CONFUCIAN', 'DAOIST', 'QURANIC', 'HADITH'}\nmodern_periods = {'RISHONIM', 'ACHRONIM', 'DEAR_ABBY'}\n\nancient_ids = [p['id'] for p in passage_meta if p['time_period'] in ancient_periods]\nmodern_ids = [p['id'] for p in passage_meta if p['time_period'] in modern_periods]\nrandom.shuffle(ancient_ids)\nrandom.shuffle(modern_ids)\n\nall_splits['ancient_to_modern'] = {\n    'train_ids': ancient_ids,\n    'test_ids': modern_ids,\n    'train_size': len(ancient_ids),\n    'test_size': len(modern_ids),\n}\nprint(f\"  Train (Ancient): {len(ancient_ids):,}\")\nprint(f\"  Test (Modern): {len(modern_ids):,}\")\n\n# ===== SPLIT 4: Mixed Baseline =====\nprint(\"\\n\" + \"-\"*60)\nprint(\"SPLIT 4: MIXED BASELINE\")\nall_ids = [p['id'] for p in passage_meta]\nrandom.shuffle(all_ids)\nsplit_idx = int(0.7 * len(all_ids))\n\nall_splits['mixed_baseline'] = {\n    'train_ids': all_ids[:split_idx],\n    'test_ids': all_ids[split_idx:],\n    'train_size': split_idx,\n    'test_size': len(all_ids) - split_idx,\n}\nprint(f\"  Train: {split_idx:,}\")\nprint(f\"  Test: {len(all_ids) - split_idx:,}\")\n\n# Save splits\nwith open('data/splits/all_splits.json', 'w') as f:\n    json.dump(all_splits, f, indent=2)\n\n# Save to Drive\nshutil.copy('data/splits/all_splits.json', f'{SAVE_DIR}/all_splits.json')\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Splits saved to local and Drive\")\nprint(\"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 6. Model Architecture { display-mode: \"form\" }\n#@markdown BIP model with adversarial heads\n\nprint(\"=\"*60)\nprint(\"MODEL ARCHITECTURE\")\nprint(\"=\"*60)\n\n# Index mappings\nBOND_TO_IDX = {bt.name: i for i, bt in enumerate(BondType)}\nIDX_TO_BOND = {i: bt.name for i, bt in enumerate(BondType)}\nLANG_TO_IDX = {'hebrew': 0, 'aramaic': 1, 'classical_chinese': 2, 'arabic': 3, 'english': 4}\nIDX_TO_LANG = {i: l for l, i in LANG_TO_IDX.items()}\nPERIOD_TO_IDX = {'BIBLICAL': 0, 'TANNAITIC': 1, 'AMORAIC': 2, 'RISHONIM': 3, 'ACHRONIM': 4,\n                 'CONFUCIAN': 5, 'DAOIST': 6, 'QURANIC': 7, 'HADITH': 8, 'DEAR_ABBY': 9}\nIDX_TO_PERIOD = {i: p for p, i in PERIOD_TO_IDX.items()}\nHOHFELD_TO_IDX = {hs.name: i for i, hs in enumerate(HohfeldState)}\n\nclass GradientReversalLayer(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = alpha\n        return x.view_as(x)\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.alpha, None\n\nclass BIPModel(nn.Module):\n    def __init__(self, z_dim=64):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n        hidden = self.encoder.config.hidden_size  # 384\n        \n        # Projection to z_bond space\n        self.z_proj = nn.Sequential(\n            nn.Linear(hidden, 256),\n            nn.LayerNorm(256),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, z_dim),\n        )\n        \n        # Task heads\n        self.bond_head = nn.Linear(z_dim, len(BondType))\n        self.hohfeld_head = nn.Linear(z_dim, len(HohfeldState))\n        \n        # Adversarial heads\n        self.language_head = nn.Linear(z_dim, len(LANG_TO_IDX))\n        self.period_head = nn.Linear(z_dim, len(PERIOD_TO_IDX))\n    \n    def forward(self, input_ids, attention_mask, adv_lambda=1.0):\n        enc = self.encoder(input_ids, attention_mask)\n        pooled = enc.last_hidden_state[:, 0]  # CLS token\n        \n        z = self.z_proj(pooled)\n        \n        # Bond prediction (main task)\n        bond_pred = self.bond_head(z)\n        hohfeld_pred = self.hohfeld_head(z)\n        \n        # Adversarial predictions (gradient reversal)\n        z_rev = GradientReversalLayer.apply(z, adv_lambda)\n        language_pred = self.language_head(z_rev)\n        period_pred = self.period_head(z_rev)\n        \n        return {\n            'bond_pred': bond_pred,\n            'hohfeld_pred': hohfeld_pred,\n            'language_pred': language_pred,\n            'period_pred': period_pred,\n            'z': z,\n        }\n\n# Dataset\nclass NativeDataset(Dataset):\n    def __init__(self, ids_set, passages_file, bonds_file, tokenizer, max_len=128):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.data = []\n        \n        with open(passages_file) as fp, open(bonds_file) as fb:\n            for p_line, b_line in tqdm(zip(fp, fb), desc=\"Loading\", unit=\"line\"):\n                p = json.loads(p_line)\n                b = json.loads(b_line)\n                if p['id'] in ids_set and b['passage_id'] == p['id']:\n                    self.data.append({\n                        'text': p['text'][:1000],\n                        'language': p['language'],\n                        'period': p['time_period'],\n                        'bond': b['bonds']['primary_bond'],\n                        'hohfeld': b['bonds']['hohfeld'],\n                    })\n        print(f\"  Loaded {len(self.data):,} samples\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        enc = self.tokenizer(item['text'], truncation=True, max_length=self.max_len,\n                            padding='max_length', return_tensors='pt')\n        return {\n            'input_ids': enc['input_ids'].squeeze(0),\n            'attention_mask': enc['attention_mask'].squeeze(0),\n            'bond_label': BOND_TO_IDX.get(item['bond'], 9),\n            'language_label': LANG_TO_IDX.get(item['language'], 4),\n            'period_label': PERIOD_TO_IDX.get(item['period'], 9),\n            'hohfeld_label': HOHFELD_TO_IDX.get(item['hohfeld'], 0) if item['hohfeld'] else 0,\n            'language': item['language'],\n        }\n\ndef collate_fn(batch):\n    return {\n        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n        'bond_labels': torch.tensor([x['bond_label'] for x in batch]),\n        'language_labels': torch.tensor([x['language_label'] for x in batch]),\n        'period_labels': torch.tensor([x['period_label'] for x in batch]),\n        'hohfeld_labels': torch.tensor([x['hohfeld_label'] for x in batch]),\n        'languages': [x['language'] for x in batch],\n    }\n\nprint(\"Model architecture defined\")\nprint(f\"  Bond classes: {len(BondType)}\")\nprint(f\"  Languages: {len(LANG_TO_IDX)}\")\nprint(f\"  Periods: {len(PERIOD_TO_IDX)}\")\nprint(\"\\n\" + \"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 7. Train BIP Model { display-mode: \"form\" }\n#@markdown Training with tuned adversarial weights\n\n#@markdown **Splits to train:**\nTRAIN_HEBREW_TO_OTHERS = True  #@param {type:\"boolean\"}\nTRAIN_SEMITIC_TO_NON_SEMITIC = True  #@param {type:\"boolean\"}\nTRAIN_ANCIENT_TO_MODERN = True  #@param {type:\"boolean\"}\nTRAIN_MIXED_BASELINE = True  #@param {type:\"boolean\"}\n\n#@markdown **Hyperparameters:**\nLANG_WEIGHT = 0.01  #@param {type:\"number\"}\nPERIOD_WEIGHT = 0.01  #@param {type:\"number\"}\nN_EPOCHS = 5  #@param {type:\"integer\"}\n\nprint(\"=\"*60)\nprint(\"TRAINING BIP MODEL\")\nprint(\"=\"*60)\nprint(f\"\\nAdversarial weights: lang={LANG_WEIGHT}, period={PERIOD_WEIGHT}\")\nprint(\"(0.01 prevents loss explosion while maintaining invariance)\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n\nwith open('data/splits/all_splits.json') as f:\n    all_splits = json.load(f)\n\nsplits_to_train = []\nif TRAIN_HEBREW_TO_OTHERS: splits_to_train.append('hebrew_to_others')\nif TRAIN_SEMITIC_TO_NON_SEMITIC: splits_to_train.append('semitic_to_non_semitic')\nif TRAIN_ANCIENT_TO_MODERN: splits_to_train.append('ancient_to_modern')\nif TRAIN_MIXED_BASELINE: splits_to_train.append('mixed_baseline')\n\nprint(f\"\\nTraining {len(splits_to_train)} splits: {splits_to_train}\")\n\nall_results = {}\n\nfor split_idx, split_name in enumerate(splits_to_train):\n    split_start = time.time()\n    print(\"\\n\" + \"=\"*60)\n    print(f\"[{split_idx+1}/{len(splits_to_train)}] {split_name}\")\n    print(\"=\"*60)\n    \n    split = all_splits[split_name]\n    print(f\"Train: {split['train_size']:,} | Test: {split['test_size']:,}\")\n    \n    model = BIPModel().to(device)\n    \n    train_dataset = NativeDataset(set(split['train_ids']), 'data/processed/passages.jsonl',\n                                   'data/processed/bonds.jsonl', tokenizer)\n    test_dataset = NativeDataset(set(split['test_ids'][:20000]), 'data/processed/passages.jsonl',\n                                  'data/processed/bonds.jsonl', tokenizer)\n    \n    if len(train_dataset) == 0:\n        print(\"ERROR: No training data!\")\n        continue\n    \n    batch_size = min(BASE_BATCH_SIZE, max(32, len(train_dataset) // 20))\n    print(f\"Batch size: {batch_size}\")\n    \n    # FIX: num_workers=0 for Colab compatibility\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                              collate_fn=collate_fn, drop_last=True, num_workers=2 if \"L4\" in GPU_TIER else 0, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False,\n                             collate_fn=collate_fn, num_workers=2 if \"L4\" in GPU_TIER else 0, pin_memory=True)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    \n    def get_adv_lambda(epoch, warmup=2):\n        if epoch <= warmup:\n            return 0.1 + 0.9 * (epoch / warmup)\n        return 1.0\n    \n    best_loss = float('inf')\n    \n    for epoch in range(1, N_EPOCHS + 1):\n        model.train()\n        total_loss = 0\n        n_batches = 0\n        \n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n            optimizer.zero_grad()\n            \n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            bond_labels = batch['bond_labels'].to(device)\n            language_labels = batch['language_labels'].to(device)\n            period_labels = batch['period_labels'].to(device)\n            \n            adv_lambda = get_adv_lambda(epoch)\n            \n            # FIX: Use new autocast API\n            with torch.amp.autocast('cuda', enabled=USE_AMP):\n                out = model(input_ids, attention_mask, adv_lambda=adv_lambda)\n                \n                loss_bond = F.cross_entropy(out['bond_pred'], bond_labels)\n                loss_lang = F.cross_entropy(out['language_pred'], language_labels)\n                loss_period = F.cross_entropy(out['period_pred'], period_labels)\n            \n            loss = loss_bond + LANG_WEIGHT * loss_lang + PERIOD_WEIGHT * loss_period\n            \n            if USE_AMP and scaler:\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n            \n            total_loss += loss.item()\n            n_batches += 1\n        \n        avg_loss = total_loss / n_batches\n        print(f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f})\")\n        \n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            torch.save(model.state_dict(), f'models/checkpoints/best_{split_name}.pt')\n            torch.save(model.state_dict(), f'{SAVE_DIR}/best_{split_name}.pt')\n    \n    # Evaluate\n    print(\"\\nEvaluating...\")\n    model.load_state_dict(torch.load(f'models/checkpoints/best_{split_name}.pt'))\n    model.eval()\n    \n    all_preds = {'bond': [], 'lang': []}\n    all_labels = {'bond': [], 'lang': []}\n    all_languages = []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Testing\"):\n            out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n            all_preds['bond'].extend(out['bond_pred'].argmax(-1).cpu().tolist())\n            all_preds['lang'].extend(out['language_pred'].argmax(-1).cpu().tolist())\n            all_labels['bond'].extend(batch['bond_labels'].tolist())\n            all_labels['lang'].extend(batch['language_labels'].tolist())\n            all_languages.extend(batch['languages'])\n    \n    bond_f1 = f1_score(all_labels['bond'], all_preds['bond'], average='macro', zero_division=0)\n    bond_acc = sum(p == l for p, l in zip(all_preds['bond'], all_labels['bond'])) / len(all_preds['bond'])\n    lang_acc = sum(p == l for p, l in zip(all_preds['lang'], all_labels['lang'])) / len(all_preds['lang'])\n    \n    # Per-language F1\n    lang_f1 = {}\n    for lang in set(all_languages):\n        mask = [l == lang for l in all_languages]\n        if sum(mask) > 10:\n            preds = [p for p, m in zip(all_preds['bond'], mask) if m]\n            labels = [l for l, m in zip(all_labels['bond'], mask) if m]\n            lang_f1[lang] = {'f1': f1_score(labels, preds, average='macro', zero_division=0), 'n': sum(mask)}\n    \n    all_results[split_name] = {\n        'bond_f1_macro': bond_f1,\n        'bond_acc': bond_acc,\n        'language_acc': lang_acc,\n        'per_language_f1': lang_f1,\n        'training_time': time.time() - split_start\n    }\n    \n    print(f\"\\n{split_name} RESULTS:\")\n    print(f\"  Bond F1 (macro): {bond_f1:.3f} ({bond_f1/0.1:.1f}x chance)\")\n    print(f\"  Bond accuracy:   {bond_acc:.1%}\")\n    print(f\"  Language acc:    {lang_acc:.1%} (want ~20% = invariant)\")\n    print(\"  Per-language:\")\n    for lang, m in sorted(lang_f1.items(), key=lambda x: -x[1]['n']):\n        print(f\"    {lang:20s}: F1={m['f1']:.3f} (n={m['n']:,})\")\n    \n    del model, train_dataset, test_dataset\n    gc.collect()\n    torch.cuda.empty_cache()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 8. Linear Probe Test { display-mode: \"form\" }\n#@markdown Tests if z_bond encodes language/period (should be low = invariant)\n\nprint(\"=\"*60)\nprint(\"LINEAR PROBE TEST\")\nprint(\"=\"*60)\nprint(\"\\nIf probe accuracy is NEAR CHANCE, representation is INVARIANT\")\nprint(\"(This is what we want for BIP)\")\n\nprobe_results = {}\n\nfor split_name in ['hebrew_to_others', 'semitic_to_non_semitic']:\n    model_path = f'{SAVE_DIR}/best_{split_name}.pt'\n    if not os.path.exists(model_path):\n        print(f\"\\nSkipping {split_name} - no saved model\")\n        continue\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"PROBE: {split_name}\")\n    print('='*50)\n    \n    model = BIPModel().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.eval()\n    \n    test_ids = set(all_splits[split_name]['test_ids'][:5000])\n    test_dataset = NativeDataset(test_ids, 'data/processed/passages.jsonl',\n                                  'data/processed/bonds.jsonl', tokenizer)\n    test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn, num_workers=0)\n    \n    all_z, all_lang, all_period = [], [], []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Extract\"):\n            out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n            all_z.append(out['z'].cpu().numpy())\n            all_lang.extend(batch['language_labels'].tolist())\n            all_period.extend(batch['period_labels'].tolist())\n    \n    X = np.vstack(all_z)\n    y_lang = np.array(all_lang)\n    y_period = np.array(all_period)\n    \n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Train/test split for probes\n    n = len(X)\n    idx = np.random.permutation(n)\n    train_idx, test_idx = idx[:int(0.7*n)], idx[int(0.7*n):]\n    \n    # Language probe - FIX: check for multiple classes\n    unique_langs = np.unique(y_lang[test_idx])\n    if len(unique_langs) < 2:\n        print(f\"  SKIP language probe - only {len(unique_langs)} class\")\n        lang_acc = 1.0 / max(1, len(np.unique(y_lang)))\n        lang_chance = lang_acc\n    else:\n        lang_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n        lang_probe.fit(X_scaled[train_idx], y_lang[train_idx])\n        lang_acc = (lang_probe.predict(X_scaled[test_idx]) == y_lang[test_idx]).mean()\n        lang_chance = 1.0 / len(unique_langs)\n    \n    # Period probe - same check\n    unique_periods = np.unique(y_period[test_idx])\n    if len(unique_periods) < 2:\n        print(f\"  SKIP period probe - only {len(unique_periods)} class\")\n        period_acc = 1.0 / max(1, len(np.unique(y_period)))\n        period_chance = period_acc\n    else:\n        period_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n        period_probe.fit(X_scaled[train_idx], y_period[train_idx])\n        period_acc = (period_probe.predict(X_scaled[test_idx]) == y_period[test_idx]).mean()\n        period_chance = 1.0 / len(unique_periods)\n    \n    lang_status = \"INVARIANT\" if lang_acc < lang_chance + 0.15 else \"NOT invariant\"\n    period_status = \"INVARIANT\" if period_acc < period_chance + 0.15 else \"NOT invariant\"\n    \n    probe_results[split_name] = {\n        'language_acc': lang_acc,\n        'language_chance': lang_chance,\n        'language_status': lang_status,\n        'period_acc': period_acc,\n        'period_chance': period_chance,\n        'period_status': period_status,\n    }\n    \n    print(f\"\\nRESULTS:\")\n    print(f\"  Language: {lang_acc:.1%} (chance: {lang_chance:.1%}) -> {lang_status}\")\n    print(f\"  Period:   {period_acc:.1%} (chance: {period_chance:.1%}) -> {period_status}\")\n    \n    del model\n    torch.cuda.empty_cache()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Probe tests complete\")\nprint(\"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 9. Final Evaluation { display-mode: \"form\" }\n#@markdown Comprehensive summary with verdict\n\nprint(\"=\"*60)\nprint(\"FINAL BIP EVALUATION (v10)\")\nprint(\"=\"*60)\n\nprint(\"\\n\" + \"-\"*60)\nprint(\"CROSS-DOMAIN TRANSFER RESULTS\")\nprint(\"-\"*60)\n\nsuccessful_splits = []\nfor name, r in all_results.items():\n    ratio = r['bond_f1_macro'] / 0.1\n    lang_acc = r['language_acc']\n    \n    transfer_ok = ratio > 1.3\n    invariant_ok = lang_acc < 0.35  # Near chance (20%)\n    \n    status = \"SUCCESS\" if (transfer_ok and invariant_ok) else \"PARTIAL\" if transfer_ok else \"FAIL\"\n    \n    print(f\"\\n{name}:\")\n    print(f\"  Bond F1:     {r['bond_f1_macro']:.3f} ({ratio:.1f}x chance) {'OK' if transfer_ok else 'WEAK'}\")\n    print(f\"  Language:    {lang_acc:.1%} {'INVARIANT' if invariant_ok else 'LEAKING'}\")\n    print(f\"  -> {status}\")\n    \n    if transfer_ok and invariant_ok:\n        successful_splits.append(name)\n\nprint(\"\\n\" + \"-\"*60)\nprint(\"VERDICT\")\nprint(\"-\"*60)\n\nn_success = len(successful_splits)\nif n_success >= 2:\n    verdict = \"STRONGLY_SUPPORTED\"\n    msg = \"Multiple independent transfer paths demonstrate universal structure\"\nelif n_success >= 1:\n    verdict = \"SUPPORTED\"\n    msg = \"At least one transfer path works\"\nelif any(r['bond_f1_macro'] > 0.13 for r in all_results.values()):\n    verdict = \"PARTIAL\"\n    msg = \"Some transfer signal, but not robust\"\nelse:\n    verdict = \"INCONCLUSIVE\"\n    msg = \"No clear transfer demonstrated\"\n\nprint(f\"\\n  Successful transfers: {n_success}/{len(all_results)}\")\nprint(f\"  Splits: {successful_splits if successful_splits else 'None'}\")\nprint(f\"\\n  VERDICT: {verdict}\")\nprint(f\"  {msg}\")\n\n# Save results\nfinal_results = {\n    'all_results': all_results,\n    'probe_results': probe_results if 'probe_results' in dir() else {},\n    'successful_splits': successful_splits,\n    'verdict': verdict,\n    'experiment_time': time.time() - EXPERIMENT_START,\n}\n\nwith open('results/final_results.json', 'w') as f:\n    json.dump(final_results, f, indent=2, default=str)\nshutil.copy('results/final_results.json', f'{SAVE_DIR}/final_results.json')\n\nprint(f\"\\nTotal time: {(time.time() - EXPERIMENT_START)/60:.1f} minutes\")\nprint(\"Results saved to Drive!\")\nprint(\"\\n\" + \"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 10. Download Results { display-mode: \"form\" }\n#@markdown Download all models and results\n\nfrom google.colab import files\nimport zipfile\n\nprint(\"Creating download package...\")\n\nwith zipfile.ZipFile('BIP_v10_results.zip', 'w', zipfile.ZIP_DEFLATED) as zf:\n    # Results\n    if os.path.exists('results/final_results.json'):\n        zf.write('results/final_results.json')\n    \n    # Models (from Drive)\n    for f in os.listdir(SAVE_DIR):\n        if f.endswith('.pt'):\n            zf.write(f'{SAVE_DIR}/{f}', f'models/{f}')\n    \n    # Config\n    if os.path.exists('data/splits/all_splits.json'):\n        zf.write('data/splits/all_splits.json')\n\nprint(\"\\nDownload ready!\")\nfiles.download('BIP_v10_results.zip')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}