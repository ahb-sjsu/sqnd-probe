{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "L4",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#",
    " ",
    "B",
    "I",
    "P",
    " ",
    "v",
    "1",
    "0",
    ".",
    "1",
    "4",
    ".",
    "1",
    " ",
    "-",
    " ",
    "B",
    "u",
    "d",
    "d",
    "h",
    "i",
    "s",
    "t",
    "-",
    "I",
    "n",
    "f",
    "o",
    "r",
    "m",
    "e",
    "d",
    " ",
    "P",
    "h",
    "i",
    "l",
    "o",
    "s",
    "o",
    "p",
    "h",
    "y",
    " ",
    "M",
    "o",
    "r",
    "a",
    "l",
    " ",
    "E",
    "x",
    "t",
    "r",
    "a",
    "c",
    "t",
    "i",
    "o",
    "n",
    "\n",
    "\n",
    "*",
    "*",
    "E",
    "x",
    "t",
    "r",
    "a",
    "c",
    "t",
    "i",
    "n",
    "g",
    " ",
    "m",
    "o",
    "r",
    "a",
    "l",
    " ",
    "k",
    "n",
    "o",
    "w",
    "l",
    "e",
    "d",
    "g",
    "e",
    " ",
    "f",
    "r",
    "o",
    "m",
    " ",
    "5",
    ",",
    "0",
    "0",
    "0",
    " ",
    "y",
    "e",
    "a",
    "r",
    "s",
    " ",
    "o",
    "f",
    " ",
    "h",
    "u",
    "m",
    "a",
    "n",
    " ",
    "e",
    "t",
    "h",
    "i",
    "c",
    "a",
    "l",
    " ",
    "r",
    "e",
    "a",
    "s",
    "o",
    "n",
    "i",
    "n",
    "g",
    "*",
    "*",
    "\n",
    "\n",
    "T",
    "h",
    "i",
    "s",
    " ",
    "n",
    "o",
    "t",
    "e",
    "b",
    "o",
    "o",
    "k",
    " ",
    "i",
    "m",
    "p",
    "l",
    "e",
    "m",
    "e",
    "n",
    "t",
    "s",
    " ",
    "a",
    " ",
    "c",
    "o",
    "m",
    "p",
    "l",
    "e",
    "t",
    "e",
    " ",
    "p",
    "i",
    "p",
    "e",
    "l",
    "i",
    "n",
    "e",
    " ",
    "f",
    "o",
    "r",
    ":",
    "\n",
    "1",
    ".",
    " ",
    "L",
    "o",
    "a",
    "d",
    "i",
    "n",
    "g",
    " ",
    "m",
    "u",
    "l",
    "t",
    "i",
    "-",
    "l",
    "i",
    "n",
    "g",
    "u",
    "a",
    "l",
    " ",
    "a",
    "n",
    "c",
    "i",
    "e",
    "n",
    "t",
    " ",
    "a",
    "n",
    "d",
    " ",
    "m",
    "o",
    "d",
    "e",
    "r",
    "n",
    " ",
    "e",
    "t",
    "h",
    "i",
    "c",
    "a",
    "l",
    " ",
    "t",
    "e",
    "x",
    "t",
    "s",
    "\n",
    "2",
    ".",
    " ",
    "E",
    "x",
    "t",
    "r",
    "a",
    "c",
    "t",
    "i",
    "n",
    "g",
    " ",
    "m",
    "o",
    "r",
    "a",
    "l",
    " ",
    "b",
    "o",
    "n",
    "d",
    "s",
    " ",
    "(",
    "a",
    "g",
    "e",
    "n",
    "t",
    ",",
    " ",
    "p",
    "a",
    "t",
    "i",
    "e",
    "n",
    "t",
    ",",
    " ",
    "o",
    "b",
    "l",
    "i",
    "g",
    "a",
    "t",
    "i",
    "o",
    "n",
    " ",
    "t",
    "y",
    "p",
    "e",
    ")",
    "\n",
    "3",
    ".",
    " ",
    "T",
    "r",
    "a",
    "i",
    "n",
    "i",
    "n",
    "g",
    " ",
    "c",
    "r",
    "o",
    "s",
    "s",
    "-",
    "c",
    "u",
    "l",
    "t",
    "u",
    "r",
    "a",
    "l",
    " ",
    "m",
    "o",
    "r",
    "a",
    "l",
    " ",
    "e",
    "m",
    "b",
    "e",
    "d",
    "d",
    "i",
    "n",
    "g",
    "s",
    "\n",
    "4",
    ".",
    " ",
    "A",
    "n",
    "a",
    "l",
    "y",
    "z",
    "i",
    "n",
    "g",
    " ",
    "e",
    "t",
    "h",
    "i",
    "c",
    "a",
    "l",
    " ",
    "p",
    "a",
    "t",
    "t",
    "e",
    "r",
    "n",
    "s",
    " ",
    "a",
    "c",
    "r",
    "o",
    "s",
    "s",
    " ",
    "t",
    "r",
    "a",
    "d",
    "i",
    "t",
    "i",
    "o",
    "n",
    "s",
    "\n",
    "\n",
    "*",
    "*",
    "B",
    "o",
    "n",
    "d",
    " ",
    "E",
    "x",
    "t",
    "r",
    "a",
    "c",
    "t",
    "i",
    "o",
    "n",
    " ",
    "T",
    "r",
    "a",
    "i",
    "n",
    "i",
    "n",
    "g",
    " ",
    "D",
    "a",
    "t",
    "a",
    " ",
    "(",
    "N",
    "E",
    "W",
    " ",
    "i",
    "n",
    " ",
    "v",
    "1",
    "0",
    ".",
    "1",
    "4",
    ".",
    "1",
    ")",
    ":",
    "*",
    "*",
    "\n",
    "-",
    " ",
    "[",
    "E",
    "T",
    "H",
    "I",
    "C",
    "S",
    "]",
    "(",
    "h",
    "t",
    "t",
    "p",
    "s",
    ":",
    "/",
    "/",
    "g",
    "i",
    "t",
    "h",
    "u",
    "b",
    ".",
    "c",
    "o",
    "m",
    "/",
    "h",
    "e",
    "n",
    "d",
    "r",
    "y",
    "c",
    "k",
    "s",
    "/",
    "e",
    "t",
    "h",
    "i",
    "c",
    "s",
    ")",
    ":",
    " ",
    "1",
    "3",
    "0",
    "K",
    " ",
    "s",
    "c",
    "e",
    "n",
    "a",
    "r",
    "i",
    "o",
    "s",
    " ",
    "a",
    "c",
    "r",
    "o",
    "s",
    "s",
    " ",
    "5",
    " ",
    "c",
    "a",
    "t",
    "e",
    "g",
    "o",
    "r",
    "i",
    "e",
    "s",
    "\n",
    "-",
    " ",
    "[",
    "S",
    "c",
    "r",
    "u",
    "p",
    "l",
    "e",
    "s",
    "]",
    "(",
    "h",
    "t",
    "t",
    "p",
    "s",
    ":",
    "/",
    "/",
    "g",
    "i",
    "t",
    "h",
    "u",
    "b",
    ".",
    "c",
    "o",
    "m",
    "/",
    "a",
    "l",
    "l",
    "e",
    "n",
    "a",
    "i",
    "/",
    "s",
    "c",
    "r",
    "u",
    "p",
    "l",
    "e",
    "s",
    ")",
    ":",
    " ",
    "3",
    "2",
    "K",
    " ",
    "r",
    "e",
    "a",
    "l",
    "-",
    "l",
    "i",
    "f",
    "e",
    " ",
    "a",
    "n",
    "e",
    "c",
    "d",
    "o",
    "t",
    "e",
    "s",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "e",
    "t",
    "h",
    "i",
    "c",
    "a",
    "l",
    " ",
    "j",
    "u",
    "d",
    "g",
    "m",
    "e",
    "n",
    "t",
    "s",
    "\n",
    "-",
    " ",
    "[",
    "E",
    "t",
    "h",
    "i",
    "c",
    "s",
    "S",
    "u",
    "i",
    "t",
    "e",
    "]",
    "(",
    "h",
    "t",
    "t",
    "p",
    "s",
    ":",
    "/",
    "/",
    "g",
    "i",
    "t",
    "h",
    "u",
    "b",
    ".",
    "c",
    "o",
    "m",
    "/",
    "l",
    "l",
    "m",
    "-",
    "e",
    "t",
    "h",
    "i",
    "c",
    "s",
    "/",
    "e",
    "t",
    "h",
    "i",
    "c",
    "s",
    "s",
    "u",
    "i",
    "t",
    "e",
    ")",
    ":",
    " ",
    "2",
    "0",
    "K",
    " ",
    "c",
    "o",
    "m",
    "p",
    "l",
    "e",
    "x",
    " ",
    "c",
    "o",
    "n",
    "t",
    "e",
    "x",
    "t",
    "u",
    "a",
    "l",
    "i",
    "z",
    "e",
    "d",
    " ",
    "m",
    "o",
    "r",
    "a",
    "l",
    " ",
    "s",
    "i",
    "t",
    "u",
    "a",
    "t",
    "i",
    "o",
    "n",
    "s",
    "\n",
    "\n",
    "*",
    "*",
    "C",
    "o",
    "r",
    "p",
    "u",
    "s",
    " ",
    "C",
    "o",
    "v",
    "e",
    "r",
    "a",
    "g",
    "e",
    ":",
    "*",
    "*",
    "\n",
    "\n",
    "*",
    "A",
    "n",
    "c",
    "i",
    "e",
    "n",
    "t",
    " ",
    "&",
    " ",
    "C",
    "l",
    "a",
    "s",
    "s",
    "i",
    "c",
    "a",
    "l",
    ":",
    "*",
    "\n",
    "-",
    " ",
    "H",
    "e",
    "b",
    "r",
    "e",
    "w",
    " ",
    "(",
    "B",
    "i",
    "b",
    "l",
    "i",
    "c",
    "a",
    "l",
    ",",
    " ",
    "M",
    "i",
    "s",
    "h",
    "n",
    "a",
    "i",
    "c",
    ",",
    " ",
    "T",
    "a",
    "l",
    "m",
    "u",
    "d",
    "i",
    "c",
    ")",
    " ",
    "-",
    " ",
    "S",
    "e",
    "f",
    "a",
    "r",
    "i",
    "a",
    " ",
    "(",
    "8",
    "8",
    " ",
    "t",
    "e",
    "x",
    "t",
    "s",
    ")",
    "\n",
    "-",
    " ",
    "A",
    "r",
    "a",
    "m",
    "a",
    "i",
    "c",
    " ",
    "(",
    "T",
    "a",
    "l",
    "m",
    "u",
    "d",
    " ",
    "B",
    "a",
    "v",
    "l",
    "i",
    ")",
    " ",
    "-",
    " ",
    "S",
    "e",
    "f",
    "a",
    "r",
    "i",
    "a",
    "\n",
    "-",
    " ",
    "C",
    "l",
    "a",
    "s",
    "s",
    "i",
    "c",
    "a",
    "l",
    " ",
    "C",
    "h",
    "i",
    "n",
    "e",
    "s",
    "e",
    " ",
    "(",
    "C",
    "o",
    "n",
    "f",
    "u",
    "c",
    "i",
    "a",
    "n",
    ",",
    " ",
    "D",
    "a",
    "o",
    "i",
    "s",
    "t",
    ",",
    " ",
    "L",
    "e",
    "g",
    "a",
    "l",
    "i",
    "s",
    "t",
    ",",
    " ",
    "B",
    "u",
    "d",
    "d",
    "h",
    "i",
    "s",
    "t",
    ")",
    " ",
    "-",
    " ",
    "c",
    "t",
    "e",
    "x",
    "t",
    ".",
    "o",
    "r",
    "g",
    ",",
    " ",
    "C",
    "B",
    "E",
    "T",
    "A",
    "\n",
    "-",
    " ",
    "A",
    "r",
    "a",
    "b",
    "i",
    "c",
    " ",
    "(",
    "Q",
    "u",
    "r",
    "a",
    "n",
    "i",
    "c",
    ")",
    " ",
    "-",
    " ",
    "T",
    "a",
    "n",
    "z",
    "i",
    "l",
    "\n",
    "-",
    " ",
    "S",
    "a",
    "n",
    "s",
    "k",
    "r",
    "i",
    "t",
    " ",
    "(",
    "D",
    "h",
    "a",
    "r",
    "m",
    "a",
    "s",
    "h",
    "a",
    "s",
    "t",
    "r",
    "a",
    ",",
    " ",
    "U",
    "p",
    "a",
    "n",
    "i",
    "s",
    "h",
    "a",
    "d",
    "s",
    ",",
    " ",
    "I",
    "t",
    "i",
    "h",
    "a",
    "s",
    "a",
    ")",
    " ",
    "-",
    " ",
    "G",
    "i",
    "t",
    "H",
    "u",
    "b",
    "\n",
    "-",
    " ",
    "P",
    "a",
    "l",
    "i",
    " ",
    "(",
    "T",
    "h",
    "e",
    "r",
    "a",
    "v",
    "a",
    "d",
    "a",
    " ",
    "C",
    "a",
    "n",
    "o",
    "n",
    ")",
    " ",
    "-",
    " ",
    "S",
    "u",
    "t",
    "t",
    "a",
    "C",
    "e",
    "n",
    "t",
    "r",
    "a",
    "l",
    "\n",
    "-",
    " ",
    "G",
    "r",
    "e",
    "e",
    "k",
    " ",
    "&",
    " ",
    "L",
    "a",
    "t",
    "i",
    "n",
    " ",
    "(",
    "S",
    "t",
    "o",
    "i",
    "c",
    ",",
    " ",
    "P",
    "l",
    "a",
    "t",
    "o",
    "n",
    "i",
    "c",
    ",",
    " ",
    "A",
    "r",
    "i",
    "s",
    "t",
    "o",
    "t",
    "e",
    "l",
    "i",
    "a",
    "n",
    ")",
    " ",
    "-",
    " ",
    "P",
    "e",
    "r",
    "s",
    "e",
    "u",
    "s",
    " ",
    "D",
    "i",
    "g",
    "i",
    "t",
    "a",
    "l",
    " ",
    "L",
    "i",
    "b",
    "r",
    "a",
    "r",
    "y",
    "\n",
    "\n",
    "*",
    "W",
    "e",
    "s",
    "t",
    "e",
    "r",
    "n",
    " ",
    "P",
    "h",
    "i",
    "l",
    "o",
    "s",
    "o",
    "p",
    "h",
    "y",
    " ",
    "&",
    " ",
    "R",
    "e",
    "l",
    "i",
    "g",
    "i",
    "o",
    "n",
    ":",
    "*",
    "\n",
    "-",
    " ",
    "E",
    "n",
    "g",
    "l",
    "i",
    "s",
    "h",
    ":",
    " ",
    "K",
    "a",
    "n",
    "t",
    ",",
    " ",
    "M",
    "i",
    "l",
    "l",
    ",",
    " ",
    "S",
    "p",
    "i",
    "n",
    "o",
    "z",
    "a",
    ",",
    " ",
    "A",
    "r",
    "i",
    "s",
    "t",
    "o",
    "t",
    "l",
    "e",
    ",",
    " ",
    "P",
    "l",
    "a",
    "t",
    "o",
    ",",
    " ",
    "E",
    "p",
    "i",
    "c",
    "t",
    "e",
    "t",
    "u",
    "s",
    ",",
    " ",
    "M",
    "a",
    "r",
    "c",
    "u",
    "s",
    " ",
    "A",
    "u",
    "r",
    "e",
    "l",
    "i",
    "u",
    "s",
    " ",
    "(",
    "G",
    "u",
    "t",
    "e",
    "n",
    "b",
    "e",
    "r",
    "g",
    ")",
    "\n",
    "-",
    " ",
    "B",
    "i",
    "b",
    "l",
    "e",
    " ",
    "K",
    "J",
    "V",
    ":",
    " ",
    "C",
    "o",
    "m",
    "p",
    "l",
    "e",
    "t",
    "e",
    " ",
    "(",
    "8",
    "0",
    " ",
    "b",
    "o",
    "o",
    "k",
    "s",
    " ",
    "i",
    "n",
    "c",
    "l",
    ".",
    " ",
    "A",
    "p",
    "o",
    "c",
    "r",
    "y",
    "p",
    "h",
    "a",
    ")",
    "\n",
    "-",
    " ",
    "L",
    "u",
    "t",
    "h",
    "e",
    "r",
    "'",
    "s",
    " ",
    "C",
    "a",
    "t",
    "e",
    "c",
    "h",
    "i",
    "s",
    "m",
    "s",
    " ",
    "(",
    "S",
    "m",
    "a",
    "l",
    "l",
    " ",
    "&",
    " ",
    "L",
    "a",
    "r",
    "g",
    "e",
    ")",
    "\n",
    "-",
    " ",
    "F",
    "r",
    "e",
    "n",
    "c",
    "h",
    ":",
    " ",
    "M",
    "o",
    "n",
    "t",
    "a",
    "i",
    "g",
    "n",
    "e",
    ",",
    " ",
    "V",
    "o",
    "l",
    "t",
    "a",
    "i",
    "r",
    "e",
    ",",
    " ",
    "R",
    "o",
    "u",
    "s",
    "s",
    "e",
    "a",
    "u",
    " ",
    "(",
    "G",
    "u",
    "t",
    "e",
    "n",
    "b",
    "e",
    "r",
    "g",
    ")",
    "\n",
    "-",
    " ",
    "S",
    "p",
    "a",
    "n",
    "i",
    "s",
    "h",
    ":",
    " ",
    "C",
    "e",
    "r",
    "v",
    "a",
    "n",
    "t",
    "e",
    "s",
    " ",
    "D",
    "o",
    "n",
    " ",
    "Q",
    "u",
    "i",
    "x",
    "o",
    "t",
    "e",
    " ",
    "(",
    "G",
    "u",
    "t",
    "e",
    "n",
    "b",
    "e",
    "r",
    "g",
    ")",
    "\n",
    "-",
    " ",
    "I",
    "t",
    "a",
    "l",
    "i",
    "a",
    "n",
    ":",
    " ",
    "M",
    "a",
    "c",
    "h",
    "i",
    "a",
    "v",
    "e",
    "l",
    "l",
    "i",
    ",",
    " ",
    "D",
    "a",
    "n",
    "t",
    "e",
    " ",
    "(",
    "G",
    "u",
    "t",
    "e",
    "n",
    "b",
    "e",
    "r",
    "g",
    ")",
    "\n",
    "\n",
    "*",
    "M",
    "o",
    "d",
    "e",
    "r",
    "n",
    " ",
    "E",
    "t",
    "h",
    "i",
    "c",
    "s",
    ":",
    "*",
    "\n",
    "-",
    " ",
    "D",
    "e",
    "a",
    "r",
    " ",
    "A",
    "b",
    "b",
    "y",
    " ",
    "a",
    "d",
    "v",
    "i",
    "c",
    "e",
    " ",
    "c",
    "o",
    "l",
    "u",
    "m",
    "n",
    "s",
    " ",
    "(",
    "6",
    "8",
    "K",
    " ",
    "l",
    "e",
    "t",
    "t",
    "e",
    "r",
    "s",
    ")",
    "\n",
    "-",
    " ",
    "h",
    "e",
    "n",
    "d",
    "r",
    "y",
    "c",
    "k",
    "s",
    "/",
    "e",
    "t",
    "h",
    "i",
    "c",
    "s",
    " ",
    "d",
    "a",
    "t",
    "a",
    "s",
    "e",
    "t",
    " ",
    "(",
    "1",
    "3",
    "4",
    "K",
    " ",
    "s",
    "c",
    "e",
    "n",
    "a",
    "r",
    "i",
    "o",
    "s",
    ")",
    "\n",
    "-",
    " ",
    "F",
    "o",
    "l",
    "k",
    "l",
    "o",
    "r",
    "e",
    " ",
    "&",
    " ",
    "N",
    "a",
    "t",
    "i",
    "v",
    "e",
    " ",
    "A",
    "m",
    "e",
    "r",
    "i",
    "c",
    "a",
    "n",
    " ",
    "t",
    "r",
    "a",
    "d",
    "i",
    "t",
    "i",
    "o",
    "n",
    "s",
    " ",
    "(",
    "A",
    "s",
    "h",
    "l",
    "i",
    "m",
    "a",
    "n",
    " ",
    "F",
    "o",
    "l",
    "k",
    "t",
    "e",
    "x",
    "t",
    "s",
    ")"
   ],
   "id": "cell_0"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7a39f88f-eef6-406a-919c-9569f6edabad"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "BIP v10.9 - ENVIRONMENT DETECTION\n",
      "============================================================\n",
      "\n",
      "Environment: COLAB\n",
      "GPU Quota:   Free: T4 ~12h/day, Pro: L4/A100\n",
      "Storage:     /content/drive/MyDrive\n",
      "Data Dir:    /content\n",
      "\n",
      "------------------------------------------------------------\n",
      "ENVIRONMENT TIPS:\n",
      "  Tip: Use GPU runtime (Runtime -> Change runtime type -> T4 GPU)\n",
      "  Tip: Colab Pro gives L4 GPU access (~2x faster than T4)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Installing dependencies...\n",
      "\n",
      "============================================================\n",
      "GPU DETECTION & RESOURCE ALLOCATION\n",
      "============================================================\n",
      "\n",
      "Detected Hardware:\n",
      "  GPU:  NVIDIA L4\n",
      "  VRAM: 23.8 GB\n",
      "  RAM:  56.9 GB\n",
      "  Backbone: LaBSE -> batch size 256\n",
      "\n",
      "------------------------------------------------------------\n",
      "OPTIMAL SETTINGS:\n",
      "------------------------------------------------------------\n",
      "  Environment:     COLAB\n",
      "  GPU Tier:        L4/A100\n",
      "  Backbone:        LaBSE\n",
      "  Batch size:      256\n",
      "  Max per lang:    50,000\n",
      "  DataLoader workers: 4\n",
      "  Learning rate:   2.00e-05\n",
      "\n",
      "============================================================\n",
      "PERSISTENT STORAGE SETUP\n",
      "============================================================\n",
      "Mounted at /content/drive\n",
      "Google Drive mounted successfully\n",
      "\n",
      "------------------------------------------------------------\n",
      "STORAGE STATUS:\n",
      "------------------------------------------------------------\n",
      "  Folder: /content/drive/MyDrive/BIP_v10.11\n",
      "  Folder existed: True\n",
      "  Files found: 8\n",
      "    - Sefaria-Export_RAW\n",
      "    - all_splits.json\n",
      "    - best_ancient_to_modern.pt\n",
      "    - best_hebrew_to_others.pt\n",
      "    - best_mixed_baseline.pt\n",
      "    - best_semitic_to_non_semitic.pt\n",
      "    - corpus_cache\n",
      "    - passages.jsonl\n",
      "  Pre-processed data available: False\n",
      "\n",
      "============================================================\n",
      "DATA LOADING STRATEGY: Update missing\n",
      "------------------------------------------------------------\n",
      "  -> Will download missing data, use cached where available\n",
      "     Sefaria: will download\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "SETUP COMPLETE\n",
      "============================================================\n",
      "  Environment: COLAB\n",
      "  GPU:         NVIDIA L4 (L4/A100)\n",
      "  Storage:     /content/drive/MyDrive/BIP_v10.11\n",
      "  Ready to run: Cell 2 (Imports)\n"
     ]
    }
   ],
   "source": [
    "# @title 1. Configuration & Setup { display-mode: \"form\" }\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ### Version\n",
    "BIP_VERSION = \"10.14\"  # @param {type:\"string\"}\n",
    "# @markdown Central version number - change to update all references\n",
    "# @markdown ## Data Source Configuration\n",
    "\n",
    "DATA_MODE = \"Update missing\"  # @param [\"Refresh all\", \"Update missing\", \"Cache only\"]\n",
    "# @markdown - **Refresh all**: Re-download everything from source (slow, ~2hrs)\n",
    "# @markdown - **Update missing**: Use cache, download only what's missing (recommended)\n",
    "# @markdown - **Cache only**: Use only cached data, fail if missing\n",
    "\n",
    "DRIVE_FOLDER = f\"BIP_v{BIP_VERSION}\"  # @param {type:\"string\"}\n",
    "# @markdown **Folder name for persistent storage** (edit above to change)\n",
    "\n",
    "# Derive flags from DATA_MODE\n",
    "USE_DRIVE_DATA = True  # Always use Drive for caching\n",
    "REFRESH_DATA_FROM_SOURCE = DATA_MODE == \"Refresh all\"\n",
    "CACHE_ONLY = DATA_MODE == \"Cache only\"\n",
    "# @markdown ---\n",
    "# @markdown ## Model Backbone\n",
    "BACKBONE = \"LaBSE\"  # @param [\"MiniLM\", \"LaBSE\", \"XLM-R-base\", \"XLM-R-large\"]\n",
    "# @markdown - **MiniLM**: Fast, 118M params, good baseline\n",
    "# @markdown - **LaBSE**: Best cross-lingual alignment, 471M params (recommended)\n",
    "# @markdown - **XLM-R-base**: Strong multilingual, 270M params\n",
    "# @markdown - **XLM-R-large**: Strongest representations, 550M params\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ## Output Options\n",
    "CREATE_DOWNLOAD_ZIP = False  # @param {type:\"boolean\"}\n",
    "# @markdown - **CREATE_DOWNLOAD_ZIP**: Create and download a zip file of results (optional)\n",
    "# @markdown - Results are always persisted to Google Drive regardless of this setting\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ## Training Hyperparameters\n",
    "FREEZE_ENCODER = True  # @param {type:\"boolean\"}\n",
    "# @markdown - **FREEZE_ENCODER**: Only train probe head (recommended for stability)\n",
    "# @markdown - Unfrozen: Fine-tune entire encoder (471M params, risk of catastrophic forgetting)\n",
    "\n",
    "LEARNING_RATE = 1e-5  # @param {type:\"number\"}\n",
    "# @markdown - **Frozen encoder**: 1e-4 to 1e-3 works well\n",
    "# @markdown - **Unfrozen encoder**: Use 1e-5 to 5e-6 (lower = more stable)\n",
    "\n",
    "WARMUP_RATIO = 0.1  # @param {type:\"number\"}\n",
    "# @markdown - Fraction of training for learning rate warmup (0.0 to 0.2)\n",
    "\n",
    "GRADIENT_CLIP = 1.0  # @param {type:\"number\"}\n",
    "# @markdown - Max gradient norm (prevents exploding gradients, 0 = disabled)\n",
    "\n",
    "NUM_EPOCHS = 10  # @param {type:\"integer\"}\n",
    "# @markdown - Number of training epochs per split\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 3  # @param {type:\"integer\"}\n",
    "# @markdown - Stop if no improvement for N epochs (0 = disabled)\n",
    "\n",
    "ADV_WARMUP_EPOCHS = 3  # @param {type:\"integer\"}\n",
    "# @markdown - Epochs to ramp adversarial strength (longer = more stable)\n",
    "\n",
    "ADV_MAX_LAMBDA = 0.4  # @param {type:\"number\"}\n",
    "# @markdown - Max adversarial weight (lower = less aggressive, try 0.3-0.7)\n",
    "\n",
    "# Backbone configurations\n",
    "BACKBONE_CONFIGS = {\n",
    "    \"MiniLM\": {\n",
    "        \"model_name\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        \"hidden_size\": 384,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 4096,\n",
    "            \"T4\": 512,\n",
    "            \"2xT4\": 1024,\n",
    "            \"SMALL\": 128,\n",
    "            \"MINIMAL/CPU\": 64,\n",
    "        },\n",
    "    },\n",
    "    \"LaBSE\": {\n",
    "        \"model_name\": \"sentence-transformers/LaBSE\",\n",
    "        \"hidden_size\": 768,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 4096,  # Increased: only using 2.1/22.5GB at 256\n",
    "            \"T4\": 512,\n",
    "            \"2xT4\": 1024,\n",
    "            \"SMALL\": 128,\n",
    "            \"MINIMAL/CPU\": 64,\n",
    "        },\n",
    "    },\n",
    "    \"XLM-R-base\": {\n",
    "        \"model_name\": \"xlm-roberta-base\",\n",
    "        \"hidden_size\": 768,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 2048,  # Increased for better GPU utilization\n",
    "            \"T4\": 256,\n",
    "            \"2xT4\": 512,\n",
    "            \"SMALL\": 128,\n",
    "            \"MINIMAL/CPU\": 64,\n",
    "        },\n",
    "    },\n",
    "    \"XLM-R-large\": {\n",
    "        \"model_name\": \"xlm-roberta-large\",\n",
    "        \"hidden_size\": 1024,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 256,\n",
    "            \"T4\": 64,\n",
    "            \"2xT4\": 128,\n",
    "            \"SMALL\": 32,\n",
    "            \"MINIMAL/CPU\": 16,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "BACKBONE_CONFIG = BACKBONE_CONFIGS[BACKBONE]\n",
    "MODEL_NAME = BACKBONE_CONFIG[\"model_name\"]\n",
    "BACKBONE_HIDDEN = BACKBONE_CONFIG[\"hidden_size\"]\n",
    "\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ## Run Setup\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "EXPERIMENT_START = time.time()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BIP v10.9 - ENVIRONMENT DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===== ENVIRONMENT DETECTION =====\n",
    "# Detect which cloud platform we're running on\n",
    "\n",
    "ENV_NAME = \"UNKNOWN\"\n",
    "ENV_GPU_QUOTA = \"Unknown\"\n",
    "PERSISTENT_STORAGE = None\n",
    "DATA_DIR = \"/content\"  # Default\n",
    "\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"Detect cloud environment and return (name, gpu_quota, storage_path, data_dir)\"\"\"\n",
    "\n",
    "    # 1. Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "\n",
    "        return (\"COLAB\", \"Free: T4 ~12h/day, Pro: L4/A100\", \"/content/drive/MyDrive\", \"/content\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    # 2. Kaggle Kernels\n",
    "    if os.path.exists(\"/kaggle\"):\n",
    "        # Kaggle has /kaggle/input for datasets, /kaggle/working for output\n",
    "        return (\"KAGGLE\", \"Free: 2xT4 30h/week, TPU 30h/week\", \"/kaggle/working\", \"/kaggle/working\")\n",
    "\n",
    "    # 3. Lightning.ai Studios\n",
    "    if os.environ.get(\"LIGHTNING_CLOUDSPACE_HOST\") or os.path.exists(\"/teamspace\"):\n",
    "        # Lightning.ai has /teamspace/studios for persistent storage\n",
    "        return (\n",
    "            \"LIGHTNING_AI\",\n",
    "            \"Free: 22h/month GPU, Pro: A10G/H100\",\n",
    "            \"/teamspace/studios\",\n",
    "            \"/teamspace/studios\",\n",
    "        )\n",
    "\n",
    "    # 4. Paperspace Gradient\n",
    "    if os.environ.get(\"PAPERSPACE_NOTEBOOK_REPO_ID\") or os.path.exists(\"/notebooks\"):\n",
    "        return (\"PAPERSPACE\", \"Free: M4000 6h, Pro: A100/H100\", \"/storage\", \"/notebooks\")\n",
    "\n",
    "    # 5. Saturn Cloud\n",
    "    if os.environ.get(\"SATURN_RESOURCE_ID\") or \"saturn\" in os.environ.get(\"HOSTNAME\", \"\").lower():\n",
    "        return (\n",
    "            \"SATURN_CLOUD\",\n",
    "            \"Free: T4 10h/month, Pro: A10G/A100\",\n",
    "            \"/home/jovyan/workspace\",\n",
    "            \"/home/jovyan\",\n",
    "        )\n",
    "\n",
    "    # 6. HuggingFace Spaces\n",
    "    if os.environ.get(\"SPACE_ID\") or os.environ.get(\"HF_SPACE_ID\"):\n",
    "        return (\n",
    "            \"HUGGINGFACE_SPACES\",\n",
    "            \"Free: CPU only, ZeroGPU: A10G/A100 quota\",\n",
    "            \"/data\",\n",
    "            \"/home/user/app\",\n",
    "        )\n",
    "\n",
    "    # 7. AWS SageMaker Studio Lab\n",
    "    if os.path.exists(\"/home/studio-lab-user\"):\n",
    "        return (\n",
    "            \"SAGEMAKER_STUDIO_LAB\",\n",
    "            \"Free: T4 4h/session, 24h max/day\",\n",
    "            \"/home/studio-lab-user\",\n",
    "            \"/home/studio-lab-user\",\n",
    "        )\n",
    "\n",
    "    # 8. Deepnote\n",
    "    if os.environ.get(\"DEEPNOTE_PROJECT_ID\"):\n",
    "        return (\"DEEPNOTE\", \"Free: CPU, Pro: T4/A10G\", \"/work\", \"/work\")\n",
    "\n",
    "    # 9. Local/Unknown\n",
    "    return (\"LOCAL\", \"Depends on local hardware\", os.getcwd(), os.getcwd())\n",
    "\n",
    "\n",
    "ENV_NAME, ENV_GPU_QUOTA, PERSISTENT_STORAGE, DATA_DIR = detect_environment()\n",
    "\n",
    "print(f\"\\nEnvironment: {ENV_NAME}\")\n",
    "print(f\"GPU Quota:   {ENV_GPU_QUOTA}\")\n",
    "print(f\"Storage:     {PERSISTENT_STORAGE}\")\n",
    "print(f\"Data Dir:    {DATA_DIR}\")\n",
    "\n",
    "# Environment-specific setup\n",
    "ENV_TIPS = {\n",
    "    \"COLAB\": [\n",
    "        \"Tip: Use GPU runtime (Runtime -> Change runtime type -> T4 GPU)\",\n",
    "        \"Tip: Colab Pro gives L4 GPU access (~2x faster than T4)\",\n",
    "    ],\n",
    "    \"KAGGLE\": [\n",
    "        \"Tip: Enable GPU (Settings -> Accelerator -> GPU T4 x2)\",\n",
    "        \"Tip: 30h/week GPU quota resets every Saturday\",\n",
    "        \"Tip: Upload data as a Kaggle Dataset for persistence\",\n",
    "    ],\n",
    "    \"LIGHTNING_AI\": [\n",
    "        \"Tip: Select GPU studio (A10G recommended for this workload)\",\n",
    "        \"Tip: /teamspace/studios persists across sessions\",\n",
    "    ],\n",
    "    \"PAPERSPACE\": [\n",
    "        \"Tip: Use /storage for persistent data across runs\",\n",
    "        \"Tip: Free tier has 6h/month GPU limit\",\n",
    "    ],\n",
    "    \"SATURN_CLOUD\": [\n",
    "        \"Tip: Start a T4 instance from the Resources tab\",\n",
    "        \"Tip: 10h/month free GPU quota\",\n",
    "    ],\n",
    "    \"HUGGINGFACE_SPACES\": [\n",
    "        \"Tip: ZeroGPU provides A10G/A100 access with quota system\",\n",
    "        \"Tip: Use Gradio/Streamlit for interactive demos\",\n",
    "    ],\n",
    "    \"SAGEMAKER_STUDIO_LAB\": [\n",
    "        \"Tip: Request GPU runtime from the launcher\",\n",
    "        \"Tip: Sessions timeout after 4h, max 24h/day\",\n",
    "    ],\n",
    "    \"LOCAL\": [\"Tip: Running locally - ensure CUDA is installed for GPU support\"],\n",
    "}\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(\"ENVIRONMENT TIPS:\")\n",
    "for tip in ENV_TIPS.get(ENV_NAME, [\"No specific tips for this environment\"]):\n",
    "    print(f\"  {tip}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ===== INSTALL DEPENDENCIES =====\n",
    "import subprocess\n",
    "\n",
    "print(\"\\nInstalling dependencies...\")\n",
    "for pkg in [\n",
    "    \"transformers\",\n",
    "    \"sentence-transformers\",\n",
    "    \"pandas\",\n",
    "    \"tqdm\",\n",
    "    \"scikit-learn\",\n",
    "    \"pyyaml\",\n",
    "    \"psutil\",\n",
    "    \"datasets\",\n",
    "]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GPU DETECTION & RESOURCE ALLOCATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Detect hardware\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    GPU_COUNT = torch.cuda.device_count()\n",
    "else:\n",
    "    GPU_NAME = \"CPU\"\n",
    "    VRAM_GB = 0\n",
    "    GPU_COUNT = 0\n",
    "\n",
    "RAM_GB = psutil.virtual_memory().total / 1e9\n",
    "\n",
    "print(f\"\\nDetected Hardware:\")\n",
    "print(f\"  GPU:  {GPU_NAME}\" + (f\" (x{GPU_COUNT})\" if GPU_COUNT > 1 else \"\"))\n",
    "print(\n",
    "    f\"  VRAM: {VRAM_GB:.1f} GB\" + (f\" (total: {VRAM_GB*GPU_COUNT:.1f} GB)\" if GPU_COUNT > 1 else \"\")\n",
    ")\n",
    "print(f\"  RAM:  {RAM_GB:.1f} GB\")\n",
    "\n",
    "# Set optimal parameters based on hardware\n",
    "if VRAM_GB >= 22:  # L4 (24GB) or A100\n",
    "    GPU_TIER = \"L4/A100\"\n",
    "elif VRAM_GB >= 14:  # T4 (16GB)\n",
    "    GPU_TIER = \"T4\"\n",
    "elif VRAM_GB >= 10:\n",
    "    GPU_TIER = \"SMALL\"\n",
    "else:\n",
    "    GPU_TIER = \"MINIMAL/CPU\"\n",
    "\n",
    "# Kaggle with 2xT4 can use larger batch\n",
    "if ENV_NAME == \"KAGGLE\" and GPU_COUNT >= 2:\n",
    "    GPU_TIER = \"2xT4\"\n",
    "    print(f\"  ** Kaggle 2xT4 detected **\")\n",
    "\n",
    "# Get backbone-specific batch size\n",
    "BATCH_SIZE = BACKBONE_CONFIG[\"recommended_batch\"].get(GPU_TIER, 64)\n",
    "print(f\"  Backbone: {BACKBONE} -> batch size {BATCH_SIZE}\")\n",
    "\n",
    "MAX_PER_LANG = 50000  # Language sample limit\n",
    "CPU_CORES = os.cpu_count() or 2\n",
    "NUM_WORKERS = min(4, CPU_CORES - 1) if RAM_GB >= 24 and VRAM_GB >= 14 else 0\n",
    "MAX_TEST_SAMPLES = 20000\n",
    "# Use LEARNING_RATE from UI, or scale with batch size\n",
    "if LEARNING_RATE and LEARNING_RATE != 1e-5:  # 1e-5 is the default\n",
    "    LR = LEARNING_RATE\n",
    "else:\n",
    "    LR = min(1e-4, 2e-5 * (BATCH_SIZE / 256) ** 0.5)  # Sqrt scaling, capped at 1e-4\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(f\"OPTIMAL SETTINGS:\")\n",
    "print(f\"-\" * 60)\n",
    "print(f\"  Environment:     {ENV_NAME}\")\n",
    "print(f\"  GPU Tier:        {GPU_TIER}\")\n",
    "print(f\"  Backbone:        {BACKBONE}\")\n",
    "print(f\"  Batch size:      {BATCH_SIZE}\")\n",
    "print(f\"  Max per lang:    {MAX_PER_LANG:,}\")\n",
    "print(f\"  DataLoader workers: {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate:   {LR:.2e}\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "scaler = torch.amp.GradScaler(\"cuda\") if USE_AMP else None\n",
    "\n",
    "# ===== PERSISTENT STORAGE SETUP =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERSISTENT STORAGE SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "SAVE_DIR = None\n",
    "DRIVE_HAS_DATA = False\n",
    "DRIVE_FILES = set()  # Use set for O(1) lookup\n",
    "\n",
    "if ENV_NAME == \"COLAB\":\n",
    "    # Google Colab - mount Drive\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "\n",
    "        DRIVE_MOUNT_PATH = \"/content/drive\"\n",
    "\n",
    "        if os.path.exists(f\"{DRIVE_MOUNT_PATH}/MyDrive\"):\n",
    "            print(\"Google Drive already mounted\")\n",
    "        else:\n",
    "            try:\n",
    "                drive.mount(DRIVE_MOUNT_PATH, force_remount=False)\n",
    "                print(\"Google Drive mounted successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Drive mount issue: {e}\")\n",
    "                try:\n",
    "                    drive.mount(DRIVE_MOUNT_PATH, force_remount=True)\n",
    "                    print(\"Google Drive mounted (force remount)\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"WARNING: Could not mount Drive: {e2}\")\n",
    "                    print(\"Falling back to local storage\")\n",
    "                    PERSISTENT_STORAGE = DATA_DIR\n",
    "\n",
    "        SAVE_DIR = f\"{DRIVE_MOUNT_PATH}/MyDrive/{DRIVE_FOLDER}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Colab Drive setup failed: {e}\")\n",
    "        SAVE_DIR = f\"{DATA_DIR}/{DRIVE_FOLDER}\"\n",
    "\n",
    "elif ENV_NAME == \"KAGGLE\":\n",
    "    # Kaggle - use working directory\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using Kaggle working directory: {SAVE_DIR}\")\n",
    "    print(\"Note: Data persists until kernel is reset\")\n",
    "    # Check for uploaded datasets\n",
    "    if os.path.exists(\"/kaggle/input\"):\n",
    "        datasets = os.listdir(\"/kaggle/input\")\n",
    "        if datasets:\n",
    "            print(f\"Available datasets: {datasets[:5]}\")\n",
    "\n",
    "elif ENV_NAME == \"LIGHTNING_AI\":\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using Lightning.ai studio storage: {SAVE_DIR}\")\n",
    "\n",
    "elif ENV_NAME == \"PAPERSPACE\":\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using Paperspace /storage: {SAVE_DIR}\")\n",
    "\n",
    "elif ENV_NAME == \"HUGGINGFACE_SPACES\":\n",
    "    # HF Spaces has limited persistent storage\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using HuggingFace Spaces storage: {SAVE_DIR}\")\n",
    "    print(\"Warning: HF Spaces storage is limited\")\n",
    "\n",
    "else:\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using local storage: {SAVE_DIR}\")\n",
    "\n",
    "# Check if folder exists BEFORE creating it\n",
    "folder_existed = os.path.exists(SAVE_DIR)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Check what's available in storage - use BOTH listdir AND direct exists checks\n",
    "# (Google Drive can have sync issues where listdir misses files)\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    DRIVE_FILES = set(os.listdir(SAVE_DIR))  # O(1) membership test\n",
    "\n",
    "    # Direct existence checks for key files (bypasses listdir caching issues)\n",
    "    key_files = [\"passages.jsonl\", \"bonds.jsonl\", \"dear_abby.csv\", \"all_splits.json\"]\n",
    "    for kf in key_files:\n",
    "        kf_path = os.path.join(SAVE_DIR, kf)\n",
    "        if os.path.exists(kf_path) and kf not in DRIVE_FILES:\n",
    "            print(f\"  [Drive sync fix] Found {kf} via os.path.exists() but not listdir()\")\n",
    "            DRIVE_FILES.add(kf)\n",
    "\n",
    "    DRIVE_HAS_DATA = \"passages.jsonl\" in DRIVE_FILES and \"bonds.jsonl\" in DRIVE_FILES\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(f\"STORAGE STATUS:\")\n",
    "print(f\"-\" * 60)\n",
    "print(f\"  Folder: {SAVE_DIR}\")\n",
    "print(f\"  Folder existed: {folder_existed}\")\n",
    "print(f\"  Files found: {len(DRIVE_FILES)}\")\n",
    "\n",
    "# If folder was empty/new, show what folders exist in parent to help debug\n",
    "if not DRIVE_FILES and ENV_NAME == \"COLAB\":\n",
    "    parent = os.path.dirname(SAVE_DIR)  # e.g., /content/drive/MyDrive\n",
    "    if os.path.exists(parent):\n",
    "        siblings = [d for d in os.listdir(parent) if \"bip\" in d.lower() or \"BIP\" in d]\n",
    "        if siblings:\n",
    "            print(f\"  ** Similar folders in {parent}: {siblings}\")\n",
    "        else:\n",
    "            print(f\"  ** No BIP folders found in {parent}\")\n",
    "if DRIVE_FILES:\n",
    "    for f in sorted(DRIVE_FILES)[:10]:  # sorted() converts to list for slicing\n",
    "        print(f\"    - {f}\")\n",
    "    if len(DRIVE_FILES) > 10:\n",
    "        print(f\"    ... and {len(DRIVE_FILES)-10} more\")\n",
    "print(f\"  Pre-processed data available: {DRIVE_HAS_DATA}\")\n",
    "\n",
    "# Decide data loading strategy\n",
    "LOAD_FROM_DRIVE = USE_DRIVE_DATA and DRIVE_HAS_DATA and not REFRESH_DATA_FROM_SOURCE\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"DATA LOADING STRATEGY: {DATA_MODE}\")\n",
    "print(\"-\" * 60)\n",
    "if DATA_MODE == \"Refresh all\":\n",
    "    print(f\"  -> Will re-download ALL data from online sources\")\n",
    "    print(f\"     (This takes ~2 hours, use 'Update missing' to save time)\")\n",
    "elif DATA_MODE == \"Cache only\":\n",
    "    if LOAD_FROM_DRIVE:\n",
    "        print(f\"  -> Using cached data only (no downloads)\")\n",
    "    else:\n",
    "        print(f\"  -> ERROR: Cache-only mode but no cached data found!\")\n",
    "        print(f\"     Change DATA_MODE to 'Update missing'\")\n",
    "else:  # Update missing (default)\n",
    "    if LOAD_FROM_DRIVE:\n",
    "        print(f\"  -> Using cached processed data from Drive\")\n",
    "        print(f\"     (v10.9 corpora will be added if missing)\")\n",
    "    else:\n",
    "        print(f\"  -> Will download missing data, use cached where available\")\n",
    "        print(\n",
    "            f\"     Sefaria: {'cached' if os.path.exists(f'{SAVE_DIR}/Sefaria-Export-json.tar.gz') else 'will download'}\"\n",
    "        )\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create local directories\n",
    "for d in [\"data/processed\", \"data/splits\", \"data/raw\", \"models/checkpoints\", \"results\"]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"SETUP COMPLETE\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"  Environment: {ENV_NAME}\")\n",
    "print(f\"  GPU:         {GPU_NAME} ({GPU_TIER})\")\n",
    "print(f\"  Storage:     {SAVE_DIR}\")\n",
    "print(f\"  Ready to run: Cell 2 (Imports)\")\n",
    ""
   ],
   "id": "cell_1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "outputId": "a7fd4a35-1a55-4948-f012-17b941194a01"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1585) (ipython-input-932513802.py, line 1585)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-932513802.py\"\u001b[0;36m, line \u001b[0;32m1585\u001b[0m\n\u001b[0;31m    print(\"\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1585)\n"
     ]
    }
   ],
   "source": [
    "# @title 2. Load Corpora (v10.12 - Self-Contained) { display-mode: \"form\" }\n",
    "# @markdown Downloads from verified external sources - fully self-contained, no external imports\n",
    "# @markdown\n",
    "# @markdown **Sources (9 categories):**\n",
    "# @markdown - Sanskrit: Itihasa (93K shlokas)\n",
    "# @markdown - Pali: SuttaCentral API (Full Canon)\n",
    "# @markdown - Arabic: Tanzil.net (Quran)\n",
    "# @markdown - Hebrew/Aramaic: Sefaria GitHub\n",
    "\n",
    "INCLUDE_RESPONSA = False  # @param {type:\"boolean\"}\n",
    "# @markdown - **INCLUDE_RESPONSA**: Include Responsa texts (requires 30-50 min git clone)\n",
    "# @markdown - Set to True only if you need the full Responsa collection\n",
    "# @markdown - Chinese: ctext.org API\n",
    "# @markdown - Greek/Latin: Perseus Digital Library\n",
    "# @markdown - Romance: Don Quijote, Montaigne, Voltaire, Rousseau, Machiavelli, Dante\n",
    "# @markdown - Folklore: Ashliman Folktexts (incl. Native American)\n",
    "# @markdown - English: Gutenberg philosophy, Dear Abby (68K), hendrycks/ethics (134K)\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING CORPORA (v10.12 - Self-Contained)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "DATA_DIR = Path(\"data/raw/v10.12\")\n",
    "CACHE_DIR = DATA_DIR / \"cache\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get settings from Cell 1\n",
    "try:\n",
    "    _cache_only = CACHE_ONLY\n",
    "except NameError:\n",
    "    _cache_only = False\n",
    "\n",
    "try:\n",
    "    _save_dir = SAVE_DIR\n",
    "except NameError:\n",
    "    _save_dir = \"data/processed\"\n",
    "\n",
    "# Memory limits per language (L4 GPU safe)\n",
    "MAX_PASSAGES_PER_LANG = {\n",
    "    \"sanskrit\": 15000,\n",
    "    \"pali\": 10000,\n",
    "    \"arabic\": 10000,\n",
    "    \"classical_chinese\": 10000,\n",
    "    \"hebrew\": 15000,\n",
    "    \"aramaic\": 10000,\n",
    "    \"english\": 50000,  # Increased for folklore + ethics\n",
    "    \"greek\": 10000,\n",
    "    \"latin\": 10000,\n",
    "    \"spanish\": 5000,\n",
    "    \"french\": 5000,\n",
    "    \"italian\": 5000,\n",
    "    \"default\": 5000,\n",
    "}\n",
    "\n",
    "MIN_PASSAGES = 500  # For 6-sigma confidence\n",
    "\n",
    "# ============================================================================\n",
    "# RESTORE CACHE FROM DRIVE (if available)\n",
    "# ============================================================================\n",
    "# In hybrid mode, check if Drive has cached corpus files and restore them\n",
    "# This avoids re-downloading on every Colab restart\n",
    "\n",
    "if _save_dir and os.path.exists(_save_dir):\n",
    "    drive_cache = Path(_save_dir) / \"corpus_cache\"\n",
    "    if drive_cache.exists():\n",
    "        import shutil\n",
    "        restored = 0\n",
    "        for cache_file in drive_cache.glob(\"*.json\"):\n",
    "            local_cache = CACHE_DIR / cache_file.name\n",
    "            if not local_cache.exists():\n",
    "                shutil.copy(cache_file, local_cache)\n",
    "                restored += 1\n",
    "        if restored:\n",
    "            print(f\"Restored {restored} cache files from Drive\")\n",
    "    # Note: sefaria.json IS cached to Drive after first successful load\n",
    "    # Git clone is faster than Drive copy for many small files\n",
    "\n",
    "# ============================================================================\n",
    "# RATE LIMITING\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, calls_per_minute: int = 20):\n",
    "        self.min_interval = 60.0 / calls_per_minute\n",
    "        self.last_call = 0.0\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def wait(self):\n",
    "        with self.lock:\n",
    "            elapsed = time.time() - self.last_call\n",
    "            if elapsed < self.min_interval:\n",
    "                time.sleep(self.min_interval - elapsed)\n",
    "            self.last_call = time.time()\n",
    "\n",
    "\n",
    "GITHUB_LIMITER = RateLimiter(calls_per_minute=60)\n",
    "SUTTACENTRAL_LIMITER = RateLimiter(calls_per_minute=120)\n",
    "CTEXT_LIMITER = RateLimiter(calls_per_minute=30)\n",
    "\n",
    "# ============================================================================\n",
    "# SANSKRIT - Itihasa from GitHub (VERIFIED)\n",
    "# https://github.com/rahular/itihasa - 93K shlokas\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_itihasa_github() -> list[dict]:\n",
    "    \"\"\"Load Itihasa Sanskrit shlokas from GitHub.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"itihasa.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Itihasa: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Downloading Itihasa from GitHub...\")\n",
    "    data_path = DATA_DIR / \"itihasa\"\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = [\n",
    "        (\"train.sn\", \"https://raw.githubusercontent.com/rahular/itihasa/main/data/train.sn\"),\n",
    "        (\"dev.sn\", \"https://raw.githubusercontent.com/rahular/itihasa/main/data/dev.sn\"),\n",
    "        (\"test.sn\", \"https://raw.githubusercontent.com/rahular/itihasa/main/data/test.sn\"),\n",
    "    ]\n",
    "\n",
    "    for name, url in files:\n",
    "        local_file = data_path / name\n",
    "        if not local_file.exists():\n",
    "            try:\n",
    "                GITHUB_LIMITER.wait()\n",
    "                resp = requests.get(url, timeout=120)\n",
    "                if resp.status_code == 200:\n",
    "                    with open(local_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(resp.text)\n",
    "                    print(f\"    Downloaded {name}: {len(resp.text)//1024}KB\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Failed {name}: {e}\")\n",
    "\n",
    "    # Parse .sn files\n",
    "    for sn_file in data_path.glob(\"*.sn\"):\n",
    "        with open(sn_file, encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f, 1):\n",
    "                text = line.strip()\n",
    "                if text and len(text) > 10:\n",
    "                    passages.append(\n",
    "                        {\n",
    "                            \"id\": f\"itihasa_{sn_file.stem}_{i}\",\n",
    "                            \"text\": text,\n",
    "                            \"language\": \"sanskrit\",\n",
    "                            \"source\": f\"Itihasa/{sn_file.stem}\",\n",
    "                            \"time_periods\": [\"DHARMA\", \"ANCIENT\", \"INDIC\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  Itihasa: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PALI - SuttaCentral API (VERIFIED)\n",
    "# https://suttacentral.net/api/bilarasuttas/{id}/pli\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_pali_suttacentral() -> list[dict]:\n",
    "    \"\"\"Load Pali texts from SuttaCentral API.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"suttacentral.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  SuttaCentral: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Fetching from SuttaCentral API...\")\n",
    "\n",
    "    # Expanded sutta list\n",
    "    sutta_ids = []\n",
    "    # Majjhima Nikaya (152 suttas)\n",
    "    sutta_ids.extend([f\"mn{i}\" for i in range(1, 153)])\n",
    "    # Digha Nikaya (34 suttas)\n",
    "    sutta_ids.extend([f\"dn{i}\" for i in range(1, 35)])\n",
    "    # Samyutta Nikaya (key vaggas)\n",
    "    for v in [1, 3, 6, 12, 22, 35, 45, 56]:\n",
    "        sutta_ids.extend([f\"sn{v}.{i}\" for i in range(1, 20)])\n",
    "    # Anguttara Nikaya\n",
    "    for n in [1, 2, 3, 4, 5, 6, 7, 8, 10]:\n",
    "        sutta_ids.extend([f\"an{n}.{i}\" for i in range(1, 50)])\n",
    "    # Dhammapada\n",
    "    sutta_ids.extend([f\"dhp{i}\" for i in range(1, 27)])\n",
    "\n",
    "    def fetch_sutta(sid):\n",
    "        results = []\n",
    "        try:\n",
    "            SUTTACENTRAL_LIMITER.wait()\n",
    "            url = f\"https://suttacentral.net/api/bilarasuttas/{sid}/pli\"\n",
    "            resp = requests.get(url, timeout=30)\n",
    "            if resp.status_code == 200:\n",
    "                data = resp.json()\n",
    "                if isinstance(data, dict):\n",
    "                    segments = data.get(\"root_text\", {})\n",
    "                    if isinstance(segments, dict):\n",
    "                        for seg_id, text in segments.items():\n",
    "                            if text and len(text) > 20:\n",
    "                                results.append(\n",
    "                                    {\n",
    "                                        \"id\": f\"pali_{sid}_{seg_id}\",\n",
    "                                        \"text\": text.strip(),\n",
    "                                        \"language\": \"pali\",\n",
    "                                        \"source\": sid,\n",
    "                                        \"time_periods\": [\"PALI\", \"ANCIENT\", \"INDIC\", \"BUDDHIST\"],\n",
    "                                    }\n",
    "                                )\n",
    "        except Exception:\n",
    "            pass\n",
    "        return results\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(fetch_sutta, sid) for sid in sutta_ids[:300]]\n",
    "        for done, future in enumerate(as_completed(futures), 1):\n",
    "            passages.extend(future.result())\n",
    "            if done % 50 == 0:\n",
    "                print(f\"    Fetched {done}/{min(300, len(sutta_ids))} suttas...\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  SuttaCentral: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ARABIC - Tanzil.net (VERIFIED)\n",
    "# https://tanzil.net/download/\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_quran_tanzil() -> list[dict]:\n",
    "    \"\"\"Load Quran from Tanzil.net.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"tanzil.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Tanzil Quran: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Downloading Quran from Tanzil.net...\")\n",
    "    try:\n",
    "        url = \"https://tanzil.net/pub/download/index.php?quranType=uthmani&outType=txt-2&agree=true\"\n",
    "        resp = requests.get(url, timeout=60)\n",
    "        if resp.status_code == 200:\n",
    "            for line in resp.text.strip().split(\"\\n\"):\n",
    "                if \"|\" in line:\n",
    "                    parts = line.split(\"|\")\n",
    "                    if len(parts) >= 3:\n",
    "                        surah, ayah, text = parts[0], parts[1], parts[2].strip()\n",
    "                        if len(text) > 10:\n",
    "                            passages.append(\n",
    "                                {\n",
    "                                    \"id\": f\"quran_{surah}_{ayah}\",\n",
    "                                    \"text\": text,\n",
    "                                    \"language\": \"arabic\",\n",
    "                                    \"source\": f\"Quran {surah}:{ayah}\",\n",
    "                                    \"time_periods\": [\"QURANIC\", \"MEDIEVAL\", \"SEMITIC\"],\n",
    "                                }\n",
    "                            )\n",
    "            print(f\"    Downloaded {len(passages)} verses\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  Tanzil Quran: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HEBREW/ARAMAIC - Sefaria GitHub (VERIFIED)\n",
    "# https://github.com/Sefaria/Sefaria-Export\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_sefaria_github() -> list[dict]:\n",
    "    \"\"\"Load Hebrew/Aramaic from Sefaria GitHub.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"sefaria.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Sefaria: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    base_path = DATA_DIR / \"Sefaria-Export\"\n",
    "    json_path = base_path / \"json\"\n",
    "\n",
    "    # Key texts to download (path, language, period)\n",
    "    # Pattern: path -> Hebrew/merged.json or Hebrew/Merged.json\n",
    "    key_texts = [\n",
    "        # =====================================================================\n",
    "        # TANAKH - Complete Hebrew Bible (~39 books, ~20MB)\n",
    "        # =====================================================================\n",
    "        # Torah (Pentateuch) - 5 books\n",
    "        (\"Tanakh/Torah/Genesis\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Torah/Exodus\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Torah/Leviticus\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Torah/Numbers\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Torah/Deuteronomy\", \"hebrew\", \"BIBLICAL\"),\n",
    "\n",
    "        # Former Prophets - 6 books\n",
    "        (\"Tanakh/Prophets/Joshua\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Judges\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/I Samuel\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/II Samuel\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/I Kings\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/II Kings\", \"hebrew\", \"BIBLICAL\"),\n",
    "\n",
    "        # Latter Prophets - Major - 3 books\n",
    "        (\"Tanakh/Prophets/Isaiah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Jeremiah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Ezekiel\", \"hebrew\", \"BIBLICAL\"),\n",
    "\n",
    "        # Latter Prophets - Minor (Trei Asar) - 12 books\n",
    "        (\"Tanakh/Prophets/Hosea\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Joel\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Amos\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Obadiah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Jonah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Micah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Nahum\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Habakkuk\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Zephaniah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Haggai\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Zechariah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Malachi\", \"hebrew\", \"BIBLICAL\"),\n",
    "\n",
    "        # Writings (Ketuvim) - 13 books\n",
    "        (\"Tanakh/Writings/Psalms\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Proverbs\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Job\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Song of Songs\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Ruth\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Lamentations\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Ecclesiastes\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Esther\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Daniel\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Ezra\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Nehemiah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/I Chronicles\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/II Chronicles\", \"hebrew\", \"BIBLICAL\"),\n",
    "\n",
    "        # =====================================================================\n",
    "        # MISHNAH - Complete 6 Orders (~63 tractates, ~15MB)\n",
    "        # =====================================================================\n",
    "        # Seder Zeraim (Seeds) - Agricultural ethics\n",
    "        (\"Mishnah/Seder Zeraim/Mishnah Berakhot\", \"hebrew\", \"TANNAITIC\"),\n",
    "        (\"Mishnah/Seder Zeraim/Mishnah Peah\", \"hebrew\", \"TANNAITIC\"),  # Corners for poor\n",
    "        (\"Mishnah/Seder Zeraim/Mishnah Maasrot\", \"hebrew\", \"TANNAITIC\"),  # Tithes\n",
    "\n",
    "        # Seder Moed (Festivals) - Sabbath ethics\n",
    "        (\"Mishnah/Seder Moed/Mishnah Shabbat\", \"hebrew\", \"TANNAITIC\"),\n",
    "        (\"Mishnah/Seder Moed/Mishnah Yoma\", \"hebrew\", \"TANNAITIC\"),  # Day of Atonement\n",
    "        (\"Mishnah/Seder Moed/Mishnah Taanit\", \"hebrew\", \"TANNAITIC\"),  # Fasts\n",
    "\n",
    "        # Seder Nashim (Women) - Family/gender ethics\n",
    "        (\"Mishnah/Seder Nashim/Mishnah Yevamot\", \"hebrew\", \"TANNAITIC\"),  # Levirate marriage\n",
    "        (\"Mishnah/Seder Nashim/Mishnah Ketubot\", \"hebrew\", \"TANNAITIC\"),  # Marriage contracts\n",
    "        (\"Mishnah/Seder Nashim/Mishnah Nedarim\", \"hebrew\", \"TANNAITIC\"),  # Vows\n",
    "        (\"Mishnah/Seder Nashim/Mishnah Nazir\", \"hebrew\", \"TANNAITIC\"),  # Nazirite vows\n",
    "        (\"Mishnah/Seder Nashim/Mishnah Sotah\", \"hebrew\", \"TANNAITIC\"),  # Suspected adulteress\n",
    "        (\"Mishnah/Seder Nashim/Mishnah Gittin\", \"hebrew\", \"TANNAITIC\"),  # Divorce\n",
    "        (\"Mishnah/Seder Nashim/Mishnah Kiddushin\", \"hebrew\", \"TANNAITIC\"),  # Betrothal\n",
    "\n",
    "        # Seder Nezikin (Damages) - Civil/criminal ethics (CORE)\n",
    "        # Note: Sefaria uses \"Mishnah X\" prefix for tractate folders\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Bava Kamma\", \"hebrew\", \"TANNAITIC\"),  # First Gate - damages\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Bava Metzia\", \"hebrew\", \"TANNAITIC\"),  # Middle Gate - property\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Bava Batra\", \"hebrew\", \"TANNAITIC\"),  # Last Gate - sales\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Sanhedrin\", \"hebrew\", \"TANNAITIC\"),  # Courts/capital\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Makkot\", \"hebrew\", \"TANNAITIC\"),  # Lashes\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Shevuot\", \"hebrew\", \"TANNAITIC\"),  # Oaths\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Eduyot\", \"hebrew\", \"TANNAITIC\"),  # Testimonies\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Avodah Zarah\", \"hebrew\", \"TANNAITIC\"),  # Idolatry\n",
    "        (\"Mishnah/Seder Nezikin/Pirkei Avot\", \"hebrew\", \"TANNAITIC\"),  # Ethics of Fathers (no prefix)\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Horayot\", \"hebrew\", \"TANNAITIC\"),  # Rulings\n",
    "\n",
    "        # Seder Kodashim (Holy Things) - Temple/sacred\n",
    "        (\"Mishnah/Seder Kodashim/Mishnah Zevachim\", \"hebrew\", \"TANNAITIC\"),\n",
    "        (\"Mishnah/Seder Kodashim/Mishnah Menachot\", \"hebrew\", \"TANNAITIC\"),\n",
    "\n",
    "        # Seder Tohorot (Purities) - Purity ethics\n",
    "        (\"Mishnah/Seder Tohorot/Mishnah Niddah\", \"hebrew\", \"TANNAITIC\"),  # Menstrual purity\n",
    "\n",
    "        # =====================================================================\n",
    "        # TALMUD BAVLI - Key tractates (~20MB)\n",
    "        # =====================================================================\n",
    "        # Foundational\n",
    "        (\"Talmud/Bavli/Seder Zeraim/Berakhot\", \"aramaic\", \"AMORAIC\"),\n",
    "\n",
    "        # Ethics tractates (Seder Nezikin)\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Bava Kamma\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Bava Metzia\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Bava Batra\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Sanhedrin\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Makkot\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Shevuot\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Avodah Zarah\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Horayot\", \"aramaic\", \"AMORAIC\"),\n",
    "\n",
    "        # Family ethics (Seder Nashim)\n",
    "        (\"Talmud/Bavli/Seder Nashim/Yevamot\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nashim/Ketubot\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nashim/Kiddushin\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nashim/Gittin\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nashim/Sotah\", \"aramaic\", \"AMORAIC\"),\n",
    "\n",
    "        # Sabbath (Seder Moed)\n",
    "        (\"Talmud/Bavli/Seder Moed/Shabbat\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Moed/Yoma\", \"aramaic\", \"AMORAIC\"),\n",
    "\n",
    "        # =====================================================================\n",
    "        # RESPONSA - Ethical Q&A (only if INCLUDE_RESPONSA=True)\n",
    "        # =====================================================================\n",
    "        (\"Responsa/Geonim\", \"hebrew\", \"GEONIC\"),  # 600-1000 CE\n",
    "        (\"Responsa/Rishonim\", \"hebrew\", \"RISHONIM\"),  # 1000-1500 CE\n",
    "        (\"Responsa/Acharonim\", \"hebrew\", \"ACHARONIM\"),  # 1500-1800 CE\n",
    "        (\"Responsa/Modern\", \"hebrew\", \"MODERN_RESPONSA\"),  # 1800-present\n",
    "        (\"Responsa/Teshuvot Maharsham Volume I\", \"hebrew\", \"ACHARONIM\"),\n",
    "        (\"Responsa/Teshuvot Maharsham Volume II\", \"hebrew\", \"ACHARONIM\"),\n",
    "        (\"Responsa/Teshuvot Maharsham Volume III\", \"hebrew\", \"ACHARONIM\"),\n",
    "    ]\n",
    "\n",
    "    # Download strategy depends on INCLUDE_RESPONSA setting\n",
    "    # - False: Staged download only (fast, ~2 min, core texts)\n",
    "    # - True: Full clone only (slow, 30-50 min, includes Responsa)\n",
    "\n",
    "    if INCLUDE_RESPONSA:\n",
    "        # Skip staged download - we need full clone for Responsa anyway\n",
    "        print(\"  INCLUDE_RESPONSA=True: Will do full clone for Responsa...\")\n",
    "        need_staged = False\n",
    "    else:\n",
    "        need_staged = not json_path.exists()\n",
    "\n",
    "    if need_staged:\n",
    "        print(\"  Downloading Sefaria texts (staged download)...\")\n",
    "        json_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        base_url = \"https://raw.githubusercontent.com/Sefaria/Sefaria-Export/master/json\"\n",
    "\n",
    "        def download_sefaria_text(text_info):\n",
    "            \"\"\"Download merged.json for a Sefaria text, handling various structures.\"\"\"\n",
    "            text_path, lang, period = text_info\n",
    "            url_path = text_path.replace(\" \", \"%20\")\n",
    "\n",
    "            # Different structures for different text types\n",
    "            if \"Responsa\" in text_path:\n",
    "                # Responsa have nested structure - try to get index or first collection\n",
    "                # For now, skip in staged mode - these need full clone\n",
    "                return text_path, False, 0, []\n",
    "\n",
    "            # Standard texts: try Hebrew/merged.json first\n",
    "            patterns = [\n",
    "                (\"Hebrew/merged.json\", \"Hebrew\"),\n",
    "                (\"Aramaic/merged.json\", \"Aramaic\"),  # For Talmud\n",
    "                (\"merged.json\", \"\"),  # Direct merged.json\n",
    "            ]\n",
    "\n",
    "            for pattern, subdir in patterns:\n",
    "                try:\n",
    "                    url = f\"{base_url}/{url_path}/{pattern}\"\n",
    "                    GITHUB_LIMITER.wait()\n",
    "                    resp = requests.get(url, timeout=60)\n",
    "                    if resp.status_code == 200 and len(resp.text) > 100:\n",
    "                        local_dir = json_path / text_path\n",
    "                        if subdir:\n",
    "                            local_dir = local_dir / subdir\n",
    "                        local_dir.mkdir(parents=True, exist_ok=True)\n",
    "                        local_file = local_dir / \"merged.json\"\n",
    "                        with open(local_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(resp.text)\n",
    "                        return text_path, True, len(resp.text), []\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            return text_path, False, 0, []\n",
    "\n",
    "        # Download in parallel\n",
    "        downloaded = 0\n",
    "        total_size = 0\n",
    "        responsa_skipped = []\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(download_sefaria_text, t) for t in key_texts]\n",
    "            for future in as_completed(futures):\n",
    "                text_path, success, size, _ = future.result()\n",
    "                if \"Responsa\" in text_path:\n",
    "                    responsa_skipped.append(text_path)\n",
    "                elif success:\n",
    "                    downloaded += 1\n",
    "                    total_size += size\n",
    "                    print(\n",
    "                        f\"    Downloaded {downloaded}: {text_path.split('/')[-1][:30]} ({size//1024}KB)\"\n",
    "                    )\n",
    "\n",
    "        non_responsa_texts = [t for t in key_texts if \"Responsa\" not in t[0]]\n",
    "        responsa_texts = [t for t in key_texts if \"Responsa\" in t[0]]\n",
    "        print(\n",
    "            f\"    Staged: {downloaded}/{len(non_responsa_texts)} core texts, {total_size//1024}KB\"\n",
    "        )\n",
    "        if not INCLUDE_RESPONSA:\n",
    "            # Show what we got from staged download\n",
    "            if responsa_texts:\n",
    "                print(f\"    Responsa skipped (set INCLUDE_RESPONSA=True to include)\")\n",
    "            print(f\"    Using staged download results ({downloaded} texts)\")\n",
    "            need_clone = False  # Don't clone if INCLUDE_RESPONSA is False\n",
    "        else:\n",
    "            # INCLUDE_RESPONSA is True - we need to clone\n",
    "            print(f\"    Responsa ({len(responsa_texts)} collections) require full clone\")\n",
    "            need_clone = True\n",
    "\n",
    "        if need_clone:\n",
    "            print(\"    Starting full clone (this takes 30-50 min)...\")\n",
    "            # Clone to a temp location, then move\n",
    "            import shutil\n",
    "\n",
    "            clone_path = DATA_DIR / \"Sefaria-Clone-Temp\"\n",
    "            if clone_path.exists():\n",
    "                shutil.rmtree(clone_path)\n",
    "\n",
    "            print(\"  Cloning Sefaria-Export from GitHub (~2GB, 30-50 min)...\")\n",
    "            try:\n",
    "                import re\n",
    "\n",
    "                proc = subprocess.Popen(\n",
    "                    [\n",
    "                        \"git\",\n",
    "                        \"clone\",\n",
    "                        \"--depth\",\n",
    "                        \"1\",\n",
    "                        \"--progress\",\n",
    "                        \"https://github.com/Sefaria/Sefaria-Export.git\",\n",
    "                        str(clone_path),\n",
    "                    ],\n",
    "                    stdout=subprocess.PIPE,\n",
    "                    stderr=subprocess.PIPE,  # Git progress goes to stderr\n",
    "                    text=True,\n",
    "                    bufsize=1,\n",
    "                )\n",
    "                last_progress_time = time.time()\n",
    "                last_pct = -1\n",
    "                current_phase = \"\"\n",
    "                stall_timeout = 120  # Kill if no progress for 2 minutes\n",
    "\n",
    "                while proc.poll() is None:\n",
    "                    # Check for stall (no progress for stall_timeout seconds)\n",
    "                    if time.time() - last_progress_time > stall_timeout:\n",
    "                        proc.kill()\n",
    "                        print(f\"\\n    Stalled (no progress for {stall_timeout}s)\")\n",
    "                        raise subprocess.TimeoutExpired(\"git clone\", stall_timeout)\n",
    "\n",
    "                    # Read stderr for progress (git writes progress there)\n",
    "                    try:\n",
    "                        line = proc.stderr.readline()\n",
    "                        if line:\n",
    "                            line = line.strip()\n",
    "                            last_progress_time = time.time()  # Got output = progress\n",
    "\n",
    "                            # Detect phase changes\n",
    "                            if \"Receiving objects\" in line and current_phase != \"receiving\":\n",
    "                                current_phase = \"receiving\"\n",
    "                                print(\"    Receiving objects: \", end=\"\", flush=True)\n",
    "                                last_pct = -1\n",
    "                            elif \"Resolving deltas\" in line and current_phase != \"resolving\":\n",
    "                                current_phase = \"resolving\"\n",
    "                                print(\"\\n    Resolving deltas:  \", end=\"\", flush=True)\n",
    "                                last_pct = -1\n",
    "                            elif \"Updating files\" in line and current_phase != \"updating\":\n",
    "                                current_phase = \"updating\"\n",
    "                                print(\"\\n    Updating files:    \", end=\"\", flush=True)\n",
    "                                last_pct = -1\n",
    "\n",
    "                            # Extract and print percentage (every 10%)\n",
    "                            if \"%\" in line:\n",
    "                                match = re.search(r\"(\\d+)%\", line)\n",
    "                                if match:\n",
    "                                    pct = int(match.group(1))\n",
    "                                    # Print at 0, 10, 20, ... 100\n",
    "                                    if pct // 10 > last_pct // 10 or pct == 100:\n",
    "                                        print(f\"{pct}% \", end=\"\", flush=True)\n",
    "                                        last_pct = pct\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    time.sleep(0.05)\n",
    "\n",
    "                print()  # Newline after progress\n",
    "                # Drain any remaining output\n",
    "                _, stderr = proc.communicate(timeout=5)\n",
    "                if proc.returncode == 0:\n",
    "                    print(\"    Clone successful!\")\n",
    "                    # Move cloned json to base_path\n",
    "                    cloned_json = clone_path / \"json\"\n",
    "                    if cloned_json.exists():\n",
    "                        import shutil\n",
    "\n",
    "                        if json_path.exists():\n",
    "                            shutil.rmtree(json_path)\n",
    "                        shutil.move(str(cloned_json), str(json_path))\n",
    "                        shutil.rmtree(clone_path)  # Clean up\n",
    "                else:\n",
    "                    print(f\"    Clone failed (code {proc.returncode})\")\n",
    "                    if stderr:\n",
    "                        print(f\"    {stderr[:200]}\")\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"    Using staged results (Responsa unavailable)\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n    Clone failed: {e} - using staged results\")\n",
    "        else:\n",
    "            print(\"    Staged download sufficient, skipping full clone\")\n",
    "\n",
    "    def extract_text(obj, depth=0):\n",
    "        if depth > 5:\n",
    "            return []\n",
    "        texts = []\n",
    "        if isinstance(obj, str) and len(obj) > 20:\n",
    "            texts.append(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                texts.extend(extract_text(item, depth + 1))\n",
    "        elif isinstance(obj, dict):\n",
    "            for key in [\"he\", \"text\", \"content\"]:\n",
    "                if key in obj:\n",
    "                    texts.extend(extract_text(obj[key], depth + 1))\n",
    "        return texts\n",
    "\n",
    "    for text_path, lang, period in key_texts:\n",
    "        full_path = json_path / text_path\n",
    "        if not full_path.exists():\n",
    "            json_file = json_path / f\"{text_path}.json\"\n",
    "            if json_file.exists():\n",
    "                full_path = json_file\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            files_to_parse = []\n",
    "            if full_path.is_file():\n",
    "                files_to_parse = [full_path]\n",
    "            elif full_path.is_dir():\n",
    "                # Responsa have many nested files - allow more\n",
    "                max_files = 500 if \"Responsa\" in text_path else 100\n",
    "                files_to_parse = list(full_path.rglob(\"*.json\"))[:max_files]\n",
    "\n",
    "            text_count = 0\n",
    "            for jf in files_to_parse:\n",
    "                with open(jf, encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                texts = extract_text(data)\n",
    "                # More texts per file for Responsa (rich ethical Q&A)\n",
    "                max_per_file = 500 if \"Responsa\" in text_path else 200\n",
    "                for text in texts[:max_per_file]:\n",
    "                    if len(passages) >= MAX_PASSAGES_PER_LANG.get(lang, 5000):\n",
    "                        break\n",
    "                    passages.append(\n",
    "                        {\n",
    "                            \"id\": f\"sefaria_{len(passages)}\",\n",
    "                            \"text\": text.strip(),\n",
    "                            \"language\": lang,\n",
    "                            \"source\": text_path.split(\"/\")[-1],\n",
    "                            \"time_periods\": [period],\n",
    "                        }\n",
    "                    )\n",
    "                    text_count += 1\n",
    "            if text_count > 0 and \"Responsa\" in text_path:\n",
    "                print(f\"    {text_path.split('/')[-1]}: {text_count} responsa\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "        # Immediately cache to Drive (clone takes 30-50 min - don't lose this!)\n",
    "        try:\n",
    "            if '_save_dir' in dir() and _save_dir and os.path.exists(_save_dir):\n",
    "                import shutil\n",
    "                drive_cache = Path(_save_dir) / \"corpus_cache\"\n",
    "                drive_cache.mkdir(exist_ok=True)\n",
    "                drive_sefaria = drive_cache / \"sefaria.json\"\n",
    "                if not drive_sefaria.exists():\n",
    "                    shutil.copy(cache_file, drive_sefaria)\n",
    "                    print(f\"    -> Cached sefaria.json to Drive for future runs\")\n",
    "        except Exception as e:\n",
    "            pass  # Drive cache is optional\n",
    "\n",
    "    print(f\"  Sefaria: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CHINESE - ctext.org API (VERIFIED)\n",
    "# https://api.ctext.org/gettext?urn=ctp:analects/xue-er\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_chinese_ctext() -> list[dict]:\n",
    "    \"\"\"Load Chinese classics from ctext.org API.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"ctext.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            # Show breakdown by period\n",
    "            by_period = {}\n",
    "            for p in passages:\n",
    "                period = p.get(\"time_periods\", [\"UNKNOWN\"])[0]\n",
    "                by_period[period] = by_period.get(period, 0) + 1\n",
    "            print(f\"  ctext.org: {len(passages):,} passages (cached)\")\n",
    "            for period, count in sorted(by_period.items()):\n",
    "                print(f\"    {period}: {count}\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Fetching from ctext.org API...\")\n",
    "\n",
    "    # ctext.org requires chapter-level URNs\n",
    "    texts = [\n",
    "        (\n",
    "            \"ctp:analects\",\n",
    "            \"Analects\",\n",
    "            \"CONFUCIAN\",\n",
    "            [\n",
    "                \"xue-er\", \"wei-zheng\", \"ba-yi\", \"li-ren\", \"gong-ye-chang\",\n",
    "                \"yong-ye\", \"shu-er\", \"tai-bo\", \"zi-han\", \"xiang-dang\",\n",
    "                \"xian-jin\", \"yan-yuan\", \"zi-lu\", \"xian-wen\", \"wei-ling-gong\",\n",
    "                \"ji-shi\", \"yang-huo\", \"wei-zi\", \"zi-zhang\", \"yao-yue\",\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"ctp:mengzi\",\n",
    "            \"Mencius\",\n",
    "            \"CONFUCIAN\",\n",
    "            [\n",
    "                \"liang-hui-wang-i\", \"liang-hui-wang-ii\", \"gong-sun-chou-i\",\n",
    "                \"gong-sun-chou-ii\", \"teng-wen-gong-i\", \"teng-wen-gong-ii\",\n",
    "                \"li-lou-i\", \"li-lou-ii\", \"wan-zhang-i\", \"wan-zhang-ii\",\n",
    "                \"gaozi-i\", \"gaozi-ii\", \"jin-xin-i\", \"jin-xin-ii\",\n",
    "            ],\n",
    "        ),\n",
    "        (\"ctp:dao-de-jing\", \"Daodejing\", \"DAOIST\", []),  # Book-level fetch\n",
    "        (\"ctp:zhuangzi\", \"Zhuangzi\", \"DAOIST\", []),  # Book-level fetch\n",
    "        (\n",
    "            \"ctp:xunzi\",\n",
    "            \"Xunzi\",\n",
    "            \"CONFUCIAN\",\n",
    "            [\"quan-xue\", \"xiu-shen\", \"bu-gou\", \"rong-ru\", \"fei-xiang\", \"wang-zhi\"],\n",
    "        ),\n",
    "        (\n",
    "            \"ctp:mozi\",\n",
    "            \"Mozi\",\n",
    "            \"MOHIST\",\n",
    "            [\"qin-shi\", \"xiu-shen\", \"suo-ran\", \"fa-yi\", \"qi-huan\", \"ci-guo\"],\n",
    "        ),\n",
    "        (\n",
    "            \"ctp:hanfeizi\",\n",
    "            \"Han Feizi\",\n",
    "            \"LEGALIST\",\n",
    "            [\"chu-jian\", \"cun-han\", \"nan-yan\", \"ai-chen\", \"zhu-dao\",\n",
    "             \"yang-quan\", \"er-bing\", \"jian-jie-shi-chen\"],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    errors_by_text = {}\n",
    "\n",
    "    for text_id, name, period, chapters in texts:\n",
    "        count = 0\n",
    "        errors = []\n",
    "        # If no chapters specified, fetch book-level\n",
    "        if not chapters:\n",
    "            try:\n",
    "                CTEXT_LIMITER.wait()\n",
    "                url = f\"https://api.ctext.org/gettext?urn={text_id}\"\n",
    "                resp = requests.get(url, timeout=30)\n",
    "                if resp.status_code == 200:\n",
    "                    data = resp.json()\n",
    "                    if isinstance(data, dict) and \"fulltext\" in data:\n",
    "                        for text in data[\"fulltext\"]:\n",
    "                            if text and len(text) > 10:\n",
    "                                passages.append({\n",
    "                                    \"id\": f\"ctext_{len(passages)}\",\n",
    "                                    \"text\": text,\n",
    "                                    \"language\": \"classical_chinese\",\n",
    "                                    \"source\": name,\n",
    "                                    \"time_periods\": [period],\n",
    "                                })\n",
    "                                count += 1\n",
    "            except Exception as e:\n",
    "                errors.append(f\"book-level: {type(e).__name__}\")\n",
    "        \n",
    "        for chapter in chapters:\n",
    "            try:\n",
    "                CTEXT_LIMITER.wait()\n",
    "                urn = f\"{text_id}/{chapter}\"\n",
    "                url = f\"https://api.ctext.org/gettext?urn={urn}\"\n",
    "                resp = requests.get(url, timeout=30)\n",
    "\n",
    "                if resp.status_code != 200:\n",
    "                    errors.append(f\"{chapter}: HTTP {resp.status_code}\")\n",
    "                    continue\n",
    "\n",
    "                data = resp.json()\n",
    "\n",
    "                # Check for API error response\n",
    "                if isinstance(data, dict) and \"error\" in data:\n",
    "                    errors.append(f\"{chapter}: {data['error']}\")\n",
    "                    continue\n",
    "\n",
    "                # API returns {\"fulltext\": [...], \"title\": \"...\"}\n",
    "                if isinstance(data, dict) and \"fulltext\" in data:\n",
    "                    for text in data[\"fulltext\"]:\n",
    "                        if text and len(text) > 10:\n",
    "                            passages.append({\n",
    "                                \"id\": f\"ctext_{len(passages)}\",\n",
    "                                \"text\": text,\n",
    "                                \"language\": \"classical_chinese\",\n",
    "                                \"source\": f\"{name}/{chapter}\",\n",
    "                                \"time_periods\": [period],\n",
    "                            })\n",
    "                            count += 1\n",
    "                # Fallback: list format\n",
    "                elif isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        text = item.get(\"text\", \"\") if isinstance(item, dict) else str(item)\n",
    "                        if text and len(text) > 10:\n",
    "                            passages.append({\n",
    "                                \"id\": f\"ctext_{len(passages)}\",\n",
    "                                \"text\": text,\n",
    "                                \"language\": \"classical_chinese\",\n",
    "                                \"source\": f\"{name}/{chapter}\",\n",
    "                                \"time_periods\": [period],\n",
    "                            })\n",
    "                            count += 1\n",
    "                else:\n",
    "                    errors.append(f\"{chapter}: unexpected response format\")\n",
    "\n",
    "            except requests.exceptions.Timeout:\n",
    "                errors.append(f\"{chapter}: timeout\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                errors.append(f\"{chapter}: {type(e).__name__}\")\n",
    "            except json.JSONDecodeError:\n",
    "                errors.append(f\"{chapter}: invalid JSON\")\n",
    "            except Exception as e:\n",
    "                errors.append(f\"{chapter}: {type(e).__name__}: {e}\")\n",
    "\n",
    "        # Always print status for each text\n",
    "        if count > 0:\n",
    "            print(f\"    {name} ({period}): {count} passages\")\n",
    "        else:\n",
    "            print(f\"    {name} ({period}): 0 passages [FAILED]\")\n",
    "\n",
    "        if errors:\n",
    "            errors_by_text[name] = errors\n",
    "\n",
    "    # Print error summary\n",
    "    if errors_by_text:\n",
    "        print(f\"\\n  ctext.org API errors:\")\n",
    "        for name, errs in errors_by_text.items():\n",
    "            print(f\"    {name}: {len(errs)} failed chapters\")\n",
    "            for err in errs[:3]:  # Show first 3 errors\n",
    "                print(f\"      - {err}\")\n",
    "            if len(errs) > 3:\n",
    "                print(f\"      - ... and {len(errs) - 3} more\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  ctext.org: {len(passages):,} passages total\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ============================================================================\n",
    "# CHINESE BUDDHIST - CBETA (Chinese Buddhist Electronic Text Association)\n",
    "# Key sutras for Buddhist moral philosophy\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_chinese_buddhist() -> list[dict]:\n",
    "    \"\"\"Load Chinese Buddhist texts from CBETA via CLTK GitHub mirrors.\n",
    "\n",
    "    Sources key sutras representing Buddhist ethics/philosophy:\n",
    "    - Diamond Sutra (\u91d1\u525b\u7d93) - Prajnaparamita\n",
    "    - Heart Sutra (\u5fc3\u7d93) - Core emptiness teaching\n",
    "    - Platform Sutra (\u516d\u7956\u58c7\u7d93) - Chan/Zen ethics\n",
    "    - Sutra of 42 Sections (\u56db\u5341\u4e8c\u7ae0\u7d93) - Basic moral teachings\n",
    "    - Vimalakirti Sutra (\u7dad\u6469\u8a70\u7d93) - Lay Buddhist ethics\n",
    "    - Lotus Sutra (\u5999\u6cd5\u84ee\u83ef\u7d93) - Devotional Buddhism\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"cbeta_buddhist.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  CBETA Buddhist: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Fetching Chinese Buddhist texts from CBETA/CLTK...\")\n",
    "\n",
    "    # CLTK uses format: cbeta__taisho-tripitaka-electronic-version-no-XXXX__chinese.json\n",
    "    base_url = \"https://raw.githubusercontent.com/cltk/chinese_text_cbeta_02/master/cltk_json\"\n",
    "    texts = [\n",
    "        (\"0235\", \"Diamond Sutra\"),\n",
    "        (\"0251\", \"Heart Sutra\"),\n",
    "        (\"0262\", \"Lotus Sutra\"),\n",
    "        (\"0475\", \"Vimalakirti Sutra\"),\n",
    "        (\"0784\", \"42 Sections Sutra\"),\n",
    "        (\"2008\", \"Platform Sutra\"),\n",
    "    ]\n",
    "\n",
    "    for text_num, name in texts:\n",
    "        try:\n",
    "            url = f\"{base_url}/cbeta__taisho-tripitaka-electronic-version-no-{text_num}__chinese.json\"\n",
    "            resp = requests.get(url, timeout=60)\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"    {name}: HTTP {resp.status_code}\")\n",
    "                continue\n",
    "\n",
    "            data = resp.json()\n",
    "            count = 0\n",
    "\n",
    "            # CLTK format: {\"text\": {\"0\": \"line\", \"1\": \"line\", ...}}\n",
    "            if isinstance(data, dict) and \"text\" in data:\n",
    "                text_dict = data[\"text\"]\n",
    "                if isinstance(text_dict, dict):\n",
    "                    for key in sorted(text_dict.keys(), key=lambda x: int(x) if x.isdigit() else 0):\n",
    "                        text = text_dict[key]\n",
    "                        if isinstance(text, str) and len(text) > 10:\n",
    "                            # Skip metadata lines\n",
    "                            if text.startswith(\"No.\") or text.startswith(\"[\") or text.startswith(\"\u3010\"):\n",
    "                                continue\n",
    "                            if \"CBETA\" in text or \"Taisho\" in text:\n",
    "                                continue\n",
    "                            passages.append({\n",
    "                                \"id\": f\"cbeta_T{text_num}_{count}\",\n",
    "                                \"text\": text.strip(),\n",
    "                                \"language\": \"classical_chinese\",\n",
    "                                \"source\": f\"CBETA/{name}\",\n",
    "                                \"time_periods\": [\"BUDDHIST\"],\n",
    "                            })\n",
    "                            count += 1\n",
    "\n",
    "            if count > 0:\n",
    "                print(f\"    {name}: {count} passages\")\n",
    "            else:\n",
    "                print(f\"    {name}: 0 (format issue)\")\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"    {name}: timeout\")\n",
    "        except Exception as e:\n",
    "            print(f\"    {name}: {type(e).__name__}: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  CBETA Buddhist: {len(passages):,} passages total\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# GREEK/LATIN - Perseus Digital Library (VERIFIED)\n",
    "# https://github.com/PerseusDL/canonical-greekLit\n",
    "# https://github.com/PerseusDL/canonical-latinLit\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_perseus_github() -> list[dict]:\n",
    "    \"\"\"Load Greek and Latin philosophy from Perseus Digital Library.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"perseus.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Perseus: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Fetching from Perseus GitHub...\")\n",
    "\n",
    "    # Key philosophical texts\n",
    "    # Format: (author_id, work_id, name, period)\n",
    "    # URL pattern: /data/{author_id}/{work_id}/{author_id}.{work_id}.perseus-{lang}2.xml\n",
    "    greek_texts = [\n",
    "        (\"tlg0059\", \"tlg030\", \"Plato Republic\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0059\", \"tlg031\", \"Plato Laws\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0059\", \"tlg017\", \"Plato Apology\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0059\", \"tlg004\", \"Plato Symposium\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0059\", \"tlg003\", \"Plato Phaedo\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0086\", \"tlg010\", \"Aristotle Nicomachean Ethics\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0086\", \"tlg028\", \"Aristotle Politics\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0086\", \"tlg035\", \"Aristotle Rhetoric\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0555\", \"tlg001\", \"Epictetus Discourses\", \"HELLENISTIC\"),\n",
    "        (\"tlg0555\", \"tlg002\", \"Epictetus Enchiridion\", \"HELLENISTIC\"),\n",
    "        (\"tlg0562\", \"tlg001\", \"Marcus Aurelius Meditations\", \"HELLENISTIC\"),\n",
    "    ]\n",
    "\n",
    "    latin_texts = [\n",
    "        (\"phi0474\", \"phi038\", \"Cicero De Officiis\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0474\", \"phi044\", \"Cicero Tusculan Disputations\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0474\", \"phi019\", \"Cicero De Finibus\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0690\", \"phi003\", \"Seneca Epistles\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0690\", \"phi001\", \"Seneca De Beneficiis\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0959\", \"phi001\", \"Lucretius De Rerum Natura\", \"CLASSICAL_LATIN\"),\n",
    "    ]\n",
    "\n",
    "    def fetch_perseus_text(author_id, work_id, name, period, lang_code):\n",
    "        \"\"\"Fetch text from Perseus using correct URL pattern.\"\"\"\n",
    "        results = []\n",
    "        repo = \"greekLit\" if lang_code == \"grc\" else \"latinLit\"\n",
    "        text_id = f\"{author_id}.{work_id}\"\n",
    "\n",
    "        # Try multiple filename patterns\n",
    "        patterns = [\n",
    "            f\"{text_id}.perseus-{lang_code}2.xml\",  # Most common: tlg0059.tlg030.perseus-grc2.xml\n",
    "            f\"{text_id}.perseus-{lang_code}1.xml\",\n",
    "            f\"{text_id}.{lang_code}1.xml\",\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            try:\n",
    "                url = f\"https://raw.githubusercontent.com/PerseusDL/canonical-{repo}/master/data/{author_id}/{work_id}/{pattern}\"\n",
    "                GITHUB_LIMITER.wait()\n",
    "                resp = requests.get(url, timeout=60)\n",
    "                if resp.status_code == 200:\n",
    "                    import re\n",
    "\n",
    "                    # Extract text between tags, remove markup\n",
    "                    text_content = re.sub(r\"<[^>]+>\", \" \", resp.text)\n",
    "                    text_content = re.sub(r\"\\s+\", \" \", text_content).strip()\n",
    "\n",
    "                    # Split into chunks of ~500 chars\n",
    "                    chunks = []\n",
    "                    words = text_content.split()\n",
    "                    current = []\n",
    "                    current_len = 0\n",
    "                    for word in words:\n",
    "                        current.append(word)\n",
    "                        current_len += len(word) + 1\n",
    "                        if current_len > 400:\n",
    "                            chunks.append(\" \".join(current))\n",
    "                            current = []\n",
    "                            current_len = 0\n",
    "                    if current:\n",
    "                        chunks.append(\" \".join(current))\n",
    "\n",
    "                    lang = \"greek\" if lang_code == \"grc\" else \"latin\"\n",
    "                    for i, chunk in enumerate(chunks[:500]):  # Limit per text\n",
    "                        if len(chunk) > 50:\n",
    "                            results.append(\n",
    "                                {\n",
    "                                    \"id\": f\"perseus_{text_id}_{i}\",\n",
    "                                    \"text\": chunk,\n",
    "                                    \"language\": lang,\n",
    "                                    \"source\": name,\n",
    "                                    \"time_periods\": [period],\n",
    "                                }\n",
    "                            )\n",
    "                    return results  # Success, stop trying patterns\n",
    "            except Exception:\n",
    "                continue\n",
    "        return results\n",
    "\n",
    "    # Fetch Greek texts\n",
    "    for author_id, work_id, name, period in greek_texts:\n",
    "        result = fetch_perseus_text(author_id, work_id, name, period, \"grc\")\n",
    "        passages.extend(result)\n",
    "        if result:\n",
    "            print(f\"    {name}: {len(result)} passages\")\n",
    "\n",
    "    # Fetch Latin texts\n",
    "    for author_id, work_id, name, period in latin_texts:\n",
    "        result = fetch_perseus_text(author_id, work_id, name, period, \"lat\")\n",
    "        passages.extend(result)\n",
    "        if result:\n",
    "            print(f\"    {name}: {len(result)} passages\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  Perseus: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# WESTERN PHILOSOPHY - Project Gutenberg (direct download by ID)\n",
    "# Like R's gutenbergr::gutenberg_download() - just give it a list of IDs\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_gutenberg_philosophy(target_passages: int = 5000) -> list[dict]:\n",
    "    \"\"\"Load Western philosophy classics from Project Gutenberg by ID.\n",
    "\n",
    "    Uses gutenberg_download(id) like R's gutenbergr package - just give it IDs.\n",
    "    JIT loading: fetches texts one at a time, caches individually.\n",
    "\n",
    "    Args:\n",
    "        target_passages: Stop fetching after reaching this many passages.\n",
    "                        Set to 0 for unlimited (fetch all texts).\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    cache_dir = CACHE_DIR / \"gutenberg_texts\"\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Check for combined cache first (legacy)\n",
    "    legacy_cache = CACHE_DIR / \"gutenberg.json\"\n",
    "    if legacy_cache.exists():\n",
    "        with open(legacy_cache, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Gutenberg: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Fetching from GITenberg mirrors (JIT)...\")\n",
    "\n",
    "    # Western Philosophy texts by Gutenberg ID\n",
    "    # Format: (gutenberg_id, title, period)\n",
    "    texts = [\n",
    "        # Kant\n",
    "        (5683, \"Kant Critique of Practical Reason\", \"MODERN_ETHICS\"),\n",
    "        (5684, \"Kant Metaphysical Elements of Ethics\", \"MODERN_ETHICS\"),\n",
    "        (4280, \"Kant Critique of Pure Reason\", \"MODERN_ETHICS\"),\n",
    "        # Mill\n",
    "        (11224, \"Mill Utilitarianism\", \"MODERN_ETHICS\"),\n",
    "        (34901, \"Mill On Liberty\", \"MODERN_ETHICS\"),\n",
    "        # Spinoza\n",
    "        (3800, \"Spinoza Ethics\", \"MODERN_ETHICS\"),\n",
    "        # Aristotle\n",
    "        (8438, \"Aristotle Nicomachean Ethics\", \"CLASSICAL_GREEK\"),\n",
    "        # Plato\n",
    "        (1497, \"Plato Republic\", \"CLASSICAL_GREEK\"),\n",
    "        (1656, \"Plato Apology\", \"CLASSICAL_GREEK\"),\n",
    "        # Stoics\n",
    "        (10661, \"Epictetus Discourses\", \"HELLENISTIC\"),\n",
    "        (2680, \"Marcus Aurelius Meditations\", \"HELLENISTIC\"),\n",
    "        # New Testament & Apocrypha (KJV) - Christian ethics\n",
    "        (10, \"Bible KJV Complete\", \"BIBLICAL_CHRISTIAN\"),  # Complete Bible (80 books)\n",
    "        (124, \"Apocrypha Deuterocanonical\", \"APOCRYPHA\"),  # Tobit, Judith, Wisdom, Sirach, Maccabees\n",
    "        # Catechisms - Christian doctrine/ethics\n",
    "        (1670, \"Luther Small Catechism\", \"REFORMATION\"),\n",
    "        (1722, \"Luther Large Catechism\", \"REFORMATION\"),\n",
    "        # American practical ethics\n",
    "    ]\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; BIP-Corpus/1.0)\"}\n",
    "\n",
    "    def gutenberg_download(gutenberg_id: int) -> str | None:\n",
    "        \"\"\"Download text from Project Gutenberg by ID. Like R's gutenbergr::gutenberg_download().\"\"\"\n",
    "        # Primary: direct UTF-8 URL (works from most locations)\n",
    "        urls = [\n",
    "            f\"https://www.gutenberg.org/ebooks/{gutenberg_id}.txt.utf-8\",\n",
    "            f\"https://www.gutenberg.org/cache/epub/{gutenberg_id}/pg{gutenberg_id}.txt\",\n",
    "        ]\n",
    "        for url in urls:\n",
    "            try:\n",
    "                r = requests.get(url, headers=headers, timeout=60)\n",
    "                if r.status_code == 200:\n",
    "                    return r.text\n",
    "            except Exception:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "    def extract_passages(content: str, guten_id: int, name: str, period: str) -> list[dict]:\n",
    "        \"\"\"Extract paragraphs from Gutenberg text content.\"\"\"\n",
    "        results = []\n",
    "        # Normalize line endings (Gutenberg uses \\r\\n)\n",
    "        content = content.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "        # Skip Gutenberg header/footer\n",
    "        start_marker = \"*** START OF\"\n",
    "        end_marker = \"*** END OF\"\n",
    "        start_idx = content.find(start_marker)\n",
    "        end_idx = content.find(end_marker)\n",
    "        if start_idx > 0:\n",
    "            content = content[start_idx:]\n",
    "            newline_idx = content.find(\"\\n\")\n",
    "            if newline_idx > 0:\n",
    "                content = content[newline_idx + 1 :]\n",
    "        if end_idx > 0 and start_idx > 0:\n",
    "            content = content[: end_idx - start_idx - 100]\n",
    "        elif end_idx > 0:\n",
    "            content = content[:end_idx]\n",
    "\n",
    "        # Split into paragraphs\n",
    "        paras = content.split(\"\\n\\n\")\n",
    "        count = 0\n",
    "        for para in paras:\n",
    "            para = para.strip().replace(\"\\n\", \" \")\n",
    "            para = \" \".join(para.split())\n",
    "            if len(para) > 100 and len(para) < 2000:\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"id\": f\"gutenberg_{guten_id}_{count}\",\n",
    "                        \"text\": para,\n",
    "                        \"language\": \"english\",\n",
    "                        \"source\": name,\n",
    "                        \"time_periods\": [period],\n",
    "                    }\n",
    "                )\n",
    "                count += 1\n",
    "        return results\n",
    "\n",
    "    # JIT loading: fetch texts one at a time, stop when we have enough\n",
    "    for guten_id, name, period in texts:\n",
    "        # Check individual cache first\n",
    "        text_cache = cache_dir / f\"{guten_id}.json\"\n",
    "        if text_cache.exists():\n",
    "            with open(text_cache, encoding=\"utf-8\") as f:\n",
    "                text_passages = json.load(f)\n",
    "                passages.extend(text_passages)\n",
    "                print(f\"    {name}: {len(text_passages):,} passages (cached)\")\n",
    "        else:\n",
    "            # Download by ID (like R's gutenbergr::gutenberg_download)\n",
    "            time.sleep(0.3)  # Rate limit\n",
    "            content = gutenberg_download(guten_id)\n",
    "\n",
    "            if content:\n",
    "                text_passages = extract_passages(content, guten_id, name, period)\n",
    "                if text_passages:\n",
    "                    # Cache this text individually\n",
    "                    with open(text_cache, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(text_passages, f, ensure_ascii=False)\n",
    "                    passages.extend(text_passages)\n",
    "                    print(f\"    {name}: {len(text_passages):,} passages\")\n",
    "                else:\n",
    "                    print(f\"    {name}: no passages extracted\")\n",
    "            else:\n",
    "                print(f\"    {name}: download failed (ID {guten_id})\")\n",
    "\n",
    "        # JIT early stop: if we have enough, stop fetching\n",
    "        if target_passages > 0 and len(passages) >= target_passages:\n",
    "            print(f\"    (reached {target_passages:,} target, stopping)\")\n",
    "            break\n",
    "\n",
    "    print(f\"  Gutenberg: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# NATIVE AMERICAN & WORLD FOLKLORE - HuggingFace (VERIFIED)\n",
    "# Source: merve/folk-mythology-tales (246K stories from Ashliman Folktexts)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_folk_mythology() -> list[dict]:\n",
    "    \"\"\"Load folk tales and mythology including Native American from HuggingFace.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"folk_mythology.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Folk/Mythology: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Loading folk-mythology-tales from HuggingFace...\")\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "\n",
    "        ds = load_dataset(\"merve/folk-mythology-tales\", split=\"train\")\n",
    "\n",
    "        for i, item in enumerate(ds):\n",
    "            text = item.get(\"text\", \"\")\n",
    "            if text and len(text) > 50:\n",
    "                passages.append(\n",
    "                    {\n",
    "                        \"id\": f\"folk_{i}\",\n",
    "                        \"text\": text.strip()[:2000],  # Limit length\n",
    "                        \"language\": \"english\",\n",
    "                        \"source\": \"Ashliman Folktexts\",\n",
    "                        \"time_periods\": [\"FOLKLORE\", \"TRADITIONAL\"],\n",
    "                    }\n",
    "                )\n",
    "                if len(passages) >= 50000:  # Limit total\n",
    "                    break\n",
    "        print(f\"    Loaded {len(passages):,} folk tales\")\n",
    "    except ImportError:\n",
    "        print(\"    datasets library not available, skipping\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  Folk/Mythology: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ROMANCE LANGUAGE PHILOSOPHY - Project Gutenberg (direct download by ID)\n",
    "# Spanish: Don Quixote, La Celestina, Lazarillo de Tormes\n",
    "# French: Montaigne, Voltaire, Rousseau, Pascal\n",
    "# Italian: Machiavelli, Dante, Boccaccio | Portuguese: Cam\u00f5es\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_romance_philosophy(target_passages: int = 10000) -> list[dict]:\n",
    "    \"\"\"Load Romance language philosophy from Project Gutenberg by ID.\n",
    "\n",
    "    Uses gutenberg_download(id) like R's gutenbergr package.\n",
    "    JIT loading: fetches texts one at a time, caches individually.\n",
    "\n",
    "    Args:\n",
    "        target_passages: Stop fetching after reaching this many passages.\n",
    "                        Set to 0 for unlimited (fetch all texts).\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    cache_dir = CACHE_DIR / \"romance_texts\"\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Check for legacy combined cache\n",
    "    legacy_cache = CACHE_DIR / \"romance_philosophy.json\"\n",
    "    if legacy_cache.exists():\n",
    "        with open(legacy_cache, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Romance Philosophy: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Fetching Romance philosophy (JIT)...\")\n",
    "\n",
    "    # Romance language texts by Gutenberg ID\n",
    "    # Format: (gutenberg_id, title, language, period)\n",
    "    texts = [\n",
    "        # Spanish\n",
    "        (996, \"Don Quixote\", \"spanish\", \"SPANISH_GOLDEN_AGE\"),\n",
    "        (1619, \"La Celestina\", \"spanish\", \"SPANISH_GOLDEN_AGE\"),\n",
    "        (320, \"Lazarillo de Tormes\", \"spanish\", \"SPANISH_GOLDEN_AGE\"),\n",
    "        # French\n",
    "        (19942, \"Candide (Voltaire)\", \"french\", \"FRENCH_ENLIGHTENMENT\"),\n",
    "        (46333, \"Social Contract (Rousseau)\", \"french\", \"FRENCH_ENLIGHTENMENT\"),\n",
    "        (3600, \"Essais de Montaigne\", \"french\", \"FRENCH_RENAISSANCE\"),\n",
    "        (18269, \"Pens\u00e9es (Pascal)\", \"french\", \"FRENCH_ENLIGHTENMENT\"),\n",
    "        # Italian\n",
    "        (1232, \"The Prince (Machiavelli)\", \"italian\", \"ITALIAN_RENAISSANCE\"),\n",
    "        (1004, \"Divine Comedy (Dante)\", \"italian\", \"MEDIEVAL_ITALIAN\"),\n",
    "        (3726, \"Decameron Vol I (Boccaccio)\", \"italian\", \"MEDIEVAL_ITALIAN\"),\n",
    "        (13102, \"Decameron Vol II (Boccaccio)\", \"italian\", \"MEDIEVAL_ITALIAN\"),\n",
    "        # Portuguese\n",
    "        (3333, \"Os Lus\u00edadas (Cam\u00f5es)\", \"portuguese\", \"PORTUGUESE_RENAISSANCE\"),\n",
    "    ]\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; BIP-Corpus/1.0)\"}\n",
    "\n",
    "    def gutenberg_download(gutenberg_id: int) -> str | None:\n",
    "        \"\"\"Download text from Project Gutenberg by ID.\"\"\"\n",
    "        urls = [\n",
    "            f\"https://www.gutenberg.org/ebooks/{gutenberg_id}.txt.utf-8\",\n",
    "            f\"https://www.gutenberg.org/cache/epub/{gutenberg_id}/pg{gutenberg_id}.txt\",\n",
    "        ]\n",
    "        for url in urls:\n",
    "            try:\n",
    "                r = requests.get(url, headers=headers, timeout=60)\n",
    "                if r.status_code == 200:\n",
    "                    return r.text\n",
    "            except Exception:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "    def extract_passages(content: str, guten_id: int, name: str, lang: str, period: str) -> list[dict]:\n",
    "        \"\"\"Extract paragraphs from Gutenberg text content.\"\"\"\n",
    "        results = []\n",
    "        # Normalize line endings (Gutenberg uses \\r\\n)\n",
    "        content = content.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "        # Skip Gutenberg header/footer\n",
    "        start_marker = \"*** START OF\"\n",
    "        end_marker = \"*** END OF\"\n",
    "        start_idx = content.find(start_marker)\n",
    "        end_idx = content.find(end_marker)\n",
    "        if start_idx > 0:\n",
    "            content = content[start_idx:]\n",
    "            nl = content.find(\"\\n\")\n",
    "            if nl > 0:\n",
    "                content = content[nl + 1 :]\n",
    "        if end_idx > 0 and start_idx > 0:\n",
    "            content = content[: end_idx - start_idx - 100]\n",
    "        elif end_idx > 0:\n",
    "            content = content[:end_idx]\n",
    "\n",
    "        # Split into paragraphs\n",
    "        paras = content.split(\"\\n\\n\")\n",
    "        count = 0\n",
    "        for para in paras:\n",
    "            para = para.strip().replace(\"\\n\", \" \")\n",
    "            para = \" \".join(para.split())\n",
    "            if len(para) > 100 and len(para) < 2000:\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"id\": f\"romance_{guten_id}_{count}\",\n",
    "                        \"text\": para,\n",
    "                        \"language\": lang,\n",
    "                        \"source\": name,\n",
    "                        \"time_periods\": [period],\n",
    "                    }\n",
    "                )\n",
    "                count += 1\n",
    "        return results\n",
    "\n",
    "    # JIT loading: fetch texts one at a time\n",
    "    for guten_id, name, lang, period in texts:\n",
    "        text_cache = cache_dir / f\"{guten_id}.json\"\n",
    "        if text_cache.exists():\n",
    "            with open(text_cache, encoding=\"utf-8\") as f:\n",
    "                text_passages = json.load(f)\n",
    "                passages.extend(text_passages)\n",
    "                print(f\"    {name}: {len(text_passages):,} passages (cached)\")\n",
    "        else:\n",
    "            # Download by ID (like R's gutenbergr::gutenberg_download)\n",
    "            time.sleep(0.3)  # Rate limit\n",
    "            content = gutenberg_download(guten_id)\n",
    "\n",
    "            if content:\n",
    "                text_passages = extract_passages(content, guten_id, name, lang, period)\n",
    "                if text_passages:\n",
    "                    with open(text_cache, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(text_passages, f, ensure_ascii=False)\n",
    "                    passages.extend(text_passages)\n",
    "                    print(f\"    {name}: {len(text_passages):,} passages\")\n",
    "                else:\n",
    "                    print(f\"    {name}: no passages extracted\")\n",
    "            else:\n",
    "                print(f\"    {name}: download failed (ID {guten_id})\")\n",
    "\n",
    "        # JIT early stop\n",
    "        if target_passages > 0 and len(passages) >= target_passages:\n",
    "            print(f\"    (reached {target_passages:,} target, stopping)\")\n",
    "            break\n",
    "\n",
    "    print(f\"  Romance Philosophy: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ENGLISH ETHICS - Dear Abby (68K letters)\n",
    "# Source: https://github.com/Mac-STAT/data (VERIFIED)\n",
    "# ============================================================================\n",
    "\n",
    "DEAR_ABBY_URL = \"https://raw.githubusercontent.com/Mac-STAT/data/main/dear_abby.csv\"\n",
    "\n",
    "\n",
    "def load_dear_abby() -> list[dict]:\n",
    "    \"\"\"Load Dear Abby advice columns (68K letters) from Mac-STAT GitHub.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"dear_abby.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Dear Abby: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    # Check local file first\n",
    "    local_paths = [\n",
    "        Path(\"data/raw/dear_abby.csv\"),\n",
    "        DATA_DIR / \"dear_abby.csv\",\n",
    "    ]\n",
    "\n",
    "    csv_file = None\n",
    "    for p in local_paths:\n",
    "        if p.exists():\n",
    "            csv_file = p\n",
    "            print(f\"  Found local: {csv_file}\")\n",
    "            break\n",
    "\n",
    "    # Download if not local\n",
    "    if not csv_file:\n",
    "        print(\"  Downloading Dear Abby from GitHub (17.9MB)...\")\n",
    "        csv_file = DATA_DIR / \"dear_abby.csv\"\n",
    "        try:\n",
    "            resp = requests.get(DEAR_ABBY_URL, timeout=120)\n",
    "            if resp.status_code == 200:\n",
    "                csv_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                with open(csv_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(resp.text)\n",
    "                print(f\"    Downloaded to {csv_file}\")\n",
    "            else:\n",
    "                print(f\"    Failed: HTTP {resp.status_code}\")\n",
    "                return passages\n",
    "        except Exception as e:\n",
    "            print(f\"    Download failed: {e}\")\n",
    "            return passages\n",
    "\n",
    "    # Parse CSV using pandas for better handling of multi-line fields\n",
    "    print(\"  Parsing Dear Abby CSV...\")\n",
    "    try:\n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.read_csv(csv_file, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "        print(f\"    CSV has {len(df):,} rows, columns: {list(df.columns)}\")\n",
    "\n",
    "        skipped_empty = 0\n",
    "        skipped_short = 0\n",
    "        for _, row in df.iterrows():\n",
    "            # Primary column is \"question_only\"\n",
    "            text = str(row.get(\"question_only\", \"\"))\n",
    "            if not text or text == \"nan\" or pd.isna(row.get(\"question_only\")):\n",
    "                skipped_empty += 1\n",
    "                continue\n",
    "            if len(text) <= 50:\n",
    "                skipped_short += 1\n",
    "                continue\n",
    "            year = row.get(\"year\", \"\")\n",
    "            passages.append(\n",
    "                {\n",
    "                    \"id\": f\"abby_{row.get('letterId', len(passages))}\",\n",
    "                    \"text\": text.strip(),\n",
    "                    \"language\": \"english\",\n",
    "                    \"source\": f\"Dear Abby {year}\",\n",
    "                    \"time_periods\": [\"DEAR_ABBY\", \"MODERN\", \"ENGLISH_ETHICS\"],\n",
    "                }\n",
    "            )\n",
    "        print(\n",
    "            f\"    Loaded {len(passages):,} letters (skipped: {skipped_empty} empty, {skipped_short} short)\"\n",
    "        )\n",
    "    except ImportError:\n",
    "        # Fallback to csv module if pandas not available\n",
    "        print(\"    pandas not available, using csv module\")\n",
    "        with open(csv_file, encoding=\"utf-8\", newline=\"\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                text = row.get(\"question_only\", \"\")\n",
    "                if text and len(text) > 50:\n",
    "                    passages.append(\n",
    "                        {\n",
    "                            \"id\": f\"abby_{row.get('letterId', len(passages))}\",\n",
    "                            \"text\": text.strip(),\n",
    "                            \"language\": \"english\",\n",
    "                            \"source\": f\"Dear Abby {row.get('year', '')}\",\n",
    "                            \"time_periods\": [\"DEAR_ABBY\", \"MODERN\", \"ENGLISH_ETHICS\"],\n",
    "                        }\n",
    "                    )\n",
    "        print(f\"    Loaded {len(passages):,} letters\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed to parse CSV: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  Dear Abby: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ENGLISH ETHICS - hendrycks/ethics (134K examples) - SUPPLEMENTAL\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def load_hendrycks_ethics() -> list[dict]:\n",
    "    \"\"\"Load ethics scenarios from hendrycks/ethics dataset (supplemental).\n",
    "\n",
    "    Downloads directly from Berkeley (HuggingFace loader is deprecated).\n",
    "    \"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"hendrycks_ethics.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  hendrycks/ethics: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Downloading hendrycks/ethics from Berkeley...\")\n",
    "\n",
    "    import tarfile\n",
    "    import io\n",
    "\n",
    "    TAR_URL = \"https://people.eecs.berkeley.edu/~hendrycks/ethics.tar\"\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(TAR_URL, timeout=120)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"    Download failed: {e}\")\n",
    "        return passages\n",
    "\n",
    "    # Extract CSVs from tar\n",
    "    subsets = {\n",
    "        \"commonsense\": (\"cm_train.csv\", \"input\"),\n",
    "        \"deontology\": (\"deontology_train.csv\", \"scenario\"),\n",
    "        \"justice\": (\"justice_train.csv\", \"scenario\"),\n",
    "        \"utilitarianism\": (\"util_train.csv\", \"baseline\"),  # Note: different name\n",
    "        \"virtue\": (\"virtue_train.csv\", \"scenario\"),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        tar_bytes = io.BytesIO(resp.content)\n",
    "        with tarfile.open(fileobj=tar_bytes, mode=\"r:\") as tar:\n",
    "            for subset, (filename, text_col) in subsets.items():\n",
    "                # Find the file in the archive (may be in subdirectory)\n",
    "                csv_member = None\n",
    "                for member in tar.getmembers():\n",
    "                    if member.name.endswith(filename):\n",
    "                        csv_member = member\n",
    "                        break\n",
    "\n",
    "                if not csv_member:\n",
    "                    print(f\"    {subset}: file not found ({filename})\")\n",
    "                    continue\n",
    "\n",
    "                # Extract and parse CSV\n",
    "                csv_file = tar.extractfile(csv_member)\n",
    "                if csv_file is None:\n",
    "                    continue\n",
    "\n",
    "                csv_content = csv_file.read().decode(\"utf-8\")\n",
    "                reader = csv.DictReader(io.StringIO(csv_content))\n",
    "\n",
    "                count = 0\n",
    "                for row in reader:\n",
    "                    text = row.get(text_col, \"\")\n",
    "                    if text and len(text) > 30:\n",
    "                        passages.append({\n",
    "                            \"id\": f\"ethics_{subset}_{count}\",\n",
    "                            \"text\": text.strip(),\n",
    "                            \"language\": \"english\",\n",
    "                            \"source\": f\"hendrycks/ethics/{subset}\",\n",
    "                            \"time_periods\": [\"MODERN_ETHICS\", \"MODERN\", \"ENGLISH_ETHICS\"],\n",
    "                        })\n",
    "                        count += 1\n",
    "\n",
    "                print(f\"    {subset}: {count} passages\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    Tar extraction failed: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  hendrycks/ethics: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN LOADER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Fetching from verified external sources...\")\n",
    "print(f\"Cache only mode: {_cache_only}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "by_language = defaultdict(list)\n",
    "\n",
    "# Load all sources\n",
    "print(\"\\n[SANSKRIT]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"sanskrit\"].extend(load_itihasa_github())\n",
    "print(f\"  Elapsed: {time.time()-_t0:.1f}s\")\n",
    "\n",
    "print(\"\\n[PALI]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"pali\"].extend(load_pali_suttacentral())\n",
    "print(f\"  Elapsed: {time.time()-_t0:.1f}s\")\n",
    "\n",
    "print(\"\\n[ARABIC]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"arabic\"].extend(load_quran_tanzil())\n",
    "print(f\"  Elapsed: {time.time()-_t0:.1f}s\")\n",
    "\n",
    "print(\"\\n[HEBREW/ARAMAIC]\")\n",
    "_t0 = time.time()\n",
    "sefaria = load_sefaria_github()\n",
    "print(f\"  Elapsed: {time.time()-_t0:.1f}s\")\n",
    "for p in sefaria:\n",
    "    by_language[p[\"language\"]].append(p)\n",
    "\n",
    "print(\"\\n[CHINESE]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"classical_chinese\"].extend(load_chinese_ctext())\n",
    "print(f\"  Elapsed: {time.time()-_t0:.1f}s\")\n",
    "print(\"\\n[CHINESE BUDDHIST]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"classical_chinese\"].extend(load_chinese_buddhist())\n",
    "print(f\"  Elapsed: {time.time()-_t0:.1f}s\")  # CBETA Buddhist\n",
    "\n",
    "print(\"\\n[GREEK/LATIN]\")\n",
    "_t0 = time.time()\n",
    "perseus = load_perseus_github()\n",
    "print(f\"  Elapsed: {time.time()-_t0:.1f}s\")\n",
    "for p in perseus:\n",
    "    by_language[p[\"language\"]].append(p)\n",
    "\n",
    "print(\"\\n[WESTERN PHILOSOPHY]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"english\"].extend(load_gutenberg_philosophy())\n",
    "print(f\"  Elapsed: {time.time()-_t0:.1f}s\")\n",
    "\n",
    "print(\"\\n[ROMANCE LANGUAGES]\")\n",
    "_t0 = time.time()\n",
    "romance = load_romance_philosophy()\n",
    "print(f\"  Elapsed: {time.time()-_t0:.1f}s\")\n",
    "for p in romance:\n",
    "    by_language[p[\"language\"]].append(p)\n",
    "\n",
    "print(\"\\n[ENGLISH ETHICS]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"english\"].extend(load_dear_abby())\n",
    "by_language[\"english\"].extend(load_hendrycks_ethics())\n",
    "print(f\"  Elapsed: {time.time()-_t0:.1f}s\")  # Supplemental\n",
    "\n",
    "print(\"\\n[FOLKLORE/NATIVE AMERICAN]\")\n",
    "_t0 = time.time()\n",
    "by_language[\"english\"].extend(load_folk_mythology())\n",
    "print(f\"  Elapsed: {time.time()-_t0:.1f}s\")\n",
    "\n",
    "# ============================================================================\n",
    "# APPLY MEMORY LIMITS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Applying memory limits...\")\n",
    "for lang in list(by_language.keys()):\n",
    "    max_count = MAX_PASSAGES_PER_LANG.get(lang, MAX_PASSAGES_PER_LANG[\"default\"])\n",
    "    if len(by_language[lang]) > max_count:\n",
    "        original = len(by_language[lang])\n",
    "        by_language[lang] = by_language[lang][:max_count]\n",
    "        print(f\"  {lang}: {original:,} -> {max_count:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CORPUS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total = 0\n",
    "for lang, passages in sorted(by_language.items(), key=lambda x: -len(x[1])):\n",
    "    count = len(passages)\n",
    "    total += count\n",
    "    status = \"OK\" if count >= MIN_PASSAGES else f\"NEED {MIN_PASSAGES - count} MORE\"\n",
    "    print(f\"  {lang:20s}: {count:6,} passages  [{status}]\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"  {'TOTAL':20s}: {total:6,} passages\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE TO JSONL FOR LATER CELLS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nConverting to training format...\")\n",
    "\n",
    "all_passages = []\n",
    "for _lang, passages in by_language.items():\n",
    "    for p in passages:\n",
    "        all_passages.append(\n",
    "            {\n",
    "                \"id\": p[\"id\"],\n",
    "                \"text\": p[\"text\"],\n",
    "                \"language\": p[\"language\"],\n",
    "                \"source\": p[\"source\"],\n",
    "                \"time_periods\": p.get(\"time_periods\", [p.get(\"time_period\", \"UNKNOWN\")]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "with open(\"data/processed/passages.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in all_passages:\n",
    "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "print(\"Saved to data/processed/passages.jsonl\")\n",
    "\n",
    "# Cache to Drive if available\n",
    "if _save_dir and os.path.exists(os.path.dirname(_save_dir)):\n",
    "    os.makedirs(_save_dir, exist_ok=True)\n",
    "    import shutil\n",
    "\n",
    "    shutil.copy(\"data/processed/passages.jsonl\", f\"{_save_dir}/passages.jsonl\")\n",
    "    print(f\"Cached to {_save_dir}/passages.jsonl\")\n",
    "\n",
    "    # Also cache the corpus cache files to Drive for faster restarts\n",
    "    drive_cache = Path(_save_dir) / \"corpus_cache\"\n",
    "    drive_cache.mkdir(exist_ok=True)\n",
    "    cached_count = 0\n",
    "    for cache_file in CACHE_DIR.glob(\"*.json\"):\n",
    "        dest = drive_cache / cache_file.name\n",
    "        if not dest.exists() or dest.stat().st_size != cache_file.stat().st_size:\n",
    "            shutil.copy(cache_file, dest)\n",
    "            cached_count += 1\n",
    "    if cached_count:\n",
    "        print(f\"  Cached {cached_count} corpus files to Drive\")\n",
    "\n",
    "    # Note: sefaria.json (processed output) IS cached to Drive\n",
    "    # Raw Sefaria-Export not cached (50K files), but processed cache restores instantly\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CORPUS LOADING COMPLETE\")\n",
    "print(\"=\" * 60)\n"
   ],
   "id": "cell_2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2b. Load Ethics Datasets for Bond Extraction { display-mode: \"form\" }\n",
    "# @markdown Load ETHICS, Scruples, and EthicsSuite datasets for bond extraction training\n",
    "# @markdown These provide modern English moral reasoning examples with labeled judgments\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ### Dataset Selection\n",
    "LOAD_ETHICS_DATASET = True  # @param {type:\"boolean\"}\n",
    "# @markdown hendrycks/ethics: Justice, deontology, virtue, utilitarianism, commonsense\n",
    "\n",
    "LOAD_SCRUPLES_DATASET = True  # @param {type:\"boolean\"}\n",
    "# @markdown allenai/scruples: 32K real-life anecdotes with ethical judgments\n",
    "\n",
    "LOAD_ETHICSUITE_DATASET = True  # @param {type:\"boolean\"}\n",
    "# @markdown LLM-Ethics/EthicsSuite: 20K complex contextualized moral situations\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ### Size Limits (0 = unlimited)\n",
    "MAX_ETHICS_ITEMS = 50000  # @param {type:\"integer\"}\n",
    "MAX_SCRUPLES_ITEMS = 30000  # @param {type:\"integer\"}\n",
    "MAX_ETHICSUITE_ITEMS = 20000  # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ### Output Options\n",
    "EXPORT_BIP_FORMAT = True  # @param {type:\"boolean\"}\n",
    "# @markdown Export as BIP passages for integration with Cell 2 corpus\n",
    "\n",
    "CREATE_TRAIN_TEST_SPLIT = True  # @param {type:\"boolean\"}\n",
    "TEST_SPLIT_RATIO = 0.2  # @param {type:\"number\"}\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BOND EXTRACTION TRAINING DATA (v10.14)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BOND SCHEMA\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class BondAnnotation:\n",
    "    \"\"\"A moral bond extracted from text.\"\"\"\n",
    "    text: str\n",
    "    agent: Optional[str]\n",
    "    patient: Optional[str]\n",
    "    bond_type: str\n",
    "    hohfeld_state: str\n",
    "    context: str\n",
    "    confidence: float\n",
    "    source_dataset: str\n",
    "    source_category: str\n",
    "    raw_label: str\n",
    "\n",
    "\n",
    "BOND_TYPES = [\n",
    "    \"OBLIGATION\", \"PROHIBITION\", \"PERMISSION\", \"CLAIM\",\n",
    "    \"POWER\", \"IMMUNITY\", \"VIRTUE\", \"VICE\", \"SUPEREROGATORY\"\n",
    "]\n",
    "\n",
    "HOHFELD_STATES = [\n",
    "    \"DUTY\", \"CLAIM\", \"LIBERTY\", \"NO_CLAIM\",\n",
    "    \"POWER\", \"LIABILITY\", \"IMMUNITY\", \"DISABILITY\"\n",
    "]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ETHICS DATASET LOADER\n",
    "# =============================================================================\n",
    "\n",
    "class EthicsLoader:\n",
    "    \"\"\"Load hendrycks/ethics dataset.\"\"\"\n",
    "\n",
    "    CATEGORY_TO_BOND = {\n",
    "        \"deontology\": (\"OBLIGATION\", \"DUTY\"),\n",
    "        \"justice\": (\"CLAIM\", \"CLAIM\"),\n",
    "        \"virtue\": (\"VIRTUE\", \"DUTY\"),\n",
    "        \"utilitarianism\": (\"PERMISSION\", \"LIBERTY\"),\n",
    "        \"commonsense\": (\"OBLIGATION\", \"DUTY\"),\n",
    "    }\n",
    "\n",
    "    def load(self, max_items: int = 0) -> List[BondAnnotation]:\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "        except ImportError:\n",
    "            print(\"  Installing datasets library...\")\n",
    "            os.system(\"pip install datasets -q\")\n",
    "            from datasets import load_dataset\n",
    "\n",
    "        annotations = []\n",
    "        categories = [\"commonsense\", \"deontology\", \"justice\", \"utilitarianism\", \"virtue\"]\n",
    "\n",
    "        for category in categories:\n",
    "            if max_items > 0 and len(annotations) >= max_items:\n",
    "                break\n",
    "\n",
    "            print(f\"  Loading ETHICS/{category}...\")\n",
    "            try:\n",
    "                dataset = load_dataset(\"hendrycks/ethics\", category)\n",
    "\n",
    "                for split in [\"train\", \"test\"]:\n",
    "                    if split not in dataset:\n",
    "                        continue\n",
    "                    for item in dataset[split]:\n",
    "                        if max_items > 0 and len(annotations) >= max_items:\n",
    "                            break\n",
    "\n",
    "                        text = item.get(\"input\") or item.get(\"scenario\") or item.get(\"text\", \"\")\n",
    "                        if not text or len(text) < 10:\n",
    "                            continue\n",
    "\n",
    "                        label = item.get(\"label\", 0)\n",
    "                        bond_type, hohfeld = self.CATEGORY_TO_BOND.get(category, (\"OBLIGATION\", \"DUTY\"))\n",
    "\n",
    "                        # Extract agent/patient\n",
    "                        agent, patient = self._extract_roles(text)\n",
    "\n",
    "                        if label == 1:\n",
    "                            context = \"descriptive\"\n",
    "                            if bond_type == \"OBLIGATION\":\n",
    "                                bond_type = \"PROHIBITION\"\n",
    "                        else:\n",
    "                            context = \"prescriptive\"\n",
    "\n",
    "                        annotations.append(BondAnnotation(\n",
    "                            text=text[:500],\n",
    "                            agent=agent,\n",
    "                            patient=patient,\n",
    "                            bond_type=bond_type,\n",
    "                            hohfeld_state=hohfeld,\n",
    "                            context=context,\n",
    "                            confidence=0.8,\n",
    "                            source_dataset=\"ethics\",\n",
    "                            source_category=category,\n",
    "                            raw_label=str(label),\n",
    "                        ))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    Warning: {e}\")\n",
    "\n",
    "        return annotations\n",
    "\n",
    "    def _extract_roles(self, text: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "        agent = patient = None\n",
    "\n",
    "        if re.match(r\"^I\\s+(should|must|ought)\", text, re.I):\n",
    "            agent = \"speaker\"\n",
    "        elif re.match(r\"^You\\s+(should|must|ought)\", text, re.I):\n",
    "            agent = \"addressee\"\n",
    "        else:\n",
    "            match = re.match(r\"^([A-Z][a-z]+)\\s+(should|must|ought)\", text)\n",
    "            if match:\n",
    "                agent = match.group(1).lower()\n",
    "\n",
    "        patient_match = re.search(r\"(help|protect|harm|hurt)\\s+(\\w+)\", text, re.I)\n",
    "        if patient_match:\n",
    "            p = patient_match.group(2).lower()\n",
    "            if p not in [\"the\", \"a\", \"an\", \"my\", \"your\"]:\n",
    "                patient = p\n",
    "\n",
    "        return agent, patient\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SCRUPLES DATASET LOADER\n",
    "# =============================================================================\n",
    "\n",
    "class ScruplesLoader:\n",
    "    \"\"\"Load allenai/scruples dataset.\"\"\"\n",
    "\n",
    "    LABEL_TO_BOND = {\n",
    "        \"AUTHOR_WRONG\": (\"PROHIBITION\", \"DUTY\", \"author\"),\n",
    "        \"OTHER_WRONG\": (\"PROHIBITION\", \"DUTY\", \"other\"),\n",
    "        \"EVERYBODY_WRONG\": (\"PROHIBITION\", \"DUTY\", \"both\"),\n",
    "        \"NOBODY_WRONG\": (\"PERMISSION\", \"LIBERTY\", None),\n",
    "        \"INFO\": (\"OBLIGATION\", \"DUTY\", None),\n",
    "    }\n",
    "\n",
    "    ANECDOTES_URL = \"https://storage.googleapis.com/ai2-mosaic-public/projects/scruples/v1.0/data/anecdotes.tar.gz\"\n",
    "    DILEMMAS_URL = \"https://storage.googleapis.com/ai2-mosaic-public/projects/scruples/v1.0/data/dilemmas.tar.gz\"\n",
    "\n",
    "    def load(self, max_items: int = 0) -> List[BondAnnotation]:\n",
    "        import tarfile\n",
    "        import requests\n",
    "\n",
    "        cache_dir = Path(\"data/ethics_cache\")\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        annotations = []\n",
    "\n",
    "        # Load anecdotes from Google Cloud\n",
    "        print(\"  Loading Scruples/anecdotes...\")\n",
    "        anecdotes_cache = cache_dir / \"scruples_anecdotes.tar.gz\"\n",
    "        try:\n",
    "            if not anecdotes_cache.exists():\n",
    "                print(\"    Downloading from Google Cloud...\")\n",
    "                resp = requests.get(self.ANECDOTES_URL, timeout=120)\n",
    "                resp.raise_for_status()\n",
    "                with open(anecdotes_cache, \"wb\") as f:\n",
    "                    f.write(resp.content)\n",
    "                print(f\"    Downloaded {len(resp.content)//1024}KB\")\n",
    "\n",
    "            with tarfile.open(anecdotes_cache, \"r:gz\") as tar:\n",
    "                for member in tar.getmembers():\n",
    "                    if not member.name.endswith(\".jsonl\"):\n",
    "                        continue\n",
    "                    f = tar.extractfile(member)\n",
    "                    if f is None:\n",
    "                        continue\n",
    "                    for line in f:\n",
    "                        if max_items > 0 and len(annotations) >= max_items:\n",
    "                            break\n",
    "                        try:\n",
    "                            item = json.loads(line.decode(\"utf-8\"))\n",
    "                        except:\n",
    "                            continue\n",
    "                        title = item.get(\"title\", \"\")\n",
    "                        text = item.get(\"text\", \"\")\n",
    "                        full_text = f\"{title} {text}\".strip()\n",
    "                        if len(full_text) < 20:\n",
    "                            continue\n",
    "                        label = item.get(\"binarized_label\", item.get(\"label\", 0))\n",
    "                        bond_info = self.LABEL_TO_BOND.get(label, (\"OBLIGATION\", \"DUTY\", None))\n",
    "                        if len(bond_info) == 3:\n",
    "                            bond_type, hohfeld, violator = bond_info\n",
    "                        else:\n",
    "                            bond_type, hohfeld = bond_info[:2]\n",
    "                            violator = None\n",
    "                        agent = patient = None\n",
    "                        if violator == \"author\":\n",
    "                            agent, patient = \"author\", \"other\"\n",
    "                        elif violator == \"other\":\n",
    "                            agent, patient = \"other\", \"author\"\n",
    "                        annotations.append(BondAnnotation(\n",
    "                            text=full_text[:500],\n",
    "                            agent=agent,\n",
    "                            patient=patient,\n",
    "                            bond_type=bond_type,\n",
    "                            hohfeld_state=hohfeld,\n",
    "                            context=\"descriptive\",\n",
    "                            confidence=0.7,\n",
    "                            source_dataset=\"scruples\",\n",
    "                            source_category=\"anecdotes\",\n",
    "                            raw_label=str(label),\n",
    "                        ))\n",
    "            print(f\"    Loaded {len(annotations)} anecdotes\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Dataset 'allenai/scruples' - {e}\")\n",
    "\n",
    "        # Load dilemmas\n",
    "        print(\"  Loading Scruples/dilemmas...\")\n",
    "        dilemmas_cache = cache_dir / \"scruples_dilemmas.tar.gz\"\n",
    "        dilemma_start = len(annotations)\n",
    "        try:\n",
    "            if not dilemmas_cache.exists():\n",
    "                resp = requests.get(self.DILEMMAS_URL, timeout=120)\n",
    "                resp.raise_for_status()\n",
    "                with open(dilemmas_cache, \"wb\") as f:\n",
    "                    f.write(resp.content)\n",
    "\n",
    "            with tarfile.open(dilemmas_cache, \"r:gz\") as tar:\n",
    "                for member in tar.getmembers():\n",
    "                    if not member.name.endswith(\".jsonl\"):\n",
    "                        continue\n",
    "                    f = tar.extractfile(member)\n",
    "                    if f is None:\n",
    "                        continue\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            item = json.loads(line.decode(\"utf-8\"))\n",
    "                        except:\n",
    "                            continue\n",
    "                        action1 = item.get(\"action1\", \"\")\n",
    "                        action2 = item.get(\"action2\", \"\")\n",
    "                        text = f\"Choice A: {action1} Choice B: {action2}\"\n",
    "                        if len(text) < 20:\n",
    "                            continue\n",
    "                        annotations.append(BondAnnotation(\n",
    "                            text=text[:500],\n",
    "                            agent=\"actor\",\n",
    "                            patient=\"affected\",\n",
    "                            bond_type=\"OBLIGATION\",\n",
    "                            hohfeld_state=\"DUTY\",\n",
    "                            context=\"hypothetical\",\n",
    "                            confidence=0.6,\n",
    "                            source_dataset=\"scruples\",\n",
    "                            source_category=\"dilemmas\",\n",
    "                            raw_label=str(item.get(\"label\", 0)),\n",
    "                        ))\n",
    "            print(f\"    Loaded {len(annotations) - dilemma_start} dilemmas\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Dataset 'allenai/scruples' - {e}\")\n",
    "\n",
    "        return annotations\n",
    "\n",
    "    def _load_hf_legacy(self, max_items: int = 0) -> List[BondAnnotation]:\n",
    "        \"\"\"Legacy HuggingFace loader (no longer works).\"\"\"\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "            dataset = load_dataset(\"allenai/scruples\", \"anecdotes\")\n",
    "\n",
    "            for split in [\"train\", \"dev\", \"test\"]:\n",
    "                if split not in dataset:\n",
    "                    continue\n",
    "                for item in dataset[split]:\n",
    "                    if max_items > 0 and len(annotations) >= max_items:\n",
    "                        break\n",
    "\n",
    "                    title = item.get(\"title\", \"\")\n",
    "                    text = item.get(\"text\", \"\")\n",
    "                    full_text = f\"{title}\\n{text}\" if title else text\n",
    "\n",
    "                    if len(full_text) < 20:\n",
    "                        continue\n",
    "\n",
    "                    label = item.get(\"binarized_label\") or item.get(\"label\", \"INFO\")\n",
    "                    if isinstance(label, int):\n",
    "                        label = \"AUTHOR_WRONG\" if label == 1 else \"NOBODY_WRONG\"\n",
    "\n",
    "                    bond_type, hohfeld, violator = self.LABEL_TO_BOND.get(label, (\"OBLIGATION\", \"DUTY\", None))\n",
    "\n",
    "                    agent = patient = None\n",
    "                    if violator == \"author\":\n",
    "                        agent, patient = \"author\", \"other\"\n",
    "                    elif violator == \"other\":\n",
    "                        agent, patient = \"other\", \"author\"\n",
    "                    elif violator == \"both\":\n",
    "                        agent = patient = \"both\"\n",
    "\n",
    "                    annotations.append(BondAnnotation(\n",
    "                        text=full_text[:500],\n",
    "                        agent=agent,\n",
    "                        patient=patient,\n",
    "                        bond_type=bond_type,\n",
    "                        hohfeld_state=hohfeld,\n",
    "                        context=\"descriptive\",\n",
    "                        confidence=0.7,\n",
    "                        source_dataset=\"scruples\",\n",
    "                        source_category=\"anecdotes\",\n",
    "                        raw_label=label,\n",
    "                    ))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: {e}\")\n",
    "\n",
    "        # Load dilemmas\n",
    "        print(\"  Loading Scruples/dilemmas...\")\n",
    "        try:\n",
    "            dataset = load_dataset(\"allenai/scruples\", \"dilemmas\")\n",
    "            dilemma_limit = max_items // 3 if max_items > 0 else 0\n",
    "\n",
    "            count = 0\n",
    "            for split in [\"train\", \"dev\", \"test\"]:\n",
    "                if split not in dataset:\n",
    "                    continue\n",
    "                for item in dataset[split]:\n",
    "                    if dilemma_limit > 0 and count >= dilemma_limit:\n",
    "                        break\n",
    "\n",
    "                    action1 = item.get(\"action1\", \"\")\n",
    "                    action2 = item.get(\"action2\", \"\")\n",
    "                    text = f\"Choice A: {action1}\\nChoice B: {action2}\"\n",
    "\n",
    "                    if len(text) < 20:\n",
    "                        continue\n",
    "\n",
    "                    annotations.append(BondAnnotation(\n",
    "                        text=text[:500],\n",
    "                        agent=\"actor\",\n",
    "                        patient=\"affected\",\n",
    "                        bond_type=\"OBLIGATION\",\n",
    "                        hohfeld_state=\"DUTY\",\n",
    "                        context=\"hypothetical\",\n",
    "                        confidence=0.6,\n",
    "                        source_dataset=\"scruples\",\n",
    "                        source_category=\"dilemmas\",\n",
    "                        raw_label=str(item.get(\"label\", 0)),\n",
    "                    ))\n",
    "                    count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: {e}\")\n",
    "\n",
    "        return annotations\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ETHICSUITE LOADER\n",
    "# =============================================================================\n",
    "\n",
    "class EthicsSuiteLoader:\n",
    "    \"\"\"Load LLM-Ethics/EthicsSuite dataset.\"\"\"\n",
    "\n",
    "    def load(self, max_items: int = 0) -> List[BondAnnotation]:\n",
    "        import urllib.request\n",
    "\n",
    "        url = \"https://raw.githubusercontent.com/LLM-Ethics/EthicsSuite/main/data.jsonl\"\n",
    "        cache_dir = Path(\"data/ethics_cache\")\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        cache_file = cache_dir / \"ethicsuite.jsonl\"\n",
    "\n",
    "        annotations = []\n",
    "\n",
    "        print(\"  Loading EthicsSuite...\")\n",
    "        try:\n",
    "            if not cache_file.exists():\n",
    "                print(\"    Downloading...\")\n",
    "                urllib.request.urlretrieve(url, cache_file)\n",
    "\n",
    "            category_map = {\n",
    "                \"deontology\": (\"OBLIGATION\", \"DUTY\"),\n",
    "                \"justice\": (\"CLAIM\", \"CLAIM\"),\n",
    "                \"virtue\": (\"VIRTUE\", \"DUTY\"),\n",
    "                \"utilitarianism\": (\"PERMISSION\", \"LIBERTY\"),\n",
    "                \"commonsense\": (\"OBLIGATION\", \"DUTY\"),\n",
    "            }\n",
    "\n",
    "            with open(cache_file, encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if max_items > 0 and len(annotations) >= max_items:\n",
    "                        break\n",
    "\n",
    "                    item = json.loads(line)\n",
    "                    text = item.get(\"text\", \"\")\n",
    "                    if len(text) < 20:\n",
    "                        continue\n",
    "\n",
    "                    source = item.get(\"source\", \"unknown\")\n",
    "                    bond_type, hohfeld = category_map.get(source, (\"OBLIGATION\", \"DUTY\"))\n",
    "\n",
    "                    annotations.append(BondAnnotation(\n",
    "                        text=text[:500],\n",
    "                        agent=None,\n",
    "                        patient=None,\n",
    "                        bond_type=bond_type,\n",
    "                        hohfeld_state=hohfeld,\n",
    "                        context=\"hypothetical\",\n",
    "                        confidence=0.75,\n",
    "                        source_dataset=\"ethicsuite\",\n",
    "                        source_category=source,\n",
    "                        raw_label=item.get(\"original_text\", \"\")[:100],\n",
    "                    ))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: {e}\")\n",
    "\n",
    "        return annotations\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN LOADING LOGIC\n",
    "# =============================================================================\n",
    "\n",
    "all_bond_annotations = []\n",
    "\n",
    "if LOAD_ETHICS_DATASET:\n",
    "    print(\"\\n[1] ETHICS Dataset (hendrycks/ethics)\")\n",
    "    loader = EthicsLoader()\n",
    "    ethics_anns = loader.load(MAX_ETHICS_ITEMS)\n",
    "    print(f\"    Loaded: {len(ethics_anns):,} annotations\")\n",
    "    all_bond_annotations.extend(ethics_anns)\n",
    "\n",
    "if LOAD_SCRUPLES_DATASET:\n",
    "    print(\"\\n[2] Scruples Dataset (allenai/scruples)\")\n",
    "    loader = ScruplesLoader()\n",
    "    scruples_anns = loader.load(MAX_SCRUPLES_ITEMS)\n",
    "    print(f\"    Loaded: {len(scruples_anns):,} annotations\")\n",
    "    all_bond_annotations.extend(scruples_anns)\n",
    "\n",
    "if LOAD_ETHICSUITE_DATASET:\n",
    "    print(\"\\n[3] EthicsSuite Dataset (LLM-Ethics/EthicsSuite)\")\n",
    "    loader = EthicsSuiteLoader()\n",
    "    suite_anns = loader.load(MAX_ETHICSUITE_ITEMS)\n",
    "    print(f\"    Loaded: {len(suite_anns):,} annotations\")\n",
    "    all_bond_annotations.extend(suite_anns)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BOND EXTRACTION DATA STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stats = {\n",
    "    \"by_dataset\": defaultdict(int),\n",
    "    \"by_bond_type\": defaultdict(int),\n",
    "    \"by_hohfeld\": defaultdict(int),\n",
    "    \"by_context\": defaultdict(int),\n",
    "    \"by_category\": defaultdict(int),\n",
    "    \"has_agent\": 0,\n",
    "    \"has_patient\": 0,\n",
    "}\n",
    "\n",
    "for ann in all_bond_annotations:\n",
    "    stats[\"by_dataset\"][ann.source_dataset] += 1\n",
    "    stats[\"by_bond_type\"][ann.bond_type] += 1\n",
    "    stats[\"by_hohfeld\"][ann.hohfeld_state] += 1\n",
    "    stats[\"by_context\"][ann.context] += 1\n",
    "    stats[\"by_category\"][ann.source_category] += 1\n",
    "    if ann.agent:\n",
    "        stats[\"has_agent\"] += 1\n",
    "    if ann.patient:\n",
    "        stats[\"has_patient\"] += 1\n",
    "\n",
    "print(f\"\\nTotal annotations: {len(all_bond_annotations):,}\")\n",
    "\n",
    "print(\"\\nBy Dataset:\")\n",
    "for ds, count in sorted(stats[\"by_dataset\"].items()):\n",
    "    print(f\"  {ds}: {count:,}\")\n",
    "\n",
    "print(\"\\nBy Bond Type:\")\n",
    "for bt, count in sorted(stats[\"by_bond_type\"].items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {bt}: {count:,}\")\n",
    "\n",
    "print(\"\\nBy Context:\")\n",
    "for ctx, count in sorted(stats[\"by_context\"].items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {ctx}: {count:,}\")\n",
    "\n",
    "print(f\"\\nAgent extracted: {stats['has_agent']:,} ({100*stats['has_agent']/max(1,len(all_bond_annotations)):.1f}%)\")\n",
    "print(f\"Patient extracted: {stats['has_patient']:,} ({100*stats['has_patient']/max(1,len(all_bond_annotations)):.1f}%)\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT\n",
    "# =============================================================================\n",
    "\n",
    "output_dir = Path(\"data/bond_training\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save all annotations\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with open(output_dir / \"bond_annotations.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for ann in all_bond_annotations:\n",
    "        f.write(json.dumps(asdict(ann), ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved: {output_dir / 'bond_annotations.jsonl'}\")\n",
    "\n",
    "# Train/test split\n",
    "if CREATE_TRAIN_TEST_SPLIT:\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    shuffled = all_bond_annotations.copy()\n",
    "    random.shuffle(shuffled)\n",
    "    split_idx = int(len(shuffled) * (1 - TEST_SPLIT_RATIO))\n",
    "    train_anns = shuffled[:split_idx]\n",
    "    test_anns = shuffled[split_idx:]\n",
    "\n",
    "    with open(output_dir / \"bond_train.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for ann in train_anns:\n",
    "            f.write(json.dumps(asdict(ann), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    with open(output_dir / \"bond_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for ann in test_anns:\n",
    "            f.write(json.dumps(asdict(ann), ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Train/Test split: {len(train_anns):,} / {len(test_anns):,}\")\n",
    "\n",
    "# Export in BIP format\n",
    "if EXPORT_BIP_FORMAT:\n",
    "    bip_passages = []\n",
    "    for i, ann in enumerate(all_bond_annotations):\n",
    "        passage = {\n",
    "            \"id\": f\"ethics_{ann.source_dataset}_{i}\",\n",
    "            \"text\": ann.text,\n",
    "            \"language\": \"english\",\n",
    "            \"time_periods\": [\"MODERN_ETHICS\"],\n",
    "            \"tags\": [\"modern\", \"english\", \"western\", \"ethics\", ann.source_category],\n",
    "            \"bonds\": [{\n",
    "                \"agent\": ann.agent or \"unspecified\",\n",
    "                \"patient\": ann.patient or \"unspecified\",\n",
    "                \"bond_type\": ann.bond_type,\n",
    "                \"hohfeld_state\": ann.hohfeld_state,\n",
    "                \"context\": ann.context,\n",
    "                \"confidence\": ann.confidence,\n",
    "            }],\n",
    "            \"source\": ann.source_dataset,\n",
    "            \"category\": ann.source_category,\n",
    "        }\n",
    "        bip_passages.append(passage)\n",
    "\n",
    "    with open(output_dir / \"ethics_corpus.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for p in bip_passages:\n",
    "            f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"BIP format: {output_dir / 'ethics_corpus.jsonl'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BOND EXTRACTION DATA READY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Use data/bond_training/bond_train.jsonl for training\")\n",
    "print(f\"Use data/bond_training/ethics_corpus.jsonl for BIP integration\")\n",
    ""
   ],
   "id": "cell_3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3. Patterns + Normalization { display-mode: \"form\" }\n",
    "# @markdown BIP v10.9: Complete native patterns for moral concepts in 7 languages\n",
    "# @markdown - Added: Sanskrit, Pali patterns\n",
    "# @markdown - Added: NLP improvements (negation detection, modal classification)\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from enum import Enum, auto\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT NORMALIZATION & PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ===== TEXT NORMALIZATION =====\n",
    "def normalize_hebrew(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"[\\u0591-\\u05C7]\", \"\", text)  # Remove nikud\n",
    "    for final, regular in [\n",
    "        (\"\\u05da\", \"\\u05db\"),\n",
    "        (\"\\u05dd\", \"\\u05de\"),\n",
    "        (\"\\u05df\", \"\\u05e0\"),\n",
    "        (\"\\u05e3\", \"\\u05e4\"),\n",
    "        (\"\\u05e5\", \"\\u05e6\"),\n",
    "    ]:\n",
    "        text = text.replace(final, regular)\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"[\\u064B-\\u065F]\", \"\", text)  # Remove tashkeel\n",
    "    text = text.replace(\"\\u0640\", \"\")  # Remove tatweel\n",
    "    for v in [\"\\u0623\", \"\\u0625\", \"\\u0622\", \"\\u0671\"]:\n",
    "        text = text.replace(v, \"\\u0627\")\n",
    "    text = text.replace(\"\\u0629\", \"\\u0647\").replace(\"\\u0649\", \"\\u064a\")\n",
    "    return text\n",
    "\n",
    "\n",
    "# NEW in v10.9: Sanskrit normalization\n",
    "def normalize_sanskrit(text):\n",
    "    \"\"\"Normalize Sanskrit/Devanagari text.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    # Remove vedic accents and other diacriticals\n",
    "    text = re.sub(r\"[\\u0951-\\u0954]\", \"\", text)  # Vedic tone marks\n",
    "    text = re.sub(r\"[\\u0900-\\u0902]\", \"\", text)  # Chandrabindu variants\n",
    "    return text\n",
    "\n",
    "\n",
    "# NEW in v10.9: Pali normalization\n",
    "def normalize_pali(text):\n",
    "    \"\"\"Normalize Pali text (romanized or script).\"\"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    # Normalize romanized Pali diacritics\n",
    "    text = text.lower()\n",
    "    # Handle common Pali romanization variations\n",
    "    text = text.replace(\"\u1e43\", \"m\").replace(\"\u1e45\", \"n\").replace(\"\u00f1\", \"n\")\n",
    "    text = text.replace(\"\u1e6d\", \"t\").replace(\"\u1e0d\", \"d\").replace(\"\u1e47\", \"n\")\n",
    "    text = text.replace(\"\u1e37\", \"l\").replace(\"\u0101\", \"a\").replace(\"\u012b\", \"i\").replace(\"\u016b\", \"u\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_text(text, language):\n",
    "    if language in [\"hebrew\", \"aramaic\"]:\n",
    "        return normalize_hebrew(text)\n",
    "    elif language == \"arabic\":\n",
    "        return normalize_arabic(text)\n",
    "    elif language == \"classical_chinese\":\n",
    "        return unicodedata.normalize(\"NFKC\", text)\n",
    "    elif language == \"sanskrit\":\n",
    "        return normalize_sanskrit(text)\n",
    "    elif language == \"pali\":\n",
    "        return normalize_pali(text)\n",
    "    else:\n",
    "        return unicodedata.normalize(\"NFKC\", text.lower())\n",
    "\n",
    "\n",
    "# ===== BOND AND HOHFELD TYPES =====\n",
    "class BondType(Enum):\n",
    "    HARM_PREVENTION = auto()\n",
    "    RECIPROCITY = auto()\n",
    "    AUTONOMY = auto()\n",
    "    PROPERTY = auto()\n",
    "    FAMILY = auto()\n",
    "    AUTHORITY = auto()\n",
    "    CARE = auto()\n",
    "    FAIRNESS = auto()\n",
    "    CONTRACT = auto()\n",
    "    NONE = auto()\n",
    "\n",
    "\n",
    "class HohfeldState(Enum):\n",
    "    OBLIGATION = auto()\n",
    "    RIGHT = auto()\n",
    "    LIBERTY = auto()\n",
    "    NO_RIGHT = auto()\n",
    "\n",
    "\n",
    "# ===== COMPLETE BOND PATTERNS =====\n",
    "ALL_BOND_PATTERNS = {\n",
    "    \"hebrew\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u05d4\\u05e8\\u05d2\",\n",
    "            r\"\\u05e8\\u05e6\\u05d7\",\n",
    "            r\"\\u05e0\\u05d6\\u05e7\",\n",
    "            r\"\\u05d4\\u05db\\u05d4\",\n",
    "            r\"\\u05d4\\u05e6\\u05d9\\u05dc\",\n",
    "            r\"\\u05e9\\u05de\\u05e8\",\n",
    "            r\"\\u05e4\\u05e7\\u05d5\\u05d7.\\u05e0\\u05e4\\u05e9\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\u05d2\\u05de\\u05d5\\u05dc\",\n",
    "            r\"\\u05d4\\u05e9\\u05d9\\u05d1\",\n",
    "            r\"\\u05e4\\u05e8\\u05e2\",\n",
    "            r\"\\u05e0\\u05ea\\u05df.*\\u05e7\\u05d1\\u05dc\",\n",
    "            r\"\\u05de\\u05d3\\u05d4.\\u05db\\u05e0\\u05d2\\u05d3\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\u05d1\\u05d7\\u05e8\",\n",
    "            r\"\\u05e8\\u05e6\\u05d5\\u05df\",\n",
    "            r\"\\u05d7\\u05e4\\u05e9\",\n",
    "            r\"\\u05e2\\u05e6\\u05de\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u05e7\\u05e0\\u05d4\",\n",
    "            r\"\\u05de\\u05db\\u05e8\",\n",
    "            r\"\\u05d2\\u05d6\\u05dc\",\n",
    "            r\"\\u05d2\\u05e0\\u05d1\",\n",
    "            r\"\\u05de\\u05de\\u05d5\\u05df\",\n",
    "            r\"\\u05e0\\u05db\\u05e1\",\n",
    "            r\"\\u05d9\\u05e8\\u05e9\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u05d0\\u05d1\",\n",
    "            r\"\\u05d0\\u05dd\",\n",
    "            r\"\\u05d1\\u05df\",\n",
    "            r\"\\u05db\\u05d1\\u05d3.*\\u05d0\\u05d1\",\n",
    "            r\"\\u05db\\u05d1\\u05d3.*\\u05d0\\u05dd\",\n",
    "            r\"\\u05de\\u05e9\\u05e4\\u05d7\\u05d4\",\n",
    "            r\"\\u05d0\\u05d7\",\n",
    "            r\"\\u05d0\\u05d7\\u05d5\\u05ea\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u05de\\u05dc\\u05db\",\n",
    "            r\"\\u05e9\\u05d5\\u05e4\\u05d8\",\n",
    "            r\"\\u05e6\\u05d5\\u05d4\",\n",
    "            r\"\\u05ea\\u05d5\\u05e8\\u05d4\",\n",
    "            r\"\\u05de\\u05e6\\u05d5\\u05d4\",\n",
    "            r\"\\u05d3\\u05d9\\u05df\",\n",
    "            r\"\\u05d7\\u05e7\",\n",
    "        ],\n",
    "        BondType.CARE: [\n",
    "            r\"\\u05d7\\u05e1\\u05d3\",\n",
    "            r\"\\u05e8\\u05d7\\u05dd\",\n",
    "            r\"\\u05e2\\u05d6\\u05e8\",\n",
    "            r\"\\u05ea\\u05de\\u05db\",\n",
    "            r\"\\u05e6\\u05d3\\u05e7\\u05d4\",\n",
    "        ],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u05e6\\u05d3\\u05e7\",\n",
    "            r\"\\u05de\\u05e9\\u05e4\\u05d8\",\n",
    "            r\"\\u05d9\\u05e9\\u05e8\",\n",
    "            r\"\\u05e9\\u05d5\\u05d4\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u05d1\\u05e8\\u05d9\\u05ea\",\n",
    "            r\"\\u05e0\\u05d3\\u05e8\",\n",
    "            r\"\\u05e9\\u05d1\\u05d5\\u05e2\",\n",
    "            r\"\\u05d4\\u05ea\\u05d7\\u05d9\\u05d1\",\n",
    "            r\"\\u05e2\\u05e8\\u05d1\",\n",
    "        ],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u05e7\\u05d8\\u05dc\",\n",
    "            r\"\\u05e0\\u05d6\\u05e7\",\n",
    "            r\"\\u05d7\\u05d1\\u05dc\",\n",
    "            r\"\\u05e9\\u05d6\\u05d9\\u05d1\",\n",
    "            r\"\\u05e4\\u05e6\\u05d9\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [r\"\\u05e4\\u05e8\\u05e2\", r\"\\u05e9\\u05dc\\u05dd\", r\"\\u05d0\\u05d2\\u05e8\"],\n",
    "        BondType.AUTONOMY: [r\"\\u05e6\\u05d1\\u05d9\", r\"\\u05e8\\u05e2\\u05d5\"],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u05d6\\u05d1\\u05df\",\n",
    "            r\"\\u05e7\\u05e0\\u05d4\",\n",
    "            r\"\\u05d2\\u05d6\\u05dc\",\n",
    "            r\"\\u05de\\u05de\\u05d5\\u05e0\\u05d0\",\n",
    "            r\"\\u05e0\\u05db\\u05e1\\u05d9\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u05d0\\u05d1\\u05d0\",\n",
    "            r\"\\u05d0\\u05de\\u05d0\",\n",
    "            r\"\\u05d1\\u05e8\\u05d0\",\n",
    "            r\"\\u05d1\\u05e8\\u05ea\\u05d0\",\n",
    "            r\"\\u05d9\\u05e7\\u05e8\",\n",
    "            r\"\\u05d0\\u05d7\\u05d0\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u05de\\u05dc\\u05db\\u05d0\",\n",
    "            r\"\\u05d3\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05d3\\u05d9\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05e4\\u05e7\\u05d5\\u05d3\\u05d0\",\n",
    "            r\"\\u05d0\\u05d5\\u05e8\\u05d9\\u05ea\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\u05d7\\u05e1\\u05d3\", r\"\\u05e8\\u05d7\\u05dd\", r\"\\u05e1\\u05e2\\u05d3\"],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u05d3\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05e7\\u05e9\\u05d5\\u05d8\",\n",
    "            r\"\\u05ea\\u05e8\\u05d9\\u05e6\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u05e7\\u05d9\\u05de\\u05d0\",\n",
    "            r\"\\u05e9\\u05d1\\u05d5\\u05e2\\u05d4\",\n",
    "            r\"\\u05e0\\u05d3\\u05e8\\u05d0\",\n",
    "            r\"\\u05e2\\u05e8\\u05d1\\u05d0\",\n",
    "        ],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u6bba\",\n",
    "            r\"\\u5bb3\",\n",
    "            r\"\\u50b7\",\n",
    "            r\"\\u6551\",\n",
    "            r\"\\u8b77\",\n",
    "            r\"\\u885b\",\n",
    "            r\"\\u66b4\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [r\"\\u5831\", r\"\\u9084\", r\"\\u511f\", r\"\\u8ced\", r\"\\u7b54\"],\n",
    "        BondType.AUTONOMY: [r\"\\u81ea\", r\"\\u7531\", r\"\\u4efb\", r\"\\u610f\", r\"\\u5fd7\"],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u8ca1\",\n",
    "            r\"\\u7269\",\n",
    "            r\"\\u7522\",\n",
    "            r\"\\u76dc\",\n",
    "            r\"\\u7aca\",\n",
    "            r\"\\u8ce3\",\n",
    "            r\"\\u8cb7\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u5b5d\",\n",
    "            r\"\\u7236\",\n",
    "            r\"\\u6bcd\",\n",
    "            r\"\\u89aa\",\n",
    "            r\"\\u5b50\",\n",
    "            r\"\\u5f1f\",\n",
    "            r\"\\u5144\",\n",
    "            r\"\\u5bb6\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u541b\",\n",
    "            r\"\\u81e3\",\n",
    "            r\"\\u738b\",\n",
    "            r\"\\u547d\",\n",
    "            r\"\\u4ee4\",\n",
    "            r\"\\u6cd5\",\n",
    "            r\"\\u6cbb\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\u4ec1\", r\"\\u611b\", r\"\\u6148\", r\"\\u60e0\", r\"\\u6069\", r\"\\u6190\"],\n",
    "        BondType.FAIRNESS: [r\"\\u7fa9\", r\"\\u6b63\", r\"\\u516c\", r\"\\u5e73\", r\"\\u5747\"],\n",
    "        BondType.CONTRACT: [r\"\\u7d04\", r\"\\u76df\", r\"\\u8a93\", r\"\\u8afe\", r\"\\u4fe1\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u0642\\u062a\\u0644\",\n",
    "            r\"\\u0636\\u0631\\u0631\",\n",
    "            r\"\\u0627\\u0630[\\u064a\\u0649]\",\n",
    "            r\"\\u0638\\u0644\\u0645\",\n",
    "            r\"\\u0627\\u0646\\u0642\\u0630\",\n",
    "            r\"\\u062d\\u0641\\u0638\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0646\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\u062c\\u0632\\u0627\",\n",
    "            r\"\\u0631\\u062f\",\n",
    "            r\"\\u0642\\u0635\\u0627\\u0635\",\n",
    "            r\"\\u0645\\u062b\\u0644\",\n",
    "            r\"\\u0639\\u0648\\u0636\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\u062d\\u0631\",\n",
    "            r\"\\u0627\\u0631\\u0627\\u062f\\u0629\",\n",
    "            r\"\\u0627\\u062e\\u062a\\u064a\\u0627\\u0631\",\n",
    "            r\"\\u0645\\u0634\\u064a\\u0626\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u0645\\u0627\\u0644\",\n",
    "            r\"\\u0645\\u0644\\u0643\",\n",
    "            r\"\\u0633\\u0631\\u0642\",\n",
    "            r\"\\u0628\\u064a\\u0639\",\n",
    "            r\"\\u0634\\u0631\\u0627\",\n",
    "            r\"\\u0645\\u064a\\u0631\\u0627\\u062b\",\n",
    "            r\"\\u063a\\u0635\\u0628\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u0648\\u0627\\u0644\\u062f\",\n",
    "            r\"\\u0627\\u0628\\u0648\",\n",
    "            r\"\\u0627\\u0645\",\n",
    "            r\"\\u0627\\u0628\\u0646\",\n",
    "            r\"\\u0628\\u0646\\u062a\",\n",
    "            r\"\\u0627\\u0647\\u0644\",\n",
    "            r\"\\u0642\\u0631\\u0628[\\u064a\\u0649]\",\n",
    "            r\"\\u0631\\u062d\\u0645\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u0637\\u0627\\u0639\",\n",
    "            r\"\\u0627\\u0645\\u0631\",\n",
    "            r\"\\u062d\\u0643\\u0645\",\n",
    "            r\"\\u0633\\u0644\\u0637\\u0627\\u0646\",\n",
    "            r\"\\u062e\\u0644\\u064a\\u0641\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0645\",\n",
    "            r\"\\u0634\\u0631\\u064a\\u0639\",\n",
    "        ],\n",
    "        BondType.CARE: [\n",
    "            r\"\\u0631\\u062d\\u0645\",\n",
    "            r\"\\u0627\\u062d\\u0633\\u0627\\u0646\",\n",
    "            r\"\\u0639\\u0637\\u0641\",\n",
    "            r\"\\u0635\\u062f\\u0642\",\n",
    "            r\"\\u0632\\u0643\\u0627\",\n",
    "        ],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u0639\\u062f\\u0644\",\n",
    "            r\"\\u0642\\u0633\\u0637\",\n",
    "            r\"\\u062d\\u0642\",\n",
    "            r\"\\u0627\\u0646\\u0635\\u0627\\u0641\",\n",
    "            r\"\\u0633\\u0648[\\u064a\\u0649]\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u0639\\u0647\\u062f\",\n",
    "            r\"\\u0639\\u0642\\u062f\",\n",
    "            r\"\\u0646\\u0630\\u0631\",\n",
    "            r\"\\u064a\\u0645\\u064a\\u0646\",\n",
    "            r\"\\u0648\\u0641\\u0627\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0646\",\n",
    "        ],\n",
    "    },\n",
    "    \"english\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\bkill\",\n",
    "            r\"\\bmurder\",\n",
    "            r\"\\bharm\",\n",
    "            r\"\\bhurt\",\n",
    "            r\"\\bsave\",\n",
    "            r\"\\bprotect\",\n",
    "            r\"\\bviolence\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\breturn\",\n",
    "            r\"\\brepay\",\n",
    "            r\"\\bexchange\",\n",
    "            r\"\\bgive.*back\",\n",
    "            r\"\\breciproc\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\bfree\",\n",
    "            r\"\\bchoice\",\n",
    "            r\"\\bchoose\",\n",
    "            r\"\\bconsent\",\n",
    "            r\"\\bautonomy\",\n",
    "            r\"\\bright to\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\bsteal\",\n",
    "            r\"\\btheft\",\n",
    "            r\"\\bown\",\n",
    "            r\"\\bproperty\",\n",
    "            r\"\\bbelong\",\n",
    "            r\"\\binherit\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\bfather\",\n",
    "            r\"\\bmother\",\n",
    "            r\"\\bparent\",\n",
    "            r\"\\bchild\",\n",
    "            r\"\\bfamily\",\n",
    "            r\"\\bhonor.*parent\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\bobey\",\n",
    "            r\"\\bcommand\",\n",
    "            r\"\\bauthority\",\n",
    "            r\"\\blaw\",\n",
    "            r\"\\brule\",\n",
    "            r\"\\bgovern\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\bcare\", r\"\\bhelp\", r\"\\bkind\", r\"\\bcompassion\", r\"\\bcharity\", r\"\\bmercy\"],\n",
    "        BondType.FAIRNESS: [r\"\\bfair\", r\"\\bjust\", r\"\\bequal\", r\"\\bequity\", r\"\\bright\\b\"],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\bpromise\",\n",
    "            r\"\\bcontract\",\n",
    "            r\"\\bagreem\",\n",
    "            r\"\\bvow\",\n",
    "            r\"\\boath\",\n",
    "            r\"\\bcommit\",\n",
    "        ],\n",
    "    },\n",
    "    \"sanskrit\": {\n",
    "        BondType.HARM_PREVENTION: [r\"\u0939\u093f\u0902\u0938\u093e\", r\"\u0905\u0939\u093f\u0902\u0938\u093e\", r\"\u0935\u0927\", r\"\u0930\u0915\u094d\u0937\u093e\", r\"\u0924\u094d\u0930\u093e\u0923\"],\n",
    "        BondType.RECIPROCITY: [r\"\u092a\u094d\u0930\u0924\u093f\u0926\u093e\u0928\", r\"\u092a\u094d\u0930\u0924\u094d\u092f\u0941\u092a\u0915\u093e\u0930\", r\"\u0926\u093e\u0928\", r\"\u090b\u0923\"],\n",
    "        BondType.AUTONOMY: [r\"\u0938\u094d\u0935\u0924\u0902\u0924\u094d\u0930\", r\"\u092e\u094b\u0915\u094d\u0937\", r\"\u0938\u094d\u0935\u0947\u091a\u094d\u091b\u093e\"],\n",
    "        BondType.PROPERTY: [r\"\u0927\u0928\", r\"\u0938\u094d\u0935\", r\"\u091a\u094b\u0930\", r\"\u0926\u093e\u092f\"],\n",
    "        BondType.FAMILY: [r\"\u092a\u093f\u0924\u0943\", r\"\u092e\u093e\u0924\u0943\", r\"\u092a\u0941\u0924\u094d\u0930\", r\"\u0915\u0941\u0932\", r\"\u0917\u0943\u0939\"],\n",
    "        BondType.AUTHORITY: [r\"\u0930\u093e\u091c\", r\"\u0927\u0930\u094d\u092e\", r\"\u0935\u093f\u0927\u093f\", r\"\u0928\u093f\u092f\u092e\", r\"\u0936\u093e\u0938\u094d\u0924\u094d\u0930\"],\n",
    "        BondType.CARE: [r\"\u0915\u0930\u0941\u0923\u093e\", r\"\u0926\u092f\u093e\", r\"\u092a\u094d\u0930\u0947\u092e\", r\"\u092e\u0948\u0924\u094d\u0930\u0940\", r\"\u0938\u0947\u0935\u093e\"],\n",
    "        BondType.FAIRNESS: [r\"\u0928\u094d\u092f\u093e\u092f\", r\"\u0938\u092e\u0924\u093e\", r\"\u0927\u0930\u094d\u092e\", r\"\u090b\u0924\"],\n",
    "        BondType.CONTRACT: [r\"\u092a\u094d\u0930\u0924\u093f\u091c\u094d\u091e\u093e\", r\"\u0938\u0902\u0935\u093f\u0926\", r\"\u0935\u091a\u0928\", r\"\u0936\u092a\u0925\"],\n",
    "    },\n",
    "    \"pali\": {\n",
    "        BondType.HARM_PREVENTION: [r\"himsa\", r\"ahimsa\", r\"panatipata\", r\"rakkhati\"],\n",
    "        BondType.RECIPROCITY: [r\"dana\", r\"patidana\", r\"ina\"],\n",
    "        BondType.AUTONOMY: [r\"vimutti\", r\"nibbana\", r\"attadhipa\"],\n",
    "        BondType.PROPERTY: [r\"dhana\", r\"theyya\", r\"adinnadana\"],\n",
    "        BondType.FAMILY: [r\"mata\", r\"pita\", r\"putta\", r\"kula\"],\n",
    "        BondType.AUTHORITY: [r\"raja\", r\"dhamma\", r\"vinaya\", r\"sikkhapada\"],\n",
    "        BondType.CARE: [r\"karuna\", r\"metta\", r\"mudita\", r\"upekkha\"],\n",
    "        BondType.FAIRNESS: [r\"samma\", r\"dhamma\", r\"sacca\"],\n",
    "        BondType.CONTRACT: [r\"patijna\", r\"vacana\", r\"sacca\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# ===== COMPLETE HOHFELD PATTERNS =====\n",
    "ALL_HOHFELD_PATTERNS = {\n",
    "    \"hebrew\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u05d7\\u05d9\\u05d9\\u05d1\",\n",
    "            r\"\\u05e6\\u05e8\\u05d9\\u05db\",\n",
    "            r\"\\u05de\\u05d5\\u05db\\u05e8\\u05d7\",\n",
    "            r\"\\u05de\\u05e6\\u05d5\\u05d5\\u05d4\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "            r\"\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d6\\u05db\\u05d0\\u05d9\",\n",
    "            r\"\\u05de\\u05d2\\u05d9\\u05e2\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u05de\\u05d5\\u05ea\\u05e8\",\n",
    "            r\"\\u05e8\\u05e9\\u05d5\\u05ea\",\n",
    "            r\"\\u05e4\\u05d8\\u05d5\\u05e8\",\n",
    "            r\"\\u05d9\\u05db\\u05d5\\u05dc\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u05d0\\u05e1\\u05d5\\u05e8\",\n",
    "            r\"\\u05d0\\u05d9\\u05e0\\u05d5 \\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d0\\u05d9\\u05df.*\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "        ],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u05d7\\u05d9\\u05d9\\u05d1\",\n",
    "            r\"\\u05de\\u05d7\\u05d5\\u05d9\\u05d1\",\n",
    "            r\"\\u05d1\\u05e2\\u05d9\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "            r\"\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d6\\u05db\\u05d9\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u05e9\\u05e8\\u05d9\",\n",
    "            r\"\\u05de\\u05d5\\u05ea\\u05e8\",\n",
    "            r\"\\u05e4\\u05d8\\u05d5\\u05e8\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u05d0\\u05e1\\u05d5\\u05e8\",\n",
    "            r\"\\u05dc\\u05d0.*\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "        ],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        HohfeldState.OBLIGATION: [r\"\\u5fc5\", r\"\\u9808\", r\"\\u7576\", r\"\\u61c9\", r\"\\u5b9c\"],\n",
    "        HohfeldState.RIGHT: [r\"\\u53ef\", r\"\\u5f97\", r\"\\u6b0a\", r\"\\u5b9c\"],\n",
    "        HohfeldState.LIBERTY: [r\"\\u8a31\", r\"\\u4efb\", r\"\\u807d\", r\"\\u514d\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"\\u4e0d\\u53ef\", r\"\\u52ff\", r\"\\u7981\", r\"\\u83ab\", r\"\\u975e\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u064a\\u062c\\u0628\",\n",
    "            r\"\\u0648\\u0627\\u062c\\u0628\",\n",
    "            r\"\\u0641\\u0631\\u0636\",\n",
    "            r\"\\u0644\\u0627\\u0632\\u0645\",\n",
    "            r\"\\u0648\\u062c\\u0648\\u0628\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u062d\\u0642\",\n",
    "            r\"\\u064a\\u062d\\u0642\",\n",
    "            r\"\\u062c\\u0627\\u0626\\u0632\",\n",
    "            r\"\\u064a\\u062c\\u0648\\u0632\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u0645\\u0628\\u0627\\u062d\",\n",
    "            r\"\\u062d\\u0644\\u0627\\u0644\",\n",
    "            r\"\\u062c\\u0627\\u0626\\u0632\",\n",
    "            r\"\\u0627\\u0628\\u0627\\u062d\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u062d\\u0631\\u0627\\u0645\",\n",
    "            r\"\\u0645\\u062d\\u0631\\u0645\",\n",
    "            r\"\\u0645\\u0645\\u0646\\u0648\\u0639\",\n",
    "            r\"\\u0644\\u0627 \\u064a\\u062c\\u0648\\u0632\",\n",
    "            r\"\\u0646\\u0647[\\u064a\\u0649]\",\n",
    "        ],\n",
    "    },\n",
    "    \"english\": {\n",
    "        HohfeldState.OBLIGATION: [r\"\\bmust\\b\", r\"\\bshall\\b\", r\"\\bobligat\", r\"\\bduty\", r\"\\brequir\"],\n",
    "        HohfeldState.RIGHT: [r\"\\bright\\b\", r\"\\bentitle\", r\"\\bdeserve\", r\"\\bclaim\"],\n",
    "        HohfeldState.LIBERTY: [r\"\\bmay\\b\", r\"\\bpermit\", r\"\\ballow\", r\"\\bfree to\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"\\bforbid\", r\"\\bprohibit\", r\"\\bmust not\", r\"\\bshall not\"],\n",
    "    },\n",
    "    \"sanskrit\": {\n",
    "        HohfeldState.OBLIGATION: [r\"\u0915\u0930\u094d\u0924\u0935\u094d\u092f\", r\"\u0905\u0935\u0936\u094d\u092f\", r\"\u0928\u093f\u092f\u092e\", r\"\u0935\u093f\u0927\u093f\"],\n",
    "        HohfeldState.RIGHT: [r\"\u0905\u0927\u093f\u0915\u093e\u0930\", r\"\u0938\u094d\u0935\u0924\u094d\u0935\"],\n",
    "        HohfeldState.LIBERTY: [r\"\u0936\u0915\u094d\u092f\", r\"\u0905\u0928\u0941\u091c\u094d\u091e\u093e\", r\"\u0909\u091a\u093f\u0924\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"\u0928\u093f\u0937\u093f\u0926\u094d\u0927\", r\"\u0935\u0930\u094d\u091c\u093f\u0924\", r\"\u0905\u0915\u0930\u094d\u0924\u0935\u094d\u092f\"],\n",
    "    },\n",
    "    \"pali\": {\n",
    "        HohfeldState.OBLIGATION: [r\"kicca\", r\"karaniiya\", r\"dhammo\"],\n",
    "        HohfeldState.RIGHT: [r\"adhikaara\", r\"bhaaga\"],\n",
    "        HohfeldState.LIBERTY: [r\"anujaanati\", r\"kappati\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"nisiddha\", r\"akaraniya\", r\"na kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ===== CONTEXT MARKERS FOR GRAMMAR-AWARE EXTRACTION =====\n",
    "CONTEXT_MARKERS = {\n",
    "    \"hebrew\": {\n",
    "        \"negation\": [r\"\u05dc\u05d0\", r\"\u05d0\u05dc\", r\"\u05d0\u05d9\u05df\", r\"\u05d1\u05dc\u05d9\", r\"\u05d0\u05d9\u05e0\"],\n",
    "        \"obligation\": [r\"\u05d7\u05d9\u05d9\u05d1\", r\"\u05e6\u05e8\u05d9\u05da\", r\"\u05de\u05d5\u05db\u05e8\u05d7\", r\"\u05e6\u05d5\u05d5\u05d4\"],\n",
    "        \"prohibition\": [r\"\u05d0\u05e1\u05d5\u05e8\", r\"\u05d0\u05dc.*\u05ea\"],\n",
    "        \"permission\": [r\"\u05de\u05d5\u05ea\u05e8\", r\"\u05e8\u05e9\u05d0\u05d9\", r\"\u05e4\u05d8\u05d5\u05e8\"],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        \"negation\": [r\"\u05dc\u05d0\", r\"\u05dc\u05d9\u05ea\", r\"\u05dc\u05d0\u05d5\"],\n",
    "        \"obligation\": [r\"\u05d7\u05d9\u05d9\u05d1\", r\"\u05d1\u05e2\u05d9\"],\n",
    "        \"prohibition\": [r\"\u05d0\u05e1\u05d5\u05e8\"],\n",
    "        \"permission\": [r\"\u05e9\u05e8\u05d9\", r\"\u05de\u05d5\u05ea\u05e8\"],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        \"negation\": [r\"\u4e0d\", r\"\u975e\", r\"\u7121\", r\"\u672a\", r\"\u6bcb\"],\n",
    "        \"obligation\": [r\"\u5fc5\", r\"\u7576\", r\"\u9808\", r\"\u61c9\", r\"\u5b9c\"],\n",
    "        \"prohibition\": [r\"\u52ff\", r\"\u7981\", r\"\u83ab\", r\"\u4e0d\u53ef\"],\n",
    "        \"permission\": [r\"\u53ef\", r\"\u5f97\", r\"\u8a31\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        \"negation\": [r\"\u0644\u0627\", r\"\u0645\u0627\", r\"\u0644\u064a\u0633\", r\"\u0644\u0645\", r\"\u063a\u064a\u0631\"],\n",
    "        \"obligation\": [r\"\u064a\u062c\u0628\", r\"\u0648\u0627\u062c\u0628\", r\"\u0641\u0631\u0636\", r\"\u0639\u0644\u064a\u0647\"],\n",
    "        \"prohibition\": [r\"\u062d\u0631\u0627\u0645\", r\"\u0645\u062d\u0631\u0645\", r\"\u0644\u0627 \u064a\u062c\u0648\u0632\", r\"\u0646\u0647\u0649\"],\n",
    "        \"permission\": [r\"\u062d\u0644\u0627\u0644\", r\"\u0645\u0628\u0627\u062d\", r\"\u062c\u0627\u0626\u0632\"],\n",
    "    },\n",
    "    \"english\": {\n",
    "        \"negation\": [r\"not\", r\"no\", r\"never\", r\"neither\", r\"n't\"],\n",
    "        \"obligation\": [r\"must\", r\"shall\", r\"should\", r\"ought\", r\"required\"],\n",
    "        \"prohibition\": [r\"forbid\", r\"prohibit\", r\"must not\", r\"shall not\", r\"don't\"],\n",
    "        \"permission\": [r\"may\", r\"can\", r\"allowed\", r\"permit\"],\n",
    "    },\n",
    "    \"sanskrit\": {\n",
    "        \"negation\": [r\"\u0928\", r\"\u092e\u093e\", r\"\u0905\"],\n",
    "        \"obligation\": [r\"\u0915\u0930\u094d\u0924\u0935\u094d\u092f\", r\"\u0905\u0935\u0936\u094d\u092f\", r\"\u0935\u093f\u0927\u093f\"],\n",
    "        \"prohibition\": [r\"\u0928\u093f\u0937\u093f\u0926\u094d\u0927\", r\"\u0935\u0930\u094d\u091c\u093f\u0924\", r\"\u092e\u093e\"],\n",
    "        \"permission\": [r\"\u0936\u0915\u094d\u092f\", r\"\u0905\u0928\u0941\u091c\u094d\u091e\u093e\"],\n",
    "    },\n",
    "    \"pali\": {\n",
    "        \"negation\": [r\"na\", r\"ma\", r\"a-\"],\n",
    "        \"obligation\": [r\"kicca\", r\"karaniya\"],\n",
    "        \"prohibition\": [r\"nisiddha\", r\"akaraniya\"],\n",
    "        \"permission\": [r\"anujaanati\", r\"kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def detect_context(text, language, match_pos, window=30):\n",
    "    \"\"\"Detect grammatical context around a pattern match.\"\"\"\n",
    "    markers = CONTEXT_MARKERS.get(language, {})\n",
    "    if not markers:\n",
    "        return \"unknown\", None\n",
    "\n",
    "    start = max(0, match_pos - window)\n",
    "    end = min(len(text), match_pos + window)\n",
    "    window_text = text[start:end]\n",
    "\n",
    "    for marker_type in [\"prohibition\", \"obligation\", \"permission\"]:\n",
    "        for pattern in markers.get(marker_type, []):\n",
    "            if re.search(pattern, window_text):\n",
    "                return \"prescriptive\", marker_type\n",
    "\n",
    "    for pattern in markers.get(\"negation\", []):\n",
    "        if re.search(pattern, window_text):\n",
    "            return \"descriptive\", \"negated\"\n",
    "\n",
    "    return \"descriptive\", None\n",
    "\n",
    "\n",
    "# ===== NLP IMPROVEMENTS (v10.9 Phase 1) =====\n",
    "NEGATION_CUES = {\n",
    "    \"english\": [\"not\", \"no\", \"never\", \"neither\", \"nor\", \"n't\", \"without\", \"lack\", \"none\"],\n",
    "    \"classical_chinese\": [\"\u4e0d\", \"\u975e\", \"\u7121\", \"\u83ab\", \"\u52ff\", \"\u672a\", \"\u5f17\", \"\u6bcb\", \"\u5426\"],\n",
    "    \"arabic\": [\"\u0644\u0627\", \"\u0645\u0627\", \"\u0644\u0645\", \"\u0644\u0646\", \"\u0644\u064a\u0633\", \"\u063a\u064a\u0631\", \"\u0628\u062f\u0648\u0646\"],\n",
    "    \"hebrew\": [\"\u05dc\u05d0\", \"\u05d0\u05dc\", \"\u05d1\u05dc\u05d9\", \"\u05d0\u05d9\u05df\", \"\u05de\u05d1\u05dc\u05d9\"],\n",
    "    \"aramaic\": [\"\u05dc\u05d0\", \"\u05dc\u05d9\u05ea\", \"\u05dc\u05d0\u05d5\"],\n",
    "    \"sanskrit\": [\"\u0928\", \"\u092e\u093e\", \"\u0905\"],\n",
    "    \"pali\": [\"na\", \"ma\", \"a\", \"an\"],\n",
    "}\n",
    "\n",
    "MODAL_CLASSIFICATION = {\n",
    "    \"english\": {\n",
    "        \"obligation\": [\"must\", \"shall\", \"have to\", \"ought to\", \"need to\", \"required\", \"obligated\"],\n",
    "        \"permission\": [\"may\", \"can\", \"allowed\", \"permitted\", \"free to\", \"entitled\"],\n",
    "        \"prohibition\": [\"must not\", \"shall not\", \"cannot\", \"forbidden\", \"prohibited\", \"banned\"],\n",
    "        \"supererogation\": [\"should\", \"ought\", \"would be good\", \"ideally\", \"preferably\"],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        \"obligation\": [\"\u5fc5\", \"\u7576\", \"\u5b9c\", \"\u9808\", \"\u61c9\", \"\u8981\"],\n",
    "        \"permission\": [\"\u53ef\", \"\u5f97\", \"\u8a31\", \"\u5bb9\", \"\u80fd\"],\n",
    "        \"prohibition\": [\"\u4e0d\u53ef\", \"\u4e0d\u5f97\", \"\u52ff\", \"\u83ab\", \"\u7981\", \"\u4e0d\u8a31\", \"\u4e0d\u5b9c\"],\n",
    "        \"supererogation\": [\"\u5584\", \"\u7f8e\", \"\u5fb7\", \"\u5b9c\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        \"obligation\": [\"\u064a\u062c\u0628\", \"\u0641\u0631\u0636\", \"\u0648\u0627\u062c\u0628\", \"\u0644\u0627\u0632\u0645\", \"\u0641\u0631\u064a\u0636\u0629\"],\n",
    "        \"permission\": [\"\u064a\u062c\u0648\u0632\", \"\u0645\u0628\u0627\u062d\", \"\u062d\u0644\u0627\u0644\", \"\u062c\u0627\u0626\u0632\"],\n",
    "        \"prohibition\": [\"\u062d\u0631\u0627\u0645\", \"\u0645\u062d\u0631\u0645\", \"\u0645\u0645\u0646\u0648\u0639\", \"\u0644\u0627 \u064a\u062c\u0648\u0632\", \"\u0645\u062d\u0638\u0648\u0631\"],\n",
    "        \"supererogation\": [\"\u0645\u0633\u062a\u062d\u0628\", \"\u0633\u0646\u0629\", \"\u0645\u0646\u062f\u0648\u0628\", \"\u0646\u0627\u0641\u0644\u0629\"],\n",
    "    },\n",
    "    \"hebrew\": {\n",
    "        \"obligation\": [\"\u05d7\u05d9\u05d9\u05d1\", \"\u05de\u05e6\u05d5\u05d5\u05d4\", \"\u05e6\u05e8\u05d9\u05da\", \"\u05de\u05d5\u05db\u05e8\u05d7\", \"\u05d7\u05d5\u05d1\u05d4\"],\n",
    "        \"permission\": [\"\u05de\u05d5\u05ea\u05e8\", \"\u05e8\u05e9\u05d0\u05d9\", \"\u05d9\u05db\u05d5\u05dc\", \"\u05d4\u05d9\u05ea\u05e8\"],\n",
    "        \"prohibition\": [\"\u05d0\u05e1\u05d5\u05e8\", \"\u05dc\u05d0 \u05d9\u05e2\u05e9\u05d4\", \"\u05d0\u05dc\", \"\u05d0\u05d9\u05e1\u05d5\u05e8\"],\n",
    "        \"supererogation\": [\"\u05e8\u05d0\u05d5\u05d9\", \"\u05d8\u05d5\u05d1\", \"\u05de\u05d9\u05d3\u05ea \u05d7\u05e1\u05d9\u05d3\u05d5\u05ea\", \"\u05dc\u05e4\u05e0\u05d9\u05dd \u05de\u05e9\u05d5\u05e8\u05ea \u05d4\u05d3\u05d9\u05df\"],\n",
    "    },\n",
    "    \"sanskrit\": {\n",
    "        \"obligation\": [\"\u0915\u0930\u094d\u0924\u0935\u094d\u092f\", \"\u0905\u0935\u0936\u094d\u092f\", \"\u0928\u093f\u092f\u092e\"],\n",
    "        \"permission\": [\"\u0936\u0915\u094d\u092f\", \"\u0905\u0928\u0941\u091c\u094d\u091e\u093e\"],\n",
    "        \"prohibition\": [\"\u0928\u093f\u0937\u093f\u0926\u094d\u0927\", \"\u0935\u0930\u094d\u091c\u093f\u0924\", \"\u092e\u093e\"],\n",
    "    },\n",
    "    \"pali\": {\n",
    "        \"obligation\": [\"kicca\", \"karaniya\", \"dhamma\"],\n",
    "        \"permission\": [\"kappati\", \"anujanati\"],\n",
    "        \"prohibition\": [\"akappiya\", \"akaraniya\", \"na kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def enhanced_extract_bond(text: str, language: str) -> dict:\n",
    "    \"\"\"Enhanced bond extraction with negation + modal detection.\"\"\"\n",
    "    normalized = normalize_text(text, language)\n",
    "\n",
    "    negation_cues = NEGATION_CUES.get(language, [])\n",
    "    is_negated = any(cue in normalized for cue in negation_cues)\n",
    "\n",
    "    modal_status = \"unknown\"\n",
    "    modal_text = None\n",
    "    for status, markers in MODAL_CLASSIFICATION.get(language, {}).items():\n",
    "        for marker in markers:\n",
    "            if marker in normalized:\n",
    "                modal_status = status\n",
    "                modal_text = marker\n",
    "                break\n",
    "        if modal_status != \"unknown\":\n",
    "            break\n",
    "\n",
    "    hohfeld_map = {\n",
    "        \"obligation\": \"OBLIGATION\",\n",
    "        \"permission\": \"LIBERTY\",\n",
    "        \"prohibition\": \"NO_RIGHT\",\n",
    "        \"supererogation\": \"LIBERTY\",\n",
    "        \"unknown\": \"OBLIGATION\",\n",
    "    }\n",
    "    hohfeld = hohfeld_map[modal_status]\n",
    "\n",
    "    bond_type = None\n",
    "    confidence = 0.5\n",
    "    for bt, patterns in ALL_BOND_PATTERNS.get(language, {}).items():\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, normalized):\n",
    "                bond_type = bt\n",
    "                confidence = 0.9\n",
    "                break\n",
    "        if bond_type:\n",
    "            break\n",
    "\n",
    "    if is_negated:\n",
    "        confidence *= 0.8\n",
    "\n",
    "    if modal_status in [\"obligation\", \"prohibition\"]:\n",
    "        context = \"prescriptive\"\n",
    "    elif modal_status == \"permission\":\n",
    "        context = \"descriptive\"\n",
    "    else:\n",
    "        context = \"unknown\"\n",
    "\n",
    "    return {\n",
    "        \"bond_type\": bond_type.name if bond_type else None,\n",
    "        \"hohfeld_state\": hohfeld,\n",
    "        \"negated\": is_negated,\n",
    "        \"modal\": modal_text,\n",
    "        \"confidence\": confidence,\n",
    "        \"context\": context,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\nContext markers defined for grammar-aware extraction\")\n",
    "print(\"  Detects: negation, obligation, prohibition, permission\")\n",
    "\n",
    "print(f\"\\nPatterns defined for {len(ALL_BOND_PATTERNS)} languages:\")\n",
    "for lang in ALL_BOND_PATTERNS:\n",
    "    n = sum(len(p) for p in ALL_BOND_PATTERNS[lang].values())\n",
    "    print(f\"  {lang}: {n} bond patterns\")\n",
    "\n",
    "print(\"\\nNLP improvements (Phase 1):\")\n",
    "print(f\"  NEGATION_CUES: {len(NEGATION_CUES)} languages\")\n",
    "print(f\"  MODAL_CLASSIFICATION: {len(MODAL_CLASSIFICATION)} languages\")\n",
    "print(\"  enhanced_extract_bond() ready\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACT BONDS FROM PASSAGES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"EXTRACTING BONDS FROM PASSAGES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "passages_file = Path(\"data/processed/passages.jsonl\")\n",
    "bonds_file = Path(\"data/processed/bonds.jsonl\")\n",
    "\n",
    "if bonds_file.exists():\n",
    "    with open(bonds_file, encoding=\"utf-8\") as f:\n",
    "        bond_count = sum(1 for _ in f)\n",
    "    print(f\"  bonds.jsonl exists with {bond_count:,} bonds (cached)\")\n",
    "else:\n",
    "    passages = []\n",
    "    with open(passages_file, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            passages.append(json.loads(line))\n",
    "\n",
    "    print(f\"  Processing {len(passages):,} passages...\")\n",
    "\n",
    "    bonds = []\n",
    "    bond_type_counts = {}\n",
    "\n",
    "    for i, p in enumerate(passages):\n",
    "        try:\n",
    "            text = normalize_text(p[\"text\"], p[\"language\"])\n",
    "            bond = enhanced_extract_bond(text, p[\"language\"])\n",
    "            bond[\"passage_id\"] = p[\"id\"]\n",
    "            bonds.append(bond)\n",
    "\n",
    "            bt = bond.get(\"bond_type\") or \"NEUTRAL\"\n",
    "            bond_type_counts[bt] = bond_type_counts.get(bt, 0) + 1\n",
    "\n",
    "        except Exception as e:\n",
    "            bonds.append({\n",
    "                \"passage_id\": p[\"id\"],\n",
    "                \"bond_type\": \"NEUTRAL\",\n",
    "                \"hohfeld_state\": \"LIBERTY\",\n",
    "                \"negated\": False,\n",
    "                \"modal\": None,\n",
    "                \"confidence\": 0.1,\n",
    "                \"context\": \"unknown\"\n",
    "            })\n",
    "\n",
    "        if (i + 1) % 20000 == 0:\n",
    "            print(f\"    {i+1:,} processed...\")\n",
    "\n",
    "    bonds_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(bonds_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for b in bonds:\n",
    "            f.write(json.dumps(b, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\nSaved {len(bonds):,} bonds to {bonds_file}\")\n",
    "    print(f\"\\nBond type distribution:\")\n",
    "    for bt, count in sorted(bond_type_counts.items(), key=lambda x: -x[1]):\n",
    "        pct = 100 * count / len(bonds)\n",
    "        print(f\"    {bt:12s}: {count:6,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BOND EXTRACTION COMPLETE\")\n",
    "print(\"=\" * 60)\n"
   ],
   "id": "cell_4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 4. Generate Splits { display-mode: \"form\" }\n",
    "# @markdown v10.13: Tag-based splits with matrix selection\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ## Split Matrix\n",
    "# @markdown Select train/test tags using dropdowns. Use \"none\" to disable.\n",
    "\n",
    "# @markdown ### Experiment 1\n",
    "EXP1_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP1_NAME = \"hebrew_to_others\"  # @param {type:\"string\"}\n",
    "EXP1_TRAIN = \"hebrew\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP1_TEST = \"all-other\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 2\n",
    "EXP2_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP2_NAME = \"semitic_to_indic\"  # @param {type:\"string\"}\n",
    "EXP2_TRAIN = \"semitic\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP2_TEST = \"indic\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 3\n",
    "EXP3_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP3_NAME = \"confucian_to_buddhist\"  # @param {type:\"string\"}\n",
    "EXP3_TRAIN = \"confucian\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP3_TEST = \"buddhist\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 4\n",
    "EXP4_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP4_NAME = \"ancient_to_modern\"  # @param {type:\"string\"}\n",
    "EXP4_TRAIN = \"ancient\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP4_TEST = \"modern\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 5\n",
    "EXP5_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP5_NAME = \"east_to_west\"  # @param {type:\"string\"}\n",
    "EXP5_TRAIN = \"east-asia\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP5_TEST = \"western\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 6\n",
    "EXP6_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP6_NAME = \"semitic_to_chinese\"  # @param {type:\"string\"}\n",
    "EXP6_TRAIN = \"semitic\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP6_TEST = \"chinese\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 7\n",
    "EXP7_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP7_NAME = \"jewish_to_islamic\"  # @param {type:\"string\"}\n",
    "EXP7_TRAIN = \"hebrew\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP7_TEST = \"arabic\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 8\n",
    "EXP8_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP8_NAME = \"stoic_to_confucian\"  # @param {type:\"string\"}\n",
    "EXP8_TRAIN = \"stoic\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP8_TEST = \"confucian\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 9\n",
    "EXP9_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP9_NAME = \"daoist_to_buddhist\"  # @param {type:\"string\"}\n",
    "EXP9_TRAIN = \"daoist\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP9_TEST = \"buddhist\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 10\n",
    "EXP10_ENABLE = True  # @param {type:\"boolean\"}\n",
    "EXP10_NAME = \"hindu_to_buddhist\"  # @param {type:\"string\"}\n",
    "EXP10_TRAIN = \"hindu\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP10_TEST = \"buddhist\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 11\n",
    "EXP11_ENABLE = False  # @param {type:\"boolean\"}\n",
    "EXP11_NAME = \"custom_11\"  # @param {type:\"string\"}\n",
    "EXP11_TRAIN = \"none\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP11_TEST = \"none\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ### Experiment 12\n",
    "EXP12_ENABLE = False  # @param {type:\"boolean\"}\n",
    "EXP12_NAME = \"custom_12\"  # @param {type:\"string\"}\n",
    "EXP12_TRAIN = \"none\"  # @param [\"none\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "EXP12_TEST = \"none\"  # @param [\"none\", \"all-other\", \"hebrew\", \"aramaic\", \"arabic\", \"semitic\", \"chinese\", \"confucian\", \"daoist\", \"buddhist\", \"hindu\", \"sanskrit\", \"pali\", \"indic\", \"greek\", \"latin\", \"stoic\", \"english\", \"western\", \"modern\", \"ancient\", \"classical\", \"east-asia\", \"south-asia\", \"middle-east\"]\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ## Options\n",
    "INCLUDE_MIXED_BASELINE = True  # @param {type:\"boolean\"}\n",
    "MIN_SPLIT_SIZE = 50  # @param {type:\"integer\"}\n",
    "\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATING SPLITS (v10.13)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =============================================================================\n",
    "# TAG DEFINITIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Compound tag groups\n",
    "TAG_GROUPS = {\n",
    "    \"semitic\": [\"hebrew\", \"aramaic\", \"arabic\"],\n",
    "    \"indic\": [\"sanskrit\", \"pali\", \"hindi\"],\n",
    "    \"east-asia\": [\"chinese\", \"confucian\", \"daoist\"],\n",
    "    \"south-asia\": [\"sanskrit\", \"pali\", \"hindu\", \"buddhist\"],\n",
    "    \"middle-east\": [\"hebrew\", \"aramaic\", \"arabic\", \"jewish\", \"islamic\"],\n",
    "    \"western\": [\"english\", \"greek\", \"latin\", \"stoic\"],\n",
    "    \"ancient\": [\"ancient\", \"classical\"],\n",
    "    \"modern\": [\"modern\", \"advice\", \"american\"],\n",
    "}\n",
    "\n",
    "# Period to tags mapping\n",
    "PERIOD_TO_TAGS = {\n",
    "    \"CONFUCIAN\": [\"confucian\", \"east-asia\", \"classical\", \"chinese\"],\n",
    "    \"DAOIST\": [\"daoist\", \"east-asia\", \"classical\", \"chinese\"],\n",
    "    \"BUDDHIST\": [\"buddhist\"],\n",
    "    \"PALI\": [\"buddhist\", \"south-asia\", \"ancient\", \"pali\"],\n",
    "    \"DHARMA\": [\"hindu\", \"south-asia\", \"ancient\", \"sanskrit\"],\n",
    "    \"BIBLICAL\": [\"jewish\", \"middle-east\", \"ancient\", \"hebrew\"],\n",
    "    \"TANNAITIC\": [\"jewish\", \"middle-east\", \"classical\", \"hebrew\"],\n",
    "    \"AMORAIC\": [\"jewish\", \"middle-east\", \"classical\", \"aramaic\"],\n",
    "    \"QURANIC\": [\"islamic\", \"middle-east\", \"medieval\", \"arabic\"],\n",
    "    \"HADITH\": [\"islamic\", \"middle-east\", \"medieval\", \"arabic\"],\n",
    "    \"CLASSICAL_GREEK\": [\"stoic\", \"mediterranean\", \"classical\", \"greek\"],\n    \"HELLENISTIC\": [\"stoic\", \"mediterranean\", \"classical\", \"greek\"],  # Epictetus, Marcus Aurelius\n    \"CLASSICAL_LATIN\": [\"stoic\", \"mediterranean\", \"classical\", \"latin\"],  # Seneca, Cicero\n",
    "    \"DEAR_ABBY\": [\"american\", \"modern\", \"advice\", \"english\", \"western\"],\n",
    "    \"MODERN_ETHICS\": [\"western\", \"modern\", \"ethics\", \"english\"],\n",
    "}\n",
    "\n",
    "LANG_TO_TAGS = {\n",
    "    \"classical_chinese\": [\"chinese\", \"east-asia\"],\n",
    "    \"hebrew\": [\"hebrew\", \"middle-east\"],\n",
    "    \"aramaic\": [\"aramaic\", \"middle-east\"],\n",
    "    \"arabic\": [\"arabic\", \"middle-east\"],\n",
    "    \"sanskrit\": [\"sanskrit\", \"south-asia\"],\n",
    "    \"pali\": [\"pali\", \"south-asia\"],\n",
    "    \"greek\": [\"greek\", \"mediterranean\"],\n",
    "    \"latin\": [\"latin\", \"mediterranean\"],\n",
    "    \"english\": [\"english\", \"western\"],\n",
    "}\n",
    "\n",
    "\n",
    "def add_tags(p: dict) -> list:\n",
    "    \"\"\"Generate tags for a passage.\"\"\"\n",
    "    tags = set()\n",
    "\n",
    "    lang = p.get(\"language\", \"\")\n",
    "    if lang in LANG_TO_TAGS:\n",
    "        tags.update(LANG_TO_TAGS[lang])\n",
    "\n",
    "    for period in p.get(\"time_periods\", []):\n",
    "        if period in PERIOD_TO_TAGS:\n",
    "            tags.update(PERIOD_TO_TAGS[period])\n",
    "\n",
    "    return sorted(tags)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD PASSAGES\n",
    "# =============================================================================\n",
    "\n",
    "passages_file = Path(\"data/processed/passages.jsonl\")\n",
    "if not passages_file.exists():\n",
    "    raise FileNotFoundError(\"Run Cell 2 first to generate passages.jsonl\")\n",
    "\n",
    "passage_meta = []\n",
    "with open(passages_file, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        p = json.loads(line)\n",
    "        passage_meta.append({\n",
    "            \"id\": p[\"id\"],\n",
    "            \"language\": p.get(\"language\", \"\"),\n",
    "            \"tags\": add_tags(p),\n",
    "        })\n",
    "\n",
    "print(f\"Loaded {len(passage_meta):,} passages\")\n",
    "\n",
    "# Count tags\n",
    "tag_counts = defaultdict(int)\n",
    "for p in passage_meta:\n",
    "    for tag in p[\"tags\"]:\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "print(\"\\nTag counts:\")\n",
    "for tag, count in sorted(tag_counts.items(), key=lambda x: -x[1])[:15]:\n",
    "    print(f\"  {tag}: {count:,}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SPLIT HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "def expand_tag(tag: str) -> list:\n",
    "    \"\"\"Expand compound tags like 'semitic' to individual tags.\"\"\"\n",
    "    if tag in TAG_GROUPS:\n",
    "        return TAG_GROUPS[tag]\n",
    "    return [tag]\n",
    "\n",
    "\n",
    "def ids_with_tags(tags: list) -> list:\n",
    "    \"\"\"Get passage IDs with ANY of the tags.\"\"\"\n",
    "    tag_set = set()\n",
    "    for t in tags:\n",
    "        tag_set.update(expand_tag(t))\n",
    "    return [p[\"id\"] for p in passage_meta if set(p[\"tags\"]) & tag_set]\n",
    "\n",
    "\n",
    "def ids_without_tags(tags: list) -> list:\n",
    "    \"\"\"Get passage IDs with NONE of the tags.\"\"\"\n",
    "    tag_set = set()\n",
    "    for t in tags:\n",
    "        tag_set.update(expand_tag(t))\n",
    "    return [p[\"id\"] for p in passage_meta if not (set(p[\"tags\"]) & tag_set)]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATE SPLITS FROM MATRIX\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING SPLITS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_splits = {}\n",
    "random.seed(42)\n",
    "\n",
    "experiments = [\n",
    "    (EXP1_ENABLE, EXP1_NAME, EXP1_TRAIN, EXP1_TEST),\n",
    "    (EXP2_ENABLE, EXP2_NAME, EXP2_TRAIN, EXP2_TEST),\n",
    "    (EXP3_ENABLE, EXP3_NAME, EXP3_TRAIN, EXP3_TEST),\n",
    "    (EXP4_ENABLE, EXP4_NAME, EXP4_TRAIN, EXP4_TEST),\n",
    "    (EXP5_ENABLE, EXP5_NAME, EXP5_TRAIN, EXP5_TEST),\n",
    "    (EXP6_ENABLE, EXP6_NAME, EXP6_TRAIN, EXP6_TEST),\n",
    "    (EXP7_ENABLE, EXP7_NAME, EXP7_TRAIN, EXP7_TEST),\n",
    "    (EXP8_ENABLE, EXP8_NAME, EXP8_TRAIN, EXP8_TEST),\n",
    "    (EXP9_ENABLE, EXP9_NAME, EXP9_TRAIN, EXP9_TEST),\n",
    "    (EXP10_ENABLE, EXP10_NAME, EXP10_TRAIN, EXP10_TEST),\n",
    "    (EXP11_ENABLE, EXP11_NAME, EXP11_TRAIN, EXP11_TEST),\n",
    "    (EXP12_ENABLE, EXP12_NAME, EXP12_TRAIN, EXP12_TEST),\n",
    "]\n",
    "\n",
    "for enabled, name, train_tag, test_tag in experiments:\n",
    "    if not enabled or train_tag == \"none\" or not name.strip():\n",
    "        continue\n",
    "\n",
    "    name = name.strip().replace(\" \", \"_\")\n",
    "\n",
    "    # Get train IDs\n",
    "    train_ids = ids_with_tags([train_tag])\n",
    "\n",
    "    # Get test IDs\n",
    "    if test_tag == \"all-other\":\n",
    "        test_ids = ids_without_tags([train_tag])\n",
    "    elif test_tag == \"none\":\n",
    "        continue\n",
    "    else:\n",
    "        test_ids = ids_with_tags([test_tag])\n",
    "        # Remove overlap\n",
    "        overlap = set(train_ids) & set(test_ids)\n",
    "        train_ids = [x for x in train_ids if x not in overlap]\n",
    "        test_ids = [x for x in test_ids if x not in overlap]\n",
    "\n",
    "    if len(train_ids) < MIN_SPLIT_SIZE or len(test_ids) < MIN_SPLIT_SIZE:\n",
    "        print(f\"  SKIP {name}: insufficient data (train={len(train_ids)}, test={len(test_ids)})\")\n",
    "        continue\n",
    "\n",
    "    random.shuffle(train_ids)\n",
    "    random.shuffle(test_ids)\n",
    "\n",
    "    all_splits[name] = {\n",
    "        \"train_ids\": train_ids,\n",
    "        \"test_ids\": test_ids,\n",
    "        \"train_size\": len(train_ids),\n",
    "        \"test_size\": len(test_ids),\n",
    "        \"train_tags\": expand_tag(train_tag),\n",
    "        \"test_tags\": expand_tag(test_tag) if test_tag != \"all-other\" else [\"*\"],\n",
    "    }\n",
    "    print(f\"  {name}: {len(train_ids):,} -> {len(test_ids):,}\")\n",
    "\n",
    "# Add mixed baseline\n",
    "if INCLUDE_MIXED_BASELINE:\n",
    "    all_ids = [p[\"id\"] for p in passage_meta]\n",
    "    random.shuffle(all_ids)\n",
    "    split_pt = int(len(all_ids) * 0.7)\n",
    "    all_splits[\"mixed_baseline\"] = {\n",
    "        \"train_ids\": all_ids[:split_pt],\n",
    "        \"test_ids\": all_ids[split_pt:],\n",
    "        \"train_size\": split_pt,\n",
    "        \"test_size\": len(all_ids) - split_pt,\n",
    "        \"train_tags\": [\"*\"],\n",
    "        \"test_tags\": [\"*\"],\n",
    "    }\n",
    "    print(f\"  mixed_baseline: {split_pt:,} -> {len(all_ids)-split_pt:,}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE\n",
    "# =============================================================================\n",
    "\n",
    "splits_file = Path(\"data/splits/all_splits.json\")\n",
    "splits_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(splits_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_splits, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"SAVED {len(all_splits)} SPLITS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(f\"{'Experiment':<25} {'Train':>10} {'Test':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for name, split in sorted(all_splits.items()):\n",
    "    print(f\"{name:<25} {split['train_size']:>10,} {split['test_size']:>10,}\")\n",
    "print(\"-\" * 50)\n",
    "\n"
   ],
   "id": "cell_5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 5. Model Architecture { display-mode: \"form\" }\n",
    "# @markdown BIP v10.9 model with configurable backbone and adversarial heads\n",
    "# @markdown - Updated: 8 languages, 26 periods\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Backbone: {BACKBONE} ({MODEL_NAME})\")\n",
    "print(f\"Hidden size: {BACKBONE_HIDDEN}\")\n",
    "\n",
    "# Index mappings\n",
    "BOND_TO_IDX = {bt.name: i for i, bt in enumerate(BondType)}\n",
    "IDX_TO_BOND = {i: bt.name for i, bt in enumerate(BondType)}\n",
    "# v10.9: 8 languages (added Sanskrit, Pali, Greek placeholder)\n",
    "LANG_TO_IDX = {\n",
    "    \"hebrew\": 0,\n",
    "    \"aramaic\": 1,\n",
    "    \"classical_chinese\": 2,\n",
    "    \"arabic\": 3,\n",
    "    \"english\": 4,\n",
    "    \"sanskrit\": 5,  # NEW in v10.9\n",
    "    \"pali\": 6,  # NEW in v10.9\n",
    "    \"greek\": 7,  # FUTURE (placeholder)\n",
    "}\n",
    "IDX_TO_LANG = {i: l for l, i in LANG_TO_IDX.items()}\n",
    "\n",
    "# v10.9: 26 periods (expanded Chinese, Arabic, added Sanskrit/Pali traditions)\n",
    "PERIOD_TO_IDX = {\n",
    "    # Semitic traditions\n",
    "    \"BIBLICAL\": 0,\n",
    "    \"TANNAITIC\": 1,\n",
    "    \"AMORAIC\": 2,\n",
    "    \"RISHONIM\": 3,\n",
    "    \"ACHRONIM\": 4,\n",
    "    # Chinese traditions (expanded)\n",
    "    \"CONFUCIAN\": 5,\n",
    "    \"DAOIST\": 6,\n",
    "    \"MOHIST\": 7,  # NEW in v10.9\n",
    "    \"LEGALIST\": 8,  # NEW in v10.9\n",
    "    \"BUDDHIST\": 9,  # NEW in v10.9 (Chinese Buddhism)\n",
    "    \"NEO_CONFUCIAN\": 10,  # NEW in v10.9\n",
    "    # Arabic/Islamic traditions (expanded)\n",
    "    \"QURANIC\": 11,\n",
    "    \"HADITH\": 12,\n",
    "    \"FIQH\": 13,  # NEW in v10.9 (Islamic jurisprudence)\n",
    "    \"SUFI\": 14,  # NEW in v10.9\n",
    "    \"FALSAFA\": 15,  # NEW in v10.9 (Arabic philosophy)\n",
    "    # Sanskrit/Pali traditions (NEW in v10.9)\n",
    "    \"DHARMA\": 16,  # Dharmashastra\n",
    "    \"UPANISHAD\": 17,\n",
    "    \"GITA\": 18,\n",
    "    \"ARTHA\": 19,  # Arthashastra\n",
    "    \"PALI\": 20,  # Pali Canon\n",
    "    # Western traditions\n",
    "    \"WESTERN_CLASSICAL\": 21,\n",
    "    \"MEDIEVAL\": 22,\n",
    "    # Modern\n",
    "    \"DEAR_ABBY\": 23,\n",
    "    \"MODERN\": 24,\n",
    "    \"CLASSICAL\": 25,  # Generic classical (fallback)\n",
    "}  # 26 periods total (0-25)\n",
    "IDX_TO_PERIOD = {i: p for p, i in PERIOD_TO_IDX.items()}\n",
    "HOHFELD_TO_IDX = {hs.name: i for i, hs in enumerate(HohfeldState)}\n",
    "IDX_TO_HOHFELD = {i: hs.name for i, hs in enumerate(HohfeldState)}\n",
    "CONTEXT_TO_IDX = {\"prescriptive\": 0, \"descriptive\": 1, \"unknown\": 2}\n",
    "IDX_TO_CONTEXT = {i: c for c, i in CONTEXT_TO_IDX.items()}\n",
    "\n",
    "\n",
    "def get_confidence_weight(conf):\n",
    "    \"\"\"Map confidence to sample weight. Handles both string ('high'/'medium'/'low') and numeric (0.0-1.0) values.\"\"\"\n",
    "    if isinstance(conf, str):\n",
    "        return {\"high\": 2.0, \"medium\": 1.0, \"low\": 0.5}.get(conf, 1.0)\n",
    "    elif isinstance(conf, (int, float)):\n",
    "        return 2.0 if conf >= 0.8 else 1.0\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "class GradientReversalLayer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.alpha, None\n",
    "\n",
    "\n",
    "class BIPModel(nn.Module):\n",
    "    def __init__(self, model_name=None, hidden_size=None, z_dim=64):\n",
    "        super().__init__()\n",
    "        # Use global config if not specified\n",
    "        model_name = model_name or MODEL_NAME\n",
    "        hidden_size = hidden_size or BACKBONE_HIDDEN\n",
    "\n",
    "        print(f\"  Loading encoder: {model_name}\")\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Freeze encoder if configured (probe-only training)\n",
    "        try:\n",
    "            if FREEZE_ENCODER:\n",
    "                for param in self.encoder.parameters():\n",
    "                    param.requires_grad = False\n",
    "                print(f\"  Encoder FROZEN (probe-only mode)\")\n",
    "            else:\n",
    "                print(f\"  Encoder UNFROZEN (full fine-tuning)\")\n",
    "        except NameError:\n",
    "            print(f\"  Encoder unfrozen (FREEZE_ENCODER not set)\")\n",
    "\n",
    "        # Get actual hidden size from model config\n",
    "        actual_hidden = self.encoder.config.hidden_size\n",
    "        if actual_hidden != hidden_size:\n",
    "            print(f\"  Note: Using actual hidden size {actual_hidden}\")\n",
    "            hidden_size = actual_hidden\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Projection to z_bond space (scales with backbone size)\n",
    "        proj_hidden = min(512, hidden_size)\n",
    "        self.z_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_size, proj_hidden),\n",
    "            nn.LayerNorm(proj_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(proj_hidden, z_dim),\n",
    "        )\n",
    "\n",
    "        # Task heads\n",
    "        self.bond_head = nn.Linear(z_dim, len(BondType))\n",
    "        self.hohfeld_head = nn.Linear(z_dim, len(HohfeldState))\n",
    "\n",
    "        # Adversarial heads\n",
    "        self.language_head = nn.Linear(z_dim, len(LANG_TO_IDX))\n",
    "        self.period_head = nn.Linear(z_dim, len(PERIOD_TO_IDX))\n",
    "\n",
    "        # Context prediction head (auxiliary task)\n",
    "        self.context_head = nn.Linear(z_dim, len(CONTEXT_TO_IDX))\n",
    "\n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"  Total params: {total_params:,}\")\n",
    "        print(f\"  Trainable: {trainable_params:,}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, adv_lambda=1.0):\n",
    "        enc = self.encoder(input_ids, attention_mask)\n",
    "\n",
    "        # Handle different pooling strategies\n",
    "        if hasattr(enc, \"pooler_output\") and enc.pooler_output is not None:\n",
    "            pooled = enc.pooler_output\n",
    "        else:\n",
    "            pooled = enc.last_hidden_state[:, 0]\n",
    "\n",
    "        z = self.z_proj(pooled)\n",
    "\n",
    "        # Bond prediction (main task)\n",
    "        bond_pred = self.bond_head(z)\n",
    "        hohfeld_pred = self.hohfeld_head(z)\n",
    "\n",
    "        # Adversarial predictions (gradient reversal)\n",
    "        z_rev = GradientReversalLayer.apply(z, adv_lambda)\n",
    "        language_pred = self.language_head(z_rev)\n",
    "        period_pred = self.period_head(z_rev)\n",
    "\n",
    "        return {\n",
    "            \"bond_pred\": bond_pred,\n",
    "            \"hohfeld_pred\": hohfeld_pred,\n",
    "            \"language_pred\": language_pred,\n",
    "            \"period_pred\": period_pred,\n",
    "            \"context_pred\": self.context_head(z),\n",
    "            \"z\": z,\n",
    "        }\n",
    "\n",
    "    def get_bond_embedding(self, input_ids, attention_mask):\n",
    "        \"\"\"Get z_bond embedding for geometric analysis.\"\"\"\n",
    "        enc = self.encoder(input_ids, attention_mask)\n",
    "        if hasattr(enc, \"pooler_output\") and enc.pooler_output is not None:\n",
    "            pooled = enc.pooler_output\n",
    "        else:\n",
    "            pooled = enc.last_hidden_state[:, 0]\n",
    "        return self.z_proj(pooled)\n",
    "\n",
    "\n",
    "# Initialize tokenizer for selected backbone\n",
    "print(f\"\\nLoading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "\n",
    "# Dataset with Hohfeld support\n",
    "class NativeDataset(Dataset):\n",
    "    def __init__(self, ids_set, passages_file, bonds_file, tokenizer, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = []\n",
    "\n",
    "        bonds_by_id = {}\n",
    "        with open(bonds_file) as fb:\n",
    "            for line in fb:\n",
    "                b = json.loads(line)\n",
    "                bonds_by_id[b[\"passage_id\"]] = b\n",
    "\n",
    "        with open(passages_file) as fp:\n",
    "            for line in tqdm(fp, desc=\"Loading\", unit=\"line\"):\n",
    "                p = json.loads(line)\n",
    "                if p[\"id\"] in ids_set and p[\"id\"] in bonds_by_id:\n",
    "                    b = bonds_by_id[p[\"id\"]]\n",
    "                    self.data.append(\n",
    "                        {\n",
    "                            \"text\": p[\"text\"][:1000],\n",
    "                            \"language\": p[\"language\"],\n",
    "                            \"period\": p.get(\"time_periods\", [\"UNKNOWN\"])[0],\n",
    "                            \"bond\": b.get(\"bond_type\") or b.get(\"bonds\", {}).get(\"primary_bond\"),\n",
    "                            \"hohfeld\": None,\n",
    "                            \"context\": b.get(\"context\")\n",
    "                            or b.get(\"bonds\", {}).get(\"context\", \"unknown\"),\n",
    "                            \"confidence\": b.get(\"confidence\")\n",
    "                            or b.get(\"bonds\", {}).get(\"confidence\", \"medium\"),\n",
    "                        }\n",
    "                    )\n",
    "        print(f\"  Loaded {len(self.data):,} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        enc = self.tokenizer(\n",
    "            item[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"bond_label\": BOND_TO_IDX.get(item[\"bond\"], 9),\n",
    "            \"language_label\": LANG_TO_IDX.get(item[\"language\"], 4),\n",
    "            \"period_label\": PERIOD_TO_IDX.get(item[\"period\"], 9),\n",
    "            \"hohfeld_label\": HOHFELD_TO_IDX.get(item[\"hohfeld\"], 0) if item[\"hohfeld\"] else 0,\n",
    "            \"context_label\": CONTEXT_TO_IDX.get(item[\"context\"], 2),\n",
    "            \"sample_weight\": get_confidence_weight(item[\"confidence\"]),\n",
    "            \"language\": item[\"language\"],\n",
    "            \"context\": item[\"context\"],\n",
    "            \"confidence\": item[\"confidence\"],\n",
    "            \"text\": item[\"text\"],  # Raw text for role augmentation\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
    "        \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n",
    "        \"bond_labels\": torch.tensor([x[\"bond_label\"] for x in batch]),\n",
    "        \"language_labels\": torch.tensor([x[\"language_label\"] for x in batch]),\n",
    "        \"period_labels\": torch.tensor([x[\"period_label\"] for x in batch]),\n",
    "        \"hohfeld_labels\": torch.tensor([x[\"hohfeld_label\"] for x in batch]),\n",
    "        \"context_labels\": torch.tensor([x[\"context_label\"] for x in batch]),\n",
    "        \"sample_weights\": torch.tensor([x[\"sample_weight\"] for x in batch], dtype=torch.float),\n",
    "        \"languages\": [x[\"language\"] for x in batch],\n",
    "        \"contexts\": [x[\"context\"] for x in batch],\n",
    "        \"confidences\": [x[\"confidence\"] for x in batch],\n",
    "        \"texts\": [x[\"text\"] for x in batch],  # v10.10: raw texts for role augmentation\n",
    "    }\n",
    "\n",
    "\n",
    "print(f\"\\nArchitecture ready for {BACKBONE}\")\n",
    "print(f\"  Bond classes: {len(BondType)}\")\n",
    "print(f\"  Languages: {len(LANG_TO_IDX)}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ],
   "id": "cell_6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 6. Train BIP Model { display-mode: \"form\" }\n",
    "# @markdown Training with tuned adversarial weights and hardware-optimized parameters\n",
    "# @markdown v10.9: Added new splits (confucian_to_buddhist, all_to_sanskrit, etc.)\n",
    "\n",
    "# ===== SUPPRESS DATALOADER MULTIPROCESSING WARNINGS =====\n",
    "# These occur during garbage collection and bypass normal exception handling\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Method 1: Filter warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*can only test a child process.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.data\")\n",
    "\n",
    "# Method 2: Suppress logging\n",
    "logging.getLogger(\"torch.utils.data.dataloader\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "# Method 3: Redirect stderr during DataLoader cleanup (most effective)\n",
    "class StderrFilter(io.TextIOWrapper):\n",
    "    \"\"\"Filters out DataLoader multiprocessing cleanup messages from stderr\"\"\"\n",
    "\n",
    "    def __init__(self, original):\n",
    "        self.original = original\n",
    "        self.buffer_lines = []\n",
    "\n",
    "    def write(self, text):\n",
    "        # Filter out the specific error patterns\n",
    "        skip_patterns = [\n",
    "            \"can only test a child process\",\n",
    "            \"_MultiProcessingDataLoaderIter.__del__\",\n",
    "            \"_shutdown_workers\",\n",
    "            \"Exception ignored in:\",\n",
    "            \"w.is_alive()\",\n",
    "        ]\n",
    "        # Buffer multi-line error messages\n",
    "        if any(p in text for p in skip_patterns):\n",
    "            return len(text)  # Pretend we wrote it\n",
    "        # Also skip if it looks like part of a traceback for these errors\n",
    "        if text.strip().startswith(\"^\") and len(text.strip()) < 80:\n",
    "            return len(text)\n",
    "        if text.strip().startswith('File \"/usr') and \"dataloader.py\" in text:\n",
    "            return len(text)\n",
    "        if text.strip() == \"Traceback (most recent call last):\":\n",
    "            self.buffer_lines = [text]\n",
    "            return len(text)\n",
    "        if self.buffer_lines:\n",
    "            self.buffer_lines.append(text)\n",
    "            # Check if this is the DataLoader error traceback\n",
    "            full_msg = \"\".join(self.buffer_lines)\n",
    "            if any(p in full_msg for p in skip_patterns):\n",
    "                self.buffer_lines = []\n",
    "                return len(text)\n",
    "            # After 10 lines, flush if not the target error\n",
    "            if len(self.buffer_lines) > 10:\n",
    "                for line in self.buffer_lines:\n",
    "                    self.original.write(line)\n",
    "                self.buffer_lines = []\n",
    "        return self.original.write(text)\n",
    "\n",
    "    def flush(self):\n",
    "        if self.buffer_lines:\n",
    "            # Flush any remaining buffered content\n",
    "            for line in self.buffer_lines:\n",
    "                self.original.write(line)\n",
    "            self.buffer_lines = []\n",
    "        self.original.flush()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.original, name)\n",
    "\n",
    "\n",
    "# Install the stderr filter\n",
    "_original_stderr = sys.stderr\n",
    "sys.stderr = StderrFilter(_original_stderr)\n",
    "\n",
    "# Method 4: Patch the DataLoader cleanup function directly\n",
    "try:\n",
    "    import torch.utils.data.dataloader as dl_module\n",
    "\n",
    "    _original_del = dl_module._MultiProcessingDataLoaderIter.__del__\n",
    "\n",
    "    def _patched_del(self):\n",
    "        try:\n",
    "            _original_del(self)\n",
    "        except (AssertionError, AttributeError, RuntimeError):\n",
    "            pass  # Silently ignore cleanup errors\n",
    "\n",
    "    dl_module._MultiProcessingDataLoaderIter.__del__ = _patched_del\n",
    "except Exception:\n",
    "    pass  # If patching fails, the stderr filter will still work\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import gc\n",
    "\n",
    "# ===== INITIAL MEMORY CLEANUP =====\n",
    "# Clean up any leftover GPU memory from previous runs before starting\n",
    "print(\"Cleaning up GPU memory from previous runs...\")\n",
    "if torch.cuda.is_available():\n",
    "    # Clear any existing models/tensors from globals\n",
    "    for var_name in list(globals().keys()):\n",
    "        obj = globals().get(var_name)\n",
    "        if isinstance(obj, torch.nn.Module):\n",
    "            try:\n",
    "                obj.cpu()\n",
    "                del globals()[var_name]\n",
    "            except:\n",
    "                pass\n",
    "        elif isinstance(obj, torch.Tensor) and obj.is_cuda:\n",
    "            try:\n",
    "                del globals()[var_name]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # Force garbage collection\n",
    "    for _ in range(5):\n",
    "        gc.collect()\n",
    "\n",
    "    # Clear CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Check memory status\n",
    "    mem_alloc = torch.cuda.memory_allocated() / 1e9\n",
    "    mem_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"  GPU memory: {mem_alloc:.2f} GB allocated, {mem_reserved:.2f} GB reserved\")\n",
    "\n",
    "    if mem_alloc > 1.0:\n",
    "        print(f\"  WARNING: {mem_alloc:.1f} GB still allocated - consider restarting runtime\")\n",
    "        # Try more aggressive cleanup\n",
    "        torch.cuda.ipc_collect()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"  No GPU detected\")\n",
    "\n",
    "print()\n",
    "\n",
    "# @markdown **Splits to train:**\n",
    "# @markdown v10.13: Automatically uses splits generated in Cell 4\n",
    "TRAIN_ALL_SPLITS = True  # @param {type:\"boolean\"}\n",
    "# @markdown Train all splits from Cell 4. If False, specify splits below.\n",
    "\n",
    "SPECIFIC_SPLITS = \"\"  # @param {type:\"string\"}\n",
    "# @markdown Comma-separated split names (only used if TRAIN_ALL_SPLITS=False)\n",
    "# @markdown Example: \"hebrew_to_others, confucian_to_buddhist, mixed_baseline\"\n",
    "\n",
    "MAX_SPLITS = 0  # @param {type:\"integer\"}\n",
    "# @markdown Limit number of splits (0 = no limit). Useful for quick testing.\n",
    "\n",
    "# @markdown **Reproducibility:**\n",
    "USE_FIXED_SEED = True  # @param {type:\"boolean\"}\n",
    "RANDOM_SEED = 42  # @param {type:\"integer\"}\n",
    "# @markdown Set USE_FIXED_SEED=True for reproducible results, False for random initialization\n",
    "\n",
    "if USE_FIXED_SEED:\n",
    "    import numpy as np\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Using fixed seed: {RANDOM_SEED}\")\n",
    "else:\n",
    "    torch.backends.cudnn.benchmark = True  # Faster but non-deterministic\n",
    "    print(\"Using random initialization\")\n",
    "\n",
    "# @markdown **Hyperparameters:**\n",
    "LANG_WEIGHT = 0.1  # @param {type:\"number\"}\n",
    "PERIOD_WEIGHT = 0.066  # @param {type:\"number\"}\n",
    "# Use NUM_EPOCHS from Cell 1, or default\n",
    "try:\n",
    "    N_EPOCHS = NUM_EPOCHS\n",
    "except NameError:\n",
    "    N_EPOCHS = 10  # Default fallback\n",
    "\n",
    "# @markdown **Context-Aware Training:**\n",
    "USE_CONFIDENCE_WEIGHTING = True  # @param {type:\"boolean\"}\n",
    "# @markdown Weight prescriptive (high confidence) examples 2x in loss\n",
    "\n",
    "USE_CONTEXT_AUXILIARY = True  # @param {type:\"boolean\"}\n",
    "# @markdown Add context prediction as auxiliary training target\n",
    "\n",
    "CONTEXT_LOSS_WEIGHT = 0.33  # @param {type:\"number\"}\n",
    "# @markdown Weight for context prediction loss\n",
    "\n",
    "STRICT_PRESCRIPTIVE_TEST = False  # @param {type:\"boolean\"}\n",
    "# @markdown Only evaluate on prescriptive examples (reduces test set ~97%!)\n",
    "\n",
    "# @markdown **v10.10: Role-Aware Data Augmentation:**\n",
    "USE_ROLE_AUGMENTATION = True  # @param {type:\"boolean\"}\n",
    "# @markdown Adds contrastive loss for agent/patient role sensitivity\n",
    "ROLE_AUGMENT_PROB = 0.3  # @param {type:\"number\"}\n",
    "# @markdown Probability of augmenting each batch\n",
    "ROLE_CONTRASTIVE_WEIGHT = 0.2  # @param {type:\"number\"}\n",
    "# @markdown Weight for role contrastive loss\n",
    "ROLE_CONTRASTIVE_MARGIN = 0.5  # @param {type:\"number\"}\n",
    "# @markdown Minimum embedding distance for role-swapped pairs\n",
    "\n",
    "\n",
    "def swap_roles_simple(text, language):\n",
    "    \"\"\"Simple role swap using word order reversal for common patterns.\n",
    "    v10.10: Addresses weak role_swap sensitivity (0.003) from fuzz testing.\"\"\"\n",
    "    patterns = {\n",
    "        \"english\": [\n",
    "            (r\"(\\w+) must (\\w+) (\\w+)\", r\"\\3 must \\2 \\1\"),\n",
    "            (r\"(\\w+) should (\\w+) (\\w+)\", r\"\\3 should \\2 \\1\"),\n",
    "            (r\"(\\w+) shall (\\w+) (\\w+)\", r\"\\3 shall \\2 \\1\"),\n",
    "            (r\"the (\\w+) must (\\w+) the (\\w+)\", r\"the \\3 must \\2 the \\1\"),\n",
    "            (r\"(\\w+) is obligated to (\\w+) (\\w+)\", r\"\\3 is obligated to \\2 \\1\"),\n",
    "            (r\"(\\w+) has a duty to (\\w+) (\\w+)\", r\"\\3 has a duty to \\2 \\1\"),\n",
    "        ],\n",
    "        \"hebrew\": [\n",
    "            (r\"\u05e2\u05dc (\\S+) \u05dc(\\S+) \u05d0\u05ea (\\S+)\", r\"\u05e2\u05dc \\3 \u05dc\\2 \u05d0\u05ea \\1\"),\n",
    "        ],\n",
    "        \"classical_chinese\": [\n",
    "            (r\"(\\S)\u7576(\\S)(\\S)\", r\"\\3\u7576\\2\\1\"),\n",
    "            (r\"(\\S)\u9808(\\S)(\\S)\", r\"\\3\u9808\\2\\1\"),\n",
    "            (r\"(\\S)\u61c9(\\S)(\\S)\", r\"\\3\u61c9\\2\\1\"),\n",
    "        ],\n",
    "        \"arabic\": [\n",
    "            (r\"\u064a\u062c\u0628 \u0639\u0644\u0649 (\\S+) \u0623\u0646 (\\S+) (\\S+)\", r\"\u064a\u062c\u0628 \u0639\u0644\u0649 \\3 \u0623\u0646 \\2 \\1\"),\n",
    "            (r\"(\\S+) \u0639\u0644\u064a\u0647 \u0623\u0646 (\\S+) (\\S+)\", r\"\\3 \u0639\u0644\u064a\u0647 \u0623\u0646 \\2 \\1\"),\n",
    "        ],\n",
    "        \"sanskrit\": [\n",
    "            (r\"(\\S+)\u0903 (\\S+)\u092e\u094d (\\S+)\u0924\u093f\", r\"\\3\u0903 \\2\u092e\u094d \\1\u0924\u093f\"),\n",
    "        ],\n",
    "        \"pali\": [\n",
    "            (r\"(\\S+)o (\\S+)a\u1e43 (\\S+)ti\", r\"\\3o \\2a\u1e43 \\1ti\"),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    lang_patterns = patterns.get(language, patterns[\"english\"])\n",
    "    for pattern, replacement in lang_patterns:\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            swapped = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "            if swapped != text:\n",
    "                return swapped\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING BIP MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSettings:\")\n",
    "print(f\"  Backbone:     {BACKBONE}\")\n",
    "print(f\"  GPU Tier:     {GPU_TIER}\")\n",
    "print(f\"  Batch size:   {BATCH_SIZE}\")\n",
    "print(f\"  Workers:      {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate: {LR:.2e}\")\n",
    "print(f\"  Adv weights:  lang={LANG_WEIGHT}, period={PERIOD_WEIGHT}\")\n",
    "print(\"(0.01 prevents loss explosion while maintaining invariance)\")\n",
    "print(f\"  Confidence weighting: {USE_CONFIDENCE_WEIGHTING}\")\n",
    "print(f\"  Context auxiliary: {USE_CONTEXT_AUXILIARY} (weight={CONTEXT_LOSS_WEIGHT})\")\n",
    "print(f\"  Strict prescriptive test: {STRICT_PRESCRIPTIVE_TEST}\")\n",
    "print(f\"  Role augmentation: {USE_ROLE_AUGMENTATION} (prob={ROLE_AUGMENT_PROB}, weight={ROLE_CONTRASTIVE_WEIGHT})\")\n",
    "\n",
    "# tokenizer loaded in Cell 6 based on BACKBONE selection\n",
    "\n",
    "with open(\"data/splits/all_splits.json\") as f:\n",
    "    all_splits = json.load(f)\n",
    "\n",
    "# Build splits_to_train from Cell 4 output\n",
    "if TRAIN_ALL_SPLITS:\n",
    "    splits_to_train = list(all_splits.keys())\n",
    "else:\n",
    "    # Parse comma-separated list\n",
    "    splits_to_train = [s.strip() for s in SPECIFIC_SPLITS.split(\",\") if s.strip()]\n",
    "    # Filter to only splits that exist\n",
    "    splits_to_train = [s for s in splits_to_train if s in all_splits]\n",
    "\n",
    "# Apply max limit if set\n",
    "if MAX_SPLITS > 0:\n",
    "    splits_to_train = splits_to_train[:MAX_SPLITS]\n",
    "\n",
    "print(f\"\\nTraining {len(splits_to_train)} splits: {splits_to_train}\")\n",
    "\n",
    "all_results = {}\n",
    "MIN_TEST_SIZE = 100  # Lowered to allow smaller test sets like Chinese\n",
    "\n",
    "for split_idx, split_name in enumerate(splits_to_train):\n",
    "    split_start = time.time()\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"[{split_idx+1}/{len(splits_to_train)}] {split_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    split = all_splits[split_name]\n",
    "    print(f\"Train: {split['train_size']:,} | Test: {split['test_size']:,}\")\n",
    "\n",
    "    if split[\"test_size\"] < MIN_TEST_SIZE:\n",
    "        print(f\"WARNING: Test set only {split['test_size']} samples (need {MIN_TEST_SIZE})\")\n",
    "        print(\"Skipping this split - results would be unreliable\")\n",
    "        print(\"To fix: Add more data to the test languages/periods\")\n",
    "        continue\n",
    "\n",
    "    # Create model with OOM recovery\n",
    "    def create_model_with_retry():\n",
    "        \"\"\"Create model, cleaning up GPU memory if OOM occurs.\"\"\"\n",
    "        try:\n",
    "            return BIPModel().to(device)\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(\"  OOM on model creation - cleaning up and retrying...\")\n",
    "            # Clean up any existing model in globals\n",
    "            _g = globals()\n",
    "            for _var in [\"model\", \"analyzer\", \"encoder\"]:\n",
    "                if _var in _g and _g[_var] is not None:\n",
    "                    try:\n",
    "                        if hasattr(_g[_var], \"cpu\"):\n",
    "                            _g[_var].cpu()\n",
    "                        _g[_var] = None\n",
    "                    except:\n",
    "                        pass\n",
    "            # Force cleanup\n",
    "            gc.collect()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            # Retry\n",
    "            return BIPModel().to(device)\n",
    "\n",
    "    model = create_model_with_retry()\n",
    "\n",
    "    train_dataset = NativeDataset(\n",
    "        set(split[\"train_ids\"]),\n",
    "        \"data/processed/passages.jsonl\",\n",
    "        \"data/processed/bonds.jsonl\",\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    test_ids_to_use = split[\"test_ids\"][:MAX_TEST_SAMPLES]\n",
    "\n",
    "    # Optional: strict prescriptive-only test\n",
    "    if STRICT_PRESCRIPTIVE_TEST:\n",
    "        print(\"Filtering to prescriptive examples only...\")\n",
    "        # Load bonds to filter\n",
    "        prescriptive_ids = set()\n",
    "        with open(\"data/processed/bonds.jsonl\") as f:\n",
    "            for line in f:\n",
    "                b = json.loads(line)\n",
    "                if b.get(\"context\") == \"prescriptive\":\n",
    "                    prescriptive_ids.add(b[\"passage_id\"])\n",
    "        test_ids_to_use = [tid for tid in test_ids_to_use if tid in prescriptive_ids]\n",
    "        print(f\"  Filtered to {len(test_ids_to_use):,} prescriptive samples\")\n",
    "\n",
    "    test_dataset = NativeDataset(\n",
    "        set(test_ids_to_use),\n",
    "        \"data/processed/passages.jsonl\",\n",
    "        \"data/processed/bonds.jsonl\",\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"ERROR: No training data!\")\n",
    "        continue\n",
    "\n",
    "    # Use hardware-optimized batch size\n",
    "    actual_batch = min(BATCH_SIZE, len(train_dataset) // 2)  # Allow larger batches, min 2 batches/epoch\n",
    "    print(f\"Actual batch size: {actual_batch}\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=actual_batch,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=actual_batch * 2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "    # Gradient clipping setup\n",
    "    try:\n",
    "        grad_clip = GRADIENT_CLIP if GRADIENT_CLIP > 0 else None\n",
    "    except NameError:\n",
    "        grad_clip = 1.0  # Default\n",
    "\n",
    "    # Early stopping setup\n",
    "    try:\n",
    "        early_stop_patience = EARLY_STOPPING_PATIENCE if EARLY_STOPPING_PATIENCE > 0 else None\n",
    "    except NameError:\n",
    "        early_stop_patience = 3  # Default\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    def get_adv_lambda(epoch, warmup=ADV_WARMUP_EPOCHS, max_lambda=ADV_MAX_LAMBDA):\n",
    "        \"\"\"Ramp adversarial strength: 0.1 -> 1.0 over warmup, then hold at 1.0\"\"\"\n",
    "        if epoch <= warmup:\n",
    "            return 0.1 + (max_lambda - 0.1) * (epoch / warmup)\n",
    "        return max_lambda\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    no_improve_count = 0\n",
    "    start_epoch = 1\n",
    "\n",
    "    # Check for existing checkpoint to resume from\n",
    "    checkpoint_path = f\"models/checkpoints/latest_{split_name}.pt\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"  Found checkpoint, resuming...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        best_loss = checkpoint[\"best_loss\"]\n",
    "        print(f\"  Resuming from epoch {start_epoch}, best_loss={best_loss:.4f}\")\n",
    "\n",
    "    for epoch in range(start_epoch, N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            bond_labels = batch[\"bond_labels\"].to(device)\n",
    "            language_labels = batch[\"language_labels\"].to(device)\n",
    "            period_labels = batch[\"period_labels\"].to(device)\n",
    "\n",
    "            adv_lambda = get_adv_lambda(epoch)\n",
    "\n",
    "            # Use new autocast API\n",
    "            with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n",
    "                out = model(input_ids, attention_mask, adv_lambda=adv_lambda)\n",
    "\n",
    "                # Weighted bond loss\n",
    "                if USE_CONFIDENCE_WEIGHTING:\n",
    "                    sample_weights = batch[\"sample_weights\"].to(device)\n",
    "                    loss_bond = F.cross_entropy(out[\"bond_pred\"], bond_labels, reduction=\"none\")\n",
    "                    loss_bond = (loss_bond * sample_weights).mean()\n",
    "                else:\n",
    "                    loss_bond = F.cross_entropy(out[\"bond_pred\"], bond_labels)\n",
    "\n",
    "                # Context auxiliary loss\n",
    "                if USE_CONTEXT_AUXILIARY:\n",
    "                    context_labels = batch[\"context_labels\"].to(device)\n",
    "                    loss_context = F.cross_entropy(out[\"context_pred\"], context_labels)\n",
    "                else:\n",
    "                    loss_context = 0\n",
    "\n",
    "                loss_lang = F.cross_entropy(out[\"language_pred\"], language_labels)\n",
    "                loss_period = F.cross_entropy(out[\"period_pred\"], period_labels)\n",
    "\n",
    "            loss = (\n",
    "                loss_bond\n",
    "                + LANG_WEIGHT * loss_lang\n",
    "                + PERIOD_WEIGHT * loss_period\n",
    "                + CONTEXT_LOSS_WEIGHT * loss_context\n",
    "            )\n",
    "\n",
    "            # v10.10: Role contrastive loss for agent/patient sensitivity\n",
    "            loss_role = torch.tensor(0.0, device=device)\n",
    "            if USE_ROLE_AUGMENTATION and random.random() < ROLE_AUGMENT_PROB:\n",
    "                batch_texts = batch.get(\"texts\", [])\n",
    "                batch_languages = batch.get(\"languages\", [])\n",
    "\n",
    "                swapped_texts = []\n",
    "                original_indices = []\n",
    "\n",
    "                for i, (text, lang) in enumerate(zip(batch_texts, batch_languages)):\n",
    "                    swapped = swap_roles_simple(text, lang)\n",
    "                    if swapped:\n",
    "                        swapped_texts.append(swapped)\n",
    "                        original_indices.append(i)\n",
    "\n",
    "                if swapped_texts and len(swapped_texts) >= 2:\n",
    "                    # Tokenize swapped texts\n",
    "                    swapped_encoded = tokenizer(\n",
    "                        swapped_texts,\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=128,\n",
    "                        return_tensors=\"pt\",\n",
    "                    )\n",
    "                    swapped_ids = swapped_encoded[\"input_ids\"].to(device)\n",
    "                    swapped_mask = swapped_encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "                    # Get embeddings for swapped texts (no gradients needed - saves memory!)\n",
    "                    # We only need gradients through z_original, not z_swapped\n",
    "                    with torch.no_grad():\n",
    "                        swapped_out = model(swapped_ids, swapped_mask, adv_lambda=0)\n",
    "                        z_swapped = swapped_out[\"z\"].detach()\n",
    "\n",
    "                    # Get original embeddings for corresponding indices (keeps gradients)\n",
    "                    z_original = out[\"z\"][original_indices]\n",
    "\n",
    "                    # Contrastive loss: push role-swapped embeddings apart\n",
    "                    # Hinge loss: max(0, margin - distance)\n",
    "                    # Gradients flow through z_original only\n",
    "                    distances = F.pairwise_distance(z_original, z_swapped)\n",
    "                    loss_role = F.relu(ROLE_CONTRASTIVE_MARGIN - distances).mean()\n",
    "\n",
    "                    # Clean up to prevent memory accumulation\n",
    "                    del swapped_ids, swapped_mask, swapped_out, swapped_encoded\n",
    "                    del z_original, z_swapped, distances\n",
    "\n",
    "            loss = loss + ROLE_CONTRASTIVE_WEIGHT * loss_role\n",
    "\n",
    "            if USE_AMP and scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if grad_clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "            # Delete intermediate tensors to prevent memory accumulation\n",
    "            del input_ids, attention_mask, bond_labels, language_labels, period_labels\n",
    "            del out, loss, loss_bond, loss_lang, loss_period\n",
    "            if USE_CONFIDENCE_WEIGHTING:\n",
    "                del sample_weights\n",
    "            if USE_CONTEXT_AUXILIARY:\n",
    "                del context_labels, loss_context\n",
    "            if USE_ROLE_AUGMENTATION:\n",
    "                del loss_role\n",
    "\n",
    "            # Periodic memory cleanup every 50 batches\n",
    "            if n_batches % 50 == 0:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        avg_loss = total_loss / n_batches\n",
    "\n",
    "        # Aggressive memory cleanup after each epoch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            mem_alloc = torch.cuda.memory_allocated() / 1e9\n",
    "            mem_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "            print(\n",
    "                f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f}) [GPU: {mem_alloc:.1f}GB alloc, {mem_reserved:.1f}GB reserved]\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f})\")\n",
    "\n",
    "        # Save checkpoint every epoch (for crash recovery)\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"loss\": avg_loss,\n",
    "            \"best_loss\": best_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"models/checkpoints/latest_{split_name}.pt\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), f\"models/checkpoints/best_{split_name}.pt\")\n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/best_{split_name}.pt\")\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "            if early_stop_patience and no_improve_count >= early_stop_patience:\n",
    "                print(f\"Early stopping: no improvement for {no_improve_count} epochs\")\n",
    "                break\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.load_state_dict(torch.load(f\"models/checkpoints/best_{split_name}.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = {\"bond\": [], \"lang\": []}\n",
    "    all_labels = {\"bond\": [], \"lang\": []}\n",
    "    all_languages = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            out = model(batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), 0)\n",
    "            all_preds[\"bond\"].extend(out[\"bond_pred\"].argmax(-1).cpu().tolist())\n",
    "            all_preds[\"lang\"].extend(out[\"language_pred\"].argmax(-1).cpu().tolist())\n",
    "            all_labels[\"bond\"].extend(batch[\"bond_labels\"].tolist())\n",
    "            all_labels[\"lang\"].extend(batch[\"language_labels\"].tolist())\n",
    "            all_languages.extend(batch[\"languages\"])\n",
    "\n",
    "    bond_f1 = f1_score(all_labels[\"bond\"], all_preds[\"bond\"], average=\"macro\", zero_division=0)\n",
    "    bond_acc = sum(p == l for p, l in zip(all_preds[\"bond\"], all_labels[\"bond\"])) / len(\n",
    "        all_preds[\"bond\"]\n",
    "    )\n",
    "    lang_acc = sum(p == l for p, l in zip(all_preds[\"lang\"], all_labels[\"lang\"])) / len(\n",
    "        all_preds[\"lang\"]\n",
    "    )\n",
    "\n",
    "    # Per-language F1\n",
    "    lang_f1 = {}\n",
    "    for lang in set(all_languages):\n",
    "        mask = [l == lang for l in all_languages]\n",
    "        if sum(mask) > 10:\n",
    "            preds = [p for p, m in zip(all_preds[\"bond\"], mask) if m]\n",
    "            labels = [l for l, m in zip(all_labels[\"bond\"], mask) if m]\n",
    "            lang_f1[lang] = {\n",
    "                \"f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "                \"n\": sum(mask),\n",
    "            }\n",
    "\n",
    "    all_results[split_name] = {\n",
    "        \"bond_f1_macro\": bond_f1,\n",
    "        \"bond_acc\": bond_acc,\n",
    "        \"language_acc\": lang_acc,\n",
    "        \"per_language_f1\": lang_f1,\n",
    "        \"training_time\": time.time() - split_start,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{split_name} RESULTS:\")\n",
    "    print(f\"  Bond F1 (macro): {bond_f1:.3f} ({bond_f1/0.1:.1f}x chance)\")\n",
    "    print(f\"  Bond accuracy:   {bond_acc:.1%}\")\n",
    "    print(f\"  Language acc:    {lang_acc:.1%} (want ~20% = invariant)\")\n",
    "    print(\"  Per-language:\")\n",
    "    for lang, m in sorted(lang_f1.items(), key=lambda x: -x[1][\"n\"]):\n",
    "        print(f\"    {lang:20s}: F1={m['f1']:.3f} (n={m['n']:,})\")\n",
    "\n",
    "    # Context analysis\n",
    "    high_conf = sum(1 for c in test_dataset.data if c[\"confidence\"] == \"high\")\n",
    "    prescriptive = sum(1 for c in test_dataset.data if c[\"context\"] == \"prescriptive\")\n",
    "    print(\n",
    "        f\"  Context: {prescriptive:,}/{len(test_dataset):,} prescriptive ({prescriptive/len(test_dataset)*100:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  High confidence: {high_conf:,}/{len(test_dataset):,} ({high_conf/len(test_dataset)*100:.1f}%)\"\n",
    "    )\n",
    "\n",
    "    # GPU memory usage before cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        mem = torch.cuda.memory_allocated() / 1e9\n",
    "        print(\n",
    "            f\"\\n  GPU memory (before cleanup): {mem:.1f} GB / {VRAM_GB:.1f} GB ({mem/VRAM_GB*100:.0f}%)\"\n",
    "        )\n",
    "\n",
    "    # Aggressive memory cleanup between splits\n",
    "    # Step 1: Zero out gradients to release gradient memory\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "\n",
    "    # Step 2: Clear optimizer state (can hold significant memory)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    optimizer_state = optimizer.state\n",
    "    for state in optimizer_state.values():\n",
    "        for k, v in list(state.items()):\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = None\n",
    "\n",
    "    # Step 3: Move model to CPU to release GPU memory\n",
    "    model.cpu()\n",
    "\n",
    "    # Step 4: Delete all references\n",
    "    del model, train_dataset, test_dataset, train_loader, test_loader, optimizer\n",
    "    if USE_AMP and scaler:\n",
    "        del scaler\n",
    "\n",
    "    # Step 5: Force garbage collection (multiple passes)\n",
    "    for _ in range(5):\n",
    "        gc.collect()\n",
    "\n",
    "    # Step 6: Clear CUDA cache and reset memory stats\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        # If memory is still high, try more aggressive cleanup\n",
    "        mem_check = torch.cuda.memory_allocated() / 1e9\n",
    "        if mem_check > 2.0:\n",
    "            print(f\"  Memory still high ({mem_check:.1f}GB), attempting deeper cleanup...\")\n",
    "            # Clear all cached allocations\n",
    "            torch.cuda.memory._dump_snapshot = lambda: None  # Disable snapshot if enabled\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "\n",
    "    # Step 7: Re-create scaler for next split\n",
    "    if USE_AMP:\n",
    "        scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "    # GPU memory after cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"  GPU memory (after cleanup): {mem_after:.1f} GB (freed {mem - mem_after:.1f} GB)\")\n",
    "        if mem_after > 1.0:\n",
    "            print(f\"  WARNING: {mem_after:.1f} GB still allocated - may cause OOM on next split\")\n",
    "            print(f\"  Consider running with BACKBONE='MiniLM' for lower memory usage\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    ""
   ],
   "id": "cell_7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 7. Geometric Analysis & Linear Probe { display-mode: \"form\" }\n",
    "# @markdown v10.9: New geometric analysis module + linear probe test\n",
    "# @markdown Tests latent space structure (axis discovery, role swap analysis)\n",
    "# @markdown Tests if z_bond encodes language/period (should be low = invariant)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "# ===== v10.9: GEOMETRIC ANALYZER CLASS =====\n",
    "class GeometricAnalyzer:\n",
    "    \"\"\"\n",
    "    Probe the latent space geometry to discover moral structure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        inputs = self.tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, max_length=128, padding=\"max_length\"\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        z = self.model.get_bond_embedding(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        return z.cpu().numpy().flatten()\n",
    "\n",
    "    def find_direction(self, positive_texts: List[str], negative_texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Find the direction in z-space that separates two concepts.\n",
    "        E.g., obligation vs permission, harm vs care.\n",
    "        \"\"\"\n",
    "        pos_embs = np.array([self.get_embedding(t) for t in positive_texts])\n",
    "        neg_embs = np.array([self.get_embedding(t) for t in negative_texts])\n",
    "\n",
    "        pos_mean = pos_embs.mean(axis=0)\n",
    "        neg_mean = neg_embs.mean(axis=0)\n",
    "\n",
    "        direction = pos_mean - neg_mean\n",
    "        direction = direction / (np.linalg.norm(direction) + 1e-9)\n",
    "        return direction\n",
    "\n",
    "    def test_direction_transfer(\n",
    "        self, direction: np.ndarray, test_pairs: List[Tuple[str, str]]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Test if a direction generalizes to new examples.\n",
    "        Returns accuracy of direction-based classification.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for pos_text, neg_text in test_pairs:\n",
    "            pos_proj = np.dot(self.get_embedding(pos_text), direction)\n",
    "            neg_proj = np.dot(self.get_embedding(neg_text), direction)\n",
    "            scores.append(1.0 if pos_proj > neg_proj else 0.0)\n",
    "        return np.mean(scores)\n",
    "\n",
    "    def pca_on_pairs(self, concept_pairs: Dict[str, List[Tuple[str, str]]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Run PCA on difference vectors to find dominant axes.\n",
    "\n",
    "        concept_pairs: {\"obligation_permission\": [(obl1, perm1), ...], ...}\n",
    "        \"\"\"\n",
    "        all_diffs = []\n",
    "        labels = []\n",
    "\n",
    "        for concept, pairs in concept_pairs.items():\n",
    "            for pos, neg in pairs:\n",
    "                diff = self.get_embedding(pos) - self.get_embedding(neg)\n",
    "                all_diffs.append(diff)\n",
    "                labels.append(concept)\n",
    "\n",
    "        X = np.array(all_diffs)\n",
    "\n",
    "        pca = PCA(n_components=min(10, len(X)))\n",
    "        pca.fit(X)\n",
    "\n",
    "        return {\n",
    "            \"components\": pca.components_,\n",
    "            \"explained_variance_ratio\": pca.explained_variance_ratio_,\n",
    "            \"labels\": labels,\n",
    "            \"transformed\": pca.transform(X),\n",
    "        }\n",
    "\n",
    "    def role_swap_analysis(self, agent_patient_pairs: List[Tuple[str, str]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Test if swapping agent/patient produces consistent transformation.\n",
    "\n",
    "        agent_patient_pairs: [(\"A harmed B\", \"B harmed A\"), ...]\n",
    "        \"\"\"\n",
    "        transformations = []\n",
    "\n",
    "        for original, swapped in agent_patient_pairs:\n",
    "            orig_emb = self.get_embedding(original)\n",
    "            swap_emb = self.get_embedding(swapped)\n",
    "            transformations.append(swap_emb - orig_emb)\n",
    "\n",
    "        T = np.array(transformations)\n",
    "\n",
    "        # Check consistency: are all transformations similar?\n",
    "        mean_transform = T.mean(axis=0)\n",
    "        cosines = [\n",
    "            np.dot(t, mean_transform) / (np.linalg.norm(t) * np.linalg.norm(mean_transform) + 1e-9)\n",
    "            for t in T\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"mean_transform\": mean_transform,\n",
    "            \"consistency\": np.mean(cosines),\n",
    "            \"consistency_std\": np.std(cosines),\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LINEAR PROBE TEST\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nIf probe accuracy is NEAR CHANCE, representation is INVARIANT\")\n",
    "print(\"(This is what we want for BIP)\")\n",
    "\n",
    "probe_results = {}\n",
    "\n",
    "for split_name in [\"hebrew_to_others\", \"semitic_to_non_semitic\"]:\n",
    "    model_path = f\"{SAVE_DIR}/best_{split_name}.pt\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"\\nSkipping {split_name} - no saved model\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PROBE: {split_name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model = BIPModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    test_ids = set(all_splits[split_name][\"test_ids\"][:5000])\n",
    "    test_dataset = NativeDataset(\n",
    "        test_ids, \"data/processed/passages.jsonl\", \"data/processed/bonds.jsonl\", tokenizer\n",
    "    )\n",
    "\n",
    "    if len(test_dataset) < 50:\n",
    "        print(f\"  Skip - only {len(test_dataset)} samples\")\n",
    "        continue\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "    all_z, all_lang, all_period = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Extract\"):\n",
    "            out = model(batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), 0)\n",
    "            all_z.append(out[\"z\"].cpu().numpy())\n",
    "            all_lang.extend(batch[\"language_labels\"].tolist())\n",
    "            all_period.extend(batch[\"period_labels\"].tolist())\n",
    "\n",
    "    X = np.vstack(all_z)\n",
    "    y_lang = np.array(all_lang)\n",
    "    y_period = np.array(all_period)\n",
    "\n",
    "    scaler_probe = StandardScaler()\n",
    "    X_scaled = scaler_probe.fit_transform(X)\n",
    "\n",
    "    # Train/test split for probes\n",
    "    n = len(X)\n",
    "    idx = np.random.permutation(n)\n",
    "    train_idx, test_idx = idx[: int(0.7 * n)], idx[int(0.7 * n) :]\n",
    "\n",
    "    # Language probe - check for multiple classes\n",
    "    unique_langs = np.unique(y_lang[test_idx])\n",
    "    if len(unique_langs) < 2:\n",
    "        print(f\"  SKIP language probe - only {len(unique_langs)} class\")\n",
    "        lang_acc = 1.0 / max(1, len(np.unique(y_lang)))\n",
    "        lang_chance = lang_acc\n",
    "    else:\n",
    "        lang_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        lang_probe.fit(X_scaled[train_idx], y_lang[train_idx])\n",
    "        lang_acc = (lang_probe.predict(X_scaled[test_idx]) == y_lang[test_idx]).mean()\n",
    "        lang_chance = 1.0 / len(unique_langs)\n",
    "\n",
    "    # Period probe - same check\n",
    "    unique_periods = np.unique(y_period[test_idx])\n",
    "    if len(unique_periods) < 2:\n",
    "        print(f\"  SKIP period probe - only {len(unique_periods)} class\")\n",
    "        period_acc = 1.0 / max(1, len(np.unique(y_period)))\n",
    "        period_chance = period_acc\n",
    "    else:\n",
    "        period_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        period_probe.fit(X_scaled[train_idx], y_period[train_idx])\n",
    "        period_acc = (period_probe.predict(X_scaled[test_idx]) == y_period[test_idx]).mean()\n",
    "        period_chance = 1.0 / len(unique_periods)\n",
    "\n",
    "    lang_status = \"INVARIANT\" if lang_acc < lang_chance + 0.15 else \"NOT invariant\"\n",
    "    period_status = \"INVARIANT\" if period_acc < period_chance + 0.15 else \"NOT invariant\"\n",
    "\n",
    "    probe_results[split_name] = {\n",
    "        \"language_acc\": lang_acc,\n",
    "        \"language_chance\": lang_chance,\n",
    "        \"language_status\": lang_status,\n",
    "        \"period_acc\": period_acc,\n",
    "        \"period_chance\": period_chance,\n",
    "        \"period_status\": period_status,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nRESULTS:\")\n",
    "    print(f\"  Language: {lang_acc:.1%} (chance: {lang_chance:.1%}) -> {lang_status}\")\n",
    "    print(f\"  Period:   {period_acc:.1%} (chance: {period_chance:.1%}) -> {period_status}\")\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Probe tests complete\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===== v10.9: GEOMETRIC ANALYSIS =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GEOMETRIC ANALYSIS (v10.9)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nDiscovering interpretable axes in latent space...\")\n",
    "\n",
    "# Test pairs for axis discovery (cross-lingual)\n",
    "OBLIGATION_PERMISSION_TRAIN = [\n",
    "    # English - training set\n",
    "    (\"You must help the elderly\", \"You may help the elderly\"),\n",
    "    (\"He is required to pay\", \"He is allowed to pay\"),\n",
    "    (\"Parents must protect children\", \"Parents may protect children\"),\n",
    "]\n",
    "\n",
    "OBLIGATION_PERMISSION_TEST = [\n",
    "    # Chinese\n",
    "    (\"\u541b\u5b50\u5fc5\u5b5d\", \"\u541b\u5b50\u53ef\u5b5d\"),  # Gentleman must/may be filial\n",
    "    (\"\u6c11\u5fc5\u5f9e\u6cd5\", \"\u6c11\u53ef\u5f9e\u6cd5\"),  # People must/may follow law\n",
    "    # Arabic\n",
    "    (\"\u064a\u062c\u0628 \u0639\u0644\u064a\u0643 \u0623\u0646 \u062a\u0633\u0627\u0639\u062f\", \"\u064a\u062c\u0648\u0632 \u0644\u0643 \u0623\u0646 \u062a\u0633\u0627\u0639\u062f\"),  # You must/may help\n",
    "    # Hebrew\n",
    "    (\"\u05d7\u05d9\u05d9\u05d1 \u05dc\u05db\u05d1\u05d3\", \"\u05de\u05d5\u05ea\u05e8 \u05dc\u05db\u05d1\u05d3\"),  # Obligated/permitted to honor\n",
    "    # English - held out\n",
    "    (\"She must attend\", \"She may attend\"),\n",
    "]\n",
    "\n",
    "HARM_CARE_PAIRS = [\n",
    "    (\"He injured the child\", \"He protected the child\"),\n",
    "    (\"\u6bba\u4eba\u8005\", \"\u6551\u4eba\u8005\"),  # One who kills / one who saves\n",
    "    (\"\u0638\u0644\u0645 \u0627\u0644\u0636\u0639\u064a\u0641\", \"\u0631\u062d\u0645 \u0627\u0644\u0636\u0639\u064a\u0641\"),  # Oppressed / showed mercy to the weak\n",
    "    (\"She hurt the patient\", \"She healed the patient\"),\n",
    "]\n",
    "\n",
    "ROLE_SWAP_PAIRS = [\n",
    "    (\"The master commands the servant\", \"The servant commands the master\"),\n",
    "    (\"\u541b\u547d\u81e3\", \"\u81e3\u547d\u541b\"),  # Lord commands minister / minister commands lord\n",
    "    (\"\u0627\u0644\u0623\u0628 \u064a\u0623\u0645\u0631 \u0627\u0644\u0627\u0628\u0646\", \"\u0627\u0644\u0627\u0628\u0646 \u064a\u0623\u0645\u0631 \u0627\u0644\u0623\u0628\"),  # Father commands son / son commands father\n",
    "    (\"The parent guides the child\", \"The child guides the parent\"),\n",
    "]\n",
    "\n",
    "geometry_results = {}\n",
    "\n",
    "# Use the best model from mixed_baseline split for geometric analysis\n",
    "model_path = f\"{SAVE_DIR}/best_mixed_baseline.pt\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"\\nLoading model for geometric analysis...\")\n",
    "    model = BIPModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    analyzer = GeometricAnalyzer(model, tokenizer, device)\n",
    "\n",
    "    # 1. Find obligation/permission axis\n",
    "    print(\"\\n--- Obligation/Permission Axis ---\")\n",
    "    obl_texts = [p[0] for p in OBLIGATION_PERMISSION_TRAIN]\n",
    "    perm_texts = [p[1] for p in OBLIGATION_PERMISSION_TRAIN]\n",
    "    obl_perm_axis = analyzer.find_direction(obl_texts, perm_texts)\n",
    "\n",
    "    # Test transfer to other languages\n",
    "    transfer_acc = analyzer.test_direction_transfer(obl_perm_axis, OBLIGATION_PERMISSION_TEST)\n",
    "    print(f\"  Direction found from English training pairs\")\n",
    "    print(f\"  Transfer accuracy to other languages: {transfer_acc:.1%}\")\n",
    "    axis_status = \"STRONG\" if transfer_acc > 0.8 else \"WEAK\" if transfer_acc > 0.5 else \"FAILED\"\n",
    "    print(f\"  Status: {axis_status} deontic axis\")\n",
    "\n",
    "    geometry_results[\"obligation_permission\"] = {\n",
    "        \"transfer_accuracy\": transfer_acc,\n",
    "        \"status\": axis_status,\n",
    "    }\n",
    "\n",
    "    # 2. Find harm/care axis\n",
    "    print(\"\\n--- Harm/Care Axis ---\")\n",
    "    harm_texts = [p[0] for p in HARM_CARE_PAIRS]\n",
    "    care_texts = [p[1] for p in HARM_CARE_PAIRS]\n",
    "    harm_care_axis = analyzer.find_direction(harm_texts, care_texts)\n",
    "\n",
    "    # Check axis orthogonality\n",
    "    axis_correlation = abs(np.dot(obl_perm_axis, harm_care_axis))\n",
    "    print(f\"  Axis found\")\n",
    "    print(f\"  Correlation with obl/perm axis: {axis_correlation:.3f}\")\n",
    "    orthogonal = \"ORTHOGONAL\" if axis_correlation < 0.3 else \"CORRELATED\"\n",
    "    print(f\"  Status: {orthogonal}\")\n",
    "\n",
    "    geometry_results[\"harm_care\"] = {\n",
    "        \"axis_correlation\": axis_correlation,\n",
    "        \"orthogonal\": axis_correlation < 0.3,\n",
    "    }\n",
    "\n",
    "    # 3. Role swap analysis\n",
    "    print(\"\\n--- Role Swap Analysis ---\")\n",
    "    role_analysis = analyzer.role_swap_analysis(ROLE_SWAP_PAIRS)\n",
    "    print(\n",
    "        f\"  Mean consistency: {role_analysis['consistency']:.3f} +/- {role_analysis['consistency_std']:.3f}\"\n",
    "    )\n",
    "    role_status = \"CONSISTENT\" if role_analysis[\"consistency\"] > 0.9 else \"VARIABLE\"\n",
    "    print(f\"  Status: {role_status} agent/patient transformation\")\n",
    "\n",
    "    geometry_results[\"role_swap\"] = {\n",
    "        \"consistency\": role_analysis[\"consistency\"],\n",
    "        \"consistency_std\": role_analysis[\"consistency_std\"],\n",
    "        \"status\": role_status,\n",
    "    }\n",
    "\n",
    "    # 4. PCA on all structural pairs\n",
    "    print(\"\\n--- PCA Analysis ---\")\n",
    "    all_concept_pairs = {\n",
    "        \"obligation_permission\": OBLIGATION_PERMISSION_TRAIN + OBLIGATION_PERMISSION_TEST,\n",
    "        \"harm_care\": HARM_CARE_PAIRS,\n",
    "    }\n",
    "    pca_results = analyzer.pca_on_pairs(all_concept_pairs)\n",
    "\n",
    "    cumsum = np.cumsum(pca_results[\"explained_variance_ratio\"])\n",
    "    n_components_90 = np.argmax(cumsum > 0.9) + 1 if any(cumsum > 0.9) else len(cumsum)\n",
    "\n",
    "    print(f\"  Explained variance ratio: {pca_results['explained_variance_ratio'][:5]}\")\n",
    "    print(f\"  Components for 90% variance: {n_components_90}\")\n",
    "    pca_status = \"LOW-DIM\" if n_components_90 <= 3 else \"HIGH-DIM\"\n",
    "    print(f\"  Status: {pca_status} moral structure\")\n",
    "\n",
    "    geometry_results[\"pca\"] = {\n",
    "        \"explained_variance\": pca_results[\"explained_variance_ratio\"].tolist(),\n",
    "        \"n_components_90pct\": n_components_90,\n",
    "        \"status\": pca_status,\n",
    "    }\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(f\"\\nSkipping geometric analysis - no model at {model_path}\")\n",
    "    geometry_results = {\"error\": \"No model available\"}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Geometric analysis complete\")\n",
    "print(\"=\" * 60)\n"
   ],
   "id": "cell_8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Fuzz Testing v10.12: Structural vs Surface Perturbations { display-mode: \"form\" }\n",
    "#@markdown Tests whether structural perturbations move embeddings more than surface perturbations.\n",
    "#@markdown **Run immediately after Cell 6/7 training completes - uses model in memory.**\n",
    "#@markdown\n",
    "#@markdown v10.12 enhancements:\n",
    "#@markdown - **30+ samples per category** for 6-sigma statistical confidence\n",
    "#@markdown - **Runtime-adaptive thresholds** based on GPU type (L4/A100/T4)\n",
    "#@markdown - **Extended bond type coverage** including cross-cultural scenarios\n",
    "#@markdown - **Bootstrap confidence intervals** for robust statistics\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Enable Fuzz Testing\n",
    "RUN_FUZZ_TEST = True  #@param {type:\"boolean\"}\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "\n",
    "# ============================================================================\n",
    "# RUNTIME DETECTION AND ADAPTIVE THRESHOLDS\n",
    "# ============================================================================\n",
    "\n",
    "def detect_runtime() -> Dict:\n",
    "    \"\"\"Detect GPU type and set appropriate thresholds.\"\"\"\n",
    "    runtime_config = {\n",
    "        \"gpu_type\": \"unknown\",\n",
    "        \"vram_gb\": 0,\n",
    "        \"batch_size\": 16,\n",
    "        \"max_scenarios\": 50,\n",
    "        \"bootstrap_samples\": 1000,\n",
    "    }\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0).lower()\n",
    "        vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        runtime_config[\"vram_gb\"] = vram\n",
    "\n",
    "        if \"a100\" in gpu_name:\n",
    "            runtime_config[\"gpu_type\"] = \"A100\"\n",
    "            runtime_config[\"batch_size\"] = 64\n",
    "            runtime_config[\"max_scenarios\"] = 100\n",
    "            runtime_config[\"bootstrap_samples\"] = 5000\n",
    "        elif \"l4\" in gpu_name:\n",
    "            runtime_config[\"gpu_type\"] = \"L4\"\n",
    "            runtime_config[\"batch_size\"] = 32\n",
    "            runtime_config[\"max_scenarios\"] = 75\n",
    "            runtime_config[\"bootstrap_samples\"] = 2000\n",
    "        elif \"t4\" in gpu_name:\n",
    "            runtime_config[\"gpu_type\"] = \"T4\"\n",
    "            runtime_config[\"batch_size\"] = 16\n",
    "            runtime_config[\"max_scenarios\"] = 50\n",
    "            runtime_config[\"bootstrap_samples\"] = 1000\n",
    "        elif \"v100\" in gpu_name:\n",
    "            runtime_config[\"gpu_type\"] = \"V100\"\n",
    "            runtime_config[\"batch_size\"] = 32\n",
    "            runtime_config[\"max_scenarios\"] = 60\n",
    "            runtime_config[\"bootstrap_samples\"] = 2000\n",
    "        else:\n",
    "            # Default based on VRAM\n",
    "            if vram >= 40:\n",
    "                runtime_config[\"gpu_type\"] = \"high_vram\"\n",
    "                runtime_config[\"batch_size\"] = 64\n",
    "                runtime_config[\"max_scenarios\"] = 100\n",
    "            elif vram >= 20:\n",
    "                runtime_config[\"gpu_type\"] = \"medium_vram\"\n",
    "                runtime_config[\"batch_size\"] = 32\n",
    "                runtime_config[\"max_scenarios\"] = 75\n",
    "            else:\n",
    "                runtime_config[\"gpu_type\"] = \"low_vram\"\n",
    "                runtime_config[\"batch_size\"] = 16\n",
    "                runtime_config[\"max_scenarios\"] = 40\n",
    "\n",
    "    return runtime_config\n",
    "\n",
    "if not RUN_FUZZ_TEST:\n",
    "    print(\"Fuzz testing disabled. Check RUN_FUZZ_TEST to enable.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FUZZ TESTING v10.12: STRUCTURAL VS SURFACE PERTURBATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "\n",
    "    # Detect runtime and set thresholds\n",
    "    RUNTIME = detect_runtime()\n",
    "    print(f\"Runtime detected: {RUNTIME['gpu_type']} ({RUNTIME['vram_gb']:.1f} GB VRAM)\")\n",
    "    print(f\"Batch size: {RUNTIME['batch_size']}, Max scenarios: {RUNTIME['max_scenarios']}\")\n",
    "    print(f\"Bootstrap samples: {RUNTIME['bootstrap_samples']}\")\n",
    "    print()\n",
    "\n",
    "    # ========================================================================\n",
    "    # USE EXISTING MODEL FROM TRAINING SESSION\n",
    "    # ========================================================================\n",
    "\n",
    "    try:\n",
    "        if hasattr(model, 'module'):\n",
    "            _fuzz_model = model.module\n",
    "            print(\"Using unwrapped model from Accelerator\")\n",
    "        else:\n",
    "            _fuzz_model = model\n",
    "            print(\"Using model from training session\")\n",
    "\n",
    "        _fuzz_model.eval()\n",
    "        device = next(_fuzz_model.parameters()).device\n",
    "        print(f\"Device: {device}\")\n",
    "\n",
    "    except NameError:\n",
    "        print(\"ERROR: No model found in memory!\")\n",
    "        print(\"Please run training (Cell 6/7) first, or load from checkpoint manually.\")\n",
    "        RUN_FUZZ_TEST = False\n",
    "\n",
    "    if RUN_FUZZ_TEST:\n",
    "        print()\n",
    "\n",
    "        # ====================================================================\n",
    "        # EMBEDDING FUNCTIONS\n",
    "        # ====================================================================\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def get_embedding(text: str) -> np.ndarray:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                              max_length=128, padding=\"max_length\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            z = _fuzz_model.get_bond_embedding(inputs['input_ids'], inputs['attention_mask'])\n",
    "            return z.cpu().numpy().flatten()\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def get_embeddings_batch(texts: List[str]) -> np.ndarray:\n",
    "            \"\"\"Batch embedding for efficiency.\"\"\"\n",
    "            all_embeddings = []\n",
    "            batch_size = RUNTIME['batch_size']\n",
    "\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i+batch_size]\n",
    "                inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True,\n",
    "                                  max_length=128, padding=\"max_length\")\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                z = _fuzz_model.get_bond_embedding(inputs['input_ids'], inputs['attention_mask'])\n",
    "                all_embeddings.append(z.cpu().numpy())\n",
    "\n",
    "            return np.vstack(all_embeddings)\n",
    "\n",
    "        def cosine_distance(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "            sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-9)\n",
    "            return 1 - sim\n",
    "\n",
    "        # ====================================================================\n",
    "        # EXPANDED BASE SCENARIOS (30+ for statistical power)\n",
    "        # ====================================================================\n",
    "\n",
    "        BASE_SCENARIOS = [\n",
    "            # OBLIGATION / DUTY (8 scenarios)\n",
    "            {\"text\": \"John borrowed money from Mary and promised to repay it by Friday.\", \"bond_type\": \"OBLIGATION\", \"category\": \"promise\"},\n",
    "            {\"text\": \"The doctor has a duty to keep patient information confidential.\", \"bond_type\": \"DUTY\", \"category\": \"professional\"},\n",
    "            {\"text\": \"Parents must protect their children from harm.\", \"bond_type\": \"DUTY\", \"category\": \"familial\"},\n",
    "            {\"text\": \"The teacher promised to grade all exams by Monday.\", \"bond_type\": \"OBLIGATION\", \"category\": \"promise\"},\n",
    "            {\"text\": \"Soldiers are required to follow orders from superior officers.\", \"bond_type\": \"DUTY\", \"category\": \"institutional\"},\n",
    "            {\"text\": \"The witness swore to tell the truth in court.\", \"bond_type\": \"OBLIGATION\", \"category\": \"oath\"},\n",
    "            {\"text\": \"Citizens must pay their taxes to the government.\", \"bond_type\": \"DUTY\", \"category\": \"civic\"},\n",
    "            {\"text\": \"The contractor agreed to complete the building within six months.\", \"bond_type\": \"OBLIGATION\", \"category\": \"contract\"},\n",
    "\n",
    "            # CARE / HELP (8 scenarios)\n",
    "            {\"text\": \"Sarah helped her neighbor carry groceries, expecting nothing in return.\", \"bond_type\": \"CARE\", \"category\": \"altruism\"},\n",
    "            {\"text\": \"The nurse stayed late to comfort the dying patient.\", \"bond_type\": \"CARE\", \"category\": \"compassion\"},\n",
    "            {\"text\": \"She donated her savings to help earthquake victims.\", \"bond_type\": \"CARE\", \"category\": \"charity\"},\n",
    "            {\"text\": \"The mentor guided the young artist without asking for payment.\", \"bond_type\": \"CARE\", \"category\": \"guidance\"},\n",
    "            {\"text\": \"He gave his coat to the homeless man shivering in the cold.\", \"bond_type\": \"CARE\", \"category\": \"generosity\"},\n",
    "            {\"text\": \"The stranger stopped to help change the flat tire.\", \"bond_type\": \"CARE\", \"category\": \"assistance\"},\n",
    "            {\"text\": \"She listened patiently as he shared his troubles.\", \"bond_type\": \"CARE\", \"category\": \"empathy\"},\n",
    "            {\"text\": \"The community gathered to rebuild the family's burned house.\", \"bond_type\": \"CARE\", \"category\": \"solidarity\"},\n",
    "\n",
    "            # HARM / VIOLATION (8 scenarios)\n",
    "            {\"text\": \"He stole the wallet from the elderly woman.\", \"bond_type\": \"HARM\", \"category\": \"theft\"},\n",
    "            {\"text\": \"The company violated the contract by delivering late.\", \"bond_type\": \"VIOLATION\", \"category\": \"breach\"},\n",
    "            {\"text\": \"She spread false rumors to destroy his reputation.\", \"bond_type\": \"HARM\", \"category\": \"slander\"},\n",
    "            {\"text\": \"The politician broke his campaign promises after election.\", \"bond_type\": \"VIOLATION\", \"category\": \"betrayal\"},\n",
    "            {\"text\": \"He poisoned the well that the village depended on.\", \"bond_type\": \"HARM\", \"category\": \"sabotage\"},\n",
    "            {\"text\": \"The trustee embezzled funds from the charity.\", \"bond_type\": \"VIOLATION\", \"category\": \"fraud\"},\n",
    "            {\"text\": \"She abandoned her children to pursue her own interests.\", \"bond_type\": \"VIOLATION\", \"category\": \"abandonment\"},\n",
    "            {\"text\": \"The invaders destroyed the sacred temple.\", \"bond_type\": \"HARM\", \"category\": \"desecration\"},\n",
    "\n",
    "            # FAIRNESS / JUSTICE (8 scenarios)\n",
    "            {\"text\": \"The judge ruled fairly, giving each side equal consideration.\", \"bond_type\": \"FAIRNESS\", \"category\": \"impartiality\"},\n",
    "            {\"text\": \"She forgave him for breaking his promise.\", \"bond_type\": \"FORGIVENESS\", \"category\": \"mercy\"},\n",
    "            {\"text\": \"The council distributed resources equally among all villages.\", \"bond_type\": \"FAIRNESS\", \"category\": \"equity\"},\n",
    "            {\"text\": \"He returned the extra change the shopkeeper gave by mistake.\", \"bond_type\": \"FAIRNESS\", \"category\": \"honesty\"},\n",
    "            {\"text\": \"The elder mediated the dispute without favoring either party.\", \"bond_type\": \"FAIRNESS\", \"category\": \"mediation\"},\n",
    "            {\"text\": \"She gave credit to her assistant for the discovery.\", \"bond_type\": \"FAIRNESS\", \"category\": \"attribution\"},\n",
    "            {\"text\": \"The king pardoned the rebels who surrendered peacefully.\", \"bond_type\": \"FORGIVENESS\", \"category\": \"clemency\"},\n",
    "            {\"text\": \"They compensated the wrongly accused man for his suffering.\", \"bond_type\": \"FAIRNESS\", \"category\": \"restitution\"},\n",
    "\n",
    "            # CROSS-CULTURAL BOND TYPES (8 scenarios)\n",
    "            {\"text\": \"The student honored his teacher by caring for him in old age.\", \"bond_type\": \"PIETY\", \"category\": \"filial\"},\n",
    "            {\"text\": \"She upheld the family honor by keeping her grandfather's promise.\", \"bond_type\": \"LOYALTY\", \"category\": \"ancestral\"},\n",
    "            {\"text\": \"The warrior spared his defeated enemy as custom demanded.\", \"bond_type\": \"HONOR\", \"category\": \"chivalry\"},\n",
    "            {\"text\": \"He returned the sacred artifact to the temple it was taken from.\", \"bond_type\": \"REVERENCE\", \"category\": \"restoration\"},\n",
    "            {\"text\": \"The host provided shelter to the stranger as hospitality required.\", \"bond_type\": \"HOSPITALITY\", \"category\": \"xenia\"},\n",
    "            {\"text\": \"She maintained ritual purity to preserve cosmic order.\", \"bond_type\": \"PURITY\", \"category\": \"ritual\"},\n",
    "            {\"text\": \"The merchant kept his word even when it meant financial loss.\", \"bond_type\": \"INTEGRITY\", \"category\": \"commercial\"},\n",
    "            {\"text\": \"The community shunned him for violating the ancestral taboo.\", \"bond_type\": \"TABOO\", \"category\": \"prohibition\"},\n",
    "        ]\n",
    "\n",
    "        # ====================================================================\n",
    "        # PERTURBATION GENERATORS\n",
    "        # ====================================================================\n",
    "\n",
    "        # Name substitution pools for variety\n",
    "        NAME_POOLS = {\n",
    "            \"male\": [\"John\", \"Michael\", \"David\", \"James\", \"Robert\", \"William\", \"Thomas\", \"Daniel\"],\n",
    "            \"female\": [\"Mary\", \"Sarah\", \"Emma\", \"Lisa\", \"Anna\", \"Rachel\", \"Rebecca\", \"Hannah\"],\n",
    "        }\n",
    "\n",
    "        IRRELEVANT_DETAILS = [\n",
    "            \" It was Tuesday.\", \" The room was blue.\", \" Last summer.\",\n",
    "            \" The weather was pleasant.\", \" It happened at noon.\",\n",
    "            \" The year was uncertain.\", \" Birds sang nearby.\",\n",
    "            \" The moon was full.\", \" Rain had fallen earlier.\",\n",
    "            \" The road was dusty.\", \" Flowers bloomed outside.\",\n",
    "        ]\n",
    "\n",
    "        SYNONYMS = {\n",
    "            \"money\": [\"cash\", \"funds\", \"currency\"],\n",
    "            \"groceries\": [\"bags\", \"supplies\", \"provisions\"],\n",
    "            \"house\": [\"home\", \"dwelling\", \"residence\"],\n",
    "            \"promise\": [\"vow\", \"pledge\", \"commitment\"],\n",
    "            \"help\": [\"assist\", \"aid\", \"support\"],\n",
    "            \"truth\": [\"facts\", \"reality\", \"honesty\"],\n",
    "        }\n",
    "\n",
    "        def surface_perturbations(scenario: Dict) -> List[Dict]:\n",
    "            \"\"\"Generate surface perturbations that shouldn't change moral meaning.\"\"\"\n",
    "            text = scenario[\"text\"]\n",
    "            perturbs = []\n",
    "\n",
    "            # Name changes (multiple variations)\n",
    "            for old_name in NAME_POOLS[\"male\"] + NAME_POOLS[\"female\"]:\n",
    "                if old_name in text:\n",
    "                    for new_name in (NAME_POOLS[\"male\"] if old_name in NAME_POOLS[\"male\"] else NAME_POOLS[\"female\"]):\n",
    "                        if new_name != old_name:\n",
    "                            new_text = text.replace(old_name, new_name)\n",
    "                            if new_text != text:\n",
    "                                perturbs.append({\"text\": new_text, \"type\": \"name_change\", \"original\": old_name, \"new\": new_name})\n",
    "                                if len(perturbs) >= 3:  # Limit per scenario\n",
    "                                    break\n",
    "\n",
    "            # Irrelevant detail additions\n",
    "            for detail in IRRELEVANT_DETAILS[:4]:\n",
    "                perturbs.append({\"text\": text + detail, \"type\": \"irrelevant_detail\", \"detail\": detail})\n",
    "\n",
    "            # Synonym substitutions\n",
    "            new_text = text\n",
    "            for word, synonyms in SYNONYMS.items():\n",
    "                if word in new_text.lower():\n",
    "                    for syn in synonyms[:2]:\n",
    "                        test_text = new_text.replace(word, syn)\n",
    "                        if test_text != new_text:\n",
    "                            perturbs.append({\"text\": test_text, \"type\": \"synonym\", \"original\": word, \"new\": syn})\n",
    "                            break\n",
    "\n",
    "            return perturbs\n",
    "\n",
    "        def structural_perturbations(scenario: Dict) -> List[Dict]:\n",
    "            \"\"\"Generate structural perturbations that SHOULD change moral meaning.\"\"\"\n",
    "            text = scenario[\"text\"]\n",
    "            perturbs = []\n",
    "\n",
    "            # Role swaps (agent/patient reversal)\n",
    "            role_swaps = [\n",
    "                (\"John borrowed money from Mary\", \"Mary borrowed money from John\"),\n",
    "                (\"He stole the wallet from the elderly woman\", \"The elderly woman stole the wallet from him\"),\n",
    "                (\"She spread false rumors to destroy his reputation\", \"He spread false rumors to destroy her reputation\"),\n",
    "                (\"Sarah helped her neighbor\", \"Her neighbor helped Sarah\"),\n",
    "                (\"The teacher promised to grade\", \"The students promised to grade\"),\n",
    "                (\"He gave his coat to the homeless man\", \"The homeless man gave his coat to him\"),\n",
    "                (\"She donated her savings to help\", \"They donated their savings to help her\"),\n",
    "                (\"The host provided shelter to the stranger\", \"The stranger provided shelter to the host\"),\n",
    "            ]\n",
    "            for orig, swap in role_swaps:\n",
    "                if orig in text:\n",
    "                    perturbs.append({\"text\": text.replace(orig, swap), \"type\": \"role_swap\", \"swap\": (orig, swap)})\n",
    "\n",
    "            # Obligation to permission\n",
    "            obl_to_perm = [\n",
    "                (\"must protect\", \"may protect\"),\n",
    "                (\"has a duty to\", \"is allowed to\"),\n",
    "                (\"are required to\", \"are permitted to\"),\n",
    "                (\"swore to\", \"considered whether to\"),\n",
    "                (\"must pay\", \"may pay\"),\n",
    "                (\"agreed to\", \"considered whether to\"),\n",
    "            ]\n",
    "            for obl, perm in obl_to_perm:\n",
    "                if obl in text:\n",
    "                    perturbs.append({\"text\": text.replace(obl, perm), \"type\": \"obligation_to_permission\", \"change\": (obl, perm)})\n",
    "\n",
    "            # Positive to negative (harm introduction)\n",
    "            pos_to_neg = [\n",
    "                (\"helped\", \"refused to help\"),\n",
    "                (\"ruled fairly\", \"ruled unfairly\"),\n",
    "                (\"forgave\", \"refused to forgive\"),\n",
    "                (\"stayed late to comfort\", \"left early despite\"),\n",
    "                (\"donated\", \"hoarded\"),\n",
    "                (\"guided\", \"misled\"),\n",
    "                (\"gave\", \"took\"),\n",
    "                (\"stopped to help\", \"drove past without helping\"),\n",
    "                (\"listened patiently\", \"ignored\"),\n",
    "                (\"gathered to rebuild\", \"refused to rebuild\"),\n",
    "            ]\n",
    "            for pos, neg in pos_to_neg:\n",
    "                if pos in text:\n",
    "                    perturbs.append({\"text\": text.replace(pos, neg), \"type\": \"add_harm\", \"change\": (pos, neg)})\n",
    "\n",
    "            # Violation to fulfillment\n",
    "            viol_to_fulf = [\n",
    "                (\"violated\", \"honored\"),\n",
    "                (\"stole\", \"returned\"),\n",
    "                (\"breaking\", \"keeping\"),\n",
    "                (\"spread false rumors\", \"defended his reputation\"),\n",
    "                (\"broke his campaign promises\", \"kept his campaign promises\"),\n",
    "                (\"poisoned\", \"purified\"),\n",
    "                (\"embezzled\", \"safeguarded\"),\n",
    "                (\"abandoned\", \"cared for\"),\n",
    "                (\"destroyed\", \"preserved\"),\n",
    "            ]\n",
    "            for viol, fulf in viol_to_fulf:\n",
    "                if viol in text:\n",
    "                    perturbs.append({\"text\": text.replace(viol, fulf), \"type\": \"violation_to_fulfillment\", \"change\": (viol, fulf)})\n",
    "\n",
    "            return perturbs\n",
    "\n",
    "        # ====================================================================\n",
    "        # STATISTICAL ANALYSIS FUNCTIONS\n",
    "        # ====================================================================\n",
    "\n",
    "        def bootstrap_ci(data: np.ndarray, n_bootstrap: int = 1000,\n",
    "                        confidence: float = 0.95) -> Tuple[float, float, float]:\n",
    "            \"\"\"Calculate bootstrap confidence interval.\"\"\"\n",
    "            n = len(data)\n",
    "            if n < 2:\n",
    "                return data.mean(), data.mean(), data.mean()\n",
    "\n",
    "            boot_means = []\n",
    "            for _ in range(n_bootstrap):\n",
    "                sample = np.random.choice(data, size=n, replace=True)\n",
    "                boot_means.append(sample.mean())\n",
    "\n",
    "            boot_means = np.array(boot_means)\n",
    "            alpha = (1 - confidence) / 2\n",
    "            lower = np.percentile(boot_means, alpha * 100)\n",
    "            upper = np.percentile(boot_means, (1 - alpha) * 100)\n",
    "\n",
    "            return lower, data.mean(), upper\n",
    "\n",
    "        def effect_size_cohens_d(group1: np.ndarray, group2: np.ndarray) -> float:\n",
    "            \"\"\"Calculate Cohen's d effect size.\"\"\"\n",
    "            n1, n2 = len(group1), len(group2)\n",
    "            var1, var2 = group1.var(), group2.var()\n",
    "            pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n",
    "            return (group1.mean() - group2.mean()) / (pooled_std + 1e-9)\n",
    "\n",
    "        # ====================================================================\n",
    "        # RUN TESTS\n",
    "        # ====================================================================\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "        print(\"RUNNING FUZZ TESTS\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "\n",
    "        # Organize results by perturbation type\n",
    "        results_by_type = {\n",
    "            \"structural_obligation_to_permission\": [],\n",
    "            \"structural_add_harm\": [],\n",
    "            \"structural_role_swap\": [],\n",
    "            \"structural_violation_to_fulfillment\": [],\n",
    "            \"surface_name_change\": [],\n",
    "            \"surface_irrelevant_detail\": [],\n",
    "            \"surface_synonym\": [],\n",
    "        }\n",
    "\n",
    "        all_surface_distances = []\n",
    "        all_structural_distances = []\n",
    "\n",
    "        scenarios_to_run = BASE_SCENARIOS[:RUNTIME['max_scenarios']]\n",
    "        print(f\"Processing {len(scenarios_to_run)} scenarios...\")\n",
    "        print()\n",
    "\n",
    "        for i, scenario in enumerate(scenarios_to_run):\n",
    "            base_emb = get_embedding(scenario[\"text\"])\n",
    "\n",
    "            # Process surface perturbations\n",
    "            surface_perturbs = surface_perturbations(scenario)\n",
    "            for p in surface_perturbs:\n",
    "                dist = cosine_distance(base_emb, get_embedding(p[\"text\"]))\n",
    "                all_surface_distances.append(dist)\n",
    "                key = f\"surface_{p['type']}\"\n",
    "                if key in results_by_type:\n",
    "                    results_by_type[key].append(dist)\n",
    "\n",
    "            # Process structural perturbations\n",
    "            structural_perturbs = structural_perturbations(scenario)\n",
    "            for p in structural_perturbs:\n",
    "                dist = cosine_distance(base_emb, get_embedding(p[\"text\"]))\n",
    "                all_structural_distances.append(dist)\n",
    "                key = f\"structural_{p['type']}\"\n",
    "                if key in results_by_type:\n",
    "                    results_by_type[key].append(dist)\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Processed {i+1}/{len(scenarios_to_run)} scenarios...\")\n",
    "\n",
    "        print()\n",
    "        print(f\"Total surface perturbations: {len(all_surface_distances)}\")\n",
    "        print(f\"Total structural perturbations: {len(all_structural_distances)}\")\n",
    "        print()\n",
    "\n",
    "        # ====================================================================\n",
    "        # DETAILED RESULTS\n",
    "        # ====================================================================\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "        print(\"RESULTS BY PERTURBATION TYPE\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "\n",
    "        fuzz_results = {}\n",
    "\n",
    "        for ptype, distances in results_by_type.items():\n",
    "            if len(distances) > 0:\n",
    "                distances = np.array(distances)\n",
    "                lower, mean, upper = bootstrap_ci(distances, RUNTIME['bootstrap_samples'])\n",
    "                fuzz_results[ptype] = {\n",
    "                    \"mean_distance\": str(mean),\n",
    "                    \"std\": str(distances.std()),\n",
    "                    \"ci_lower\": str(lower),\n",
    "                    \"ci_upper\": str(upper),\n",
    "                    \"n\": len(distances),\n",
    "                }\n",
    "                category = \"STRUCTURAL\" if \"structural\" in ptype else \"SURFACE\"\n",
    "                print(f\"{ptype}:\")\n",
    "                print(f\"  n={len(distances)}, mean={mean:.4f}, std={distances.std():.4f}\")\n",
    "                print(f\"  95% CI: [{lower:.4f}, {upper:.4f}]\")\n",
    "                print()\n",
    "\n",
    "        # ====================================================================\n",
    "        # AGGREGATE COMPARISON\n",
    "        # ====================================================================\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "        print(\"AGGREGATE COMPARISON\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "\n",
    "        surface_arr = np.array(all_surface_distances)\n",
    "        structural_arr = np.array(all_structural_distances)\n",
    "\n",
    "        surf_lower, surf_mean, surf_upper = bootstrap_ci(surface_arr, RUNTIME['bootstrap_samples'])\n",
    "        struct_lower, struct_mean, struct_upper = bootstrap_ci(structural_arr, RUNTIME['bootstrap_samples'])\n",
    "\n",
    "        print(f\"Surface (should be SMALL):\")\n",
    "        print(f\"  mean={surf_mean:.4f}, std={surface_arr.std():.4f}\")\n",
    "        print(f\"  95% CI: [{surf_lower:.4f}, {surf_upper:.4f}]\")\n",
    "        print()\n",
    "        print(f\"Structural (should be LARGE):\")\n",
    "        print(f\"  mean={struct_mean:.4f}, std={structural_arr.std():.4f}\")\n",
    "        print(f\"  95% CI: [{struct_lower:.4f}, {struct_upper:.4f}]\")\n",
    "        print()\n",
    "\n",
    "        # Statistical tests\n",
    "        from scipy import stats\n",
    "        t_stat, p_value = stats.ttest_ind(structural_arr, surface_arr)\n",
    "\n",
    "        # Mann-Whitney U for non-parametric comparison\n",
    "        u_stat, u_pvalue = stats.mannwhitneyu(structural_arr, surface_arr, alternative='greater')\n",
    "\n",
    "        ratio = struct_mean / (surf_mean + 1e-9)\n",
    "        cohens_d = effect_size_cohens_d(structural_arr, surface_arr)\n",
    "\n",
    "        print(f\"Ratio (structural/surface): {ratio:.2f}x\")\n",
    "        print(f\"Cohen's d effect size: {cohens_d:.3f}\")\n",
    "        print(f\"t-statistic: {t_stat:.4f}, p-value: {p_value:.6f}\")\n",
    "        print(f\"Mann-Whitney U: {u_stat:.0f}, p-value: {u_pvalue:.6f}\")\n",
    "        print()\n",
    "\n",
    "        # Store comparison results\n",
    "        fuzz_results[\"comparison\"] = {\n",
    "            \"structural_mean\": str(struct_mean),\n",
    "            \"structural_ci\": [str(struct_lower), str(struct_upper)],\n",
    "            \"surface_mean\": str(surf_mean),\n",
    "            \"surface_ci\": [str(surf_lower), str(surf_upper)],\n",
    "            \"ratio\": str(ratio),\n",
    "            \"cohens_d\": str(cohens_d),\n",
    "            \"t_statistic\": t_stat,\n",
    "            \"p_value\": p_value,\n",
    "            \"mann_whitney_u\": float(u_stat),\n",
    "            \"mann_whitney_p\": float(u_pvalue),\n",
    "            \"n_structural\": len(structural_arr),\n",
    "            \"n_surface\": len(surface_arr),\n",
    "        }\n",
    "\n",
    "        # ====================================================================\n",
    "        # VERDICT (RUNTIME-ADAPTIVE THRESHOLDS)\n",
    "        # ====================================================================\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "        print(\"VERDICT\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "\n",
    "        # Adaptive thresholds based on sample size and runtime\n",
    "        if len(structural_arr) >= 30 and len(surface_arr) >= 30:\n",
    "            # Strong statistical power - use stricter thresholds\n",
    "            strong_ratio = 3.0\n",
    "            moderate_ratio = 2.0\n",
    "            weak_ratio = 1.5\n",
    "            p_threshold = 0.01\n",
    "        elif len(structural_arr) >= 15:\n",
    "            # Medium statistical power\n",
    "            strong_ratio = 2.5\n",
    "            moderate_ratio = 1.8\n",
    "            weak_ratio = 1.3\n",
    "            p_threshold = 0.05\n",
    "        else:\n",
    "            # Low statistical power - use looser thresholds but note uncertainty\n",
    "            strong_ratio = 2.0\n",
    "            moderate_ratio = 1.5\n",
    "            weak_ratio = 1.2\n",
    "            p_threshold = 0.10\n",
    "\n",
    "        verdict = \"NOT_SUPPORTED\"\n",
    "        verdict_detail = \"\"\n",
    "\n",
    "        if ratio >= strong_ratio and p_value < p_threshold and cohens_d > 0.8:\n",
    "            verdict = \"STRONG_SUPPORT\"\n",
    "            verdict_detail = f\"Model learned moral structure (ratio={ratio:.1f}x, d={cohens_d:.2f}, p={p_value:.4f})\"\n",
    "        elif ratio >= moderate_ratio and p_value < 0.05 and cohens_d > 0.5:\n",
    "            verdict = \"MODERATE_SUPPORT\"\n",
    "            verdict_detail = f\"Evidence for moral structure (ratio={ratio:.1f}x, d={cohens_d:.2f})\"\n",
    "        elif ratio >= weak_ratio and p_value < 0.10:\n",
    "            verdict = \"WEAK_SUPPORT\"\n",
    "            verdict_detail = f\"Weak evidence (ratio={ratio:.1f}x, needs more data)\"\n",
    "        else:\n",
    "            verdict = \"NOT_SUPPORTED\"\n",
    "            verdict_detail = \"May be encoding surface features rather than moral structure\"\n",
    "\n",
    "        print(f\"Verdict: {verdict}\")\n",
    "        print(f\"Detail: {verdict_detail}\")\n",
    "        print()\n",
    "        print(f\"Runtime: {RUNTIME['gpu_type']}\")\n",
    "        print(f\"Thresholds used: strong>{strong_ratio}x, moderate>{moderate_ratio}x, p<{p_threshold}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        fuzz_results[\"verdict\"] = verdict\n",
    "        fuzz_results[\"verdict_detail\"] = verdict_detail\n",
    "        fuzz_results[\"runtime\"] = RUNTIME\n",
    "\n",
    "        # Make results available for integration\n",
    "        FUZZ_RESULTS_V1011 = fuzz_results\n",
    "\n"
   ],
   "id": "cell_9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 9. Save & Download Results { display-mode: \"form\" }\n",
    "# @markdown Persist results to Google Drive and optionally download as zip\n",
    "\n",
    "import shutil\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Always persist results to Drive\n",
    "if SAVE_DIR and os.path.exists(SAVE_DIR):\n",
    "    print(f\"\\nPersisting to: {SAVE_DIR}\")\n",
    "\n",
    "    # Save final results JSON\n",
    "    if os.path.exists(\"results/final_results.json\"):\n",
    "        dest = f\"{SAVE_DIR}/final_results.json\"\n",
    "        shutil.copy(\"results/final_results.json\", dest)\n",
    "        print(f\"  Saved: final_results.json\")\n",
    "\n",
    "    # Save splits config\n",
    "    if os.path.exists(\"data/splits/all_splits.json\"):\n",
    "        dest = f\"{SAVE_DIR}/all_splits.json\"\n",
    "        shutil.copy(\"data/splits/all_splits.json\", dest)\n",
    "        print(f\"  Saved: all_splits.json\")\n",
    "\n",
    "    # Models are already saved to SAVE_DIR during training\n",
    "    model_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(\".pt\")]\n",
    "    if model_files:\n",
    "        print(f\"  Models already in Drive: {len(model_files)} files\")\n",
    "        for mf in model_files[:5]:\n",
    "            print(f\"    - {mf}\")\n",
    "        if len(model_files) > 5:\n",
    "            print(f\"    ... and {len(model_files)-5} more\")\n",
    "\n",
    "    print(f\"\\nResults persisted to Google Drive: {SAVE_DIR}\")\n",
    "else:\n",
    "    print(\"WARNING: SAVE_DIR not available, results only in local directories\")\n",
    "\n",
    "# Optional: Create download zip\n",
    "if CREATE_DOWNLOAD_ZIP:\n",
    "    import zipfile\n",
    "\n",
    "    zip_path = f\"BIP_v{BIP_VERSION}_results.zip\"\n",
    "    print(f\"\\n\" + \"-\" * 60)\n",
    "    print(\"Creating download package...\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "        # Results\n",
    "        if os.path.exists(\"results/final_results.json\"):\n",
    "            zf.write(\"results/final_results.json\")\n",
    "\n",
    "        # Models (from Drive)\n",
    "        if SAVE_DIR and os.path.exists(SAVE_DIR):\n",
    "            for f in os.listdir(SAVE_DIR):\n",
    "                if f.endswith(\".pt\"):\n",
    "                    zf.write(f\"{SAVE_DIR}/{f}\", f\"models/{f}\")\n",
    "\n",
    "        # Config\n",
    "        if os.path.exists(\"data/splits/all_splits.json\"):\n",
    "            zf.write(\"data/splits/all_splits.json\")\n",
    "\n",
    "    print(f\"Download package ready: {zip_path}\")\n",
    "\n",
    "    # Download in Colab, or show path otherwise\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(zip_path)\n",
    "    except ImportError:\n",
    "        print(f\"Not running in Colab. Zip saved to: {os.path.abspath(zip_path)}\")\n",
    "else:\n",
    "    print(f\"\\n(Zip download disabled - set CREATE_DOWNLOAD_ZIP=True in cell 1 to enable)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    ""
   ],
   "id": "cell_10"
  }
 ]
}