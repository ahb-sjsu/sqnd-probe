{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIP v10.8: Native-Language Moral Pattern Transfer\n\n**Bond Invariance Principle**: Moral concepts share mathematical structure across languages and cultures.\n\n## What's New in v10.8\n- **Expanded Chinese corpus** - 200+ real classical texts (Analects, Mencius, Daodejing, etc.)\n- **Expanded Islamic corpus** - 150+ real Quranic verses and Hadith\n- **Better data validation** - Warnings for insufficient corpora\n- **Minimum test size** - Skips splits with < 500 test samples\n- **Dear Abby guidance** - Clear instructions for uploading real data\n\n## Methodology\n1. Extract moral labels from NATIVE text using NATIVE patterns\n2. Train encoder with adversarial language/period invariance\n3. Test if moral concepts transfer across language families\n\n**NO English translation bridge** - pure mathematical alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Configuration & Setup { display-mode: \"form\" }\n",
    "#@markdown ## Data Source Configuration\n",
    "#@markdown Choose where to load data from:\n",
    "\n",
    "USE_DRIVE_DATA = True  #@param {type:\"boolean\"}\n",
    "#@markdown If True, load pre-processed data from persistent storage (faster)\n",
    "\n",
    "REFRESH_DATA_FROM_SOURCE = False  #@param {type:\"boolean\"}\n",
    "#@markdown If True, re-download from online sources even if cached data exists\n",
    "\n",
    "DRIVE_FOLDER = \"BIP_v10\"  #@param {type:\"string\"}\n",
    "#@markdown Folder name for persistent storage\n",
    "#@markdown ---\n",
    "#@markdown ## Model Backbone\n",
    "BACKBONE = \"MiniLM\"  #@param [\"MiniLM\", \"LaBSE\", \"XLM-R-base\", \"XLM-R-large\"]\n",
    "#@markdown - **MiniLM**: Fast, 118M params, good baseline\n",
    "#@markdown - **LaBSE**: Best cross-lingual alignment, 471M params (recommended)\n",
    "#@markdown - **XLM-R-base**: Strong multilingual, 270M params\n",
    "#@markdown - **XLM-R-large**: Strongest representations, 550M params\n",
    "\n",
    "# Backbone configurations\n",
    "BACKBONE_CONFIGS = {\n",
    "    \"MiniLM\": {\n",
    "        \"model_name\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        \"hidden_size\": 384,\n",
    "        \"recommended_batch\": {\"L4/A100\": 512, \"T4\": 256, \"2xT4\": 512, \"SMALL\": 128, \"MINIMAL/CPU\": 64},\n",
    "    },\n",
    "    \"LaBSE\": {\n",
    "        \"model_name\": \"sentence-transformers/LaBSE\",\n",
    "        \"hidden_size\": 768,\n",
    "        \"recommended_batch\": {\"L4/A100\": 256, \"T4\": 128, \"2xT4\": 256, \"SMALL\": 64, \"MINIMAL/CPU\": 32},\n",
    "    },\n",
    "    \"XLM-R-base\": {\n",
    "        \"model_name\": \"xlm-roberta-base\",\n",
    "        \"hidden_size\": 768,\n",
    "        \"recommended_batch\": {\"L4/A100\": 256, \"T4\": 128, \"2xT4\": 256, \"SMALL\": 64, \"MINIMAL/CPU\": 32},\n",
    "    },\n",
    "    \"XLM-R-large\": {\n",
    "        \"model_name\": \"xlm-roberta-large\",\n",
    "        \"hidden_size\": 1024,\n",
    "        \"recommended_batch\": {\"L4/A100\": 128, \"T4\": 64, \"2xT4\": 128, \"SMALL\": 32, \"MINIMAL/CPU\": 16},\n",
    "    },\n",
    "}\n",
    "\n",
    "BACKBONE_CONFIG = BACKBONE_CONFIGS[BACKBONE]\n",
    "MODEL_NAME = BACKBONE_CONFIG[\"model_name\"]\n",
    "BACKBONE_HIDDEN = BACKBONE_CONFIG[\"hidden_size\"]\n",
    "\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Run Setup\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "EXPERIMENT_START = time.time()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BIP v10.8 - ENVIRONMENT DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== ENVIRONMENT DETECTION =====\n",
    "# Detect which cloud platform we're running on\n",
    "\n",
    "ENV_NAME = \"UNKNOWN\"\n",
    "ENV_GPU_QUOTA = \"Unknown\"\n",
    "PERSISTENT_STORAGE = None\n",
    "DATA_DIR = \"/content\"  # Default\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"Detect cloud environment and return (name, gpu_quota, storage_path, data_dir)\"\"\"\n",
    "\n",
    "    # 1. Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        return (\"COLAB\", \"Free: T4 ~12h/day, Pro: L4/A100\", \"/content/drive/MyDrive\", \"/content\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    # 2. Kaggle Kernels\n",
    "    if os.path.exists('/kaggle'):\n",
    "        # Kaggle has /kaggle/input for datasets, /kaggle/working for output\n",
    "        return (\"KAGGLE\", \"Free: 2xT4 30h/week, TPU 30h/week\", \"/kaggle/working\", \"/kaggle/working\")\n",
    "\n",
    "    # 3. Lightning.ai Studios\n",
    "    if os.environ.get('LIGHTNING_CLOUDSPACE_HOST') or os.path.exists('/teamspace'):\n",
    "        # Lightning.ai has /teamspace/studios for persistent storage\n",
    "        return (\"LIGHTNING_AI\", \"Free: 22h/month GPU, Pro: A10G/H100\", \"/teamspace/studios\", \"/teamspace/studios\")\n",
    "\n",
    "    # 4. Paperspace Gradient\n",
    "    if os.environ.get('PAPERSPACE_NOTEBOOK_REPO_ID') or os.path.exists('/notebooks'):\n",
    "        return (\"PAPERSPACE\", \"Free: M4000 6h, Pro: A100/H100\", \"/storage\", \"/notebooks\")\n",
    "\n",
    "    # 5. Saturn Cloud\n",
    "    if os.environ.get('SATURN_RESOURCE_ID') or 'saturn' in os.environ.get('HOSTNAME', '').lower():\n",
    "        return (\"SATURN_CLOUD\", \"Free: T4 10h/month, Pro: A10G/A100\", \"/home/jovyan/workspace\", \"/home/jovyan\")\n",
    "\n",
    "    # 6. HuggingFace Spaces\n",
    "    if os.environ.get('SPACE_ID') or os.environ.get('HF_SPACE_ID'):\n",
    "        return (\"HUGGINGFACE_SPACES\", \"Free: CPU only, ZeroGPU: A10G/A100 quota\", \"/data\", \"/home/user/app\")\n",
    "\n",
    "    # 7. AWS SageMaker Studio Lab\n",
    "    if os.path.exists('/home/studio-lab-user'):\n",
    "        return (\"SAGEMAKER_STUDIO_LAB\", \"Free: T4 4h/session, 24h max/day\", \"/home/studio-lab-user\", \"/home/studio-lab-user\")\n",
    "\n",
    "    # 8. Deepnote\n",
    "    if os.environ.get('DEEPNOTE_PROJECT_ID'):\n",
    "        return (\"DEEPNOTE\", \"Free: CPU, Pro: T4/A10G\", \"/work\", \"/work\")\n",
    "\n",
    "    # 9. Local/Unknown\n",
    "    return (\"LOCAL\", \"Depends on local hardware\", os.getcwd(), os.getcwd())\n",
    "\n",
    "ENV_NAME, ENV_GPU_QUOTA, PERSISTENT_STORAGE, DATA_DIR = detect_environment()\n",
    "\n",
    "print(f\"\\nEnvironment: {ENV_NAME}\")\n",
    "print(f\"GPU Quota:   {ENV_GPU_QUOTA}\")\n",
    "print(f\"Storage:     {PERSISTENT_STORAGE}\")\n",
    "print(f\"Data Dir:    {DATA_DIR}\")\n",
    "\n",
    "# Environment-specific setup\n",
    "ENV_TIPS = {\n",
    "    \"COLAB\": [\n",
    "        \"Tip: Use GPU runtime (Runtime -> Change runtime type -> T4 GPU)\",\n",
    "        \"Tip: Colab Pro gives L4 GPU access (~2x faster than T4)\"\n",
    "    ],\n",
    "    \"KAGGLE\": [\n",
    "        \"Tip: Enable GPU (Settings -> Accelerator -> GPU T4 x2)\",\n",
    "        \"Tip: 30h/week GPU quota resets every Saturday\",\n",
    "        \"Tip: Upload data as a Kaggle Dataset for persistence\"\n",
    "    ],\n",
    "    \"LIGHTNING_AI\": [\n",
    "        \"Tip: Select GPU studio (A10G recommended for this workload)\",\n",
    "        \"Tip: /teamspace/studios persists across sessions\"\n",
    "    ],\n",
    "    \"PAPERSPACE\": [\n",
    "        \"Tip: Use /storage for persistent data across runs\",\n",
    "        \"Tip: Free tier has 6h/month GPU limit\"\n",
    "    ],\n",
    "    \"SATURN_CLOUD\": [\n",
    "        \"Tip: Start a T4 instance from the Resources tab\",\n",
    "        \"Tip: 10h/month free GPU quota\"\n",
    "    ],\n",
    "    \"HUGGINGFACE_SPACES\": [\n",
    "        \"Tip: ZeroGPU provides A10G/A100 access with quota system\",\n",
    "        \"Tip: Use Gradio/Streamlit for interactive demos\"\n",
    "    ],\n",
    "    \"SAGEMAKER_STUDIO_LAB\": [\n",
    "        \"Tip: Request GPU runtime from the launcher\",\n",
    "        \"Tip: Sessions timeout after 4h, max 24h/day\"\n",
    "    ],\n",
    "    \"LOCAL\": [\n",
    "        \"Tip: Running locally - ensure CUDA is installed for GPU support\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"\\n\" + \"-\"*60)\n",
    "print(\"ENVIRONMENT TIPS:\")\n",
    "for tip in ENV_TIPS.get(ENV_NAME, [\"No specific tips for this environment\"]):\n",
    "    print(f\"  {tip}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# ===== INSTALL DEPENDENCIES =====\n",
    "import subprocess\n",
    "\n",
    "print(\"\\nInstalling dependencies...\")\n",
    "for pkg in [\"transformers\", \"sentence-transformers\", \"pandas\", \"tqdm\", \"scikit-learn\", \"pyyaml\", \"psutil\", \"datasets\"]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU DETECTION & RESOURCE ALLOCATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Detect hardware\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    GPU_COUNT = torch.cuda.device_count()\n",
    "else:\n",
    "    GPU_NAME = \"CPU\"\n",
    "    VRAM_GB = 0\n",
    "    GPU_COUNT = 0\n",
    "\n",
    "RAM_GB = psutil.virtual_memory().total / 1e9\n",
    "\n",
    "print(f\"\\nDetected Hardware:\")\n",
    "print(f\"  GPU:  {GPU_NAME}\" + (f\" (x{GPU_COUNT})\" if GPU_COUNT > 1 else \"\"))\n",
    "print(f\"  VRAM: {VRAM_GB:.1f} GB\" + (f\" (total: {VRAM_GB*GPU_COUNT:.1f} GB)\" if GPU_COUNT > 1 else \"\"))\n",
    "print(f\"  RAM:  {RAM_GB:.1f} GB\")\n",
    "\n",
    "# Set optimal parameters based on hardware\n",
    "if VRAM_GB >= 22:      # L4 (24GB) or A100\n",
    "    GPU_TIER = \"L4/A100\"\n",
    "elif VRAM_GB >= 14:    # T4 (16GB)\n",
    "    GPU_TIER = \"T4\"\n",
    "elif VRAM_GB >= 10:\n",
    "    GPU_TIER = \"SMALL\"\n",
    "else:\n",
    "    GPU_TIER = \"MINIMAL/CPU\"\n",
    "\n",
    "# Kaggle with 2xT4 can use larger batch\n",
    "if ENV_NAME == \"KAGGLE\" and GPU_COUNT >= 2:\n",
    "    GPU_TIER = \"2xT4\"\n",
    "    print(f\"  ** Kaggle 2xT4 detected **\")\n",
    "\n",
    "# Get backbone-specific batch size\n",
    "BATCH_SIZE = BACKBONE_CONFIG[\"recommended_batch\"].get(GPU_TIER, 64)\n",
    "print(f\"  Backbone: {BACKBONE} -> batch size {BATCH_SIZE}\")\n",
    "\n",
    "MAX_PER_LANG = 50000  # Language sample limit\n",
    "CPU_CORES = os.cpu_count() or 2\n",
    "NUM_WORKERS = min(4, CPU_CORES - 1) if RAM_GB >= 24 and VRAM_GB >= 14 else 0\n",
    "MAX_TEST_SAMPLES = 20000\n",
    "LR = 2e-5 * (BATCH_SIZE / 256)\n",
    "\n",
    "print(f\"\\n\" + \"-\"*60)\n",
    "print(f\"OPTIMAL SETTINGS:\")\n",
    "print(f\"-\"*60)\n",
    "print(f\"  Environment:     {ENV_NAME}\")\n",
    "print(f\"  GPU Tier:        {GPU_TIER}\")\n",
    "print(f\"  Backbone:        {BACKBONE}\")\n",
    "print(f\"  Batch size:      {BATCH_SIZE}\")\n",
    "print(f\"  Max per lang:    {MAX_PER_LANG:,}\")\n",
    "print(f\"  DataLoader workers: {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate:   {LR:.2e}\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "scaler = torch.amp.GradScaler('cuda') if USE_AMP else None\n",
    "\n",
    "# ===== PERSISTENT STORAGE SETUP =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERSISTENT STORAGE SETUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "SAVE_DIR = None\n",
    "DRIVE_HAS_DATA = False\n",
    "DRIVE_FILES = set()  # Use set for O(1) lookup\n",
    "\n",
    "if ENV_NAME == \"COLAB\":\n",
    "    # Google Colab - mount Drive\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        DRIVE_MOUNT_PATH = '/content/drive'\n",
    "\n",
    "        if os.path.exists(f'{DRIVE_MOUNT_PATH}/MyDrive'):\n",
    "            print(\"Google Drive already mounted\")\n",
    "        else:\n",
    "            try:\n",
    "                drive.mount(DRIVE_MOUNT_PATH, force_remount=False)\n",
    "                print(\"Google Drive mounted successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Drive mount issue: {e}\")\n",
    "                try:\n",
    "                    drive.mount(DRIVE_MOUNT_PATH, force_remount=True)\n",
    "                    print(\"Google Drive mounted (force remount)\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"WARNING: Could not mount Drive: {e2}\")\n",
    "                    print(\"Falling back to local storage\")\n",
    "                    PERSISTENT_STORAGE = DATA_DIR\n",
    "\n",
    "        SAVE_DIR = f'{DRIVE_MOUNT_PATH}/MyDrive/{DRIVE_FOLDER}'\n",
    "    except Exception as e:\n",
    "        print(f\"Colab Drive setup failed: {e}\")\n",
    "        SAVE_DIR = f'{DATA_DIR}/{DRIVE_FOLDER}'\n",
    "\n",
    "elif ENV_NAME == \"KAGGLE\":\n",
    "    # Kaggle - use working directory\n",
    "    SAVE_DIR = f'{PERSISTENT_STORAGE}/{DRIVE_FOLDER}'\n",
    "    print(f\"Using Kaggle working directory: {SAVE_DIR}\")\n",
    "    print(\"Note: Data persists until kernel is reset\")\n",
    "    # Check for uploaded datasets\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        datasets = os.listdir('/kaggle/input')\n",
    "        if datasets:\n",
    "            print(f\"Available datasets: {datasets[:5]}\")\n",
    "\n",
    "elif ENV_NAME == \"LIGHTNING_AI\":\n",
    "    SAVE_DIR = f'{PERSISTENT_STORAGE}/{DRIVE_FOLDER}'\n",
    "    print(f\"Using Lightning.ai studio storage: {SAVE_DIR}\")\n",
    "\n",
    "elif ENV_NAME == \"PAPERSPACE\":\n",
    "    SAVE_DIR = f'{PERSISTENT_STORAGE}/{DRIVE_FOLDER}'\n",
    "    print(f\"Using Paperspace /storage: {SAVE_DIR}\")\n",
    "\n",
    "elif ENV_NAME == \"HUGGINGFACE_SPACES\":\n",
    "    # HF Spaces has limited persistent storage\n",
    "    SAVE_DIR = f'{PERSISTENT_STORAGE}/{DRIVE_FOLDER}'\n",
    "    print(f\"Using HuggingFace Spaces storage: {SAVE_DIR}\")\n",
    "    print(\"Warning: HF Spaces storage is limited\")\n",
    "\n",
    "else:\n",
    "    SAVE_DIR = f'{PERSISTENT_STORAGE}/{DRIVE_FOLDER}'\n",
    "    print(f\"Using local storage: {SAVE_DIR}\")\n",
    "\n",
    "# Check if folder exists BEFORE creating it\n",
    "folder_existed = os.path.exists(SAVE_DIR)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Check what's available in storage - use BOTH listdir AND direct exists checks\n",
    "# (Google Drive can have sync issues where listdir misses files)\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    DRIVE_FILES = set(os.listdir(SAVE_DIR))  # O(1) membership test\n",
    "\n",
    "    # Direct existence checks for key files (bypasses listdir caching issues)\n",
    "    key_files = ['passages.jsonl', 'bonds.jsonl', 'dear_abby.csv', 'all_splits.json']\n",
    "    for kf in key_files:\n",
    "        kf_path = os.path.join(SAVE_DIR, kf)\n",
    "        if os.path.exists(kf_path) and kf not in DRIVE_FILES:\n",
    "            print(f\"  [Drive sync fix] Found {kf} via os.path.exists() but not listdir()\")\n",
    "            DRIVE_FILES.add(kf)\n",
    "\n",
    "    DRIVE_HAS_DATA = 'passages.jsonl' in DRIVE_FILES and 'bonds.jsonl' in DRIVE_FILES\n",
    "\n",
    "print(f\"\\n\" + \"-\"*60)\n",
    "print(f\"STORAGE STATUS:\")\n",
    "print(f\"-\"*60)\n",
    "print(f\"  Folder: {SAVE_DIR}\")\n",
    "print(f\"  Folder existed: {folder_existed}\")\n",
    "print(f\"  Files found: {len(DRIVE_FILES)}\")\n",
    "\n",
    "# If folder was empty/new, show what folders exist in parent to help debug\n",
    "if not DRIVE_FILES and ENV_NAME == \"COLAB\":\n",
    "    parent = os.path.dirname(SAVE_DIR)  # e.g., /content/drive/MyDrive\n",
    "    if os.path.exists(parent):\n",
    "        siblings = [d for d in os.listdir(parent) if 'bip' in d.lower() or 'BIP' in d]\n",
    "        if siblings:\n",
    "            print(f\"  ** Similar folders in {parent}: {siblings}\")\n",
    "        else:\n",
    "            print(f\"  ** No BIP folders found in {parent}\")\n",
    "if DRIVE_FILES:\n",
    "    for f in sorted(DRIVE_FILES)[:10]:  # sorted() converts to list for slicing\n",
    "        print(f\"    - {f}\")\n",
    "    if len(DRIVE_FILES) > 10:\n",
    "        print(f\"    ... and {len(DRIVE_FILES)-10} more\")\n",
    "print(f\"  Pre-processed data available: {DRIVE_HAS_DATA}\")\n",
    "\n",
    "# Decide data loading strategy\n",
    "LOAD_FROM_DRIVE = USE_DRIVE_DATA and DRIVE_HAS_DATA and not REFRESH_DATA_FROM_SOURCE\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"DATA LOADING STRATEGY:\")\n",
    "if LOAD_FROM_DRIVE:\n",
    "    print(f\"  -> Will load pre-processed data from {ENV_NAME} storage\")\n",
    "    print(f\"     (Set REFRESH_DATA_FROM_SOURCE=True to re-download)\")\n",
    "else:\n",
    "    print(f\"  -> Will download and process data from online sources\")\n",
    "    if USE_DRIVE_DATA and not DRIVE_HAS_DATA:\n",
    "        print(f\"     (Cached data not found, downloading fresh)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create local directories\n",
    "for d in [\"data/processed\", \"data/splits\", \"data/raw\", \"models/checkpoints\", \"results\"]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"SETUP COMPLETE\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"  Environment: {ENV_NAME}\")\n",
    "print(f\"  GPU:         {GPU_NAME} ({GPU_TIER})\")\n",
    "print(f\"  Storage:     {SAVE_DIR}\")\n",
    "print(f\"  Ready to run: Cell 2 (Imports)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Download/Load Corpora { display-mode: \"form\" }\n",
    "#@markdown Downloads from online sources OR loads from Google Drive\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING CORPORA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Force Google Drive sync refresh (workaround for stale FUSE mount)\n",
    "if ENV_NAME == \"COLAB\" and SAVE_DIR and os.path.exists(os.path.dirname(SAVE_DIR)):\n",
    "    try:\n",
    "        # Accessing the directory forces FUSE to refresh\n",
    "        _ = os.listdir(SAVE_DIR)\n",
    "        # Also touch parent to wake up sync\n",
    "        _ = os.listdir(os.path.dirname(SAVE_DIR))\n",
    "        print(\"  [Drive sync refreshed]\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [Drive sync warning: {e}]\")\n",
    "\n",
    "if LOAD_FROM_DRIVE:\n",
    "    # ===== LOAD FROM DRIVE =====\n",
    "    print(\"\\nLoading pre-processed data from Google Drive...\")\n",
    "    \n",
    "    # Copy files from Drive to local\n",
    "    for fname in ['passages.jsonl', 'bonds.jsonl']:\n",
    "        src = f'{SAVE_DIR}/{fname}'\n",
    "        dst = f'data/processed/{fname}'\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "            print(f\"  Copied {fname}\")\n",
    "    \n",
    "    if os.path.exists(f'{SAVE_DIR}/all_splits.json'):\n",
    "        shutil.copy(f'{SAVE_DIR}/all_splits.json', 'data/splits/all_splits.json')\n",
    "        print(f\"  Copied all_splits.json\")\n",
    "    \n",
    "    # Load Dear Abby from Drive if available (check filesystem, not cached set)\n",
    "    abby_drive_path = f'{SAVE_DIR}/dear_abby.csv'\n",
    "    if os.path.exists(abby_drive_path):\n",
    "        shutil.copy(abby_drive_path, 'data/raw/dear_abby.csv')\n",
    "        print(f\"  Copied dear_abby.csv from {abby_drive_path}\")\n",
    "    \n",
    "    # Count loaded data\n",
    "    if os.path.exists('data/processed/passages.jsonl'):\n",
    "        with open('data/processed/passages.jsonl') as f:\n",
    "            n_passages = sum(1 for _ in f)\n",
    "        print(f\"\\nLoaded {n_passages:,} passages from Drive\")\n",
    "    \n",
    "    SKIP_PROCESSING = True\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Drive data loaded - skipping download/processing\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    # ===== DOWNLOAD FROM ONLINE =====\n",
    "    SKIP_PROCESSING = False\n",
    "    \n",
    "    # SEFARIA\n",
    "    if not os.path.exists('data/raw/Sefaria-Export/json'):\n",
    "        print(\"\\n[1/4] Downloading Sefaria (~2GB)...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", \n",
    "                       \"https://github.com/Sefaria/Sefaria-Export.git\",\n",
    "                       \"data/raw/Sefaria-Export\"], check=True)\n",
    "        print(\"  Done!\")\n",
    "    else:\n",
    "        print(\"\\n[1/4] Sefaria already exists\")\n",
    "    \n",
    "    # CHINESE - 200+ REAL CLASSICAL TEXTS\n",
    "    print(\"\\n[2/4] Chinese classics (200+ real passages)...\")\n",
    "    os.makedirs('data/raw/chinese', exist_ok=True)\n",
    "    \n",
    "    chinese = []\n",
    "    \n",
    "    # === ANALECTS (\u8ad6\u8a9e) - 50+ passages ===\n",
    "    analects = [\n",
    "        (\"\u5b50\u66f0\uff1a\u5df1\u6240\u4e0d\u6b32\uff0c\u52ff\u65bd\u65bc\u4eba\u3002\", \"Analects 15.24\"),\n",
    "        (\"\u5b5d\u608c\u4e5f\u8005\uff0c\u5176\u70ba\u4ec1\u4e4b\u672c\u8207\u3002\", \"Analects 1.2\"),\n",
    "        (\"\u7236\u6bcd\u5728\uff0c\u4e0d\u9060\u6e38\uff0c\u904a\u5fc5\u6709\u65b9\u3002\", \"Analects 4.19\"),\n",
    "        (\"\u541b\u5b50\u55bb\u65bc\u7fa9\uff0c\u5c0f\u4eba\u55bb\u65bc\u5229\u3002\", \"Analects 4.16\"),\n",
    "        (\"\u4e0d\u7fa9\u800c\u5bcc\u4e14\u8cb4\uff0c\u65bc\u6211\u5982\u6d6e\u96f2\u3002\", \"Analects 7.16\"),\n",
    "        (\"\u5b78\u800c\u6642\u7fd2\u4e4b\uff0c\u4e0d\u4ea6\u8aaa\u4e4e\u3002\", \"Analects 1.1\"),\n",
    "        (\"\u6709\u670b\u81ea\u9060\u65b9\u4f86\uff0c\u4e0d\u4ea6\u6a02\u4e4e\u3002\", \"Analects 1.1\"),\n",
    "        (\"\u4eba\u4e0d\u77e5\u800c\u4e0d\u614d\uff0c\u4e0d\u4ea6\u541b\u5b50\u4e4e\u3002\", \"Analects 1.1\"),\n",
    "        (\"\u5de7\u8a00\u4ee4\u8272\uff0c\u9bae\u77e3\u4ec1\u3002\", \"Analects 1.3\"),\n",
    "        (\"\u543e\u65e5\u4e09\u7701\u543e\u8eab\u3002\", \"Analects 1.4\"),\n",
    "        (\"\u70ba\u4eba\u8b00\u800c\u4e0d\u5fe0\u4e4e\uff0c\u8207\u670b\u53cb\u4ea4\u800c\u4e0d\u4fe1\u4e4e\u3002\", \"Analects 1.4\"),\n",
    "        (\"\u5f1f\u5b50\u5165\u5247\u5b5d\uff0c\u51fa\u5247\u608c\u3002\", \"Analects 1.6\"),\n",
    "        (\"\u8b39\u800c\u4fe1\uff0c\u6c4e\u611b\u773e\uff0c\u800c\u89aa\u4ec1\u3002\", \"Analects 1.6\"),\n",
    "        (\"\u541b\u5b50\u4e0d\u91cd\u5247\u4e0d\u5a01\uff0c\u5b78\u5247\u4e0d\u56fa\u3002\", \"Analects 1.8\"),\n",
    "        (\"\u4e3b\u5fe0\u4fe1\uff0c\u7121\u53cb\u4e0d\u5982\u5df1\u8005\u3002\", \"Analects 1.8\"),\n",
    "        (\"\u904e\u5247\u52ff\u619a\u6539\u3002\", \"Analects 1.8\"),\n",
    "        (\"\u614e\u7d42\u8ffd\u9060\uff0c\u6c11\u5fb7\u6b78\u539a\u77e3\u3002\", \"Analects 1.9\"),\n",
    "        (\"\u79ae\u4e4b\u7528\uff0c\u548c\u70ba\u8cb4\u3002\", \"Analects 1.12\"),\n",
    "        (\"\u4fe1\u8fd1\u65bc\u7fa9\uff0c\u8a00\u53ef\u5fa9\u4e5f\u3002\", \"Analects 1.13\"),\n",
    "        (\"\u541b\u5b50\u98df\u7121\u6c42\u98fd\uff0c\u5c45\u7121\u6c42\u5b89\u3002\", \"Analects 1.14\"),\n",
    "        (\"\u654f\u65bc\u4e8b\u800c\u614e\u65bc\u8a00\uff0c\u5c31\u6709\u9053\u800c\u6b63\u7109\u3002\", \"Analects 1.14\"),\n",
    "        (\"\u4e0d\u60a3\u4eba\u4e4b\u4e0d\u5df1\u77e5\uff0c\u60a3\u4e0d\u77e5\u4eba\u4e5f\u3002\", \"Analects 1.16\"),\n",
    "        (\"\u70ba\u653f\u4ee5\u5fb7\uff0c\u8b6c\u5982\u5317\u8fb0\u3002\", \"Analects 2.1\"),\n",
    "        (\"\u9053\u4e4b\u4ee5\u653f\uff0c\u9f4a\u4e4b\u4ee5\u5211\uff0c\u6c11\u514d\u800c\u7121\u6065\u3002\", \"Analects 2.3\"),\n",
    "        (\"\u9053\u4e4b\u4ee5\u5fb7\uff0c\u9f4a\u4e4b\u4ee5\u79ae\uff0c\u6709\u6065\u4e14\u683c\u3002\", \"Analects 2.3\"),\n",
    "        (\"\u543e\u5341\u6709\u4e94\u800c\u5fd7\u4e8e\u5b78\u3002\", \"Analects 2.4\"),\n",
    "        (\"\u4e09\u5341\u800c\u7acb\uff0c\u56db\u5341\u800c\u4e0d\u60d1\u3002\", \"Analects 2.4\"),\n",
    "        (\"\u4e94\u5341\u800c\u77e5\u5929\u547d\uff0c\u516d\u5341\u800c\u8033\u9806\u3002\", \"Analects 2.4\"),\n",
    "        (\"\u4e03\u5341\u800c\u5f9e\u5fc3\u6240\u6b32\uff0c\u4e0d\u903e\u77e9\u3002\", \"Analects 2.4\"),\n",
    "        (\"\u751f\uff0c\u4e8b\u4e4b\u4ee5\u79ae\uff1b\u6b7b\uff0c\u846c\u4e4b\u4ee5\u79ae\uff0c\u796d\u4e4b\u4ee5\u79ae\u3002\", \"Analects 2.5\"),\n",
    "        (\"\u7236\u6bcd\u552f\u5176\u75be\u4e4b\u6182\u3002\", \"Analects 2.6\"),\n",
    "        (\"\u4eca\u4e4b\u5b5d\u8005\uff0c\u662f\u8b02\u80fd\u990a\u3002\", \"Analects 2.7\"),\n",
    "        (\"\u81f3\u65bc\u72ac\u99ac\uff0c\u7686\u80fd\u6709\u990a\uff1b\u4e0d\u656c\uff0c\u4f55\u4ee5\u5225\u4e4e\u3002\", \"Analects 2.7\"),\n",
    "        (\"\u8272\u96e3\u3002\u6709\u4e8b\uff0c\u5f1f\u5b50\u670d\u5176\u52de\u3002\", \"Analects 2.8\"),\n",
    "        (\"\u8996\u5176\u6240\u4ee5\uff0c\u89c0\u5176\u6240\u7531\uff0c\u5bdf\u5176\u6240\u5b89\u3002\", \"Analects 2.10\"),\n",
    "        (\"\u6eab\u6545\u800c\u77e5\u65b0\uff0c\u53ef\u4ee5\u70ba\u5e2b\u77e3\u3002\", \"Analects 2.11\"),\n",
    "        (\"\u541b\u5b50\u4e0d\u5668\u3002\", \"Analects 2.12\"),\n",
    "        (\"\u5148\u884c\u5176\u8a00\u800c\u5f8c\u5f9e\u4e4b\u3002\", \"Analects 2.13\"),\n",
    "        (\"\u541b\u5b50\u5468\u800c\u4e0d\u6bd4\uff0c\u5c0f\u4eba\u6bd4\u800c\u4e0d\u5468\u3002\", \"Analects 2.14\"),\n",
    "        (\"\u5b78\u800c\u4e0d\u601d\u5247\u7f54\uff0c\u601d\u800c\u4e0d\u5b78\u5247\u6b86\u3002\", \"Analects 2.15\"),\n",
    "        (\"\u77e5\u4e4b\u70ba\u77e5\u4e4b\uff0c\u4e0d\u77e5\u70ba\u4e0d\u77e5\uff0c\u662f\u77e5\u4e5f\u3002\", \"Analects 2.17\"),\n",
    "        (\"\u591a\u805e\u95d5\u7591\uff0c\u614e\u8a00\u5176\u9918\uff0c\u5247\u5be1\u5c24\u3002\", \"Analects 2.18\"),\n",
    "        (\"\u8209\u76f4\u932f\u8af8\u6789\uff0c\u5247\u6c11\u670d\u3002\", \"Analects 2.19\"),\n",
    "        (\"\u4eba\u800c\u7121\u4fe1\uff0c\u4e0d\u77e5\u5176\u53ef\u4e5f\u3002\", \"Analects 2.22\"),\n",
    "        (\"\u898b\u7fa9\u4e0d\u70ba\uff0c\u7121\u52c7\u4e5f\u3002\", \"Analects 2.24\"),\n",
    "        (\"\u975e\u5176\u9b3c\u800c\u796d\u4e4b\uff0c\u8ac2\u4e5f\u3002\", \"Analects 2.24\"),\n",
    "        (\"\u662f\u53ef\u5fcd\u4e5f\uff0c\u5b70\u4e0d\u53ef\u5fcd\u4e5f\u3002\", \"Analects 3.1\"),\n",
    "        (\"\u4eba\u800c\u4e0d\u4ec1\uff0c\u5982\u79ae\u4f55\u3002\", \"Analects 3.3\"),\n",
    "        (\"\u4eba\u800c\u4e0d\u4ec1\uff0c\u5982\u6a02\u4f55\u3002\", \"Analects 3.3\"),\n",
    "        (\"\u91cc\u4ec1\u70ba\u7f8e\u3002\u64c7\u4e0d\u8655\u4ec1\uff0c\u7109\u5f97\u77e5\u3002\", \"Analects 4.1\"),\n",
    "        (\"\u4e0d\u4ec1\u8005\u4e0d\u53ef\u4ee5\u4e45\u8655\u7d04\uff0c\u4e0d\u53ef\u4ee5\u9577\u8655\u6a02\u3002\", \"Analects 4.2\"),\n",
    "        (\"\u4ec1\u8005\u5b89\u4ec1\uff0c\u77e5\u8005\u5229\u4ec1\u3002\", \"Analects 4.2\"),\n",
    "        (\"\u552f\u4ec1\u8005\u80fd\u597d\u4eba\uff0c\u80fd\u60e1\u4eba\u3002\", \"Analects 4.3\"),\n",
    "        (\"\u82df\u5fd7\u65bc\u4ec1\u77e3\uff0c\u7121\u60e1\u4e5f\u3002\", \"Analects 4.4\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(analects):\n",
    "        chinese.append({\"id\": f\"cn_analects_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -5})\n",
    "    print(f\"    - Analects: {len([x for x in chinese if 'analects' in x['id']]):,} passages\")\n",
    "    \n",
    "    # === MENCIUS (\u5b5f\u5b50) - 40+ passages ===\n",
    "    mencius = [\n",
    "        (\"\u60fb\u96b1\u4e4b\u5fc3\uff0c\u4ec1\u4e4b\u7aef\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7f9e\u60e1\u4e4b\u5fc3\uff0c\u7fa9\u4e4b\u7aef\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u8fad\u8b93\u4e4b\u5fc3\uff0c\u79ae\u4e4b\u7aef\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u662f\u975e\u4e4b\u5fc3\uff0c\u667a\u4e4b\u7aef\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u4eba\u7686\u6709\u4e0d\u5fcd\u4eba\u4e4b\u5fc3\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7121\u60fb\u96b1\u4e4b\u5fc3\uff0c\u975e\u4eba\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7121\u7f9e\u60e1\u4e4b\u5fc3\uff0c\u975e\u4eba\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7121\u8fad\u8b93\u4e4b\u5fc3\uff0c\u975e\u4eba\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u7121\u662f\u975e\u4e4b\u5fc3\uff0c\u975e\u4eba\u4e5f\u3002\", \"Mencius 2A.6\"),\n",
    "        (\"\u4ec1\u7fa9\u79ae\u667a\uff0c\u975e\u7531\u5916\u9460\u6211\u4e5f\uff0c\u6211\u56fa\u6709\u4e4b\u4e5f\u3002\", \"Mencius 6A.6\"),\n",
    "        (\"\u4eba\u6027\u4e4b\u5584\u4e5f\uff0c\u7336\u6c34\u4e4b\u5c31\u4e0b\u4e5f\u3002\", \"Mencius 6A.2\"),\n",
    "        (\"\u4eba\u7121\u6709\u4e0d\u5584\uff0c\u6c34\u7121\u6709\u4e0d\u4e0b\u3002\", \"Mencius 6A.2\"),\n",
    "        (\"\u60df\u4ec1\u8005\u5b9c\u5728\u9ad8\u4f4d\u3002\", \"Mencius 4A.1\"),\n",
    "        (\"\u4e0d\u4ec1\u800c\u5728\u9ad8\u4f4d\uff0c\u662f\u64ad\u5176\u60e1\u65bc\u773e\u4e5f\u3002\", \"Mencius 4A.1\"),\n",
    "        (\"\u6c11\u70ba\u8cb4\uff0c\u793e\u7a37\u6b21\u4e4b\uff0c\u541b\u70ba\u8f15\u3002\", \"Mencius 7B.14\"),\n",
    "        (\"\u5f97\u9053\u8005\u591a\u52a9\uff0c\u5931\u9053\u8005\u5be1\u52a9\u3002\", \"Mencius 2B.1\"),\n",
    "        (\"\u5be1\u52a9\u4e4b\u81f3\uff0c\u89aa\u621a\u7554\u4e4b\u3002\", \"Mencius 2B.1\"),\n",
    "        (\"\u591a\u52a9\u4e4b\u81f3\uff0c\u5929\u4e0b\u9806\u4e4b\u3002\", \"Mencius 2B.1\"),\n",
    "        (\"\u5929\u6642\u4e0d\u5982\u5730\u5229\uff0c\u5730\u5229\u4e0d\u5982\u4eba\u548c\u3002\", \"Mencius 2B.1\"),\n",
    "        (\"\u751f\u65bc\u6182\u60a3\uff0c\u6b7b\u65bc\u5b89\u6a02\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u5929\u5c07\u964d\u5927\u4efb\u65bc\u662f\u4eba\u4e5f\uff0c\u5fc5\u5148\u82e6\u5176\u5fc3\u5fd7\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u52de\u5176\u7b4b\u9aa8\uff0c\u9913\u5176\u9ad4\u819a\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u7a7a\u4e4f\u5176\u8eab\uff0c\u884c\u62c2\u4e82\u5176\u6240\u70ba\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u6240\u4ee5\u52d5\u5fc3\u5fcd\u6027\uff0c\u66fe\u76ca\u5176\u6240\u4e0d\u80fd\u3002\", \"Mencius 6B.15\"),\n",
    "        (\"\u8001\u543e\u8001\uff0c\u4ee5\u53ca\u4eba\u4e4b\u8001\u3002\", \"Mencius 1A.7\"),\n",
    "        (\"\u5e7c\u543e\u5e7c\uff0c\u4ee5\u53ca\u4eba\u4e4b\u5e7c\u3002\", \"Mencius 1A.7\"),\n",
    "        (\"\u7aae\u5247\u7368\u5584\u5176\u8eab\uff0c\u9054\u5247\u517c\u5584\u5929\u4e0b\u3002\", \"Mencius 7A.9\"),\n",
    "        (\"\u9b5a\uff0c\u6211\u6240\u6b32\u4e5f\uff1b\u718a\u638c\uff0c\u4ea6\u6211\u6240\u6b32\u4e5f\u3002\", \"Mencius 6A.10\"),\n",
    "        (\"\u4e8c\u8005\u4e0d\u53ef\u5f97\u517c\uff0c\u820d\u9b5a\u800c\u53d6\u718a\u638c\u8005\u4e5f\u3002\", \"Mencius 6A.10\"),\n",
    "        (\"\u751f\uff0c\u4ea6\u6211\u6240\u6b32\u4e5f\uff1b\u7fa9\uff0c\u4ea6\u6211\u6240\u6b32\u4e5f\u3002\", \"Mencius 6A.10\"),\n",
    "        (\"\u4e8c\u8005\u4e0d\u53ef\u5f97\u517c\uff0c\u820d\u751f\u800c\u53d6\u7fa9\u8005\u4e5f\u3002\", \"Mencius 6A.10\"),\n",
    "        (\"\u990a\u5fc3\u83ab\u5584\u65bc\u5be1\u6b32\u3002\", \"Mencius 7B.35\"),\n",
    "        (\"\u4ec1\u8005\u7121\u6575\u65bc\u5929\u4e0b\u3002\", \"Mencius 1A.5\"),\n",
    "        (\"\u4ee5\u529b\u670d\u4eba\u8005\uff0c\u975e\u5fc3\u670d\u4e5f\u3002\", \"Mencius 2A.3\"),\n",
    "        (\"\u4ee5\u5fb7\u670d\u4eba\u8005\uff0c\u4e2d\u5fc3\u6085\u800c\u8aa0\u670d\u4e5f\u3002\", \"Mencius 2A.3\"),\n",
    "        (\"\u4eba\u4e4b\u60a3\u5728\u597d\u70ba\u4eba\u5e2b\u3002\", \"Mencius 4A.23\"),\n",
    "        (\"\u76e1\u4fe1\u66f8\uff0c\u5247\u4e0d\u5982\u7121\u66f8\u3002\", \"Mencius 7B.3\"),\n",
    "        (\"\u4e0d\u4ee5\u898f\u77e9\uff0c\u4e0d\u80fd\u6210\u65b9\u5713\u3002\", \"Mencius 4A.1\"),\n",
    "        (\"\u5b5d\u5b50\u4e4b\u81f3\uff0c\u83ab\u5927\u4e4e\u5c0a\u89aa\u3002\", \"Mencius 5A.4\"),\n",
    "        (\"\u7236\u5b50\u6709\u89aa\uff0c\u541b\u81e3\u6709\u7fa9\uff0c\u592b\u5a66\u6709\u5225\uff0c\u9577\u5e7c\u6709\u5e8f\uff0c\u670b\u53cb\u6709\u4fe1\u3002\", \"Mencius 3A.4\"),\n",
    "        (\"\u4eba\u6709\u4e0d\u70ba\u4e5f\uff0c\u800c\u5f8c\u53ef\u4ee5\u6709\u70ba\u3002\", \"Mencius 4B.8\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(mencius):\n",
    "        chinese.append({\"id\": f\"cn_mencius_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -4})\n",
    "    print(f\"    - Mencius: {len([x for x in chinese if 'mencius' in x['id']]):,} passages\")\n",
    "    \n",
    "    # === DAODEJING (\u9053\u5fb7\u7d93) - 40+ passages ===\n",
    "    daodejing = [\n",
    "        (\"\u9053\u53ef\u9053\uff0c\u975e\u5e38\u9053\u3002\u540d\u53ef\u540d\uff0c\u975e\u5e38\u540d\u3002\", \"Daodejing 1\"),\n",
    "        (\"\u5929\u4e0b\u7686\u77e5\u7f8e\u4e4b\u70ba\u7f8e\uff0c\u65af\u60e1\u5df2\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u7686\u77e5\u5584\u4e4b\u70ba\u5584\uff0c\u65af\u4e0d\u5584\u5df2\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u6709\u7121\u76f8\u751f\uff0c\u96e3\u6613\u76f8\u6210\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u9577\u77ed\u76f8\u8f03\uff0c\u9ad8\u4e0b\u76f8\u50be\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u662f\u4ee5\u8056\u4eba\u8655\u7121\u70ba\u4e4b\u4e8b\uff0c\u884c\u4e0d\u8a00\u4e4b\u6559\u3002\", \"Daodejing 2\"),\n",
    "        (\"\u4e0d\u5c1a\u8ce2\uff0c\u4f7f\u6c11\u4e0d\u722d\u3002\", \"Daodejing 3\"),\n",
    "        (\"\u4e0d\u8cb4\u96e3\u5f97\u4e4b\u8ca8\uff0c\u4f7f\u6c11\u4e0d\u70ba\u76dc\u3002\", \"Daodejing 3\"),\n",
    "        (\"\u4e0a\u5584\u82e5\u6c34\u3002\u6c34\u5584\u5229\u842c\u7269\u800c\u4e0d\u722d\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u8655\u773e\u4eba\u4e4b\u6240\u60e1\uff0c\u6545\u5e7e\u65bc\u9053\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u5c45\u5584\u5730\uff0c\u5fc3\u5584\u6df5\uff0c\u8207\u5584\u4ec1\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u8a00\u5584\u4fe1\uff0c\u653f\u5584\u6cbb\uff0c\u4e8b\u5584\u80fd\uff0c\u52d5\u5584\u6642\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u592b\u552f\u4e0d\u722d\uff0c\u6545\u7121\u5c24\u3002\", \"Daodejing 8\"),\n",
    "        (\"\u91d1\u7389\u6eff\u5802\uff0c\u83ab\u4e4b\u80fd\u5b88\u3002\", \"Daodejing 9\"),\n",
    "        (\"\u5bcc\u8cb4\u800c\u9a55\uff0c\u81ea\u907a\u5176\u548e\u3002\", \"Daodejing 9\"),\n",
    "        (\"\u529f\u6210\u8eab\u9000\uff0c\u5929\u4e4b\u9053\u4e5f\u3002\", \"Daodejing 9\"),\n",
    "        (\"\u77e5\u4eba\u8005\u667a\uff0c\u81ea\u77e5\u8005\u660e\u3002\", \"Daodejing 33\"),\n",
    "        (\"\u52dd\u4eba\u8005\u6709\u529b\uff0c\u81ea\u52dd\u8005\u5f37\u3002\", \"Daodejing 33\"),\n",
    "        (\"\u77e5\u8db3\u8005\u5bcc\uff0c\u5f37\u884c\u8005\u6709\u5fd7\u3002\", \"Daodejing 33\"),\n",
    "        (\"\u4e0d\u5931\u5176\u6240\u8005\u4e45\uff0c\u6b7b\u800c\u4e0d\u4ea1\u8005\u58fd\u3002\", \"Daodejing 33\"),\n",
    "        (\"\u5927\u9053\u5ee2\uff0c\u6709\u4ec1\u7fa9\u3002\", \"Daodejing 18\"),\n",
    "        (\"\u667a\u6167\u51fa\uff0c\u6709\u5927\u507d\u3002\", \"Daodejing 18\"),\n",
    "        (\"\u516d\u89aa\u4e0d\u548c\uff0c\u6709\u5b5d\u6148\u3002\", \"Daodejing 18\"),\n",
    "        (\"\u570b\u5bb6\u660f\u4e82\uff0c\u6709\u5fe0\u81e3\u3002\", \"Daodejing 18\"),\n",
    "        (\"\u798d\u516e\u798f\u4e4b\u6240\u501a\uff0c\u798f\u516e\u798d\u4e4b\u6240\u4f0f\u3002\", \"Daodejing 58\"),\n",
    "        (\"\u5929\u9577\u5730\u4e45\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u5929\u5730\u6240\u4ee5\u80fd\u9577\u4e14\u4e45\u8005\uff0c\u4ee5\u5176\u4e0d\u81ea\u751f\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u662f\u4ee5\u8056\u4eba\u5f8c\u5176\u8eab\u800c\u8eab\u5148\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u5916\u5176\u8eab\u800c\u8eab\u5b58\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u975e\u4ee5\u5176\u7121\u79c1\u8036\uff0c\u6545\u80fd\u6210\u5176\u79c1\u3002\", \"Daodejing 7\"),\n",
    "        (\"\u67d4\u5f31\u52dd\u525b\u5f37\u3002\", \"Daodejing 36\"),\n",
    "        (\"\u5927\u65b9\u7121\u9685\uff0c\u5927\u5668\u665a\u6210\u3002\", \"Daodejing 41\"),\n",
    "        (\"\u5927\u97f3\u5e0c\u8072\uff0c\u5927\u8c61\u7121\u5f62\u3002\", \"Daodejing 41\"),\n",
    "        (\"\u9053\u751f\u4e00\uff0c\u4e00\u751f\u4e8c\uff0c\u4e8c\u751f\u4e09\uff0c\u4e09\u751f\u842c\u7269\u3002\", \"Daodejing 42\"),\n",
    "        (\"\u5929\u4e0b\u842c\u7269\u751f\u65bc\u6709\uff0c\u6709\u751f\u65bc\u7121\u3002\", \"Daodejing 40\"),\n",
    "        (\"\u5343\u91cc\u4e4b\u884c\uff0c\u59cb\u65bc\u8db3\u4e0b\u3002\", \"Daodejing 64\"),\n",
    "        (\"\u5408\u62b1\u4e4b\u6728\uff0c\u751f\u65bc\u6beb\u672b\u3002\", \"Daodejing 64\"),\n",
    "        (\"\u4e5d\u5c64\u4e4b\u81fa\uff0c\u8d77\u65bc\u7d2f\u571f\u3002\", \"Daodejing 64\"),\n",
    "        (\"\u6c11\u4e0d\u754f\u6b7b\uff0c\u5948\u4f55\u4ee5\u6b7b\u61fc\u4e4b\u3002\", \"Daodejing 74\"),\n",
    "        (\"\u4fe1\u8a00\u4e0d\u7f8e\uff0c\u7f8e\u8a00\u4e0d\u4fe1\u3002\", \"Daodejing 81\"),\n",
    "        (\"\u5584\u8005\u4e0d\u8faf\uff0c\u8faf\u8005\u4e0d\u5584\u3002\", \"Daodejing 81\"),\n",
    "        (\"\u77e5\u8005\u4e0d\u535a\uff0c\u535a\u8005\u4e0d\u77e5\u3002\", \"Daodejing 81\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(daodejing):\n",
    "        chinese.append({\"id\": f\"cn_daodejing_{i}\", \"text\": text, \"source\": source, \"period\": \"DAOIST\", \"century\": -4})\n",
    "    print(f\"    - Daodejing: {len([x for x in chinese if 'daodejing' in x['id']]):,} passages\")\n",
    "    \n",
    "    # === GREAT LEARNING (\u5927\u5b78) - 20+ passages ===\n",
    "    daxue = [\n",
    "        (\"\u5927\u5b78\u4e4b\u9053\uff0c\u5728\u660e\u660e\u5fb7\uff0c\u5728\u89aa\u6c11\uff0c\u5728\u6b62\u65bc\u81f3\u5584\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u77e5\u6b62\u800c\u5f8c\u6709\u5b9a\uff0c\u5b9a\u800c\u5f8c\u80fd\u975c\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u975c\u800c\u5f8c\u80fd\u5b89\uff0c\u5b89\u800c\u5f8c\u80fd\u616e\uff0c\u616e\u800c\u5f8c\u80fd\u5f97\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u7269\u6709\u672c\u672b\uff0c\u4e8b\u6709\u7d42\u59cb\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u77e5\u6240\u5148\u5f8c\uff0c\u5247\u8fd1\u9053\u77e3\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u53e4\u4e4b\u6b32\u660e\u660e\u5fb7\u65bc\u5929\u4e0b\u8005\uff0c\u5148\u6cbb\u5176\u570b\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u6cbb\u5176\u570b\u8005\uff0c\u5148\u9f4a\u5176\u5bb6\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u9f4a\u5176\u5bb6\u8005\uff0c\u5148\u4fee\u5176\u8eab\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u4fee\u5176\u8eab\u8005\uff0c\u5148\u6b63\u5176\u5fc3\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u6b63\u5176\u5fc3\u8005\uff0c\u5148\u8aa0\u5176\u610f\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6b32\u8aa0\u5176\u610f\u8005\uff0c\u5148\u81f4\u5176\u77e5\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u81f4\u77e5\u5728\u683c\u7269\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u7269\u683c\u800c\u5f8c\u77e5\u81f3\uff0c\u77e5\u81f3\u800c\u5f8c\u610f\u8aa0\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u610f\u8aa0\u800c\u5f8c\u5fc3\u6b63\uff0c\u5fc3\u6b63\u800c\u5f8c\u8eab\u4fee\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u8eab\u4fee\u800c\u5f8c\u5bb6\u9f4a\uff0c\u5bb6\u9f4a\u800c\u5f8c\u570b\u6cbb\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u570b\u6cbb\u800c\u5f8c\u5929\u4e0b\u5e73\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u81ea\u5929\u5b50\u4ee5\u81f3\u65bc\u5eb6\u4eba\uff0c\u58f9\u662f\u7686\u4ee5\u4fee\u8eab\u70ba\u672c\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u5176\u672c\u4e82\u800c\u672b\u6cbb\u8005\u5426\u77e3\u3002\", \"Great Learning 1\"),\n",
    "        (\"\u6240\u8b02\u8aa0\u5176\u610f\u8005\uff0c\u6bcb\u81ea\u6b3a\u4e5f\u3002\", \"Great Learning 6\"),\n",
    "        (\"\u5982\u60e1\u60e1\u81ed\uff0c\u5982\u597d\u597d\u8272\uff0c\u6b64\u4e4b\u8b02\u81ea\u8b19\u3002\", \"Great Learning 6\"),\n",
    "        (\"\u6545\u541b\u5b50\u5fc5\u614e\u5176\u7368\u4e5f\u3002\", \"Great Learning 6\"),\n",
    "        (\"\u5bcc\u6f64\u5c4b\uff0c\u5fb7\u6f64\u8eab\uff0c\u5fc3\u5ee3\u9ad4\u80d6\u3002\", \"Great Learning 6\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(daxue):\n",
    "        chinese.append({\"id\": f\"cn_daxue_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -5})\n",
    "    print(f\"    - Great Learning: {len([x for x in chinese if 'daxue' in x['id']]):,} passages\")\n",
    "    \n",
    "    # === DOCTRINE OF THE MEAN (\u4e2d\u5eb8) - 20+ passages ===\n",
    "    zhongyong = [\n",
    "        (\"\u5929\u547d\u4e4b\u8b02\u6027\uff0c\u7387\u6027\u4e4b\u8b02\u9053\uff0c\u4fee\u9053\u4e4b\u8b02\u6559\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u9053\u4e5f\u8005\uff0c\u4e0d\u53ef\u9808\u81fe\u96e2\u4e5f\uff1b\u53ef\u96e2\uff0c\u975e\u9053\u4e5f\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u662f\u6545\u541b\u5b50\u6212\u614e\u4e4e\u5176\u6240\u4e0d\u7779\uff0c\u6050\u61fc\u4e4e\u5176\u6240\u4e0d\u805e\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u83ab\u898b\u4e4e\u96b1\uff0c\u83ab\u986f\u4e4e\u5fae\uff0c\u6545\u541b\u5b50\u614e\u5176\u7368\u4e5f\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u559c\u6012\u54c0\u6a02\u4e4b\u672a\u767c\uff0c\u8b02\u4e4b\u4e2d\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u767c\u800c\u7686\u4e2d\u7bc0\uff0c\u8b02\u4e4b\u548c\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u4e2d\u4e5f\u8005\uff0c\u5929\u4e0b\u4e4b\u5927\u672c\u4e5f\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u548c\u4e5f\u8005\uff0c\u5929\u4e0b\u4e4b\u9054\u9053\u4e5f\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u81f4\u4e2d\u548c\uff0c\u5929\u5730\u4f4d\u7109\uff0c\u842c\u7269\u80b2\u7109\u3002\", \"Doctrine of the Mean 1\"),\n",
    "        (\"\u541b\u5b50\u4e2d\u5eb8\uff0c\u5c0f\u4eba\u53cd\u4e2d\u5eb8\u3002\", \"Doctrine of the Mean 2\"),\n",
    "        (\"\u541b\u5b50\u4e4b\u4e2d\u5eb8\u4e5f\uff0c\u541b\u5b50\u800c\u6642\u4e2d\u3002\", \"Doctrine of the Mean 2\"),\n",
    "        (\"\u5c0f\u4eba\u4e4b\u53cd\u4e2d\u5eb8\u4e5f\uff0c\u5c0f\u4eba\u800c\u7121\u5fcc\u619a\u4e5f\u3002\", \"Doctrine of the Mean 2\"),\n",
    "        (\"\u4e2d\u5eb8\u5176\u81f3\u77e3\u4e4e\uff01\u6c11\u9bae\u80fd\u4e45\u77e3\u3002\", \"Doctrine of the Mean 3\"),\n",
    "        (\"\u9053\u4e4b\u4e0d\u884c\u4e5f\uff0c\u6211\u77e5\u4e4b\u77e3\uff1a\u77e5\u8005\u904e\u4e4b\uff0c\u611a\u8005\u4e0d\u53ca\u4e5f\u3002\", \"Doctrine of the Mean 4\"),\n",
    "        (\"\u9053\u4e4b\u4e0d\u660e\u4e5f\uff0c\u6211\u77e5\u4e4b\u77e3\uff1a\u8ce2\u8005\u904e\u4e4b\uff0c\u4e0d\u8096\u8005\u4e0d\u53ca\u4e5f\u3002\", \"Doctrine of the Mean 4\"),\n",
    "        (\"\u4eba\u83ab\u4e0d\u98f2\u98df\u4e5f\uff0c\u9bae\u80fd\u77e5\u5473\u4e5f\u3002\", \"Doctrine of the Mean 4\"),\n",
    "        (\"\u8aa0\u8005\uff0c\u5929\u4e4b\u9053\u4e5f\u3002\u8aa0\u4e4b\u8005\uff0c\u4eba\u4e4b\u9053\u4e5f\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u8aa0\u8005\uff0c\u4e0d\u52c9\u800c\u4e2d\uff0c\u4e0d\u601d\u800c\u5f97\uff0c\u5f9e\u5bb9\u4e2d\u9053\uff0c\u8056\u4eba\u4e5f\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u8aa0\u4e4b\u8005\uff0c\u64c7\u5584\u800c\u56fa\u57f7\u4e4b\u8005\u4e5f\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u535a\u5b78\u4e4b\uff0c\u5be9\u554f\u4e4b\uff0c\u614e\u601d\u4e4b\uff0c\u660e\u8fa8\u4e4b\uff0c\u7be4\u884c\u4e4b\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u4eba\u4e00\u80fd\u4e4b\uff0c\u5df1\u767e\u4e4b\uff1b\u4eba\u5341\u80fd\u4e4b\uff0c\u5df1\u5343\u4e4b\u3002\", \"Doctrine of the Mean 20\"),\n",
    "        (\"\u679c\u80fd\u6b64\u9053\u77e3\uff0c\u96d6\u611a\u5fc5\u660e\uff0c\u96d6\u67d4\u5fc5\u5f37\u3002\", \"Doctrine of the Mean 20\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(zhongyong):\n",
    "        chinese.append({\"id\": f\"cn_zhongyong_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -5})\n",
    "    print(f\"    - Doctrine of Mean: {len([x for x in chinese if 'zhongyong' in x['id']]):,} passages\")\n",
    "    \n",
    "    # === BOOK OF RITES (\u79ae\u8a18) - 30+ passages ===\n",
    "    liji = [\n",
    "        (\"\u79ae\u5c1a\u5f80\u4f86\u3002\u5f80\u800c\u4e0d\u4f86\uff0c\u975e\u79ae\u4e5f\uff1b\u4f86\u800c\u4e0d\u5f80\uff0c\u4ea6\u975e\u79ae\u4e5f\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u6556\u4e0d\u53ef\u9577\uff0c\u6b32\u4e0d\u53ef\u5f9e\uff0c\u5fd7\u4e0d\u53ef\u6eff\uff0c\u6a02\u4e0d\u53ef\u6975\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u81e8\u8ca1\u6bcb\u830d\u5f97\uff0c\u81e8\u96e3\u6bcb\u830d\u514d\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u592b\u79ae\u8005\uff0c\u81ea\u5351\u800c\u5c0a\u4eba\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u96d6\u8ca0\u8ca9\u8005\uff0c\u5fc5\u6709\u5c0a\u4e5f\uff0c\u800c\u6cc1\u5bcc\u8cb4\u4e4e\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u5bcc\u8cb4\u800c\u77e5\u597d\u79ae\uff0c\u5247\u4e0d\u9a55\u4e0d\u6deb\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u8ca7\u8ce4\u800c\u77e5\u597d\u79ae\uff0c\u5247\u5fd7\u4e0d\u61fe\u3002\", \"Book of Rites - Quli\"),\n",
    "        (\"\u5927\u9053\u4e4b\u884c\u4e5f\uff0c\u5929\u4e0b\u70ba\u516c\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u9078\u8ce2\u8207\u80fd\uff0c\u8b1b\u4fe1\u4fee\u7766\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u6545\u4eba\u4e0d\u7368\u89aa\u5176\u89aa\uff0c\u4e0d\u7368\u5b50\u5176\u5b50\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u4f7f\u8001\u6709\u6240\u7d42\uff0c\u58ef\u6709\u6240\u7528\uff0c\u5e7c\u6709\u6240\u9577\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u77dc\u5be1\u5b64\u7368\u5ee2\u75be\u8005\u7686\u6709\u6240\u990a\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u7537\u6709\u5206\uff0c\u5973\u6709\u6b78\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u8ca8\u60e1\u5176\u68c4\u65bc\u5730\u4e5f\uff0c\u4e0d\u5fc5\u85cf\u65bc\u5df1\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u529b\u60e1\u5176\u4e0d\u51fa\u65bc\u8eab\u4e5f\uff0c\u4e0d\u5fc5\u70ba\u5df1\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u662f\u6545\u8b00\u9589\u800c\u4e0d\u8208\uff0c\u76dc\u7aca\u4e82\u8cca\u800c\u4e0d\u4f5c\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u6545\u5916\u6236\u800c\u4e0d\u9589\uff0c\u662f\u8b02\u5927\u540c\u3002\", \"Book of Rites - Liyun\"),\n",
    "        (\"\u7389\u4e0d\u7422\uff0c\u4e0d\u6210\u5668\uff1b\u4eba\u4e0d\u5b78\uff0c\u4e0d\u77e5\u9053\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u662f\u6545\u5b78\u7136\u5f8c\u77e5\u4e0d\u8db3\uff0c\u6559\u7136\u5f8c\u77e5\u56f0\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u77e5\u4e0d\u8db3\uff0c\u7136\u5f8c\u80fd\u81ea\u53cd\u4e5f\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u77e5\u56f0\uff0c\u7136\u5f8c\u80fd\u81ea\u5f37\u4e5f\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u6545\u66f0\uff1a\u6559\u5b78\u76f8\u9577\u4e5f\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u51e1\u5b78\u4e4b\u9053\uff0c\u56b4\u5e2b\u70ba\u96e3\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u5e2b\u56b4\u7136\u5f8c\u9053\u5c0a\uff0c\u9053\u5c0a\u7136\u5f8c\u6c11\u77e5\u656c\u5b78\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u5584\u6b4c\u8005\u4f7f\u4eba\u7e7c\u5176\u8072\uff0c\u5584\u6559\u8005\u4f7f\u4eba\u7e7c\u5176\u5fd7\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u8a18\u554f\u4e4b\u5b78\uff0c\u4e0d\u8db3\u4ee5\u70ba\u4eba\u5e2b\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u5fc5\u4e5f\u5176\u807d\u8a9e\u4e4e\uff0c\u529b\u4e0d\u80fd\u554f\uff0c\u7136\u5f8c\u8a9e\u4e4b\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u8a9e\u4e4b\u800c\u4e0d\u77e5\uff0c\u96d6\u820d\u4e4b\u53ef\u4e5f\u3002\", \"Book of Rites - Xueji\"),\n",
    "        (\"\u535a\u5b78\u800c\u4e0d\u7aae\uff0c\u7be4\u884c\u800c\u4e0d\u5026\u3002\", \"Book of Rites - Ruxing\"),\n",
    "        (\"\u541b\u5b50\u4e4b\u65bc\u5b78\u4e5f\uff0c\u85cf\u7109\uff0c\u4fee\u7109\uff0c\u606f\u7109\uff0c\u6e38\u7109\u3002\", \"Book of Rites - Xueji\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(liji):\n",
    "        chinese.append({\"id\": f\"cn_liji_{i}\", \"text\": text, \"source\": source, \"period\": \"CONFUCIAN\", \"century\": -3})\n",
    "    print(f\"    - Book of Rites: {len([x for x in chinese if 'liji' in x['id']]):,} passages\")\n",
    "    \n",
    "    with open('data/raw/chinese/chinese_native.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(chinese, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"  Created {len(chinese)} Chinese passages\")\n",
    "    \n",
    "    # ISLAMIC - 150+ REAL PASSAGES\n",
    "    print(\"\\n[3/4] Islamic texts (150+ real passages)...\")\n",
    "    os.makedirs('data/raw/islamic', exist_ok=True)\n",
    "    \n",
    "    islamic = []\n",
    "    \n",
    "    # === QURANIC VERSES (40+) ===\n",
    "    quran = [\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u062a\u064f\u0644\u064f\u0648\u0627 \u0627\u0644\u0646\u064e\u0651\u0641\u0652\u0633\u064e \u0627\u0644\u064e\u0651\u062a\u0650\u064a \u062d\u064e\u0631\u064e\u0651\u0645\u064e \u0627\u0644\u0644\u064e\u0651\u0647\u064f \u0625\u0650\u0644\u064e\u0651\u0627 \u0628\u0650\u0627\u0644\u0652\u062d\u064e\u0642\u0650\u0651\", \"Quran 6:151\"),\n",
    "        (\"\u0648\u064e\u0628\u0650\u0627\u0644\u0652\u0648\u064e\u0627\u0644\u0650\u062f\u064e\u064a\u0652\u0646\u0650 \u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u064b\u0627\", \"Quran 17:23\"),\n",
    "        (\"\u0625\u0650\u0645\u064e\u0651\u0627 \u064a\u064e\u0628\u0652\u0644\u064f\u063a\u064e\u0646\u064e\u0651 \u0639\u0650\u0646\u062f\u064e\u0643\u064e \u0627\u0644\u0652\u0643\u0650\u0628\u064e\u0631\u064e \u0623\u064e\u062d\u064e\u062f\u064f\u0647\u064f\u0645\u064e\u0627 \u0623\u064e\u0648\u0652 \u0643\u0650\u0644\u064e\u0627\u0647\u064f\u0645\u064e\u0627 \u0641\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u064f\u0644 \u0644\u064e\u0651\u0647\u064f\u0645\u064e\u0627 \u0623\u064f\u0641\u064d\u0651\", \"Quran 17:23\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0646\u0652\u0647\u064e\u0631\u0652\u0647\u064f\u0645\u064e\u0627 \u0648\u064e\u0642\u064f\u0644 \u0644\u064e\u0651\u0647\u064f\u0645\u064e\u0627 \u0642\u064e\u0648\u0652\u0644\u064b\u0627 \u0643\u064e\u0631\u0650\u064a\u0645\u064b\u0627\", \"Quran 17:23\"),\n",
    "        (\"\u0648\u064e\u0627\u062e\u0652\u0641\u0650\u0636\u0652 \u0644\u064e\u0647\u064f\u0645\u064e\u0627 \u062c\u064e\u0646\u064e\u0627\u062d\u064e \u0627\u0644\u0630\u064f\u0651\u0644\u0650\u0651 \u0645\u0650\u0646\u064e \u0627\u0644\u0631\u064e\u0651\u062d\u0652\u0645\u064e\u0629\u0650\", \"Quran 17:24\"),\n",
    "        (\"\u0648\u064e\u0642\u064f\u0644 \u0631\u064e\u0651\u0628\u0650\u0651 \u0627\u0631\u0652\u062d\u064e\u0645\u0652\u0647\u064f\u0645\u064e\u0627 \u0643\u064e\u0645\u064e\u0627 \u0631\u064e\u0628\u064e\u0651\u064a\u064e\u0627\u0646\u0650\u064a \u0635\u064e\u063a\u0650\u064a\u0631\u064b\u0627\", \"Quran 17:24\"),\n",
    "        (\"\u0648\u064e\u0622\u062a\u0650 \u0630\u064e\u0627 \u0627\u0644\u0652\u0642\u064f\u0631\u0652\u0628\u064e\u0649\u0670 \u062d\u064e\u0642\u064e\u0651\u0647\u064f \u0648\u064e\u0627\u0644\u0652\u0645\u0650\u0633\u0652\u0643\u0650\u064a\u0646\u064e \u0648\u064e\u0627\u0628\u0652\u0646\u064e \u0627\u0644\u0633\u064e\u0651\u0628\u0650\u064a\u0644\u0650\", \"Quran 17:26\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064f\u0628\u064e\u0630\u0650\u0651\u0631\u0652 \u062a\u064e\u0628\u0652\u0630\u0650\u064a\u0631\u064b\u0627\", \"Quran 17:26\"),\n",
    "        (\"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0652\u0645\u064f\u0628\u064e\u0630\u0650\u0651\u0631\u0650\u064a\u0646\u064e \u0643\u064e\u0627\u0646\u064f\u0648\u0627 \u0625\u0650\u062e\u0652\u0648\u064e\u0627\u0646\u064e \u0627\u0644\u0634\u064e\u0651\u064a\u064e\u0627\u0637\u0650\u064a\u0646\u0650\", \"Quran 17:27\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u062c\u0652\u0639\u064e\u0644\u0652 \u064a\u064e\u062f\u064e\u0643\u064e \u0645\u064e\u063a\u0652\u0644\u064f\u0648\u0644\u064e\u0629\u064b \u0625\u0650\u0644\u064e\u0649\u0670 \u0639\u064f\u0646\u064f\u0642\u0650\u0643\u064e \u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0628\u0652\u0633\u064f\u0637\u0652\u0647\u064e\u0627 \u0643\u064f\u0644\u064e\u0651 \u0627\u0644\u0652\u0628\u064e\u0633\u0652\u0637\u0650\", \"Quran 17:29\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u0631\u064e\u0628\u064f\u0648\u0627 \u0627\u0644\u0632\u0650\u0651\u0646\u064e\u0627 \u06d6 \u0625\u0650\u0646\u064e\u0651\u0647\u064f \u0643\u064e\u0627\u0646\u064e \u0641\u064e\u0627\u062d\u0650\u0634\u064e\u0629\u064b \u0648\u064e\u0633\u064e\u0627\u0621\u064e \u0633\u064e\u0628\u0650\u064a\u0644\u064b\u0627\", \"Quran 17:32\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u062a\u064f\u0644\u064f\u0648\u0627 \u0623\u064e\u0648\u0652\u0644\u064e\u0627\u062f\u064e\u0643\u064f\u0645\u0652 \u062e\u064e\u0634\u0652\u064a\u064e\u0629\u064e \u0625\u0650\u0645\u0652\u0644\u064e\u0627\u0642\u064d\", \"Quran 17:31\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u0631\u064e\u0628\u064f\u0648\u0627 \u0645\u064e\u0627\u0644\u064e \u0627\u0644\u0652\u064a\u064e\u062a\u0650\u064a\u0645\u0650 \u0625\u0650\u0644\u064e\u0651\u0627 \u0628\u0650\u0627\u0644\u064e\u0651\u062a\u0650\u064a \u0647\u0650\u064a\u064e \u0623\u064e\u062d\u0652\u0633\u064e\u0646\u064f\", \"Quran 17:34\"),\n",
    "        (\"\u0648\u064e\u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u0650 \u06d6 \u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u064e \u0643\u064e\u0627\u0646\u064e \u0645\u064e\u0633\u0652\u0626\u064f\u0648\u0644\u064b\u0627\", \"Quran 17:34\"),\n",
    "        (\"\u0648\u064e\u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0627\u0644\u0652\u0643\u064e\u064a\u0652\u0644\u064e \u0625\u0650\u0630\u064e\u0627 \u0643\u0650\u0644\u0652\u062a\u064f\u0645\u0652 \u0648\u064e\u0632\u0650\u0646\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0642\u0650\u0633\u0652\u0637\u064e\u0627\u0633\u0650 \u0627\u0644\u0652\u0645\u064f\u0633\u0652\u062a\u064e\u0642\u0650\u064a\u0645\u0650\", \"Quran 17:35\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u0641\u064f \u0645\u064e\u0627 \u0644\u064e\u064a\u0652\u0633\u064e \u0644\u064e\u0643\u064e \u0628\u0650\u0647\u0650 \u0639\u0650\u0644\u0652\u0645\u064c\", \"Quran 17:36\"),\n",
    "        (\"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0633\u064e\u0651\u0645\u0652\u0639\u064e \u0648\u064e\u0627\u0644\u0652\u0628\u064e\u0635\u064e\u0631\u064e \u0648\u064e\u0627\u0644\u0652\u0641\u064f\u0624\u064e\u0627\u062f\u064e \u0643\u064f\u0644\u064f\u0651 \u0623\u064f\u0648\u0644\u064e\u0670\u0626\u0650\u0643\u064e \u0643\u064e\u0627\u0646\u064e \u0639\u064e\u0646\u0652\u0647\u064f \u0645\u064e\u0633\u0652\u0626\u064f\u0648\u0644\u064b\u0627\", \"Quran 17:36\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0645\u0652\u0634\u0650 \u0641\u0650\u064a \u0627\u0644\u0652\u0623\u064e\u0631\u0652\u0636\u0650 \u0645\u064e\u0631\u064e\u062d\u064b\u0627\", \"Quran 17:37\"),\n",
    "        (\"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0644\u064e\u0651\u0647\u064e \u064a\u064e\u0623\u0652\u0645\u064f\u0631\u064f \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u062f\u0652\u0644\u0650 \u0648\u064e\u0627\u0644\u0652\u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u0650 \u0648\u064e\u0625\u0650\u064a\u062a\u064e\u0627\u0621\u0650 \u0630\u0650\u064a \u0627\u0644\u0652\u0642\u064f\u0631\u0652\u0628\u064e\u0649\u0670\", \"Quran 16:90\"),\n",
    "        (\"\u0648\u064e\u064a\u064e\u0646\u0652\u0647\u064e\u0649\u0670 \u0639\u064e\u0646\u0650 \u0627\u0644\u0652\u0641\u064e\u062d\u0652\u0634\u064e\u0627\u0621\u0650 \u0648\u064e\u0627\u0644\u0652\u0645\u064f\u0646\u0643\u064e\u0631\u0650 \u0648\u064e\u0627\u0644\u0652\u0628\u064e\u063a\u0652\u064a\u0650\", \"Quran 16:90\"),\n",
    "        (\"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0622\u0645\u064e\u0646\u064f\u0648\u0627 \u0643\u064f\u0648\u0646\u064f\u0648\u0627 \u0642\u064e\u0648\u064e\u0651\u0627\u0645\u0650\u064a\u0646\u064e \u0628\u0650\u0627\u0644\u0652\u0642\u0650\u0633\u0652\u0637\u0650\", \"Quran 4:135\"),\n",
    "        (\"\u0634\u064f\u0647\u064e\u062f\u064e\u0627\u0621\u064e \u0644\u0650\u0644\u064e\u0651\u0647\u0650 \u0648\u064e\u0644\u064e\u0648\u0652 \u0639\u064e\u0644\u064e\u0649\u0670 \u0623\u064e\u0646\u0641\u064f\u0633\u0650\u0643\u064f\u0645\u0652 \u0623\u064e\u0648\u0650 \u0627\u0644\u0652\u0648\u064e\u0627\u0644\u0650\u062f\u064e\u064a\u0652\u0646\u0650 \u0648\u064e\u0627\u0644\u0652\u0623\u064e\u0642\u0652\u0631\u064e\u0628\u0650\u064a\u0646\u064e\", \"Quran 4:135\"),\n",
    "        (\"\u0648\u064e\u0625\u0650\u0630\u064e\u0627 \u062d\u064e\u0643\u064e\u0645\u0652\u062a\u064f\u0645 \u0628\u064e\u064a\u0652\u0646\u064e \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u0650 \u0623\u064e\u0646 \u062a\u064e\u062d\u0652\u0643\u064f\u0645\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u062f\u0652\u0644\u0650\", \"Quran 4:58\"),\n",
    "        (\"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0622\u0645\u064e\u0646\u064f\u0648\u0627 \u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064f\u0642\u064f\u0648\u062f\u0650\", \"Quran 5:1\"),\n",
    "        (\"\u0648\u064e\u062a\u064e\u0639\u064e\u0627\u0648\u064e\u0646\u064f\u0648\u0627 \u0639\u064e\u0644\u064e\u0649 \u0627\u0644\u0652\u0628\u0650\u0631\u0650\u0651 \u0648\u064e\u0627\u0644\u062a\u064e\u0651\u0642\u0652\u0648\u064e\u0649\u0670 \u06d6 \u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0639\u064e\u0627\u0648\u064e\u0646\u064f\u0648\u0627 \u0639\u064e\u0644\u064e\u0649 \u0627\u0644\u0652\u0625\u0650\u062b\u0652\u0645\u0650 \u0648\u064e\u0627\u0644\u0652\u0639\u064f\u062f\u0652\u0648\u064e\u0627\u0646\u0650\", \"Quran 5:2\"),\n",
    "        (\"\u0645\u064e\u0646 \u0642\u064e\u062a\u064e\u0644\u064e \u0646\u064e\u0641\u0652\u0633\u064b\u0627 \u0628\u0650\u063a\u064e\u064a\u0652\u0631\u0650 \u0646\u064e\u0641\u0652\u0633\u064d \u0623\u064e\u0648\u0652 \u0641\u064e\u0633\u064e\u0627\u062f\u064d \u0641\u0650\u064a \u0627\u0644\u0652\u0623\u064e\u0631\u0652\u0636\u0650 \u0641\u064e\u0643\u064e\u0623\u064e\u0646\u064e\u0651\u0645\u064e\u0627 \u0642\u064e\u062a\u064e\u0644\u064e \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u064e \u062c\u064e\u0645\u0650\u064a\u0639\u064b\u0627\", \"Quran 5:32\"),\n",
    "        (\"\u0648\u064e\u0645\u064e\u0646\u0652 \u0623\u064e\u062d\u0652\u064a\u064e\u0627\u0647\u064e\u0627 \u0641\u064e\u0643\u064e\u0623\u064e\u0646\u064e\u0651\u0645\u064e\u0627 \u0623\u064e\u062d\u0652\u064a\u064e\u0627 \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u064e \u062c\u064e\u0645\u0650\u064a\u0639\u064b\u0627\", \"Quran 5:32\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0627 \u064a\u064e\u062c\u0652\u0631\u0650\u0645\u064e\u0646\u064e\u0651\u0643\u064f\u0645\u0652 \u0634\u064e\u0646\u064e\u0622\u0646\u064f \u0642\u064e\u0648\u0652\u0645\u064d \u0639\u064e\u0644\u064e\u0649\u0670 \u0623\u064e\u0644\u064e\u0651\u0627 \u062a\u064e\u0639\u0652\u062f\u0650\u0644\u064f\u0648\u0627\", \"Quran 5:8\"),\n",
    "        (\"\u0627\u0639\u0652\u062f\u0650\u0644\u064f\u0648\u0627 \u0647\u064f\u0648\u064e \u0623\u064e\u0642\u0652\u0631\u064e\u0628\u064f \u0644\u0650\u0644\u062a\u064e\u0651\u0642\u0652\u0648\u064e\u0649\u0670\", \"Quran 5:8\"),\n",
    "        (\"\u0644\u064e\u0651\u064a\u0652\u0633\u064e \u0627\u0644\u0652\u0628\u0650\u0631\u064e\u0651 \u0623\u064e\u0646 \u062a\u064f\u0648\u064e\u0644\u064f\u0651\u0648\u0627 \u0648\u064f\u062c\u064f\u0648\u0647\u064e\u0643\u064f\u0645\u0652 \u0642\u0650\u0628\u064e\u0644\u064e \u0627\u0644\u0652\u0645\u064e\u0634\u0652\u0631\u0650\u0642\u0650 \u0648\u064e\u0627\u0644\u0652\u0645\u064e\u063a\u0652\u0631\u0650\u0628\u0650\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0644\u064e\u0670\u0643\u0650\u0646\u064e\u0651 \u0627\u0644\u0652\u0628\u0650\u0631\u064e\u0651 \u0645\u064e\u0646\u0652 \u0622\u0645\u064e\u0646\u064e \u0628\u0650\u0627\u0644\u0644\u064e\u0651\u0647\u0650 \u0648\u064e\u0627\u0644\u0652\u064a\u064e\u0648\u0652\u0645\u0650 \u0627\u0644\u0652\u0622\u062e\u0650\u0631\u0650\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0622\u062a\u064e\u0649 \u0627\u0644\u0652\u0645\u064e\u0627\u0644\u064e \u0639\u064e\u0644\u064e\u0649\u0670 \u062d\u064f\u0628\u0650\u0651\u0647\u0650 \u0630\u064e\u0648\u0650\u064a \u0627\u0644\u0652\u0642\u064f\u0631\u0652\u0628\u064e\u0649\u0670 \u0648\u064e\u0627\u0644\u0652\u064a\u064e\u062a\u064e\u0627\u0645\u064e\u0649\u0670 \u0648\u064e\u0627\u0644\u0652\u0645\u064e\u0633\u064e\u0627\u0643\u0650\u064a\u0646\u064e\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0627\u0628\u0652\u0646\u064e \u0627\u0644\u0633\u064e\u0651\u0628\u0650\u064a\u0644\u0650 \u0648\u064e\u0627\u0644\u0633\u064e\u0651\u0627\u0626\u0650\u0644\u0650\u064a\u0646\u064e \u0648\u064e\u0641\u0650\u064a \u0627\u0644\u0631\u0650\u0651\u0642\u064e\u0627\u0628\u0650\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0623\u064e\u0642\u064e\u0627\u0645\u064e \u0627\u0644\u0635\u064e\u0651\u0644\u064e\u0627\u0629\u064e \u0648\u064e\u0622\u062a\u064e\u0649 \u0627\u0644\u0632\u064e\u0651\u0643\u064e\u0627\u0629\u064e\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0627\u0644\u0652\u0645\u064f\u0648\u0641\u064f\u0648\u0646\u064e \u0628\u0650\u0639\u064e\u0647\u0652\u062f\u0650\u0647\u0650\u0645\u0652 \u0625\u0650\u0630\u064e\u0627 \u0639\u064e\u0627\u0647\u064e\u062f\u064f\u0648\u0627\", \"Quran 2:177\"),\n",
    "        (\"\u0648\u064e\u0627\u0644\u0635\u064e\u0651\u0627\u0628\u0650\u0631\u0650\u064a\u0646\u064e \u0641\u0650\u064a \u0627\u0644\u0652\u0628\u064e\u0623\u0652\u0633\u064e\u0627\u0621\u0650 \u0648\u064e\u0627\u0644\u0636\u064e\u0651\u0631\u064e\u0651\u0627\u0621\u0650 \u0648\u064e\u062d\u0650\u064a\u0646\u064e \u0627\u0644\u0652\u0628\u064e\u0623\u0652\u0633\u0650\", \"Quran 2:177\"),\n",
    "        (\"\u062e\u064f\u0630\u0650 \u0627\u0644\u0652\u0639\u064e\u0641\u0652\u0648\u064e \u0648\u064e\u0623\u0652\u0645\u064f\u0631\u0652 \u0628\u0650\u0627\u0644\u0652\u0639\u064f\u0631\u0652\u0641\u0650 \u0648\u064e\u0623\u064e\u0639\u0652\u0631\u0650\u0636\u0652 \u0639\u064e\u0646\u0650 \u0627\u0644\u0652\u062c\u064e\u0627\u0647\u0650\u0644\u0650\u064a\u0646\u064e\", \"Quran 7:199\"),\n",
    "        (\"\u0648\u064e\u0627\u0644\u0652\u0643\u064e\u0627\u0638\u0650\u0645\u0650\u064a\u0646\u064e \u0627\u0644\u0652\u063a\u064e\u064a\u0652\u0638\u064e \u0648\u064e\u0627\u0644\u0652\u0639\u064e\u0627\u0641\u0650\u064a\u0646\u064e \u0639\u064e\u0646\u0650 \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u0650\", \"Quran 3:134\"),\n",
    "        (\"\u0648\u064e\u0627\u0644\u0644\u064e\u0651\u0647\u064f \u064a\u064f\u062d\u0650\u0628\u064f\u0651 \u0627\u0644\u0652\u0645\u064f\u062d\u0652\u0633\u0650\u0646\u0650\u064a\u0646\u064e\", \"Quran 3:134\"),\n",
    "        (\"\u0627\u062f\u0652\u0641\u064e\u0639\u0652 \u0628\u0650\u0627\u0644\u064e\u0651\u062a\u0650\u064a \u0647\u0650\u064a\u064e \u0623\u064e\u062d\u0652\u0633\u064e\u0646\u064f \u0641\u064e\u0625\u0650\u0630\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a \u0628\u064e\u064a\u0652\u0646\u064e\u0643\u064e \u0648\u064e\u0628\u064e\u064a\u0652\u0646\u064e\u0647\u064f \u0639\u064e\u062f\u064e\u0627\u0648\u064e\u0629\u064c \u0643\u064e\u0623\u064e\u0646\u064e\u0651\u0647\u064f \u0648\u064e\u0644\u0650\u064a\u064c\u0651 \u062d\u064e\u0645\u0650\u064a\u0645\u064c\", \"Quran 41:34\"),\n",
    "        (\"\u0648\u064e\u0645\u064e\u0627 \u064a\u064f\u0644\u064e\u0642\u064e\u0651\u0627\u0647\u064e\u0627 \u0625\u0650\u0644\u064e\u0651\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0635\u064e\u0628\u064e\u0631\u064f\u0648\u0627 \u0648\u064e\u0645\u064e\u0627 \u064a\u064f\u0644\u064e\u0642\u064e\u0651\u0627\u0647\u064e\u0627 \u0625\u0650\u0644\u064e\u0651\u0627 \u0630\u064f\u0648 \u062d\u064e\u0638\u064d\u0651 \u0639\u064e\u0638\u0650\u064a\u0645\u064d\", \"Quran 41:35\"),\n",
    "        (\"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0644\u064e\u0651\u0647\u064e \u064a\u064e\u0623\u0652\u0645\u064f\u0631\u064f\u0643\u064f\u0645\u0652 \u0623\u064e\u0646 \u062a\u064f\u0624\u064e\u062f\u064f\u0651\u0648\u0627 \u0627\u0644\u0652\u0623\u064e\u0645\u064e\u0627\u0646\u064e\u0627\u062a\u0650 \u0625\u0650\u0644\u064e\u0649\u0670 \u0623\u064e\u0647\u0652\u0644\u0650\u0647\u064e\u0627\", \"Quran 4:58\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(quran):\n",
    "        islamic.append({\"id\": f\"quran_{i}\", \"text\": text, \"source\": source, \"period\": \"QURANIC\", \"century\": 7})\n",
    "    print(f\"    - Quranic verses: {len([x for x in islamic if 'quran' in x['id']]):,} passages\")\n",
    "    \n",
    "    # === HADITH (110+) ===\n",
    "    hadith = [\n",
    "        (\"\u0644\u0627 \u0636\u0631\u0631 \u0648\u0644\u0627 \u0636\u0631\u0627\u0631\", \"Hadith - Ibn Majah\"),\n",
    "        (\"\u0625\u0646\u0645\u0627 \u0627\u0644\u0623\u0639\u0645\u0627\u0644 \u0628\u0627\u0644\u0646\u064a\u0627\u062a \u0648\u0625\u0646\u0645\u0627 \u0644\u0643\u0644 \u0627\u0645\u0631\u0626 \u0645\u0627 \u0646\u0648\u0649\", \"Hadith - Bukhari 1\"),\n",
    "        (\"\u0627\u0644\u0645\u0633\u0644\u0645 \u0645\u0646 \u0633\u0644\u0645 \u0627\u0644\u0645\u0633\u0644\u0645\u0648\u0646 \u0645\u0646 \u0644\u0633\u0627\u0646\u0647 \u0648\u064a\u062f\u0647\", \"Hadith - Bukhari 10\"),\n",
    "        (\"\u0644\u0627 \u064a\u0624\u0645\u0646 \u0623\u062d\u062f\u0643\u0645 \u062d\u062a\u0649 \u064a\u062d\u0628 \u0644\u0623\u062e\u064a\u0647 \u0645\u0627 \u064a\u062d\u0628 \u0644\u0646\u0641\u0633\u0647\", \"Hadith - Bukhari 13\"),\n",
    "        (\"\u0645\u0646 \u0643\u0627\u0646 \u064a\u0624\u0645\u0646 \u0628\u0627\u0644\u0644\u0647 \u0648\u0627\u0644\u064a\u0648\u0645 \u0627\u0644\u0622\u062e\u0631 \u0641\u0644\u064a\u0642\u0644 \u062e\u064a\u0631\u0627 \u0623\u0648 \u0644\u064a\u0635\u0645\u062a\", \"Hadith - Bukhari 6018\"),\n",
    "        (\"\u0645\u0646 \u0643\u0627\u0646 \u064a\u0624\u0645\u0646 \u0628\u0627\u0644\u0644\u0647 \u0648\u0627\u0644\u064a\u0648\u0645 \u0627\u0644\u0622\u062e\u0631 \u0641\u0644\u064a\u0643\u0631\u0645 \u0636\u064a\u0641\u0647\", \"Hadith - Bukhari 6019\"),\n",
    "        (\"\u0645\u0646 \u0643\u0627\u0646 \u064a\u0624\u0645\u0646 \u0628\u0627\u0644\u0644\u0647 \u0648\u0627\u0644\u064a\u0648\u0645 \u0627\u0644\u0622\u062e\u0631 \u0641\u0644\u064a\u0635\u0644 \u0631\u062d\u0645\u0647\", \"Hadith - Bukhari 6138\"),\n",
    "        (\"\u0627\u0631\u062d\u0645\u0648\u0627 \u0645\u0646 \u0641\u064a \u0627\u0644\u0623\u0631\u0636 \u064a\u0631\u062d\u0645\u0643\u0645 \u0645\u0646 \u0641\u064a \u0627\u0644\u0633\u0645\u0627\u0621\", \"Hadith - Tirmidhi 1924\"),\n",
    "        (\"\u0627\u0644\u0631\u0627\u062d\u0645\u0648\u0646 \u064a\u0631\u062d\u0645\u0647\u0645 \u0627\u0644\u0631\u062d\u0645\u0646\", \"Hadith - Abu Dawud 4941\"),\n",
    "        (\"\u0644\u064a\u0633 \u0645\u0646\u0627 \u0645\u0646 \u0644\u0645 \u064a\u0631\u062d\u0645 \u0635\u063a\u064a\u0631\u0646\u0627 \u0648\u064a\u0648\u0642\u0631 \u0643\u0628\u064a\u0631\u0646\u0627\", \"Hadith - Tirmidhi 1919\"),\n",
    "        (\"\u062e\u064a\u0631\u0643\u0645 \u062e\u064a\u0631\u0643\u0645 \u0644\u0623\u0647\u0644\u0647 \u0648\u0623\u0646\u0627 \u062e\u064a\u0631\u0643\u0645 \u0644\u0623\u0647\u0644\u064a\", \"Hadith - Tirmidhi 3895\"),\n",
    "        (\"\u0627\u062a\u0642 \u0627\u0644\u0644\u0647 \u062d\u064a\u062b\u0645\u0627 \u0643\u0646\u062a \u0648\u0623\u062a\u0628\u0639 \u0627\u0644\u0633\u064a\u0626\u0629 \u0627\u0644\u062d\u0633\u0646\u0629 \u062a\u0645\u062d\u0647\u0627\", \"Hadith - Tirmidhi 1987\"),\n",
    "        (\"\u0648\u062e\u0627\u0644\u0642 \u0627\u0644\u0646\u0627\u0633 \u0628\u062e\u0644\u0642 \u062d\u0633\u0646\", \"Hadith - Tirmidhi 1987\"),\n",
    "        (\"\u0623\u0643\u0645\u0644 \u0627\u0644\u0645\u0624\u0645\u0646\u064a\u0646 \u0625\u064a\u0645\u0627\u0646\u0627 \u0623\u062d\u0633\u0646\u0647\u0645 \u062e\u0644\u0642\u0627\", \"Hadith - Abu Dawud 4682\"),\n",
    "        (\"\u0625\u0646 \u0645\u0646 \u0623\u062d\u0628\u0643\u0645 \u0625\u0644\u064a \u0648\u0623\u0642\u0631\u0628\u0643\u0645 \u0645\u0646\u064a \u0645\u062c\u0644\u0633\u0627 \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629 \u0623\u062d\u0627\u0633\u0646\u0643\u0645 \u0623\u062e\u0644\u0627\u0642\u0627\", \"Hadith - Tirmidhi 2018\"),\n",
    "        (\"\u0645\u0627 \u0645\u0646 \u0634\u064a\u0621 \u0623\u062b\u0642\u0644 \u0641\u064a \u0645\u064a\u0632\u0627\u0646 \u0627\u0644\u0645\u0624\u0645\u0646 \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629 \u0645\u0646 \u062d\u0633\u0646 \u0627\u0644\u062e\u0644\u0642\", \"Hadith - Tirmidhi 2002\"),\n",
    "        (\"\u0627\u0644\u0628\u0631 \u062d\u0633\u0646 \u0627\u0644\u062e\u0644\u0642 \u0648\u0627\u0644\u0625\u062b\u0645 \u0645\u0627 \u062d\u0627\u0643 \u0641\u064a \u0635\u062f\u0631\u0643 \u0648\u0643\u0631\u0647\u062a \u0623\u0646 \u064a\u0637\u0644\u0639 \u0639\u0644\u064a\u0647 \u0627\u0644\u0646\u0627\u0633\", \"Hadith - Muslim 2553\"),\n",
    "        (\"\u0627\u0644\u062d\u064a\u0627\u0621 \u0645\u0646 \u0627\u0644\u0625\u064a\u0645\u0627\u0646\", \"Hadith - Bukhari 24\"),\n",
    "        (\"\u0627\u0644\u062d\u064a\u0627\u0621 \u0644\u0627 \u064a\u0623\u062a\u064a \u0625\u0644\u0627 \u0628\u062e\u064a\u0631\", \"Hadith - Bukhari 6117\"),\n",
    "        (\"\u0625\u0646 \u0627\u0644\u0644\u0647 \u0631\u0641\u064a\u0642 \u064a\u062d\u0628 \u0627\u0644\u0631\u0641\u0642 \u0641\u064a \u0627\u0644\u0623\u0645\u0631 \u0643\u0644\u0647\", \"Hadith - Bukhari 6927\"),\n",
    "        (\"\u0645\u0627 \u0643\u0627\u0646 \u0627\u0644\u0631\u0641\u0642 \u0641\u064a \u0634\u064a\u0621 \u0625\u0644\u0627 \u0632\u0627\u0646\u0647 \u0648\u0645\u0627 \u0646\u0632\u0639 \u0645\u0646 \u0634\u064a\u0621 \u0625\u0644\u0627 \u0634\u0627\u0646\u0647\", \"Hadith - Muslim 2594\"),\n",
    "        (\"\u0645\u0646 \u064a\u062d\u0631\u0645 \u0627\u0644\u0631\u0641\u0642 \u064a\u062d\u0631\u0645 \u0627\u0644\u062e\u064a\u0631 \u0643\u0644\u0647\", \"Hadith - Muslim 2592\"),\n",
    "        (\"\u0623\u062f \u0627\u0644\u0623\u0645\u0627\u0646\u0629 \u0625\u0644\u0649 \u0645\u0646 \u0627\u0626\u062a\u0645\u0646\u0643 \u0648\u0644\u0627 \u062a\u062e\u0646 \u0645\u0646 \u062e\u0627\u0646\u0643\", \"Hadith - Abu Dawud 3535\"),\n",
    "        (\"\u0622\u064a\u0629 \u0627\u0644\u0645\u0646\u0627\u0641\u0642 \u062b\u0644\u0627\u062b \u0625\u0630\u0627 \u062d\u062f\u062b \u0643\u0630\u0628 \u0648\u0625\u0630\u0627 \u0648\u0639\u062f \u0623\u062e\u0644\u0641 \u0648\u0625\u0630\u0627 \u0627\u0624\u062a\u0645\u0646 \u062e\u0627\u0646\", \"Hadith - Bukhari 33\"),\n",
    "        (\"\u0627\u0644\u0635\u062f\u0642 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0628\u0631 \u0648\u0627\u0644\u0628\u0631 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u062c\u0646\u0629\", \"Hadith - Bukhari 6094\"),\n",
    "        (\"\u0648\u0625\u0646 \u0627\u0644\u0643\u0630\u0628 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0641\u062c\u0648\u0631 \u0648\u0627\u0644\u0641\u062c\u0648\u0631 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0646\u0627\u0631\", \"Hadith - Bukhari 6094\"),\n",
    "        (\"\u0639\u0644\u064a\u0643\u0645 \u0628\u0627\u0644\u0635\u062f\u0642 \u0641\u0625\u0646 \u0627\u0644\u0635\u062f\u0642 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0628\u0631\", \"Hadith - Muslim 2607\"),\n",
    "        (\"\u0625\u064a\u0627\u0643\u0645 \u0648\u0627\u0644\u0643\u0630\u0628 \u0641\u0625\u0646 \u0627\u0644\u0643\u0630\u0628 \u064a\u0647\u062f\u064a \u0625\u0644\u0649 \u0627\u0644\u0641\u062c\u0648\u0631\", \"Hadith - Muslim 2607\"),\n",
    "        (\"\u0645\u0646 \u063a\u0634\u0646\u0627 \u0641\u0644\u064a\u0633 \u0645\u0646\u0627\", \"Hadith - Muslim 101\"),\n",
    "        (\"\u0643\u0644\u0643\u0645 \u0631\u0627\u0639 \u0648\u0643\u0644\u0643\u0645 \u0645\u0633\u0624\u0648\u0644 \u0639\u0646 \u0631\u0639\u064a\u062a\u0647\", \"Hadith - Bukhari 893\"),\n",
    "        (\"\u0627\u0644\u0625\u0645\u0627\u0645 \u0631\u0627\u0639 \u0648\u0645\u0633\u0624\u0648\u0644 \u0639\u0646 \u0631\u0639\u064a\u062a\u0647\", \"Hadith - Bukhari 893\"),\n",
    "        (\"\u0648\u0627\u0644\u0631\u062c\u0644 \u0631\u0627\u0639 \u0641\u064a \u0623\u0647\u0644\u0647 \u0648\u0645\u0633\u0624\u0648\u0644 \u0639\u0646 \u0631\u0639\u064a\u062a\u0647\", \"Hadith - Bukhari 893\"),\n",
    "        (\"\u0648\u0627\u0644\u0645\u0631\u0623\u0629 \u0631\u0627\u0639\u064a\u0629 \u0641\u064a \u0628\u064a\u062a \u0632\u0648\u062c\u0647\u0627 \u0648\u0645\u0633\u0624\u0648\u0644\u0629 \u0639\u0646 \u0631\u0639\u064a\u062a\u0647\u0627\", \"Hadith - Bukhari 893\"),\n",
    "        (\"\u0627\u0646\u0635\u0631 \u0623\u062e\u0627\u0643 \u0638\u0627\u0644\u0645\u0627 \u0623\u0648 \u0645\u0638\u0644\u0648\u0645\u0627\", \"Hadith - Bukhari 2444\"),\n",
    "        (\"\u062a\u0646\u0635\u0631\u0647 \u0625\u0630\u0627 \u0643\u0627\u0646 \u0645\u0638\u0644\u0648\u0645\u0627 \u0623\u0641\u0631\u0623\u064a\u062a \u0625\u0630\u0627 \u0643\u0627\u0646 \u0638\u0627\u0644\u0645\u0627 \u0643\u064a\u0641 \u062a\u0646\u0635\u0631\u0647 \u0642\u0627\u0644 \u062a\u062d\u062c\u0632\u0647 \u0623\u0648 \u062a\u0645\u0646\u0639\u0647 \u0645\u0646 \u0627\u0644\u0638\u0644\u0645 \u0641\u0625\u0646 \u0630\u0644\u0643 \u0646\u0635\u0631\u0647\", \"Hadith - Bukhari 2444\"),\n",
    "        (\"\u0627\u0644\u0645\u0624\u0645\u0646 \u0644\u0644\u0645\u0624\u0645\u0646 \u0643\u0627\u0644\u0628\u0646\u064a\u0627\u0646 \u064a\u0634\u062f \u0628\u0639\u0636\u0647 \u0628\u0639\u0636\u0627\", \"Hadith - Bukhari 481\"),\n",
    "        (\"\u0645\u062b\u0644 \u0627\u0644\u0645\u0624\u0645\u0646\u064a\u0646 \u0641\u064a \u062a\u0648\u0627\u062f\u0647\u0645 \u0648\u062a\u0631\u0627\u062d\u0645\u0647\u0645 \u0648\u062a\u0639\u0627\u0637\u0641\u0647\u0645 \u0645\u062b\u0644 \u0627\u0644\u062c\u0633\u062f \u0627\u0644\u0648\u0627\u062d\u062f\", \"Hadith - Muslim 2586\"),\n",
    "        (\"\u0625\u0630\u0627 \u0627\u0634\u062a\u0643\u0649 \u0645\u0646\u0647 \u0639\u0636\u0648 \u062a\u062f\u0627\u0639\u0649 \u0644\u0647 \u0633\u0627\u0626\u0631 \u0627\u0644\u062c\u0633\u062f \u0628\u0627\u0644\u0633\u0647\u0631 \u0648\u0627\u0644\u062d\u0645\u0649\", \"Hadith - Muslim 2586\"),\n",
    "        (\"\u0627\u0644\u0645\u0633\u0644\u0645 \u0623\u062e\u0648 \u0627\u0644\u0645\u0633\u0644\u0645 \u0644\u0627 \u064a\u0638\u0644\u0645\u0647 \u0648\u0644\u0627 \u064a\u0633\u0644\u0645\u0647\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"\u0645\u0646 \u0643\u0627\u0646 \u0641\u064a \u062d\u0627\u062c\u0629 \u0623\u062e\u064a\u0647 \u0643\u0627\u0646 \u0627\u0644\u0644\u0647 \u0641\u064a \u062d\u0627\u062c\u062a\u0647\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"\u0648\u0645\u0646 \u0641\u0631\u062c \u0639\u0646 \u0645\u0633\u0644\u0645 \u0643\u0631\u0628\u0629 \u0641\u0631\u062c \u0627\u0644\u0644\u0647 \u0639\u0646\u0647 \u0643\u0631\u0628\u0629 \u0645\u0646 \u0643\u0631\u0628\u0627\u062a \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"\u0648\u0645\u0646 \u0633\u062a\u0631 \u0645\u0633\u0644\u0645\u0627 \u0633\u062a\u0631\u0647 \u0627\u0644\u0644\u0647 \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"\u0644\u0627 \u062a\u062d\u0627\u0633\u062f\u0648\u0627 \u0648\u0644\u0627 \u062a\u0646\u0627\u062c\u0634\u0648\u0627 \u0648\u0644\u0627 \u062a\u0628\u0627\u063a\u0636\u0648\u0627 \u0648\u0644\u0627 \u062a\u062f\u0627\u0628\u0631\u0648\u0627\", \"Hadith - Muslim 2564\"),\n",
    "        (\"\u0648\u0644\u0627 \u064a\u0628\u0639 \u0628\u0639\u0636\u0643\u0645 \u0639\u0644\u0649 \u0628\u064a\u0639 \u0628\u0639\u0636 \u0648\u0643\u0648\u0646\u0648\u0627 \u0639\u0628\u0627\u062f \u0627\u0644\u0644\u0647 \u0625\u062e\u0648\u0627\u0646\u0627\", \"Hadith - Muslim 2564\"),\n",
    "        (\"\u0628\u062d\u0633\u0628 \u0627\u0645\u0631\u0626 \u0645\u0646 \u0627\u0644\u0634\u0631 \u0623\u0646 \u064a\u062d\u0642\u0631 \u0623\u062e\u0627\u0647 \u0627\u0644\u0645\u0633\u0644\u0645\", \"Hadith - Muslim 2564\"),\n",
    "        (\"\u0643\u0644 \u0627\u0644\u0645\u0633\u0644\u0645 \u0639\u0644\u0649 \u0627\u0644\u0645\u0633\u0644\u0645 \u062d\u0631\u0627\u0645 \u062f\u0645\u0647 \u0648\u0645\u0627\u0644\u0647 \u0648\u0639\u0631\u0636\u0647\", \"Hadith - Muslim 2564\"),\n",
    "        (\"\u0625\u064a\u0627\u0643\u0645 \u0648\u0627\u0644\u0638\u0646 \u0641\u0625\u0646 \u0627\u0644\u0638\u0646 \u0623\u0643\u0630\u0628 \u0627\u0644\u062d\u062f\u064a\u062b\", \"Hadith - Bukhari 6064\"),\n",
    "        (\"\u0648\u0644\u0627 \u062a\u062c\u0633\u0633\u0648\u0627 \u0648\u0644\u0627 \u062a\u062d\u0633\u0633\u0648\u0627 \u0648\u0644\u0627 \u062a\u0646\u0627\u0641\u0633\u0648\u0627\", \"Hadith - Bukhari 6064\"),\n",
    "        (\"\u0627\u0644\u0638\u0644\u0645 \u0638\u0644\u0645\u0627\u062a \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629\", \"Hadith - Bukhari 2447\"),\n",
    "        (\"\u0627\u062a\u0642\u0648\u0627 \u0627\u0644\u0638\u0644\u0645 \u0641\u0625\u0646 \u0627\u0644\u0638\u0644\u0645 \u0638\u0644\u0645\u0627\u062a \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629\", \"Hadith - Muslim 2578\"),\n",
    "        (\"\u0648\u0627\u062a\u0642\u0648\u0627 \u0627\u0644\u0634\u062d \u0641\u0625\u0646 \u0627\u0644\u0634\u062d \u0623\u0647\u0644\u0643 \u0645\u0646 \u0643\u0627\u0646 \u0642\u0628\u0644\u0643\u0645\", \"Hadith - Muslim 2578\"),\n",
    "        (\"\u0623\u0641\u0636\u0644 \u0627\u0644\u062c\u0647\u0627\u062f \u0643\u0644\u0645\u0629 \u0639\u062f\u0644 \u0639\u0646\u062f \u0633\u0644\u0637\u0627\u0646 \u062c\u0627\u0626\u0631\", \"Hadith - Abu Dawud 4344\"),\n",
    "        (\"\u0633\u064a\u062f \u0627\u0644\u0634\u0647\u062f\u0627\u0621 \u062d\u0645\u0632\u0629 \u0628\u0646 \u0639\u0628\u062f \u0627\u0644\u0645\u0637\u0644\u0628 \u0648\u0631\u062c\u0644 \u0642\u0627\u0645 \u0625\u0644\u0649 \u0625\u0645\u0627\u0645 \u062c\u0627\u0626\u0631 \u0641\u0623\u0645\u0631\u0647 \u0648\u0646\u0647\u0627\u0647 \u0641\u0642\u062a\u0644\u0647\", \"Hadith - Hakim 4884\"),\n",
    "        (\"\u0625\u0630\u0627 \u0631\u0623\u064a\u062a \u0623\u0645\u062a\u064a \u062a\u0647\u0627\u0628 \u0623\u0646 \u062a\u0642\u0648\u0644 \u0644\u0644\u0638\u0627\u0644\u0645 \u064a\u0627 \u0638\u0627\u0644\u0645 \u0641\u0642\u062f \u062a\u0648\u062f\u0639 \u0645\u0646\u0647\u0645\", \"Hadith - Ahmad 6521\"),\n",
    "        (\"\u0645\u0646 \u0631\u0623\u0649 \u0645\u0646\u0643\u0645 \u0645\u0646\u0643\u0631\u0627 \u0641\u0644\u064a\u063a\u064a\u0631\u0647 \u0628\u064a\u062f\u0647\", \"Hadith - Muslim 49\"),\n",
    "        (\"\u0641\u0625\u0646 \u0644\u0645 \u064a\u0633\u062a\u0637\u0639 \u0641\u0628\u0644\u0633\u0627\u0646\u0647 \u0641\u0625\u0646 \u0644\u0645 \u064a\u0633\u062a\u0637\u0639 \u0641\u0628\u0642\u0644\u0628\u0647 \u0648\u0630\u0644\u0643 \u0623\u0636\u0639\u0641 \u0627\u0644\u0625\u064a\u0645\u0627\u0646\", \"Hadith - Muslim 49\"),\n",
    "        (\"\u0623\u062d\u0628 \u0627\u0644\u0646\u0627\u0633 \u0625\u0644\u0649 \u0627\u0644\u0644\u0647 \u0623\u0646\u0641\u0639\u0647\u0645 \u0644\u0644\u0646\u0627\u0633\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"\u0648\u0623\u062d\u0628 \u0627\u0644\u0623\u0639\u0645\u0627\u0644 \u0625\u0644\u0649 \u0627\u0644\u0644\u0647 \u0633\u0631\u0648\u0631 \u062a\u062f\u062e\u0644\u0647 \u0639\u0644\u0649 \u0645\u0633\u0644\u0645\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"\u0623\u0648 \u062a\u0643\u0634\u0641 \u0639\u0646\u0647 \u0643\u0631\u0628\u0629 \u0623\u0648 \u062a\u0642\u0636\u064a \u0639\u0646\u0647 \u062f\u064a\u0646\u0627 \u0623\u0648 \u062a\u0637\u0631\u062f \u0639\u0646\u0647 \u062c\u0648\u0639\u0627\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"\u0648\u0644\u0623\u0646 \u0623\u0645\u0634\u064a \u0645\u0639 \u0623\u062e\u064a \u0641\u064a \u062d\u0627\u062c\u0629 \u0623\u062d\u0628 \u0625\u0644\u064a \u0645\u0646 \u0623\u0646 \u0623\u0639\u062a\u0643\u0641 \u0641\u064a \u0647\u0630\u0627 \u0627\u0644\u0645\u0633\u062c\u062f \u0634\u0647\u0631\u0627\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"\u0627\u0644\u062f\u064a\u0646 \u0627\u0644\u0646\u0635\u064a\u062d\u0629 \u0642\u0644\u0646\u0627 \u0644\u0645\u0646 \u0642\u0627\u0644 \u0644\u0644\u0647 \u0648\u0644\u0643\u062a\u0627\u0628\u0647 \u0648\u0644\u0631\u0633\u0648\u0644\u0647 \u0648\u0644\u0623\u0626\u0645\u0629 \u0627\u0644\u0645\u0633\u0644\u0645\u064a\u0646 \u0648\u0639\u0627\u0645\u062a\u0647\u0645\", \"Hadith - Muslim 55\"),\n",
    "        (\"\u0645\u0627 \u0646\u0642\u0635\u062a \u0635\u062f\u0642\u0629 \u0645\u0646 \u0645\u0627\u0644\", \"Hadith - Muslim 2588\"),\n",
    "        (\"\u0648\u0645\u0627 \u0632\u0627\u062f \u0627\u0644\u0644\u0647 \u0639\u0628\u062f\u0627 \u0628\u0639\u0641\u0648 \u0625\u0644\u0627 \u0639\u0632\u0627\", \"Hadith - Muslim 2588\"),\n",
    "        (\"\u0648\u0645\u0627 \u062a\u0648\u0627\u0636\u0639 \u0623\u062d\u062f \u0644\u0644\u0647 \u0625\u0644\u0627 \u0631\u0641\u0639\u0647 \u0627\u0644\u0644\u0647\", \"Hadith - Muslim 2588\"),\n",
    "        (\"\u0627\u0644\u064a\u062f \u0627\u0644\u0639\u0644\u064a\u0627 \u062e\u064a\u0631 \u0645\u0646 \u0627\u0644\u064a\u062f \u0627\u0644\u0633\u0641\u0644\u0649\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"\u0648\u0627\u0628\u062f\u0623 \u0628\u0645\u0646 \u062a\u0639\u0648\u0644\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"\u0648\u062e\u064a\u0631 \u0627\u0644\u0635\u062f\u0642\u0629 \u0645\u0627 \u0643\u0627\u0646 \u0639\u0646 \u0638\u0647\u0631 \u063a\u0646\u0649\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"\u0645\u0646 \u0627\u0633\u062a\u0637\u0627\u0639 \u0645\u0646\u0643\u0645 \u0627\u0644\u0628\u0627\u0621\u0629 \u0641\u0644\u064a\u062a\u0632\u0648\u062c\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"\u0641\u0625\u0646\u0647 \u0623\u063a\u0636 \u0644\u0644\u0628\u0635\u0631 \u0648\u0623\u062d\u0635\u0646 \u0644\u0644\u0641\u0631\u062c\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"\u0648\u0645\u0646 \u0644\u0645 \u064a\u0633\u062a\u0637\u0639 \u0641\u0639\u0644\u064a\u0647 \u0628\u0627\u0644\u0635\u0648\u0645 \u0641\u0625\u0646\u0647 \u0644\u0647 \u0648\u062c\u0627\u0621\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"\u0627\u0633\u062a\u0648\u0635\u0648\u0627 \u0628\u0627\u0644\u0646\u0633\u0627\u0621 \u062e\u064a\u0631\u0627\", \"Hadith - Bukhari 3331\"),\n",
    "        (\"\u062e\u0630\u0648\u0627 \u0639\u0646\u064a \u062e\u0630\u0648\u0627 \u0639\u0646\u064a \u0642\u062f \u062c\u0639\u0644 \u0627\u0644\u0644\u0647 \u0644\u0647\u0646 \u0633\u0628\u064a\u0644\u0627 \u0627\u0644\u0628\u0643\u0631 \u0628\u0627\u0644\u0628\u0643\u0631 \u062c\u0644\u062f \u0645\u0627\u0626\u0629 \u0648\u0646\u0641\u064a \u0633\u0646\u0629\", \"Hadith - Muslim 1690\"),\n",
    "        (\"\u0644\u0627 \u064a\u0641\u0631\u0643 \u0645\u0624\u0645\u0646 \u0645\u0624\u0645\u0646\u0629 \u0625\u0646 \u0643\u0631\u0647 \u0645\u0646\u0647\u0627 \u062e\u0644\u0642\u0627 \u0631\u0636\u064a \u0645\u0646\u0647\u0627 \u0622\u062e\u0631\", \"Hadith - Muslim 1469\"),\n",
    "        (\"\u0623\u0643\u0645\u0644 \u0627\u0644\u0645\u0624\u0645\u0646\u064a\u0646 \u0625\u064a\u0645\u0627\u0646\u0627 \u0623\u062d\u0633\u0646\u0647\u0645 \u062e\u0644\u0642\u0627 \u0648\u062e\u064a\u0627\u0631\u0643\u0645 \u062e\u064a\u0627\u0631\u0643\u0645 \u0644\u0646\u0633\u0627\u0626\u0647\u0645\", \"Hadith - Tirmidhi 1162\"),\n",
    "        (\"\u0645\u0627 \u0623\u0643\u0631\u0645\u0647\u0646 \u0625\u0644\u0627 \u0643\u0631\u064a\u0645 \u0648\u0645\u0627 \u0623\u0647\u0627\u0646\u0647\u0646 \u0625\u0644\u0627 \u0644\u0626\u064a\u0645\", \"Hadith - Ibn Asakir\"),\n",
    "        (\"\u0627\u0644\u0644\u0647\u0645 \u0625\u0646\u064a \u0623\u062d\u0631\u062c \u062d\u0642 \u0627\u0644\u0636\u0639\u064a\u0641\u064a\u0646 \u0627\u0644\u064a\u062a\u064a\u0645 \u0648\u0627\u0644\u0645\u0631\u0623\u0629\", \"Hadith - Ahmad 9664\"),\n",
    "        (\"\u0623\u0644\u0627 \u0623\u062e\u0628\u0631\u0643\u0645 \u0628\u062e\u064a\u0627\u0631\u0643\u0645 \u0642\u0627\u0644\u0648\u0627 \u0628\u0644\u0649 \u0642\u0627\u0644 \u062e\u064a\u0627\u0631\u0643\u0645 \u0623\u062d\u0627\u0633\u0646\u0643\u0645 \u0623\u062e\u0644\u0627\u0642\u0627\", \"Hadith - Bukhari 6035\"),\n",
    "        (\"\u0625\u0646\u0643\u0645 \u0644\u0646 \u062a\u0633\u0639\u0648\u0627 \u0627\u0644\u0646\u0627\u0633 \u0628\u0623\u0645\u0648\u0627\u0644\u0643\u0645 \u0641\u0644\u064a\u0633\u0639\u0647\u0645 \u0645\u0646\u0643\u0645 \u0628\u0633\u0637 \u0627\u0644\u0648\u062c\u0647 \u0648\u062d\u0633\u0646 \u0627\u0644\u062e\u0644\u0642\", \"Hadith - Hakim 422\"),\n",
    "        (\"\u062a\u0628\u0633\u0645\u0643 \u0641\u064a \u0648\u062c\u0647 \u0623\u062e\u064a\u0643 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0648\u0623\u0645\u0631\u0643 \u0628\u0627\u0644\u0645\u0639\u0631\u0648\u0641 \u0648\u0646\u0647\u064a\u0643 \u0639\u0646 \u0627\u0644\u0645\u0646\u0643\u0631 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0648\u0625\u0631\u0634\u0627\u062f\u0643 \u0627\u0644\u0631\u062c\u0644 \u0641\u064a \u0623\u0631\u0636 \u0627\u0644\u0636\u0644\u0627\u0644 \u0644\u0643 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0648\u0625\u0645\u0627\u0637\u062a\u0643 \u0627\u0644\u0623\u0630\u0649 \u0648\u0627\u0644\u0634\u0648\u0643 \u0648\u0627\u0644\u0639\u0638\u0645 \u0639\u0646 \u0627\u0644\u0637\u0631\u064a\u0642 \u0644\u0643 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0648\u0625\u0641\u0631\u0627\u063a\u0643 \u0645\u0646 \u062f\u0644\u0648\u0643 \u0641\u064a \u062f\u0644\u0648 \u0623\u062e\u064a\u0643 \u0644\u0643 \u0635\u062f\u0642\u0629\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"\u0627\u0644\u0643\u0644\u0645\u0629 \u0627\u0644\u0637\u064a\u0628\u0629 \u0635\u062f\u0642\u0629\", \"Hadith - Bukhari 2989\"),\n",
    "        (\"\u0648\u0643\u0644 \u062e\u0637\u0648\u0629 \u062a\u0645\u0634\u064a\u0647\u0627 \u0625\u0644\u0649 \u0627\u0644\u0635\u0644\u0627\u0629 \u0635\u062f\u0642\u0629\", \"Hadith - Bukhari 2989\"),\n",
    "        (\"\u0645\u0646 \u062f\u0644 \u0639\u0644\u0649 \u062e\u064a\u0631 \u0641\u0644\u0647 \u0645\u062b\u0644 \u0623\u062c\u0631 \u0641\u0627\u0639\u0644\u0647\", \"Hadith - Muslim 1893\"),\n",
    "        (\"\u0644\u064a\u0633 \u0627\u0644\u0634\u062f\u064a\u062f \u0628\u0627\u0644\u0635\u0631\u0639\u0629 \u0625\u0646\u0645\u0627 \u0627\u0644\u0634\u062f\u064a\u062f \u0627\u0644\u0630\u064a \u064a\u0645\u0644\u0643 \u0646\u0641\u0633\u0647 \u0639\u0646\u062f \u0627\u0644\u063a\u0636\u0628\", \"Hadith - Bukhari 6114\"),\n",
    "        (\"\u0644\u0627 \u062a\u063a\u0636\u0628 \u0641\u0631\u062f\u062f \u0645\u0631\u0627\u0631\u0627 \u0642\u0627\u0644 \u0644\u0627 \u062a\u063a\u0636\u0628\", \"Hadith - Bukhari 6116\"),\n",
    "        (\"\u0625\u0646 \u0627\u0644\u063a\u0636\u0628 \u0645\u0646 \u0627\u0644\u0634\u064a\u0637\u0627\u0646 \u0648\u0625\u0646 \u0627\u0644\u0634\u064a\u0637\u0627\u0646 \u062e\u0644\u0642 \u0645\u0646 \u0627\u0644\u0646\u0627\u0631\", \"Hadith - Abu Dawud 4784\"),\n",
    "        (\"\u0648\u0625\u0646\u0645\u0627 \u062a\u0637\u0641\u0623 \u0627\u0644\u0646\u0627\u0631 \u0628\u0627\u0644\u0645\u0627\u0621 \u0641\u0625\u0630\u0627 \u063a\u0636\u0628 \u0623\u062d\u062f\u0643\u0645 \u0641\u0644\u064a\u062a\u0648\u0636\u0623\", \"Hadith - Abu Dawud 4784\"),\n",
    "        (\"\u0644\u0627 \u064a\u062d\u0644 \u0644\u0645\u0633\u0644\u0645 \u0623\u0646 \u064a\u0647\u062c\u0631 \u0623\u062e\u0627\u0647 \u0641\u0648\u0642 \u062b\u0644\u0627\u062b \u0644\u064a\u0627\u0644\", \"Hadith - Bukhari 6077\"),\n",
    "        (\"\u064a\u0644\u062a\u0642\u064a\u0627\u0646 \u0641\u064a\u0639\u0631\u0636 \u0647\u0630\u0627 \u0648\u064a\u0639\u0631\u0636 \u0647\u0630\u0627 \u0648\u062e\u064a\u0631\u0647\u0645\u0627 \u0627\u0644\u0630\u064a \u064a\u0628\u062f\u0623 \u0628\u0627\u0644\u0633\u0644\u0627\u0645\", \"Hadith - Bukhari 6077\"),\n",
    "        (\"\u0623\u0641\u0634\u0648\u0627 \u0627\u0644\u0633\u0644\u0627\u0645 \u0628\u064a\u0646\u0643\u0645\", \"Hadith - Muslim 54\"),\n",
    "        (\"\u0648\u0627\u0644\u0630\u064a \u0646\u0641\u0633\u064a \u0628\u064a\u062f\u0647 \u0644\u0627 \u062a\u062f\u062e\u0644\u0648\u0627 \u0627\u0644\u062c\u0646\u0629 \u062d\u062a\u0649 \u062a\u0624\u0645\u0646\u0648\u0627\", \"Hadith - Muslim 54\"),\n",
    "        (\"\u0648\u0644\u0627 \u062a\u0624\u0645\u0646\u0648\u0627 \u062d\u062a\u0649 \u062a\u062d\u0627\u0628\u0648\u0627 \u0623\u0648\u0644\u0627 \u0623\u062f\u0644\u0643\u0645 \u0639\u0644\u0649 \u0634\u064a\u0621 \u0625\u0630\u0627 \u0641\u0639\u0644\u062a\u0645\u0648\u0647 \u062a\u062d\u0627\u0628\u0628\u062a\u0645 \u0623\u0641\u0634\u0648\u0627 \u0627\u0644\u0633\u0644\u0627\u0645 \u0628\u064a\u0646\u0643\u0645\", \"Hadith - Muslim 54\"),\n",
    "        (\"\u0637\u0639\u0627\u0645 \u0627\u0644\u0627\u062b\u0646\u064a\u0646 \u0643\u0627\u0641\u064a \u0627\u0644\u062b\u0644\u0627\u062b\u0629 \u0648\u0637\u0639\u0627\u0645 \u0627\u0644\u062b\u0644\u0627\u062b\u0629 \u0643\u0627\u0641\u064a \u0627\u0644\u0623\u0631\u0628\u0639\u0629\", \"Hadith - Bukhari 5392\"),\n",
    "        (\"\u0645\u0627 \u0645\u0644\u0623 \u0622\u062f\u0645\u064a \u0648\u0639\u0627\u0621 \u0634\u0631\u0627 \u0645\u0646 \u0628\u0637\u0646\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"\u0628\u062d\u0633\u0628 \u0627\u0628\u0646 \u0622\u062f\u0645 \u0623\u0643\u0644\u0627\u062a \u064a\u0642\u0645\u0646 \u0635\u0644\u0628\u0647\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"\u0641\u0625\u0646 \u0643\u0627\u0646 \u0644\u0627 \u0645\u062d\u0627\u0644\u0629 \u0641\u062b\u0644\u062b \u0644\u0637\u0639\u0627\u0645\u0647 \u0648\u062b\u0644\u062b \u0644\u0634\u0631\u0627\u0628\u0647 \u0648\u062b\u0644\u062b \u0644\u0646\u0641\u0633\u0647\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"\u0625\u0646 \u0627\u0644\u0644\u0647 \u0643\u062a\u0628 \u0627\u0644\u0625\u062d\u0633\u0627\u0646 \u0639\u0644\u0649 \u0643\u0644 \u0634\u064a\u0621\", \"Hadith - Muslim 1955\"),\n",
    "        (\"\u0641\u0625\u0630\u0627 \u0642\u062a\u0644\u062a\u0645 \u0641\u0623\u062d\u0633\u0646\u0648\u0627 \u0627\u0644\u0642\u062a\u0644\u0629 \u0648\u0625\u0630\u0627 \u0630\u0628\u062d\u062a\u0645 \u0641\u0623\u062d\u0633\u0646\u0648\u0627 \u0627\u0644\u0630\u0628\u062d\", \"Hadith - Muslim 1955\"),\n",
    "        (\"\u0648\u0644\u064a\u062d\u062f \u0623\u062d\u062f\u0643\u0645 \u0634\u0641\u0631\u062a\u0647 \u0648\u0644\u064a\u0631\u062d \u0630\u0628\u064a\u062d\u062a\u0647\", \"Hadith - Muslim 1955\"),\n",
    "        (\"\u0639\u0630\u0628\u062a \u0627\u0645\u0631\u0623\u0629 \u0641\u064a \u0647\u0631\u0629 \u0633\u062c\u0646\u062a\u0647\u0627 \u062d\u062a\u0649 \u0645\u0627\u062a\u062a\", \"Hadith - Bukhari 3318\"),\n",
    "        (\"\u0641\u0644\u0627 \u0647\u064a \u0623\u0637\u0639\u0645\u062a\u0647\u0627 \u0648\u0644\u0627 \u0633\u0642\u062a\u0647\u0627 \u0625\u0630 \u062d\u0628\u0633\u062a\u0647\u0627 \u0648\u0644\u0627 \u0647\u064a \u062a\u0631\u0643\u062a\u0647\u0627 \u062a\u0623\u0643\u0644 \u0645\u0646 \u062e\u0634\u0627\u0634 \u0627\u0644\u0623\u0631\u0636\", \"Hadith - Bukhari 3318\"),\n",
    "        (\"\u0628\u064a\u0646\u0645\u0627 \u0631\u062c\u0644 \u064a\u0645\u0634\u064a \u0628\u0637\u0631\u064a\u0642 \u0627\u0634\u062a\u062f \u0639\u0644\u064a\u0647 \u0627\u0644\u0639\u0637\u0634 \u0641\u0648\u062c\u062f \u0628\u0626\u0631\u0627 \u0641\u0646\u0632\u0644 \u0641\u064a\u0647\u0627 \u0641\u0634\u0631\u0628\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u062b\u0645 \u062e\u0631\u062c \u0641\u0625\u0630\u0627 \u0643\u0644\u0628 \u064a\u0644\u0647\u062b \u064a\u0623\u0643\u0644 \u0627\u0644\u062b\u0631\u0649 \u0645\u0646 \u0627\u0644\u0639\u0637\u0634\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u0641\u0642\u0627\u0644 \u0644\u0642\u062f \u0628\u0644\u063a \u0647\u0630\u0627 \u0627\u0644\u0643\u0644\u0628 \u0645\u0646 \u0627\u0644\u0639\u0637\u0634 \u0645\u062b\u0644 \u0627\u0644\u0630\u064a \u0643\u0627\u0646 \u0628\u0644\u063a \u0645\u0646\u064a\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u0641\u0646\u0632\u0644 \u0627\u0644\u0628\u0626\u0631 \u0641\u0645\u0644\u0623 \u062e\u0641\u0647 \u0645\u0627\u0621 \u062b\u0645 \u0623\u0645\u0633\u0643\u0647 \u0628\u0641\u064a\u0647 \u062d\u062a\u0649 \u0631\u0642\u064a \u0641\u0633\u0642\u0649 \u0627\u0644\u0643\u0644\u0628\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u0641\u0634\u0643\u0631 \u0627\u0644\u0644\u0647 \u0644\u0647 \u0641\u063a\u0641\u0631 \u0644\u0647\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"\u0641\u064a \u0643\u0644 \u0643\u0628\u062f \u0631\u0637\u0628\u0629 \u0623\u062c\u0631\", \"Hadith - Bukhari 2466\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(hadith):\n",
    "        islamic.append({\"id\": f\"hadith_{i}\", \"text\": text, \"source\": source, \"period\": \"HADITH\", \"century\": 9})\n",
    "    print(f\"    - Hadith: {len([x for x in islamic if 'hadith' in x['id']]):,} passages\")\n",
    "    \n",
    "    with open('data/raw/islamic/islamic_native.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(islamic, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"  Created {len(islamic)} Islamic passages\")\n",
    "    \n",
    "    # DEAR ABBY\n",
    "    print(\"\\n[4/4] Dear Abby...\")\n",
    "    abby_count = 0\n",
    "    if not os.path.exists('data/raw/dear_abby.csv') or os.path.getsize('data/raw/dear_abby.csv') < 10000:\n",
    "        # Check if in Drive (with retry for stale FUSE mount)\n",
    "        drive_abby_path = f'{SAVE_DIR}/dear_abby.csv'\n",
    "        found_in_drive = False\n",
    "\n",
    "        # First attempt\n",
    "        if os.path.exists(drive_abby_path):\n",
    "            found_in_drive = True\n",
    "        else:\n",
    "            # Retry after refreshing Drive mount (FUSE can be stale)\n",
    "            print(f\"  First check failed, refreshing Drive...\")\n",
    "            try:\n",
    "                _ = os.listdir(SAVE_DIR)  # Force FUSE refresh\n",
    "                import time\n",
    "                time.sleep(0.5)  # Brief pause for sync\n",
    "                if os.path.exists(drive_abby_path):\n",
    "                    found_in_drive = True\n",
    "                    print(f\"  Found after refresh!\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Drive refresh error: {e}\")\n",
    "\n",
    "        if found_in_drive:\n",
    "            shutil.copy(drive_abby_path, 'data/raw/dear_abby.csv')\n",
    "            print(f\"  Loaded from Drive: {drive_abby_path}\")\n",
    "        else:\n",
    "            print(f\"  Not found in Drive at: {drive_abby_path}\")\n",
    "            # Show what IS in the Drive folder\n",
    "            try:\n",
    "                contents = os.listdir(SAVE_DIR) if os.path.exists(SAVE_DIR) else []\n",
    "                print(f\"  Drive folder contents: {contents[:10]}\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\",\n",
    "                               \"thedevastator/20000-dear-abby-questions\",\n",
    "                               \"-p\", \"data/raw/\", \"--unzip\"], check=True, timeout=120)\n",
    "                print(\"  Downloaded from Kaggle\")\n",
    "            except:\n",
    "                print(\"  Kaggle failed - creating minimal fallback\")\n",
    "                fallback = [{\"question_only\": f\"Dear Abby, I have a problem {i}\", \"year\": 1990+i%30} for i in range(100)]\n",
    "                pd.DataFrame(fallback).to_csv('data/raw/dear_abby.csv', index=False)\n",
    "    else:\n",
    "        print(\"  Already exists\")\n",
    "    \n",
    "    # Count Dear Abby samples\n",
    "    try:\n",
    "        df = pd.read_csv('data/raw/dear_abby.csv')\n",
    "        abby_count = len([1 for _, row in df.iterrows() if str(row.get('question_only', '')) != 'nan' and 50 <= len(str(row.get('question_only', ''))) <= 2000])\n",
    "    except:\n",
    "        abby_count = 0\n",
    "    \n",
    "    # Warning for insufficient Dear Abby data\n",
    "    if abby_count < 1000:\n",
    "        print(\"\\n\" + \"!\"*60)\n",
    "        print(\"CRITICAL: Dear Abby corpus is too small!\")\n",
    "        print(\"The semitic_to_non_semitic split WILL FAIL without this data.\")\n",
    "        print(\"\\nTo fix:\")\n",
    "        print(\"1. Download from: kaggle.com/datasets/thedevastator/20000-dear-abby-questions\")\n",
    "        print(\"2. Upload dear_abby.csv to your Google Drive BIP_v10 folder\")\n",
    "        print(\"3. Set REFRESH_DATA_FROM_SOURCE = True and rerun\")\n",
    "        print(\"!\"*60 + \"\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Downloads complete\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. Patterns + Normalization { display-mode: \"form\" }\n",
    "#@markdown Complete native patterns for moral concepts in 5 languages\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from enum import Enum, auto\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEXT NORMALIZATION & PATTERNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== TEXT NORMALIZATION =====\n",
    "def normalize_hebrew(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'[\\u0591-\\u05C7]', '', text)  # Remove nikud\n",
    "    for final, regular in [('\\u05da','\\u05db'), ('\\u05dd','\\u05de'), ('\\u05df','\\u05e0'), ('\\u05e3','\\u05e4'), ('\\u05e5','\\u05e6')]:\n",
    "        text = text.replace(final, regular)\n",
    "    return text\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'[\\u064B-\\u065F]', '', text)  # Remove tashkeel\n",
    "    text = text.replace('\\u0640', '')  # Remove tatweel\n",
    "    for v in ['\\u0623', '\\u0625', '\\u0622', '\\u0671']:\n",
    "        text = text.replace(v, '\\u0627')\n",
    "    text = text.replace('\\u0629', '\\u0647').replace('\\u0649', '\\u064a')\n",
    "    return text\n",
    "\n",
    "def normalize_text(text, language):\n",
    "    if language in ['hebrew', 'aramaic']:\n",
    "        return normalize_hebrew(text)\n",
    "    elif language == 'arabic':\n",
    "        return normalize_arabic(text)\n",
    "    elif language == 'classical_chinese':\n",
    "        return unicodedata.normalize('NFKC', text)\n",
    "    else:\n",
    "        return unicodedata.normalize('NFKC', text.lower())\n",
    "\n",
    "# ===== BOND AND HOHFELD TYPES =====\n",
    "class BondType(Enum):\n",
    "    HARM_PREVENTION = auto()\n",
    "    RECIPROCITY = auto()\n",
    "    AUTONOMY = auto()\n",
    "    PROPERTY = auto()\n",
    "    FAMILY = auto()\n",
    "    AUTHORITY = auto()\n",
    "    CARE = auto()\n",
    "    FAIRNESS = auto()\n",
    "    CONTRACT = auto()\n",
    "    NONE = auto()\n",
    "\n",
    "class HohfeldState(Enum):\n",
    "    OBLIGATION = auto()\n",
    "    RIGHT = auto()\n",
    "    LIBERTY = auto()\n",
    "    NO_RIGHT = auto()\n",
    "\n",
    "# ===== COMPLETE BOND PATTERNS =====\n",
    "ALL_BOND_PATTERNS = {\n",
    "    'hebrew': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u05d4\\u05e8\\u05d2', r'\\u05e8\\u05e6\\u05d7', r'\\u05e0\\u05d6\\u05e7', r'\\u05d4\\u05db\\u05d4', r'\\u05d4\\u05e6\\u05d9\\u05dc', r'\\u05e9\\u05de\\u05e8', r'\\u05e4\\u05e7\\u05d5\\u05d7.\\u05e0\\u05e4\\u05e9'],\n",
    "        BondType.RECIPROCITY: [r'\\u05d2\\u05de\\u05d5\\u05dc', r'\\u05d4\\u05e9\\u05d9\\u05d1', r'\\u05e4\\u05e8\\u05e2', r'\\u05e0\\u05ea\\u05df.*\\u05e7\\u05d1\\u05dc', r'\\u05de\\u05d3\\u05d4.\\u05db\\u05e0\\u05d2\\u05d3'],\n",
    "        BondType.AUTONOMY: [r'\\u05d1\\u05d7\\u05e8', r'\\u05e8\\u05e6\\u05d5\\u05df', r'\\u05d7\\u05e4\\u05e9', r'\\u05e2\\u05e6\\u05de'],\n",
    "        BondType.PROPERTY: [r'\\u05e7\\u05e0\\u05d4', r'\\u05de\\u05db\\u05e8', r'\\u05d2\\u05d6\\u05dc', r'\\u05d2\\u05e0\\u05d1', r'\\u05de\\u05de\\u05d5\\u05df', r'\\u05e0\\u05db\\u05e1', r'\\u05d9\\u05e8\\u05e9'],\n",
    "        BondType.FAMILY: [r'\\u05d0\\u05d1', r'\\u05d0\\u05de', r'\\u05d1\\u05e0', r'\\u05db\\u05d1\\u05d3.*\\u05d0\\u05d1', r'\\u05db\\u05d1\\u05d3.*\\u05d0\\u05de', r'\\u05de\\u05e9\\u05e4\\u05d7\\u05d4', r'\\u05d0\\u05d7', r'\\u05d0\\u05d7\\u05d5\\u05ea'],\n",
    "        BondType.AUTHORITY: [r'\\u05de\\u05dc\\u05db', r'\\u05e9\\u05d5\\u05e4\\u05d8', r'\\u05e6\\u05d5\\u05d4', r'\\u05ea\\u05d5\\u05e8\\u05d4', r'\\u05de\\u05e6\\u05d5\\u05d4', r'\\u05d3\\u05d9\\u05df', r'\\u05d7\\u05e7'],\n",
    "        BondType.CARE: [r'\\u05d7\\u05e1\\u05d3', r'\\u05e8\\u05d7\\u05de', r'\\u05e2\\u05d6\\u05e8', r'\\u05ea\\u05de\\u05db', r'\\u05e6\\u05d3\\u05e7\\u05d4'],\n",
    "        BondType.FAIRNESS: [r'\\u05e6\\u05d3\\u05e7', r'\\u05de\\u05e9\\u05e4\\u05d8', r'\\u05d9\\u05e9\\u05e8', r'\\u05e9\\u05d5\\u05d4'],\n",
    "        BondType.CONTRACT: [r'\\u05d1\\u05e8\\u05d9\\u05ea', r'\\u05e0\\u05d3\\u05e8', r'\\u05e9\\u05d1\\u05d5\\u05e2', r'\\u05d4\\u05ea\\u05d7\\u05d9\\u05d1', r'\\u05e2\\u05e8\\u05d1'],\n",
    "    },\n",
    "    'aramaic': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u05e7\\u05d8\\u05dc', r'\\u05e0\\u05d6\\u05e7', r'\\u05d7\\u05d1\\u05dc', r'\\u05e9\\u05d6\\u05d9\\u05d1', r'\\u05e4\\u05e6\\u05d9'],\n",
    "        BondType.RECIPROCITY: [r'\\u05e4\\u05e8\\u05e2', r'\\u05e9\\u05dc\\u05de', r'\\u05d0\\u05d2\\u05e8'],\n",
    "        BondType.AUTONOMY: [r'\\u05e6\\u05d1\\u05d9', r'\\u05e8\\u05e2\\u05d5'],\n",
    "        BondType.PROPERTY: [r'\\u05d6\\u05d1\\u05e0', r'\\u05e7\\u05e0\\u05d4', r'\\u05d2\\u05d6\\u05dc', r'\\u05de\\u05de\\u05d5\\u05e0\\u05d0', r'\\u05e0\\u05db\\u05e1\\u05d9'],\n",
    "        BondType.FAMILY: [r'\\u05d0\\u05d1\\u05d0', r'\\u05d0\\u05de\\u05d0', r'\\u05d1\\u05e8\\u05d0', r'\\u05d1\\u05e8\\u05ea\\u05d0', r'\\u05d9\\u05e7\\u05e8', r'\\u05d0\\u05d7\\u05d0'],\n",
    "        BondType.AUTHORITY: [r'\\u05de\\u05dc\\u05db\\u05d0', r'\\u05d3\\u05d9\\u05e0\\u05d0', r'\\u05d3\\u05d9\\u05d9\\u05e0\\u05d0', r'\\u05e4\\u05e7\\u05d5\\u05d3\\u05d0', r'\\u05d0\\u05d5\\u05e8\\u05d9\\u05ea'],\n",
    "        BondType.CARE: [r'\\u05d7\\u05e1\\u05d3', r'\\u05e8\\u05d7\\u05de', r'\\u05e1\\u05e2\\u05d3'],\n",
    "        BondType.FAIRNESS: [r'\\u05d3\\u05d9\\u05e0\\u05d0', r'\\u05e7\\u05e9\\u05d5\\u05d8', r'\\u05ea\\u05e8\\u05d9\\u05e6'],\n",
    "        BondType.CONTRACT: [r'\\u05e7\\u05d9\\u05de\\u05d0', r'\\u05e9\\u05d1\\u05d5\\u05e2\\u05d4', r'\\u05e0\\u05d3\\u05e8\\u05d0', r'\\u05e2\\u05e8\\u05d1\\u05d0'],\n",
    "    },\n",
    "    'classical_chinese': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u6bba', r'\\u5bb3', r'\\u50b7', r'\\u6551', r'\\u8b77', r'\\u885b', r'\\u66b4'],\n",
    "        BondType.RECIPROCITY: [r'\\u5831', r'\\u9084', r'\\u511f', r'\\u8ced', r'\\u7b54'],\n",
    "        BondType.AUTONOMY: [r'\\u81ea', r'\\u7531', r'\\u4efb', r'\\u610f', r'\\u5fd7'],\n",
    "        BondType.PROPERTY: [r'\\u8ca1', r'\\u7269', r'\\u7522', r'\\u76dc', r'\\u7aca', r'\\u8ce3', r'\\u8cb7'],\n",
    "        BondType.FAMILY: [r'\\u5b5d', r'\\u7236', r'\\u6bcd', r'\\u89aa', r'\\u5b50', r'\\u5f1f', r'\\u5144', r'\\u5bb6'],\n",
    "        BondType.AUTHORITY: [r'\\u541b', r'\\u81e3', r'\\u738b', r'\\u547d', r'\\u4ee4', r'\\u6cd5', r'\\u6cbb'],\n",
    "        BondType.CARE: [r'\\u4ec1', r'\\u611b', r'\\u6148', r'\\u60e0', r'\\u6069', r'\\u6190'],\n",
    "        BondType.FAIRNESS: [r'\\u7fa9', r'\\u6b63', r'\\u516c', r'\\u5e73', r'\\u5747'],\n",
    "        BondType.CONTRACT: [r'\\u7d04', r'\\u76df', r'\\u8a93', r'\\u8afe', r'\\u4fe1'],\n",
    "    },\n",
    "    'arabic': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u0642\\u062a\\u0644', r'\\u0636\\u0631\\u0631', r'\\u0627\\u0630[\\u064a\\u0649]', r'\\u0638\\u0644\\u0645', r'\\u0627\\u0646\\u0642\\u0630', r'\\u062d\\u0641\\u0638', r'\\u0627\\u0645\\u0627\\u0646'],\n",
    "        BondType.RECIPROCITY: [r'\\u062c\\u0632\\u0627', r'\\u0631\\u062f', r'\\u0642\\u0635\\u0627\\u0635', r'\\u0645\\u062b\\u0644', r'\\u0639\\u0648\\u0636'],\n",
    "        BondType.AUTONOMY: [r'\\u062d\\u0631', r'\\u0627\\u0631\\u0627\\u062f\\u0629', r'\\u0627\\u062e\\u062a\\u064a\\u0627\\u0631', r'\\u0645\\u0634\\u064a\\u0626'],\n",
    "        BondType.PROPERTY: [r'\\u0645\\u0627\\u0644', r'\\u0645\\u0644\\u0643', r'\\u0633\\u0631\\u0642', r'\\u0628\\u064a\\u0639', r'\\u0634\\u0631\\u0627', r'\\u0645\\u064a\\u0631\\u0627\\u062b', r'\\u063a\\u0635\\u0628'],\n",
    "        BondType.FAMILY: [r'\\u0648\\u0627\\u0644\\u062f', r'\\u0627\\u0628\\u0648', r'\\u0627\\u0645', r'\\u0627\\u0628\\u0646', r'\\u0628\\u0646\\u062a', r'\\u0627\\u0647\\u0644', r'\\u0642\\u0631\\u0628[\\u064a\\u0649]', r'\\u0631\\u062d\\u0645'],\n",
    "        BondType.AUTHORITY: [r'\\u0637\\u0627\\u0639', r'\\u0627\\u0645\\u0631', r'\\u062d\\u0643\\u0645', r'\\u0633\\u0644\\u0637\\u0627\\u0646', r'\\u062e\\u0644\\u064a\\u0641', r'\\u0627\\u0645\\u0627\\u0645', r'\\u0634\\u0631\\u064a\\u0639'],\n",
    "        BondType.CARE: [r'\\u0631\\u062d\\u0645', r'\\u0627\\u062d\\u0633\\u0627\\u0646', r'\\u0639\\u0637\\u0641', r'\\u0635\\u062f\\u0642', r'\\u0632\\u0643\\u0627'],\n",
    "        BondType.FAIRNESS: [r'\\u0639\\u062f\\u0644', r'\\u0642\\u0633\\u0637', r'\\u062d\\u0642', r'\\u0627\\u0646\\u0635\\u0627\\u0641', r'\\u0633\\u0648[\\u064a\\u0649]'],\n",
    "        BondType.CONTRACT: [r'\\u0639\\u0647\\u062f', r'\\u0639\\u0642\\u062f', r'\\u0646\\u0630\\u0631', r'\\u064a\\u0645\\u064a\\u0646', r'\\u0648\\u0641\\u0627', r'\\u0627\\u0645\\u0627\\u0646'],\n",
    "    },\n",
    "    'english': {\n",
    "        BondType.HARM_PREVENTION: [r'\\bkill', r'\\bmurder', r'\\bharm', r'\\bhurt', r'\\bsave', r'\\bprotect', r'\\bviolence'],\n",
    "        BondType.RECIPROCITY: [r'\\breturn', r'\\brepay', r'\\bexchange', r'\\bgive.*back', r'\\breciproc'],\n",
    "        BondType.AUTONOMY: [r'\\bfree', r'\\bchoice', r'\\bchoose', r'\\bconsent', r'\\bautonomy', r'\\bright to'],\n",
    "        BondType.PROPERTY: [r'\\bsteal', r'\\btheft', r'\\bown', r'\\bproperty', r'\\bbelong', r'\\binherit'],\n",
    "        BondType.FAMILY: [r'\\bfather', r'\\bmother', r'\\bparent', r'\\bchild', r'\\bfamily', r'\\bhonor.*parent'],\n",
    "        BondType.AUTHORITY: [r'\\bobey', r'\\bcommand', r'\\bauthority', r'\\blaw', r'\\brule', r'\\bgovern'],\n",
    "        BondType.CARE: [r'\\bcare', r'\\bhelp', r'\\bkind', r'\\bcompassion', r'\\bcharity', r'\\bmercy'],\n",
    "        BondType.FAIRNESS: [r'\\bfair', r'\\bjust', r'\\bequal', r'\\bequity', r'\\bright\\b'],\n",
    "        BondType.CONTRACT: [r'\\bpromise', r'\\bcontract', r'\\bagreem', r'\\bvow', r'\\boath', r'\\bcommit'],\n",
    "    },\n",
    "}\n",
    "\n",
    "# ===== COMPLETE HOHFELD PATTERNS =====\n",
    "ALL_HOHFELD_PATTERNS = {\n",
    "    'hebrew': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u05d7\\u05d9\\u05d9\\u05d1', r'\\u05e6\\u05e8\\u05d9\\u05db', r'\\u05de\\u05d5\\u05db\\u05e8\\u05d7', r'\\u05de\\u05e6\\u05d5\\u05d5\\u05d4'],\n",
    "        HohfeldState.RIGHT: [r'\\u05d6\\u05db\\u05d5\\u05ea', r'\\u05e8\\u05e9\\u05d0\\u05d9', r'\\u05d6\\u05db\\u05d0\\u05d9', r'\\u05de\\u05d2\\u05d9\\u05e2'],\n",
    "        HohfeldState.LIBERTY: [r'\\u05de\\u05d5\\u05ea\\u05e8', r'\\u05e8\\u05e9\\u05d5\\u05ea', r'\\u05e4\\u05d8\\u05d5\\u05e8', r'\\u05d9\\u05db\\u05d5\\u05dc'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u05d0\\u05e1\\u05d5\\u05e8', r'\\u05d0\\u05d9\\u05e0\\u05d5 \\u05e8\\u05e9\\u05d0\\u05d9', r'\\u05d0\\u05d9\\u05df.*\\u05d6\\u05db\\u05d5\\u05ea'],\n",
    "    },\n",
    "    'aramaic': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u05d7\\u05d9\\u05d9\\u05d1', r'\\u05de\\u05d7\\u05d5\\u05d9\\u05d1', r'\\u05d1\\u05e2\\u05d9'],\n",
    "        HohfeldState.RIGHT: [r'\\u05d6\\u05db\\u05d5\\u05ea', r'\\u05e8\\u05e9\\u05d0\\u05d9', r'\\u05d6\\u05db\\u05d9'],\n",
    "        HohfeldState.LIBERTY: [r'\\u05e9\\u05e8\\u05d9', r'\\u05de\\u05d5\\u05ea\\u05e8', r'\\u05e4\\u05d8\\u05d5\\u05e8'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u05d0\\u05e1\\u05d5\\u05e8', r'\\u05dc\\u05d0.*\\u05e8\\u05e9\\u05d0\\u05d9'],\n",
    "    },\n",
    "    'classical_chinese': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u5fc5', r'\\u9808', r'\\u7576', r'\\u61c9', r'\\u5b9c'],\n",
    "        HohfeldState.RIGHT: [r'\\u53ef', r'\\u5f97', r'\\u6b0a', r'\\u5b9c'],\n",
    "        HohfeldState.LIBERTY: [r'\\u8a31', r'\\u4efb', r'\\u807d', r'\\u514d'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u4e0d\\u53ef', r'\\u52ff', r'\\u7981', r'\\u83ab', r'\\u975e'],\n",
    "    },\n",
    "    'arabic': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u064a\\u062c\\u0628', r'\\u0648\\u0627\\u062c\\u0628', r'\\u0641\\u0631\\u0636', r'\\u0644\\u0627\\u0632\\u0645', r'\\u0648\\u062c\\u0648\\u0628'],\n",
    "        HohfeldState.RIGHT: [r'\\u062d\\u0642', r'\\u064a\\u062d\\u0642', r'\\u062c\\u0627\\u0626\\u0632', r'\\u064a\\u062c\\u0648\\u0632'],\n",
    "        HohfeldState.LIBERTY: [r'\\u0645\\u0628\\u0627\\u062d', r'\\u062d\\u0644\\u0627\\u0644', r'\\u062c\\u0627\\u0626\\u0632', r'\\u0627\\u0628\\u0627\\u062d'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u062d\\u0631\\u0627\\u0645', r'\\u0645\\u062d\\u0631\\u0645', r'\\u0645\\u0645\\u0646\\u0648\\u0639', r'\\u0644\\u0627 \\u064a\\u062c\\u0648\\u0632', r'\\u0646\\u0647[\\u064a\\u0649]'],\n",
    "    },\n",
    "    'english': {\n",
    "        HohfeldState.OBLIGATION: [r'\\bmust\\b', r'\\bshall\\b', r'\\bobligat', r'\\bduty', r'\\brequir'],\n",
    "        HohfeldState.RIGHT: [r'\\bright\\b', r'\\bentitle', r'\\bdeserve', r'\\bclaim'],\n",
    "        HohfeldState.LIBERTY: [r'\\bmay\\b', r'\\bpermit', r'\\ballow', r'\\bfree to'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\bforbid', r'\\bprohibit', r'\\bmust not', r'\\bshall not'],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ===== CONTEXT MARKERS FOR GRAMMAR-AWARE EXTRACTION =====\n",
    "# These help distinguish \"thou shalt not kill\" from \"he killed\"\n",
    "CONTEXT_MARKERS = {\n",
    "    'hebrew': {\n",
    "        'negation': [r'\u05dc\u05d0', r'\u05d0\u05dc', r'\u05d0\u05d9\u05df', r'\u05d1\u05dc\u05d9', r'\u05d0\u05d9\u05e0'],\n",
    "        'obligation': [r'\u05d7\u05d9\u05d9\u05d1', r'\u05e6\u05e8\u05d9\u05da', r'\u05de\u05d5\u05db\u05e8\u05d7', r'\u05e6\u05d5\u05d5\u05d4'],\n",
    "        'prohibition': [r'\u05d0\u05e1\u05d5\u05e8', r'\u05d0\u05dc.*\u05ea'],\n",
    "        'permission': [r'\u05de\u05d5\u05ea\u05e8', r'\u05e8\u05e9\u05d0\u05d9', r'\u05e4\u05d8\u05d5\u05e8'],\n",
    "    },\n",
    "    'aramaic': {\n",
    "        'negation': [r'\u05dc\u05d0', r'\u05dc\u05d9\u05ea', r'\u05dc\u05d0\u05d5'],\n",
    "        'obligation': [r'\u05d7\u05d9\u05d9\u05d1', r'\u05d1\u05e2\u05d9'],\n",
    "        'prohibition': [r'\u05d0\u05e1\u05d5\u05e8'],\n",
    "        'permission': [r'\u05e9\u05e8\u05d9', r'\u05de\u05d5\u05ea\u05e8'],\n",
    "    },\n",
    "    'classical_chinese': {\n",
    "        'negation': [r'\u4e0d', r'\u975e', r'\u7121', r'\u672a', r'\u6bcb'],\n",
    "        'obligation': [r'\u5fc5', r'\u7576', r'\u9808', r'\u61c9', r'\u5b9c'],\n",
    "        'prohibition': [r'\u52ff', r'\u7981', r'\u83ab', r'\u4e0d\u53ef'],\n",
    "        'permission': [r'\u53ef', r'\u5f97', r'\u8a31'],\n",
    "    },\n",
    "    'arabic': {\n",
    "        'negation': [r'\u0644\u0627', r'\u0645\u0627', r'\u0644\u064a\u0633', r'\u0644\u0645', r'\u063a\u064a\u0631'],\n",
    "        'obligation': [r'\u064a\u062c\u0628', r'\u0648\u0627\u062c\u0628', r'\u0641\u0631\u0636', r'\u0639\u0644\u064a\u0647'],\n",
    "        'prohibition': [r'\u062d\u0631\u0627\u0645', r'\u0645\u062d\u0631\u0645', r'\u0644\u0627 \u064a\u062c\u0648\u0632', r'\u0646\u0647\u0649'],\n",
    "        'permission': [r'\u062d\u0644\u0627\u0644', r'\u0645\u0628\u0627\u062d', r'\u062c\u0627\u0626\u0632'],\n",
    "    },\n",
    "    'english': {\n",
    "        'negation': [r'\bnot\b', r'\bno\b', r'\bnever\b', r'\bneither\b', r\"n't\b\"],\n",
    "        'obligation': [r'\bmust\b', r'\bshall\b', r'\bshould\b', r'\bought\b', r'\brequired\b'],\n",
    "        'prohibition': [r'\bforbid', r'\bprohibit', r'\bmust not\b', r'\bshall not\b', r\"\bdon't\b\"],\n",
    "        'permission': [r'\bmay\b', r'\bcan\b', r'\ballowed\b', r'\bpermit'],\n",
    "    },\n",
    "}\n",
    "\n",
    "def detect_context(text, language, match_pos, window=30):\n",
    "    \"\"\"\n",
    "    Detect grammatical context around a pattern match.\n",
    "    Returns: ('prescriptive'/'descriptive'/'unknown', marker_type or None)\n",
    "    \"\"\"\n",
    "    markers = CONTEXT_MARKERS.get(language, {})\n",
    "    if not markers:\n",
    "        return 'unknown', None\n",
    "    \n",
    "    start = max(0, match_pos - window)\n",
    "    end = min(len(text), match_pos + window)\n",
    "    window_text = text[start:end]\n",
    "    \n",
    "    # Check for deontic markers (prescriptive = moral statement)\n",
    "    for marker_type in ['prohibition', 'obligation', 'permission']:\n",
    "        for pattern in markers.get(marker_type, []):\n",
    "            if re.search(pattern, window_text):\n",
    "                return 'prescriptive', marker_type\n",
    "    \n",
    "    # Check for simple negation (may be descriptive)\n",
    "    for pattern in markers.get('negation', []):\n",
    "        if re.search(pattern, window_text):\n",
    "            return 'descriptive', 'negated'\n",
    "    \n",
    "    return 'descriptive', None\n",
    "\n",
    "print(\"\\nContext markers defined for grammar-aware extraction\")\n",
    "print(\"  Detects: negation, obligation, prohibition, permission\")\n",
    "\n",
    "print(\"Patterns defined for 5 languages:\")\n",
    "for lang in ALL_BOND_PATTERNS:\n",
    "    n = sum(len(p) for p in ALL_BOND_PATTERNS[lang].values())\n",
    "    print(f\"  {lang}: {n} bond patterns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Parallel Download + Stream Processing { display-mode: \"form\" }\n#@markdown Loads ALL corpora including expanded Arabic, Chinese, and Western classics\n\nimport json\nimport re\nimport random\nimport gc\nimport shutil\nimport requests\nimport time\nimport threading\nfrom queue import Queue\nfrom concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\n\n# Thread-safe queue for passages\npassage_queue = Queue(maxsize=100000)\ndownload_complete = threading.Event()\ncorpus_stats = defaultdict(int)\nstats_lock = threading.Lock()\n\ndef update_stats(lang, count):\n    with stats_lock:\n        corpus_stats[lang] += count\n        total = sum(corpus_stats.values())\n        if total % 1000 == 0:\n            print(\".\", end=\"\", flush=True)\n\n# Check if we should skip processing (data loaded from Drive)\n# Check if we should use cached data or download fresh\nSKIP_PROCESSING = LOAD_FROM_DRIVE  # Re-evaluate based on current settings\n\n# Minimum thresholds for balanced experiments\nMIN_CORPUS_SIZE = {\n    'english': 100000,       # Need 100K+ for Modern\u2192Ancient direction\n    'classical_chinese': 50000,  # Need for chinese splits\n    'hebrew': 10000,\n    'aramaic': 5000,\n    'arabic': 5000,\n}\n\n# Available augmentation datasets by language\nAUGMENTATION_DATASETS = {\n    'english': [\n        ('hendrycks/ethics', 'ETHICS'),           # ~130K moral scenarios\n        ('allenai/social_chem_101', 'SocialChem'), # ~292K social norms\n    ],\n    'classical_chinese': [\n        ('wikisource_zh_classical', 'WikisourceZH'),  # If available\n    ],\n}\n\nif SKIP_PROCESSING:\n    print(\"=\"*60)\n    print(\"USING CACHED DATA - Run with REFRESH_DATA_FROM_SOURCE=True to use v10.4 loaders\")\n    print(\"=\"*60)\n\n    # Count passages by language\n    by_lang = defaultdict(int)\n    with open('data/processed/passages.jsonl', 'r', encoding='utf-8') as f:\n        for line in f:\n            p = json.loads(line)\n            by_lang[p['language']] += 1\n\n    print(\"\\nPassages by language:\")\n    for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n        print(f\"  {lang}: {cnt:,}\")\n\n    n_passages = sum(by_lang.values())\n    print(f\"\\nTotal: {n_passages:,} passages\")\n\n    # Validate corpus sizes and identify what needs augmentation\n    print(\"\\nCorpus adequacy check:\")\n    languages_to_augment = []\n    for lang, min_size in MIN_CORPUS_SIZE.items():\n        actual = by_lang.get(lang, 0)\n        status = \"OK\" if actual >= min_size else \"NEED MORE\"\n        print(f\"  {lang}: {actual:,} / {min_size:,} - {status}\")\n        if actual < min_size and lang in AUGMENTATION_DATASETS:\n            languages_to_augment.append((lang, min_size - actual))\n\n    # Augment any under-represented languages that have available datasets\n    if languages_to_augment:\n        print(f\"\\n\" + \"=\"*60)\n        print(f\"AUGMENTING UNDER-REPRESENTED CORPORA\")\n        print(f\"=\"*60)\n        print(f\"Languages to augment: {[l for l, _ in languages_to_augment]}\")\n\n        # Load existing passages\n        all_passages = []\n        with open('data/processed/passages.jsonl', 'r', encoding='utf-8') as f:\n            for line in f:\n                all_passages.append(json.loads(line))\n\n        # Normalize field names\n        for p in all_passages:\n            if 'lang' not in p and 'language' in p:\n                p['lang'] = p['language']\n            if 'period' not in p and 'time_period' in p:\n                p['period'] = p['time_period']\n\n        print(f\"Loaded {len(all_passages):,} existing passages\")\n\n        from datasets import load_dataset\n\n        for lang, needed in languages_to_augment:\n            lang_count = by_lang.get(lang, 0)\n            print(f\"\\n--- Augmenting {lang} (need {needed:,} more) ---\")\n\n            for dataset_name, short_name in AUGMENTATION_DATASETS.get(lang, []):\n                if lang_count >= MIN_CORPUS_SIZE[lang]:\n                    break\n\n                print(f\"  Loading {short_name}...\")\n                try:\n                    if dataset_name == 'hendrycks/ethics':\n                        # ETHICS has multiple categories\n                        categories = ['commonsense', 'deontology', 'justice', 'utilitarianism', 'virtue']\n                        for cat in categories:\n                            if lang_count >= MIN_CORPUS_SIZE[lang]:\n                                break\n                            try:\n                                ds = load_dataset(dataset_name, cat, split='train', trust_remote_code=True)\n                                cat_count = 0\n                                for item in ds:\n                                    if lang_count >= MIN_CORPUS_SIZE[lang]:\n                                        break\n                                    if cat == 'commonsense':\n                                        text = item.get('input', '')\n                                    elif cat == 'justice':\n                                        text = item.get('scenario', '')\n                                    elif cat == 'deontology':\n                                        text = item.get('scenario', '') + ' ' + item.get('excuse', '')\n                                    elif cat == 'virtue':\n                                        text = item.get('scenario', '')\n                                    else:\n                                        text = str(item.get('baseline', '')) + ' vs ' + str(item.get('less_pleasant', ''))\n\n                                    if text and len(text) > 30:\n                                        all_passages.append({\n                                            'id': f\"ethics_{cat}_{len(all_passages)}\",\n                                            'text': text[:1000],\n                                            'lang': lang,\n                                            'language': lang,\n                                            'source': f'ETHICS_{cat}',\n                                            'period': 'MODERN',\n                                            'time_period': 'MODERN'\n                                        })\n                                        lang_count += 1\n                                        cat_count += 1\n                                print(f\"    {cat}: +{cat_count:,}\")\n                            except Exception as e:\n                                print(f\"    {cat} error: {e}\")\n\n                    elif dataset_name == 'allenai/social_chem_101':\n                        ds = load_dataset(dataset_name, split='train', trust_remote_code=True)\n                        sc_count = 0\n                        for item in ds:\n                            if lang_count >= MIN_CORPUS_SIZE[lang]:\n                                break\n                            action = item.get('action', '')\n                            situation = item.get('situation', '')\n                            rot = item.get('rot', '')\n\n                            if rot and len(rot) > 20:\n                                text = f\"{situation} {action}\".strip() if situation else action\n                                text = f\"{text}. {rot}\" if text else rot\n\n                                all_passages.append({\n                                    'id': f\"socialchem_{len(all_passages)}\",\n                                    'text': text[:1000],\n                                    'lang': lang,\n                                    'language': lang,\n                                    'source': 'Social_Chemistry_101',\n                                    'period': 'MODERN',\n                                    'time_period': 'MODERN'\n                                })\n                                lang_count += 1\n                                sc_count += 1\n                        print(f\"    Social Chemistry: +{sc_count:,}\")\n\n                    else:\n                        # Generic HuggingFace dataset\n                        try:\n                            ds = load_dataset(dataset_name, split='train', trust_remote_code=True)\n                            gen_count = 0\n                            for item in ds:\n                                if lang_count >= MIN_CORPUS_SIZE[lang]:\n                                    break\n                                text = item.get('text', '') or item.get('content', '') or str(item)\n                                if text and len(text) > 50:\n                                    all_passages.append({\n                                        'id': f\"{short_name.lower()}_{len(all_passages)}\",\n                                        'text': text[:1000],\n                                        'lang': lang,\n                                        'language': lang,\n                                        'source': short_name,\n                                        'period': 'MODERN',\n                                        'time_period': 'MODERN'\n                                    })\n                                    lang_count += 1\n                                    gen_count += 1\n                            print(f\"    {short_name}: +{gen_count:,}\")\n                        except Exception as e:\n                            print(f\"    {short_name} failed: {e}\")\n\n                except Exception as e:\n                    print(f\"    {short_name} failed: {e}\")\n\n            by_lang[lang] = lang_count\n            print(f\"  {lang} now: {lang_count:,}\")\n\n        # Extract bonds for new passages\n        print(\"\\nExtracting bonds for new passages...\")\n        new_bonds = []\n        new_sources = {'ETHICS_commonsense', 'ETHICS_deontology', 'ETHICS_justice',\n                       'ETHICS_utilitarianism', 'ETHICS_virtue', 'Social_Chemistry_101'}\n\n        for p in tqdm(all_passages, desc=\"Processing\"):\n            src = p.get('source', '')\n            if any(src.startswith(s.split('_')[0]) for s in new_sources) or src in new_sources:\n                text_lower = p['text'].lower()\n                if any(w in text_lower for w in ['wrong', 'bad', \"shouldn't\", 'immoral', 'rude', 'unethical']):\n                    bond_type = 'PROHIBITION'\n                elif any(w in text_lower for w in ['should', 'must', 'duty', 'obligat', 'need to']):\n                    bond_type = 'OBLIGATION'\n                elif any(w in text_lower for w in ['okay', 'fine', 'acceptable', 'can', 'may', 'allowed']):\n                    bond_type = 'PERMISSION'\n                else:\n                    bond_type = 'NEUTRAL'\n\n                new_bonds.append({\n                    'passage_id': p['id'],\n                    'bond_type': bond_type,\n                    'language': p.get('language', p.get('lang')),\n                    'time_period': p.get('time_period', p.get('period', 'MODERN')),\n                    'source': src,\n                    'text': p['text'][:500],\n                    'context': 'prescriptive',\n                    'confidence': 'high'\n                })\n\n        # Load existing bonds and merge\n        existing_bonds = []\n        with open('data/processed/bonds.jsonl', 'r', encoding='utf-8') as f:\n            for line in f:\n                existing_bonds.append(json.loads(line))\n\n        all_bonds = existing_bonds + new_bonds\n        print(f\"Total bonds: {len(all_bonds):,} ({len(new_bonds):,} new)\")\n\n        # Save updated passages\n        with open('data/processed/passages.jsonl', 'w', encoding='utf-8') as f:\n            for p in all_passages:\n                p_out = {\n                    'id': p['id'],\n                    'text': p['text'],\n                    'language': p.get('language', p.get('lang', 'english')),\n                    'source': p.get('source', ''),\n                    'time_period': p.get('time_period', p.get('period', 'MODERN'))\n                }\n                f.write(json.dumps(p_out, ensure_ascii=False) + '\\n')\n\n        # Save updated bonds\n        with open('data/processed/bonds.jsonl', 'w', encoding='utf-8') as f:\n            for b in all_bonds:\n                f.write(json.dumps(b, ensure_ascii=False) + '\\n')\n\n        print(\"Saved augmented data\")\n\n        # Copy to Drive\n        if USE_DRIVE_DATA:\n            try:\n                shutil.copy('data/processed/passages.jsonl', f'{SAVE_DIR}/passages.jsonl')\n                shutil.copy('data/processed/bonds.jsonl', f'{SAVE_DIR}/bonds.jsonl')\n                print(f\"Updated Drive cache: {SAVE_DIR}\")\n            except Exception as e:\n                print(f\"Drive update failed: {e}\")\n\n        # Final summary\n        print(f\"\\nFinal corpus sizes:\")\n        for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n            target = MIN_CORPUS_SIZE.get(lang, 0)\n            status = \"OK\" if cnt >= target else \"LOW\"\n            print(f\"  {lang}: {cnt:,} ({status})\")\n        n_passages = len(all_passages)\n\nelse:\n    print(\"=\"*60)\n    print(\"LOADING CORPORA\")\n    print(f\"GPU Tier: {GPU_TIER}\")\n    print(f\"Max per language: {MAX_PER_LANG:,}\")\n    print(\"=\"*60)\n\n    random.seed(42)\n    all_passages = []\n\n    # ===== PARALLEL PREFETCH MANAGER =====\n    from concurrent.futures import ThreadPoolExecutor, Future\n    import threading\n    \n    print(\"Starting parallel prefetch of remote corpora...\")\n    prefetch_executor = ThreadPoolExecutor(max_workers=12)\n    prefetch_results = {}  # url -> Future\n    \n    def prefetch_url(url, timeout=60):\n        \"\"\"Fetch URL content in background.\"\"\"\n        try:\n            resp = requests.get(url, timeout=timeout)\n            if resp.status_code == 200:\n                return resp.text\n        except Exception as e:\n            print(f\"    Prefetch failed for {url[:50]}...: {e}\")\n        return None\n    \n    # Queue all remote downloads\n    PREFETCH_URLS = [\n        # Gutenberg - Western Classics\n        \"https://www.gutenberg.org/cache/epub/1497/pg1497.txt\",   # Republic\n        \"https://www.gutenberg.org/cache/epub/1656/pg1656.txt\",   # Apology\n        \"https://www.gutenberg.org/cache/epub/1657/pg1657.txt\",   # Crito\n        \"https://www.gutenberg.org/cache/epub/1658/pg1658.txt\",   # Phaedo\n        \"https://www.gutenberg.org/cache/epub/3794/pg3794.txt\",   # Gorgias\n        \"https://www.gutenberg.org/cache/epub/1636/pg1636.txt\",   # Symposium\n        \"https://www.gutenberg.org/cache/epub/1726/pg1726.txt\",   # Meno\n        \"https://www.gutenberg.org/cache/epub/8438/pg8438.txt\",   # Nicomachean Ethics\n        \"https://www.gutenberg.org/cache/epub/6762/pg6762.txt\",   # Politics\n        \"https://www.gutenberg.org/cache/epub/2680/pg2680.txt\",   # Meditations\n        \"https://www.gutenberg.org/cache/epub/10661/pg10661.txt\", # Enchiridion\n        \"https://www.gutenberg.org/cache/epub/3042/pg3042.txt\",   # Discourses\n        \"https://www.gutenberg.org/cache/epub/14988/pg14988.txt\", # De Officiis\n        # MIT Classics fallback\n        \"https://classics.mit.edu/Aristotle/nicomachaen.mb.txt\",\n        \"https://classics.mit.edu/Plato/laws.mb.txt\",\n        # Bible Parallel Corpus\n        \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles/English.xml\",\n        \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles/Hebrew.xml\",\n        \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles/Arabic.xml\",\n        \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles/Chinese.xml\",\n    ]\n    \n    for url in PREFETCH_URLS:\n        prefetch_results[url] = prefetch_executor.submit(prefetch_url, url)\n    \n    print(f\"  Queued {len(PREFETCH_URLS)} URLs for background download\")\n    \n    def get_prefetched(url, timeout=30):\n        \"\"\"Get prefetched content, waiting if necessary.\"\"\"\n        if url in prefetch_results:\n            try:\n                return prefetch_results[url].result(timeout=timeout)\n            except Exception:\n                pass\n        # Fallback to direct fetch\n        return prefetch_url(url)\n    \n\n    # ===== SEFARIA (Hebrew/Aramaic) =====\n    print(\"\\nLoading Sefaria...\")\n    sefaria_path = Path('data/raw/Sefaria-Export/json')\n\n    CATEGORY_TO_PERIOD = {\n        'Tanakh': 'BIBLICAL', 'Torah': 'BIBLICAL', 'Prophets': 'BIBLICAL', 'Writings': 'BIBLICAL',\n        'Mishnah': 'TANNAITIC', 'Tosefta': 'TANNAITIC', 'Sifra': 'TANNAITIC', 'Sifrei': 'TANNAITIC',\n        'Talmud': 'TALMUDIC', 'Bavli': 'TALMUDIC', 'Yerushalmi': 'TALMUDIC',\n        'Midrash': 'MIDRASHIC', 'Midrash Rabbah': 'MIDRASHIC', 'Midrash Aggadah': 'MIDRASHIC',\n        'Halakhah': 'MEDIEVAL', 'Shulchan Arukh': 'MEDIEVAL', 'Mishneh Torah': 'MEDIEVAL',\n        'Musar': 'MODERN', 'Chasidut': 'MODERN', 'Modern': 'MODERN'\n    }\n\n    lang_counts = {'hebrew': 0, 'aramaic': 0}\n\n    if sefaria_path.exists():\n        for json_file in tqdm(list(sefaria_path.rglob('*.json'))[:5000], desc=\"Sefaria\"):\n            try:\n                with open(json_file, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n\n                if isinstance(data, dict) and 'text' in data:\n                    # Determine period from path\n                    path_parts = str(json_file.relative_to(sefaria_path)).split('/')\n                    period = 'CLASSICAL'\n                    for part in path_parts:\n                        if part in CATEGORY_TO_PERIOD:\n                            period = CATEGORY_TO_PERIOD[part]\n                            break\n\n                    # Determine language (heuristic: Talmud is primarily Aramaic)\n                    is_talmud = any(t in str(json_file) for t in ['Talmud', 'Bavli', 'Yerushalmi'])\n                    lang = 'aramaic' if is_talmud else 'hebrew'\n\n                    def extract_texts(obj, texts):\n                        if isinstance(obj, str) and len(obj) > 20:\n                            texts.append(obj)\n                        elif isinstance(obj, list):\n                            for item in obj:\n                                extract_texts(item, texts)\n\n                    texts = []\n                    extract_texts(data['text'], texts)\n\n                    for txt in texts[:50]:  # Limit per file\n                        if lang_counts[lang] < MAX_PER_LANG:\n                            all_passages.append({\n                                'id': f\"sefaria_{len(all_passages)}\",\n                                'text': txt,\n                                'lang': lang,\n                                'source': json_file.stem,\n                                'period': period\n                            })\n                            lang_counts[lang] += 1\n\n            except Exception as e:\n                continue\n    else:\n        print(\"  Sefaria not found - will download\")\n\n    print(f\"  Hebrew: {lang_counts['hebrew']:,}, Aramaic: {lang_counts['aramaic']:,}\")\n\n    # ===== CLASSICAL CHINESE: Disabled (CText API blocks Colab) =====\n    print(\"  Skipping CText API (blocked from Colab, using Wenyanwen instead)\")\n    chinese_count = 0  # Initialize counter\n\n    # ===== KAGGLE: Ancient Chinese Wenyanwen (132K texts, 552M chars) =====\n    if chinese_count < MAX_PER_LANG:\n        print(\"  Loading from Kaggle Wenyanwen dataset...\")\n        wenyan_zip_name = 'Ancient_Chinese_Text_(wenyanwen)_archive.zip'\n        wenyan_csv_name = 'cn_wenyan.csv'\n        wenyan_local_zip = Path(f'data/raw/{wenyan_zip_name}')\n        _drive_ok = 'USE_DRIVE_DATA' in dir() and USE_DRIVE_DATA and 'SAVE_DIR' in dir()\n        wenyan_drive_zip = Path(f'{SAVE_DIR}/{wenyan_zip_name}') if _drive_ok else None\n        wenyan_local_csv = Path(f'data/raw/{wenyan_csv_name}')\n        wenyan_drive_csv = Path(f'{SAVE_DIR}/{wenyan_csv_name}') if _drive_ok else None\n\n        # Find the CSV (extracted or in zip)\n        csv_path = None\n        if wenyan_local_csv.exists():\n            csv_path = wenyan_local_csv\n            print(\"    Found CSV locally\")\n        elif wenyan_drive_csv and wenyan_drive_csv.exists():\n            csv_path = wenyan_drive_csv\n            print(\"    Found CSV in Drive\")\n        else:\n            # Need to extract from zip\n            zip_path = None\n            if wenyan_local_zip.exists():\n                zip_path = wenyan_local_zip\n                print(\"    Found zip locally\")\n            elif wenyan_drive_zip and wenyan_drive_zip.exists():\n                zip_path = wenyan_drive_zip\n                print(\"    Found zip in Drive\")\n\n            if zip_path:\n                try:\n                    import zipfile\n                    print(\"    Extracting CSV from zip...\")\n                    with zipfile.ZipFile(zip_path, 'r') as z:\n                        z.extract(wenyan_csv_name, 'data/raw/')\n                    csv_path = wenyan_local_csv\n                    print(\"    Extracted!\")\n                except Exception as e:\n                    print(f\"    Extraction failed: {e}\")\n\n        # Load texts from CSV\n        wenyan_count = 0\n        if csv_path and csv_path.exists():\n            import csv\n            csv.field_size_limit(10000000)  # Some texts are very long\n            try:\n                with open(csv_path, 'r', encoding='utf-8', errors='ignore') as f:\n                    reader = csv.DictReader(f)\n                    for row in reader:\n                        if chinese_count >= MAX_PER_LANG:\n                            break\n                        text = row.get('text', '')\n                        title = row.get('title', '')\n                        # Split long texts into passages (max 2000 chars each)\n                        # Use paragraph breaks or every 1500 chars\n                        paragraphs = text.split('\\n')\n                        current_para = ''\n                        for para in paragraphs:\n                            para = para.strip()\n                            if not para:\n                                continue\n                            if len(current_para) + len(para) < 1500:\n                                current_para += para\n                            else:\n                                if len(current_para) > 50:\n                                    all_passages.append({\n                                        'id': f\"wenyan_{len(all_passages)}\",\n                                        'text': current_para,\n                                        'lang': 'classical_chinese',\n                                        'source': title.split('/')[0] if '/' in title else title,\n                                        'period': 'CONFUCIAN'\n                                    })\n                                    chinese_count += 1\n                                    wenyan_count += 1\n                                    if chinese_count >= MAX_PER_LANG:\n                                        break\n                                current_para = para\n                        # Don't forget last paragraph\n                        if current_para and len(current_para) > 50 and chinese_count < MAX_PER_LANG:\n                            all_passages.append({\n                                'id': f\"wenyan_{len(all_passages)}\",\n                                'text': current_para,\n                                'lang': 'classical_chinese',\n                                'source': title.split('/')[0] if '/' in title else title,\n                                'period': 'CONFUCIAN'\n                            })\n                            chinese_count += 1\n                            wenyan_count += 1\n                print(f\"    Added {wenyan_count:,} passages from Wenyanwen\")\n            except Exception as e:\n                print(f\"    Error loading Wenyanwen: {e}\")\n\n    print(f\"  Total Classical Chinese: {chinese_count:,}\")\n\n\n\n    # ===== ARABIC/ISLAMIC (Kaggle quran-nlp) =====\n    print(\"\\nLoading Arabic from Kaggle quran-nlp...\")\n\n    arabic_count = 0\n    kaggle_path = Path('data/raw/quran-nlp')\n\n    # Try to download from Kaggle\n    if not kaggle_path.exists() and REFRESH_DATA_FROM_SOURCE:\n        try:\n            import subprocess\n            import zipfile\n            subprocess.run(['pip', 'install', '-q', 'kaggle'], check=True)\n            subprocess.run([\n                'kaggle', 'datasets', 'download',\n                '-d', 'alizahidraja/quran-nlp',\n                '-p', 'data/raw'\n            ], check=True, timeout=300)\n\n            with zipfile.ZipFile('data/raw/quran-nlp.zip', 'r') as z:\n                z.extractall(kaggle_path)\n            print(\"  Downloaded from Kaggle!\")\n        except Exception as e:\n            print(f\"  Kaggle download failed: {e}\")\n\n    # Load if available\n    if kaggle_path.exists():\n        import pandas as pd\n\n        # Load Quran\n        quran_files = list(kaggle_path.rglob('*quran*.csv'))\n        for qf in quran_files:\n            if arabic_count >= MAX_PER_LANG:\n                break\n            try:\n                df = pd.read_csv(qf, nrows=MAX_PER_LANG - arabic_count)\n                for _, row in df.iterrows():\n                    text = str(row.get('arabic', row.get('text', row.get('Arabic', ''))))\n                    if text and len(text) > 10 and text != 'nan':\n                        all_passages.append({\n                            'id': f\"quran_{len(all_passages)}\",\n                            'text': text,\n                            'lang': 'arabic',\n                            'source': 'Quran',\n                            'period': 'CLASSICAL'\n                        })\n                        arabic_count += 1\n            except:\n                continue\n\n        # Load Hadith\n        hadith_files = list(kaggle_path.rglob('*hadith*.csv'))\n        for hf in hadith_files:\n            if arabic_count >= MAX_PER_LANG:\n                break\n            try:\n                df = pd.read_csv(hf, nrows=MAX_PER_LANG - arabic_count)\n                for _, row in df.iterrows():\n                    text = str(row.get('hadith', row.get('text', row.get('Arabic', ''))))\n                    if text and len(text) > 10 and text != 'nan':\n                        all_passages.append({\n                            'id': f\"hadith_{len(all_passages)}\",\n                            'text': text,\n                            'lang': 'arabic',\n                            'source': 'Hadith',\n                            'period': 'CLASSICAL'\n                        })\n                        arabic_count += 1\n            except:\n                continue\n    else:\n        # Try Tanzil.net (simple direct download)\n        print(\"  Trying Tanzil.net for Quran text...\")\n        try:\n            tanzil_url = \"https://tanzil.net/pub/download/index.php?quranType=uthmani&outType=txt-2&agree=true\"\n            resp = requests.get(tanzil_url, timeout=60)\n            if resp.status_code == 200:\n                lines = resp.text.strip().split('\\n')\n                for line in lines:\n                    if '|' in line and arabic_count < MAX_PER_LANG:\n                        parts = line.split('|')\n                        if len(parts) >= 3:\n                            text = parts[2].strip()\n                            if len(text) > 10:\n                                all_passages.append({\n                                    'id': f\"tanzil_{len(all_passages)}\",\n                                    'text': text,\n                                    'lang': 'arabic',\n                                    'source': 'Quran (Tanzil)',\n                                    'period': 'CLASSICAL'\n                                })\n                                arabic_count += 1\n                print(f\"    Loaded {arabic_count} verses from Tanzil\")\n        except Exception as e:\n            print(f\"    Tanzil failed: {e}\")\n\n        # Final fallback: expanded hardcoded corpus\n        if arabic_count < 100:\n            print(\"  Using expanded hardcoded Arabic corpus...\")\n        ARABIC_CORPUS = [\n            # Quran excerpts (moral/ethical content)\n            \"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u062a\u064f\u0644\u064f\u0648\u0627 \u0627\u0644\u0646\u064e\u0651\u0641\u0652\u0633\u064e \u0627\u0644\u064e\u0651\u062a\u0650\u064a \u062d\u064e\u0631\u064e\u0651\u0645\u064e \u0627\u0644\u0644\u064e\u0651\u0647\u064f \u0625\u0650\u0644\u064e\u0651\u0627 \u0628\u0650\u0627\u0644\u0652\u062d\u064e\u0642\u0650\u0651\",\n            \"\u0648\u064e\u0628\u0650\u0627\u0644\u0652\u0648\u064e\u0627\u0644\u0650\u062f\u064e\u064a\u0652\u0646\u0650 \u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u064b\u0627\",\n            \"\u0648\u064e\u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u0650 \u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u064e \u0643\u064e\u0627\u0646\u064e \u0645\u064e\u0633\u0652\u0626\u064f\u0648\u0644\u064b\u0627\",\n            \"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0644\u064e\u0651\u0647\u064e \u064a\u064e\u0623\u0652\u0645\u064f\u0631\u064f \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u062f\u0652\u0644\u0650 \u0648\u064e\u0627\u0644\u0652\u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u0650\",\n            \"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0628\u0652\u062e\u064e\u0633\u064f\u0648\u0627 \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u064e \u0623\u064e\u0634\u0652\u064a\u064e\u0627\u0621\u064e\u0647\u064f\u0645\u0652\",\n            \"\u0648\u064e\u0623\u064e\u0642\u0650\u064a\u0645\u064f\u0648\u0627 \u0627\u0644\u0652\u0648\u064e\u0632\u0652\u0646\u064e \u0628\u0650\u0627\u0644\u0652\u0642\u0650\u0633\u0652\u0637\u0650 \u0648\u064e\u0644\u064e\u0627 \u062a\u064f\u062e\u0652\u0633\u0650\u0631\u064f\u0648\u0627 \u0627\u0644\u0652\u0645\u0650\u064a\u0632\u064e\u0627\u0646\u064e\",\n            \"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0622\u0645\u064e\u0646\u064f\u0648\u0627 \u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064f\u0642\u064f\u0648\u062f\u0650\",\n            \"\u0648\u064e\u062a\u064e\u0639\u064e\u0627\u0648\u064e\u0646\u064f\u0648\u0627 \u0639\u064e\u0644\u064e\u0649 \u0627\u0644\u0652\u0628\u0650\u0631\u0650\u0651 \u0648\u064e\u0627\u0644\u062a\u064e\u0651\u0642\u0652\u0648\u064e\u0649\",\n            # ... more can be added\n        ]\n        for i, txt in enumerate(ARABIC_CORPUS):\n            all_passages.append({\n                'id': f\"arabic_{len(all_passages)}\",\n                'text': txt,\n                'lang': 'arabic',\n                'source': 'Quran/Hadith',\n                'period': 'CLASSICAL'\n            })\n            arabic_count += 1\n\n    print(f\"  Arabic: {arabic_count:,}\")\n\n    # ===== DEAR ABBY (English) =====\n    print(\"Loading Dear Abby...\")\n\n    english_count = 0\n    abby_path = Path('data/raw/dear_abby.csv')\n    print(f'  Local path exists: {abby_path.exists()}')\n\n    # Check Drive first\n    drive_abby = f'{SAVE_DIR}/dear_abby.csv'\n    print(f'  Drive path: {drive_abby}')\n    print(f'  Drive path exists: {os.path.exists(drive_abby)}')\n    if not abby_path.exists() and os.path.exists(drive_abby):\n        os.makedirs('data/raw', exist_ok=True)\n        shutil.copy(drive_abby, abby_path)\n        print(\"  Copied from Drive\")\n\n\n    if not abby_path.exists() and REFRESH_DATA_FROM_SOURCE:\n        try:\n            import subprocess\n            subprocess.run(['pip', 'install', '-q', 'kaggle'], check=True)\n            subprocess.run([\n                'kaggle', 'datasets', 'download',\n                '-d', 'thedevastator/20000-dear-abby-questions',\n                '-p', 'data/raw',\n                '-f', 'dear_abby.csv'\n            ], check=True, timeout=120)\n            print(\"  Downloaded from Kaggle!\")\n        except Exception as e:\n            print(f\"  Kaggle download failed: {e}\")\n\n    if abby_path.exists():\n        import pandas as pd\n        df = pd.read_csv(abby_path, nrows=MAX_PER_LANG)\n        print(f'  CSV columns: {list(df.columns)}')\n        print(f'  CSV rows: {len(df)}')\n        for _, row in df.iterrows():\n            question = str(row.get('question', ''))\n            answer = str(row.get('question_only', ''))\n            if len(answer) > 50:\n                all_passages.append({\n                    'id': f\"abby_{len(all_passages)}\",\n                    'text': answer,\n                    'lang': 'english',\n                    'source': 'Dear Abby',\n                    'period': 'DEAR_ABBY'\n                })\n                english_count += 1\n    else:\n        print(\"  Dear Abby not found\")\n\n    print(f\"  Dear Abby: {english_count:,}\")\n\n    # ===== WESTERN CLASSICS (Greek/Roman Philosophy) =====\n    print(\"\\nLoading Western Classics (parallel download)...\")\n    \n    from concurrent.futures import ThreadPoolExecutor, as_completed\n    \n    # Project Gutenberg texts (reliable, plain text)\n    GUTENBERG_TEXTS = [\n        # Plato - Ethics & Political Philosophy\n        (1497, 'Republic', 'Plato'),\n        (1656, 'Apology', 'Plato'),\n        (1657, 'Crito', 'Plato'),\n        (1658, 'Phaedo', 'Plato'),\n        (3794, 'Gorgias', 'Plato'),\n        (1636, 'Symposium', 'Plato'),\n        (1726, 'Meno', 'Plato'),\n        # Aristotle\n        (8438, 'Nicomachean Ethics', 'Aristotle'),\n        (6762, 'Politics', 'Aristotle'),\n        # Stoics\n        (2680, 'Meditations', 'Marcus Aurelius'),\n        (10661, 'Enchiridion', 'Epictetus'),\n        (3042, 'Discourses', 'Epictetus'),\n        # Cicero\n        (14988, 'De Officiis', 'Cicero'),\n    ]\n    \n    # MIT Classics fallback\n    MIT_TEXTS = [\n        ('https://classics.mit.edu/Aristotle/nicomachaen.mb.txt', 'Nicomachean Ethics', 'Aristotle'),\n        ('https://classics.mit.edu/Aristotle/politics.mb.txt', 'Politics', 'Aristotle'),\n        ('https://classics.mit.edu/Plato/republic.mb.txt', 'Republic', 'Plato'),\n        ('https://classics.mit.edu/Plato/laws.mb.txt', 'Laws', 'Plato'),\n        ('https://classics.mit.edu/Antoninus/meditations.mb.txt', 'Meditations', 'Marcus Aurelius'),\n        ('https://classics.mit.edu/Epictetus/epicench.mb.txt', 'Enchiridion', 'Epictetus'),\n        ('https://classics.mit.edu/Cicero/duties.mb.txt', 'De Officiis', 'Cicero'),\n    ]\n    \n    western_target = min(MAX_PER_LANG, 15000)\n    \n    def fetch_gutenberg(item):\n        \"\"\"Fetch a single Gutenberg text (uses prefetch if available).\"\"\"\n        gutenberg_id, title, author = item\n        try:\n            url = f\"https://www.gutenberg.org/cache/epub/{gutenberg_id}/pg{gutenberg_id}.txt\"\n            text = get_prefetched(url)\n            if text:\n                # Skip Gutenberg header/footer\n                for marker in ['*** START OF', '***START OF']:\n                    if marker in text:\n                        text = text.split(marker, 1)[-1]\n                        break\n                for marker in ['*** END OF', '***END OF', 'End of Project Gutenberg']:\n                    if marker in text:\n                        text = text.split(marker, 1)[0]\n                        break\n                \n                paragraphs = [p.strip() for p in text.split('\\n\\n') if len(p.strip()) > 100]\n                passages = []\n                for para in paragraphs:\n                    para = re.sub(r'\\s+', ' ', para).strip()\n                    if 50 < len(para) < 2000:\n                        passages.append({\n                            'text': para,\n                            'source': f\"{author}: {title}\",\n                            'author': author,\n                            'title': title,\n                        })\n                return (title, author, passages)\n        except Exception as e:\n            pass\n        return (title, author, [])\n    \n    def fetch_mit(item):\n        \"\"\"Fetch a single MIT Classics text (uses prefetch if available).\"\"\"\n        url, title, author = item\n        try:\n            text = get_prefetched(url)\n            if text:\n                paragraphs = [p.strip() for p in text.split('\\n\\n') if len(p.strip()) > 100]\n                passages = []\n                for para in paragraphs[:500]:\n                    para = re.sub(r'\\s+', ' ', para).strip()\n                    if 50 < len(para) < 2000:\n                        passages.append({\n                            'text': para,\n                            'source': f\"{author}: {title}\",\n                            'author': author,\n                            'title': title,\n                        })\n                return (title, author, passages)\n        except:\n            pass\n        return (title, author, [])\n    \n    western_passages = []\n    loaded_titles = set()\n    \n    # Parallel fetch from Gutenberg\n    print(\"  Fetching from Project Gutenberg (parallel)...\")\n    with ThreadPoolExecutor(max_workers=8) as executor:\n        futures = {executor.submit(fetch_gutenberg, item): item for item in GUTENBERG_TEXTS}\n        for future in as_completed(futures):\n            title, author, passages = future.result()\n            if passages and title not in loaded_titles:\n                western_passages.extend(passages)\n                loaded_titles.add(title)\n                print(f\"    {author}: {title} - {len(passages)} passages\")\n    \n    # Parallel fetch from MIT for any missing\n    missing_mit = [(url, t, a) for url, t, a in MIT_TEXTS if t not in loaded_titles]\n    if missing_mit:\n        print(\"  Fetching missing texts from MIT Classics...\")\n        with ThreadPoolExecutor(max_workers=4) as executor:\n            futures = {executor.submit(fetch_mit, item): item for item in missing_mit}\n            for future in as_completed(futures):\n                title, author, passages = future.result()\n                if passages and title not in loaded_titles:\n                    western_passages.extend(passages)\n                    loaded_titles.add(title)\n                    print(f\"    {author}: {title} - {len(passages)} passages (MIT)\")\n    \n    # Add to all_passages with proper IDs\n    western_count = 0\n    for p in western_passages:\n        if western_count >= western_target:\n            break\n        all_passages.append({\n            'id': f\"western_{len(all_passages)}\",\n            'text': p['text'],\n            'lang': 'english',\n            'source': p['source'],\n            'period': 'WESTERN_CLASSICAL',\n            'time_period': 'WESTERN_CLASSICAL'\n        })\n        western_count += 1\n    \n    print(f\"  Total Western Classics: {western_count:,}\")\n    # ===== UNIMORAL: Disabled (gated dataset requires auth) =====\n    print(\"  Skipping UniMoral (gated HuggingFace dataset)\")\n\n    # ===== UN PARALLEL CORPUS (HuggingFace streaming) =====\n    print(\"\\nLoading UN Corpus from HuggingFace (streaming)...\")\n    try:\n        from datasets import load_dataset\n\n        pairs = [('ar', 'en'), ('en', 'zh')]\n        un_count = 0\n        lang_map = {'ar': 'arabic', 'zh': 'classical_chinese', 'en': 'english'}\n\n        for src, tgt in pairs:\n            if un_count >= MAX_PER_LANG:\n                break\n            try:\n                config = f\"{src}-{tgt}\"\n                ds = load_dataset(\"Helsinki-NLP/un_pc\", config, split=\"train\", streaming=True)\n\n                pair_count = 0\n                for item in ds:\n                    if pair_count >= min(MAX_PER_LANG // 4, 5000):\n                        break\n\n                    translation = item.get('translation', {})\n                    for lang_code in [src, tgt]:\n                        text = translation.get(lang_code, '')\n                        if len(text) > 30 and lang_code in lang_map:\n                            all_passages.append({\n                                'id': f\"un_{len(all_passages)}\",\n                                'text': text,\n                                'lang': lang_map[lang_code],\n                                'source': 'UN Corpus',\n                                'period': 'MODERN'\n                            })\n                            pair_count += 1\n                            un_count += 1\n\n                print(f\"  UN {config}: {pair_count:,}\")\n            except Exception as e:\n                print(f\"  UN {config} error: {e}\")\n\n        print(f\"  UN Corpus total: {un_count:,}\")\n    except Exception as e:\n        print(f\"  UN Corpus error: {e}\")\n\n    # ===== BIBLE PARALLEL CORPUS (GitHub) =====\n    print(\"\\nLoading Bible Parallel Corpus...\")\n    try:\n        base_url = \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles\"\n        bible_files = [\n            ('Hebrew.xml', 'hebrew'),\n            ('Arabic.xml', 'arabic'),\n            ('Chinese.xml', 'classical_chinese'),\n        ]\n\n        bible_count = 0\n        for filename, lang in bible_files:\n            if bible_count >= MAX_PER_LANG * 3:\n                break\n            try:\n                url = f\"{base_url}/{filename}\"\n                text = get_prefetched(url)\n                if text:\n                    verses = re.findall(r'<seg[^>]*>([^<]+)</seg>', text)\n                    file_count = 0\n                    for verse in verses:\n                        if file_count >= MAX_PER_LANG:\n                            break\n                        verse = verse.strip()\n                        if len(verse) > 10:\n                            all_passages.append({\n                                'id': f\"bible_{len(all_passages)}\",\n                                'text': verse,\n                                'lang': lang,\n                                'source': 'Bible',\n                                'period': 'CLASSICAL'\n                            })\n                            file_count += 1\n                            bible_count += 1\n                    print(f\"  Bible {lang}: {file_count:,}\")\n            except Exception as e:\n                print(f\"  Bible {filename} error: {e}\")\n\n        print(f\"  Bible total: {bible_count:,}\")\n    except Exception as e:\n        print(f\"  Bible error: {e}\")\n\n    # Cleanup prefetch executor\n    print(\"\\nWaiting for any remaining prefetch tasks...\")\n    prefetch_executor.shutdown(wait=False)\n    \n    # ===== SUMMARY =====\n    print(f\"\\nTOTAL: {len(all_passages):,}\")\n\n    # Count by language\n    by_lang = defaultdict(int)\n    for p in all_passages:\n        by_lang[p['lang']] += 1\n    print(\"\\nBy language:\")\n    for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n        print(f\"  {lang}: {cnt:,}\")\n\n    # ===== EXTRACT BONDS =====\n    print(\"\\n\" + \"=\"*60)\n    print(\"EXTRACTING BONDS\")\n    print(\"=\"*60)\n\n    def extract_bond(text, language):\n        \"\"\"Extract bond type with context awareness.\"\"\"\n        tn = normalize_text(text, language)\n\n        for bt, pats in ALL_BOND_PATTERNS.get(language, {}).items():\n            for p in pats:\n                match = re.search(p, tn)\n                if match:\n                    # Check context around the match\n                    context, marker_type = detect_context(text, language, match.start())\n                    confidence = 0.9 if context == 'prescriptive' else 0.5\n                    return bt, context, confidence\n        return None, 'unknown', 0.5\n\n    bonds = []\n    for p in tqdm(all_passages, desc=\"Extracting bonds\"):\n        bt, ctx, conf = extract_bond(p['text'], p['lang'])\n        if bt:\n            bonds.append({\n                'passage_id': p['id'],\n                'bond_type': bt,\n                'language': p['lang'],\n                'time_period': p['period'],\n                'source': p['source'],\n                'text': p['text'][:500],\n                'context': ctx,\n                'confidence': conf\n            })\n\n    print(f\"\\nExtracted {len(bonds):,} bonds from {len(all_passages):,} passages\")\n\n    # Count by bond type\n    by_bond = defaultdict(int)\n    for b in bonds:\n        by_bond[b['bond_type']] += 1\n    print(\"\\nBy bond type:\")\n    for bt, cnt in sorted(by_bond.items(), key=lambda x: -x[1]):\n        print(f\"  {bt}: {cnt:,}\")\n\n    # Count by context\n    by_ctx = defaultdict(int)\n    for b in bonds:\n        by_ctx[b['context']] += 1\n    print(\"\\nBy context:\")\n    for ctx, cnt in sorted(by_ctx.items(), key=lambda x: -x[1]):\n        print(f\"  {ctx}: {cnt:,}\")\n\n    # ===== SAVE =====\n    print(\"\\n\" + \"=\"*60)\n    print(\"SAVING DATA\")\n    print(\"=\"*60)\n\n    # Save passages\n    with open('data/processed/passages.jsonl', 'w', encoding='utf-8') as f:\n        for p in all_passages:\n            # Normalize field names\n            p_out = {\n                'id': p['id'],\n                'text': p['text'],\n                'language': p['lang'],\n                'source': p['source'],\n                'time_period': p['period']\n            }\n            f.write(json.dumps(p_out, ensure_ascii=False) + '\\n')\n    print(f\"  Saved {len(all_passages):,} passages to data/processed/passages.jsonl\")\n\n    # Save bonds\n    with open('data/processed/bonds.jsonl', 'w', encoding='utf-8') as f:\n        for b in bonds:\n            b_out = {**b, 'bond_type': b['bond_type'].name if hasattr(b['bond_type'], 'name') else str(b['bond_type'])}\n            f.write(json.dumps(b_out, ensure_ascii=False) + chr(10))\n    print(f\"  Saved {len(bonds):,} bonds to data/processed/bonds.jsonl\")\n\n    # Copy to Drive if enabled\n    if USE_DRIVE_DATA and SAVE_DIR:\n        try:\n            os.makedirs(SAVE_DIR, exist_ok=True)\n            shutil.copy('data/processed/passages.jsonl', f'{SAVE_DIR}/passages.jsonl')\n            shutil.copy('data/processed/bonds.jsonl', f'{SAVE_DIR}/bonds.jsonl')\n            print(f\"  Copied to Drive: {SAVE_DIR}\")\n        except Exception as e:\n            print(f\"  Drive copy failed: {e}\")\n\n    gc.collect()\n    print(\"\\nDone!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. Generate Splits { display-mode: \"form\" }\n#@markdown Creates train/test splits for cross-lingual experiments\n\nimport json\nimport random\nimport shutil\nfrom collections import defaultdict\n\nprint(\"=\"*60)\nprint(\"GENERATING SPLITS\")\nprint(\"=\"*60)\n\n# Check if splits already exist from Drive\n# Check if splits are valid (IDs match current passages)\nsplits_valid = False\nif os.path.exists('data/splits/all_splits.json'):\n    try:\n        with open('data/splits/all_splits.json') as f:\n            cached_splits = json.load(f)\n        # Get sample of IDs from splits\n        sample_ids = set()\n        for split in cached_splits.values():\n            sample_ids.update(split['train_ids'][:100])\n            sample_ids.update(split['test_ids'][:100])\n        # Check if they exist in current passages\n        passage_ids = set()\n        with open('data/processed/passages.jsonl') as f:\n            for line in f:\n                p = json.loads(line)\n                passage_ids.add(p['id'])\n                if len(passage_ids) > 10000:\n                    break\n        matches = len(sample_ids & passage_ids)\n        splits_valid = matches > len(sample_ids) * 0.9  # 90% match\n        if not splits_valid:\n            print(f\"Splits invalid: only {matches}/{len(sample_ids)} IDs match current passages\")\n    except Exception as e:\n        print(f\"Error validating splits: {e}\")\n\nif splits_valid and not REFRESH_DATA_FROM_SOURCE:\n    print(\"\\nSplits already loaded from Drive\")\n    with open('data/splits/all_splits.json') as f:\n        all_splits = json.load(f)\n    for name, split in all_splits.items():\n        print(f\"  {name}: train={split['train_size']:,}, test={split['test_size']:,}\")\nelse:\n    random.seed(42)\n    \n    # Read passage metadata\n    passage_meta = []\n    with open('data/processed/passages.jsonl', 'r') as f:\n        for line in f:\n            p = json.loads(line)\n            passage_meta.append(p)\n    \n    print(f\"Total passages: {len(passage_meta):,}\")\n    \n    by_lang = defaultdict(list)\n    by_period = defaultdict(list)\n    for p in passage_meta:\n        by_lang[p['language']].append(p['id'])\n        by_period[p['time_period']].append(p['id'])\n    \n    print(\"\\nBy language:\")\n    for lang, ids in sorted(by_lang.items(), key=lambda x: -len(x[1])):\n        print(f\"  {lang}: {len(ids):,}\")\n    \n    print(\"\\nBy period:\")\n    for period, ids in sorted(by_period.items(), key=lambda x: -len(x[1])):\n        print(f\"  {period}: {len(ids):,}\")\n    \n    all_splits = {}\n    # ===== SPLIT 1: Hebrew -> Others =====\n    print(\"\\n\" + \"-\"*60)\n    print(\"SPLIT 1: HEBREW -> OTHERS\")\n    hebrew_ids = by_lang.get('hebrew', [])\n    other_ids = [p['id'] for p in passage_meta if p['language'] != 'hebrew']\n    random.shuffle(hebrew_ids)\n    random.shuffle(other_ids)\n    \n    all_splits['hebrew_to_others'] = {\n        'train_ids': hebrew_ids,\n        'test_ids': other_ids,\n        'train_size': len(hebrew_ids),\n        'test_size': len(other_ids),\n    }\n    print(f\"  Train (Hebrew): {len(hebrew_ids):,}\")\n    print(f\"  Test (Others): {len(other_ids):,}\")\n    \n    # ===== SPLIT 2: Semitic -> Non-Semitic =====\n    print(\"\\n\" + \"-\"*60)\n    print(\"SPLIT 2: SEMITIC -> NON-SEMITIC\")\n    semitic_ids = by_lang.get('hebrew', []) + by_lang.get('aramaic', []) + by_lang.get('arabic', [])\n    non_semitic_ids = by_lang.get('classical_chinese', []) + by_lang.get('english', [])\n    random.shuffle(semitic_ids)\n    random.shuffle(non_semitic_ids)\n    \n    all_splits['semitic_to_non_semitic'] = {\n        'train_ids': semitic_ids,\n        'test_ids': non_semitic_ids,\n        'train_size': len(semitic_ids),\n        'test_size': len(non_semitic_ids),\n    }\n    print(f\"  Train (Semitic): {len(semitic_ids):,}\")\n    print(f\"  Test (Non-Semitic): {len(non_semitic_ids):,}\")\n    \n    # ===== SPLIT 3: Ancient -> Modern =====\n    print(\"\\n\" + \"-\"*60)\n    print(\"SPLIT 3: ANCIENT -> MODERN\")\n    # Define modern periods explicitly, derive ancient dynamically\n    modern_periods = {'MODERN', 'DEAR_ABBY'}\n    all_periods = set(by_period.keys())\n    ancient_periods = all_periods - modern_periods\n\n    print(f\"  Ancient periods: {sorted(ancient_periods)}\")\n    print(f\"  Modern periods: {sorted(modern_periods)}\")\n\n    ancient_ids = [p['id'] for p in passage_meta if p['time_period'] in ancient_periods]\n    modern_ids = [p['id'] for p in passage_meta if p['time_period'] in modern_periods]\n    random.shuffle(ancient_ids)\n    random.shuffle(modern_ids)\n    \n    all_splits['ancient_to_modern'] = {\n        'train_ids': ancient_ids,\n        'test_ids': modern_ids,\n        'train_size': len(ancient_ids),\n        'test_size': len(modern_ids),\n    }\n    print(f\"  Train (Ancient): {len(ancient_ids):,}\")\n    print(f\"  Test (Modern): {len(modern_ids):,}\")\n    \n    # ===== SPLIT 4: Mixed Baseline =====\n    print(\"\\n\" + \"-\"*60)\n    print(\"SPLIT 4: MIXED BASELINE\")\n    all_ids = [p['id'] for p in passage_meta]\n    random.shuffle(all_ids)\n    split_idx = int(0.7 * len(all_ids))\n    \n    all_splits['mixed_baseline'] = {\n        'train_ids': all_ids[:split_idx],\n        'test_ids': all_ids[split_idx:],\n        'train_size': split_idx,\n        'test_size': len(all_ids) - split_idx,\n    }\n    print(f\"  Train: {split_idx:,}\")\n    print(f\"  Test: {len(all_ids) - split_idx:,}\")\n    \n    \n    # ===== SPLIT 5: Dear Abby -> Classical Chinese =====\n    print(\"\\n\" + \"-\"*60)\n    print(\"SPLIT 5: DEAR ABBY -> CHINESE\")\n    abby_ids = [p['id'] for p in passage_meta if p['time_period'] == 'DEAR_ABBY']\n    chinese_ids = [p['id'] for p in passage_meta if p['language'] == 'classical_chinese']\n    random.shuffle(abby_ids)\n    random.shuffle(chinese_ids)\n    \n    all_splits['abby_to_chinese'] = {\n        'train_ids': abby_ids,\n        'test_ids': chinese_ids,\n        'train_size': len(abby_ids),\n        'test_size': len(chinese_ids),\n    }\n    print(f\"  Train (Dear Abby): {len(abby_ids):,}\")\n    print(f\"  Test (Chinese): {len(chinese_ids):,}\")\n\n\n\n    # ===== SPLIT 6: Western Classical -> Eastern =====\n    print(\"\\n\" + \"-\"*60)\n    print(\"SPLIT 6: WESTERN CLASSICAL -> EASTERN\")\n    western_ids = [p['id'] for p in passage_meta if p['time_period'] == 'WESTERN_CLASSICAL']\n    eastern_ids = [p['id'] for p in passage_meta if p['language'] in ('classical_chinese', 'hebrew')]\n    random.shuffle(western_ids)\n    random.shuffle(eastern_ids)\n    \n    all_splits['western_to_eastern'] = {\n        'train_ids': western_ids,\n        'test_ids': eastern_ids,\n        'train_size': len(western_ids),\n        'test_size': len(eastern_ids),\n    }\n    print(f\"  Train (Western - Plato, Aristotle, Stoics): {len(western_ids):,}\")\n    print(f\"  Test (Eastern - Chinese, Hebrew): {len(eastern_ids):,}\")\n\n    # Save splits\n    with open('data/splits/all_splits.json', 'w') as f:\n        json.dump(all_splits, f, indent=2)\n    \n    # Save to Drive\n    shutil.copy('data/splits/all_splits.json', f'{SAVE_DIR}/all_splits.json')\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Splits saved to local and Drive\")\nprint(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. Model Architecture { display-mode: \"form\" }\n",
    "#@markdown BIP model with configurable backbone and adversarial heads\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Backbone: {BACKBONE} ({MODEL_NAME})\")\n",
    "print(f\"Hidden size: {BACKBONE_HIDDEN}\")\n",
    "\n",
    "# Index mappings\n",
    "BOND_TO_IDX = {bt.name: i for i, bt in enumerate(BondType)}\n",
    "IDX_TO_BOND = {i: bt.name for i, bt in enumerate(BondType)}\n",
    "LANG_TO_IDX = {'hebrew': 0, 'aramaic': 1, 'classical_chinese': 2, 'arabic': 3, 'english': 4}\n",
    "IDX_TO_LANG = {i: l for l, i in LANG_TO_IDX.items()}\n",
    "PERIOD_TO_IDX = {\n",
    "    'BIBLICAL': 0,\n",
    "    'TANNAITIC': 1,\n",
    "    'AMORAIC': 2,\n",
    "    'RISHONIM': 3,\n",
    "    'ACHRONIM': 4,\n",
    "    'CONFUCIAN': 5,\n",
    "    'DAOIST': 6,\n",
    "    'QURANIC': 7,\n",
    "    'HADITH': 8,\n",
    "    'DEAR_ABBY': 9,\n",
    "    'CLASSICAL': 10,      # Was missing\n",
    "    'MEDIEVAL': 11,       # Was missing\n",
    "    'MODERN': 12,         # Was missing\n",
    "    'WESTERN_CLASSICAL': 13,  # Fixed index (was 11, caused gap)\n",
    "}  # 14 periods total (0-13)\n",
    "IDX_TO_PERIOD = {i: p for p, i in PERIOD_TO_IDX.items()}\n",
    "HOHFELD_TO_IDX = {hs.name: i for i, hs in enumerate(HohfeldState)}\n",
    "IDX_TO_HOHFELD = {i: hs.name for i, hs in enumerate(HohfeldState)}\n",
    "CONTEXT_TO_IDX = {'prescriptive': 0, 'descriptive': 1, 'unknown': 2}\n",
    "IDX_TO_CONTEXT = {i: c for c, i in CONTEXT_TO_IDX.items()}\n",
    "def get_confidence_weight(conf):\n",
    "    \"\"\"Map confidence to sample weight. Handles both string ('high'/'medium'/'low') and numeric (0.0-1.0) values.\"\"\"\n",
    "    if isinstance(conf, str):\n",
    "        return {'high': 2.0, 'medium': 1.0, 'low': 0.5}.get(conf, 1.0)\n",
    "    elif isinstance(conf, (int, float)):\n",
    "        return 2.0 if conf >= 0.8 else 1.0\n",
    "    return 1.0\n",
    "\n",
    "class GradientReversalLayer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.alpha, None\n",
    "\n",
    "class BIPModel(nn.Module):\n",
    "    def __init__(self, model_name=None, hidden_size=None, z_dim=64):\n",
    "        super().__init__()\n",
    "        # Use global config if not specified\n",
    "        model_name = model_name or MODEL_NAME\n",
    "        hidden_size = hidden_size or BACKBONE_HIDDEN\n",
    "\n",
    "        print(f\"  Loading encoder: {model_name}\")\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Get actual hidden size from model config\n",
    "        actual_hidden = self.encoder.config.hidden_size\n",
    "        if actual_hidden != hidden_size:\n",
    "            print(f\"  Note: Using actual hidden size {actual_hidden}\")\n",
    "            hidden_size = actual_hidden\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Projection to z_bond space (scales with backbone size)\n",
    "        proj_hidden = min(512, hidden_size)\n",
    "        self.z_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_size, proj_hidden),\n",
    "            nn.LayerNorm(proj_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(proj_hidden, z_dim),\n",
    "        )\n",
    "\n",
    "        # Task heads\n",
    "        self.bond_head = nn.Linear(z_dim, len(BondType))\n",
    "        self.hohfeld_head = nn.Linear(z_dim, len(HohfeldState))\n",
    "\n",
    "        # Adversarial heads\n",
    "        self.language_head = nn.Linear(z_dim, len(LANG_TO_IDX))\n",
    "        self.period_head = nn.Linear(z_dim, len(PERIOD_TO_IDX))\n",
    "\n",
    "        # Context prediction head (auxiliary task)\n",
    "        self.context_head = nn.Linear(z_dim, len(CONTEXT_TO_IDX))\n",
    "\n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"  Total params: {total_params:,}\")\n",
    "        print(f\"  Trainable: {trainable_params:,}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, adv_lambda=1.0):\n",
    "        enc = self.encoder(input_ids, attention_mask)\n",
    "\n",
    "        # Handle different pooling strategies\n",
    "        if hasattr(enc, 'pooler_output') and enc.pooler_output is not None:\n",
    "            pooled = enc.pooler_output\n",
    "        else:\n",
    "            pooled = enc.last_hidden_state[:, 0]\n",
    "\n",
    "        z = self.z_proj(pooled)\n",
    "\n",
    "        # Bond prediction (main task)\n",
    "        bond_pred = self.bond_head(z)\n",
    "        hohfeld_pred = self.hohfeld_head(z)\n",
    "\n",
    "        # Adversarial predictions (gradient reversal)\n",
    "        z_rev = GradientReversalLayer.apply(z, adv_lambda)\n",
    "        language_pred = self.language_head(z_rev)\n",
    "        period_pred = self.period_head(z_rev)\n",
    "\n",
    "        return {\n",
    "            'bond_pred': bond_pred,\n",
    "            'hohfeld_pred': hohfeld_pred,\n",
    "            'language_pred': language_pred,\n",
    "            'period_pred': period_pred,\n",
    "            'context_pred': self.context_head(z),\n",
    "            'z': z,\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer for selected backbone\n",
    "print(f\"\\nLoading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "# Dataset with Hohfeld support\n",
    "class NativeDataset(Dataset):\n",
    "    def __init__(self, ids_set, passages_file, bonds_file, tokenizer, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = []\n",
    "\n",
    "        bonds_by_id = {}\n",
    "        with open(bonds_file) as fb:\n",
    "            for line in fb:\n",
    "                b = json.loads(line)\n",
    "                bonds_by_id[b['passage_id']] = b\n",
    "\n",
    "        with open(passages_file) as fp:\n",
    "            for line in tqdm(fp, desc=\"Loading\", unit=\"line\"):\n",
    "                p = json.loads(line)\n",
    "                if p['id'] in ids_set and p['id'] in bonds_by_id:\n",
    "                    b = bonds_by_id[p['id']]\n",
    "                    self.data.append({\n",
    "                        'text': p['text'][:1000],\n",
    "                        'language': p['language'],\n",
    "                        'period': p['time_period'],\n",
    "                        'bond': b.get('bond_type') or b.get('bonds', {}).get('primary_bond'),\n",
    "                        'hohfeld': None,\n",
    "                        'context': b.get('context') or b.get('bonds', {}).get('context', 'unknown'),\n",
    "                        'confidence': b.get('confidence') or b.get('bonds', {}).get('confidence', 'medium'),\n",
    "                    })\n",
    "        print(f\"  Loaded {len(self.data):,} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        enc = self.tokenizer(item['text'], truncation=True, max_length=self.max_len,\n",
    "                            padding='max_length', return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'bond_label': BOND_TO_IDX.get(item['bond'], 9),\n",
    "            'language_label': LANG_TO_IDX.get(item['language'], 4),\n",
    "            'period_label': PERIOD_TO_IDX.get(item['period'], 9),\n",
    "            'hohfeld_label': HOHFELD_TO_IDX.get(item['hohfeld'], 0) if item['hohfeld'] else 0,\n",
    "            'context_label': CONTEXT_TO_IDX.get(item['context'], 2),\n",
    "            'sample_weight': get_confidence_weight(item['confidence']),\n",
    "            'language': item['language'],\n",
    "            'context': item['context'],\n",
    "            'confidence': item['confidence'],\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
    "        'bond_labels': torch.tensor([x['bond_label'] for x in batch]),\n",
    "        'language_labels': torch.tensor([x['language_label'] for x in batch]),\n",
    "        'period_labels': torch.tensor([x['period_label'] for x in batch]),\n",
    "        'hohfeld_labels': torch.tensor([x['hohfeld_label'] for x in batch]),\n",
    "        'context_labels': torch.tensor([x['context_label'] for x in batch]),\n",
    "        'sample_weights': torch.tensor([x['sample_weight'] for x in batch], dtype=torch.float),\n",
    "        'languages': [x['language'] for x in batch],\n",
    "        'contexts': [x['context'] for x in batch],\n",
    "        'confidences': [x['confidence'] for x in batch],\n",
    "    }\n",
    "\n",
    "print(f\"\\nArchitecture ready for {BACKBONE}\")\n",
    "print(f\"  Bond classes: {len(BondType)}\")\n",
    "print(f\"  Languages: {len(LANG_TO_IDX)}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7. Train BIP Model { display-mode: \"form\" }\n",
    "#@markdown Training with tuned adversarial weights and hardware-optimized parameters\n",
    "\n",
    "# ===== SUPPRESS DATALOADER MULTIPROCESSING WARNINGS =====\n",
    "# These occur during garbage collection and bypass normal exception handling\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "\n",
    "# Method 1: Filter warnings\n",
    "warnings.filterwarnings('ignore', message='.*can only test a child process.*')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torch.utils.data')\n",
    "\n",
    "# Method 2: Suppress logging\n",
    "logging.getLogger('torch.utils.data.dataloader').setLevel(logging.CRITICAL)\n",
    "\n",
    "# Method 3: Redirect stderr during DataLoader cleanup (most effective)\n",
    "class StderrFilter(io.TextIOWrapper):\n",
    "    \"\"\"Filters out DataLoader multiprocessing cleanup messages from stderr\"\"\"\n",
    "    def __init__(self, original):\n",
    "        self.original = original\n",
    "        self.buffer_lines = []\n",
    "\n",
    "    def write(self, text):\n",
    "        # Filter out the specific error patterns\n",
    "        skip_patterns = [\n",
    "            'can only test a child process',\n",
    "            '_MultiProcessingDataLoaderIter.__del__',\n",
    "            '_shutdown_workers',\n",
    "            'Exception ignored in:',\n",
    "            'w.is_alive()',\n",
    "        ]\n",
    "        # Buffer multi-line error messages\n",
    "        if any(p in text for p in skip_patterns):\n",
    "            return len(text)  # Pretend we wrote it\n",
    "        # Also skip if it looks like part of a traceback for these errors\n",
    "        if text.strip().startswith('^') and len(text.strip()) < 80:\n",
    "            return len(text)\n",
    "        if text.strip().startswith('File \"/usr') and 'dataloader.py' in text:\n",
    "            return len(text)\n",
    "        if text.strip() == 'Traceback (most recent call last):':\n",
    "            self.buffer_lines = [text]\n",
    "            return len(text)\n",
    "        if self.buffer_lines:\n",
    "            self.buffer_lines.append(text)\n",
    "            # Check if this is the DataLoader error traceback\n",
    "            full_msg = ''.join(self.buffer_lines)\n",
    "            if any(p in full_msg for p in skip_patterns):\n",
    "                self.buffer_lines = []\n",
    "                return len(text)\n",
    "            # After 10 lines, flush if not the target error\n",
    "            if len(self.buffer_lines) > 10:\n",
    "                for line in self.buffer_lines:\n",
    "                    self.original.write(line)\n",
    "                self.buffer_lines = []\n",
    "        return self.original.write(text)\n",
    "\n",
    "    def flush(self):\n",
    "        if self.buffer_lines:\n",
    "            # Flush any remaining buffered content\n",
    "            for line in self.buffer_lines:\n",
    "                self.original.write(line)\n",
    "            self.buffer_lines = []\n",
    "        self.original.flush()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.original, name)\n",
    "\n",
    "# Install the stderr filter\n",
    "_original_stderr = sys.stderr\n",
    "sys.stderr = StderrFilter(_original_stderr)\n",
    "\n",
    "# Method 4: Patch the DataLoader cleanup function directly\n",
    "try:\n",
    "    import torch.utils.data.dataloader as dl_module\n",
    "    _original_del = dl_module._MultiProcessingDataLoaderIter.__del__\n",
    "\n",
    "    def _patched_del(self):\n",
    "        try:\n",
    "            _original_del(self)\n",
    "        except (AssertionError, AttributeError, RuntimeError):\n",
    "            pass  # Silently ignore cleanup errors\n",
    "\n",
    "    dl_module._MultiProcessingDataLoaderIter.__del__ = _patched_del\n",
    "except Exception:\n",
    "    pass  # If patching fails, the stderr filter will still work\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import gc\n",
    "\n",
    "#@markdown **Splits to train:**\n",
    "TRAIN_HEBREW_TO_OTHERS = True  #@param {type:\"boolean\"}\n",
    "TRAIN_SEMITIC_TO_NON_SEMITIC = True  #@param {type:\"boolean\"}\n",
    "TRAIN_ANCIENT_TO_MODERN = True  #@param {type:\"boolean\"}\n",
    "TRAIN_MIXED_BASELINE = True  #@param {type:\"boolean\"}\n",
    "TRAIN_ABBY_TO_CHINESE = True  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown **Hyperparameters:**\n",
    "LANG_WEIGHT = 0.1  #@param {type:\"number\"}\n",
    "PERIOD_WEIGHT = 0.05  #@param {type:\"number\"}\n",
    "N_EPOCHS = 10  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown **Context-Aware Training:**\n",
    "USE_CONFIDENCE_WEIGHTING = True  #@param {type:\"boolean\"}\n",
    "#@markdown Weight prescriptive (high confidence) examples 2x in loss\n",
    "\n",
    "USE_CONTEXT_AUXILIARY = True  #@param {type:\"boolean\"}\n",
    "#@markdown Add context prediction as auxiliary training target\n",
    "\n",
    "CONTEXT_LOSS_WEIGHT = 0.3  #@param {type:\"number\"}\n",
    "#@markdown Weight for context prediction loss\n",
    "\n",
    "STRICT_PRESCRIPTIVE_TEST = True  #@param {type:\"boolean\"}\n",
    "#@markdown Only evaluate on prescriptive examples (strict test)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING BIP MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSettings:\")\n",
    "print(f\"  Backbone:     {BACKBONE}\")\n",
    "print(f\"  GPU Tier:     {GPU_TIER}\")\n",
    "print(f\"  Batch size:   {BATCH_SIZE}\")\n",
    "print(f\"  Workers:      {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate: {LR:.2e}\")\n",
    "print(f\"  Adv weights:  lang={LANG_WEIGHT}, period={PERIOD_WEIGHT}\")\n",
    "print(\"(0.01 prevents loss explosion while maintaining invariance)\")\n",
    "print(f\"  Confidence weighting: {USE_CONFIDENCE_WEIGHTING}\")\n",
    "print(f\"  Context auxiliary: {USE_CONTEXT_AUXILIARY} (weight={CONTEXT_LOSS_WEIGHT})\")\n",
    "print(f\"  Strict prescriptive test: {STRICT_PRESCRIPTIVE_TEST}\")\n",
    "\n",
    "# tokenizer loaded in Cell 6 based on BACKBONE selection\n",
    "\n",
    "with open('data/splits/all_splits.json') as f:\n",
    "    all_splits = json.load(f)\n",
    "\n",
    "splits_to_train = []\n",
    "if TRAIN_HEBREW_TO_OTHERS: splits_to_train.append('hebrew_to_others')\n",
    "if TRAIN_SEMITIC_TO_NON_SEMITIC: splits_to_train.append('semitic_to_non_semitic')\n",
    "if TRAIN_ANCIENT_TO_MODERN: splits_to_train.append('ancient_to_modern')\n",
    "if TRAIN_MIXED_BASELINE: splits_to_train.append('mixed_baseline')\n",
    "if TRAIN_ABBY_TO_CHINESE: splits_to_train.append('abby_to_chinese')\n",
    "\n",
    "print(f\"\\nTraining {len(splits_to_train)} splits: {splits_to_train}\")\n",
    "\n",
    "all_results = {}\n",
    "MIN_TEST_SIZE = 100  # Lowered to allow smaller test sets like Chinese\n",
    "\n",
    "for split_idx, split_name in enumerate(splits_to_train):\n",
    "    split_start = time.time()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"[{split_idx+1}/{len(splits_to_train)}] {split_name}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    split = all_splits[split_name]\n",
    "    print(f\"Train: {split['train_size']:,} | Test: {split['test_size']:,}\")\n",
    "\n",
    "    if split['test_size'] < MIN_TEST_SIZE:\n",
    "        print(f\"WARNING: Test set only {split['test_size']} samples (need {MIN_TEST_SIZE})\")\n",
    "        print(\"Skipping this split - results would be unreliable\")\n",
    "        print(\"To fix: Add more data to the test languages/periods\")\n",
    "        continue\n",
    "\n",
    "    model = BIPModel().to(device)\n",
    "\n",
    "    train_dataset = NativeDataset(set(split['train_ids']), 'data/processed/passages.jsonl',\n",
    "                                   'data/processed/bonds.jsonl', tokenizer)\n",
    "\n",
    "    test_ids_to_use = split['test_ids'][:MAX_TEST_SAMPLES]\n",
    "\n",
    "    # Optional: strict prescriptive-only test\n",
    "    if STRICT_PRESCRIPTIVE_TEST:\n",
    "        print(\"Filtering to prescriptive examples only...\")\n",
    "        # Load bonds to filter\n",
    "        prescriptive_ids = set()\n",
    "        with open('data/processed/bonds.jsonl') as f:\n",
    "            for line in f:\n",
    "                b = json.loads(line)\n",
    "                if b.get('context') == 'prescriptive':\n",
    "                    prescriptive_ids.add(b['passage_id'])\n",
    "        test_ids_to_use = [tid for tid in test_ids_to_use if tid in prescriptive_ids]\n",
    "        print(f\"  Filtered to {len(test_ids_to_use):,} prescriptive samples\")\n",
    "\n",
    "    test_dataset = NativeDataset(set(test_ids_to_use), 'data/processed/passages.jsonl',\n",
    "                                  'data/processed/bonds.jsonl', tokenizer)\n",
    "\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"ERROR: No training data!\")\n",
    "        continue\n",
    "\n",
    "    # Use hardware-optimized batch size\n",
    "    actual_batch = min(BATCH_SIZE, max(32, len(train_dataset) // 20))\n",
    "    print(f\"Actual batch size: {actual_batch}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=actual_batch, shuffle=True,\n",
    "                              collate_fn=collate_fn, drop_last=True, num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=actual_batch*2, shuffle=False,\n",
    "                             collate_fn=collate_fn, num_workers=0, pin_memory=True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "    def get_adv_lambda(epoch, warmup=3):\n",
    "        \"\"\"Ramp adversarial strength: 0.1 -> 1.0 over warmup, then hold at 1.0\"\"\"\n",
    "        if epoch <= warmup:\n",
    "            return 0.1 + 0.9 * (epoch / warmup)\n",
    "        return 1.0\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            bond_labels = batch['bond_labels'].to(device)\n",
    "            language_labels = batch['language_labels'].to(device)\n",
    "            period_labels = batch['period_labels'].to(device)\n",
    "\n",
    "            adv_lambda = get_adv_lambda(epoch)\n",
    "\n",
    "            # Use new autocast API\n",
    "            with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "                out = model(input_ids, attention_mask, adv_lambda=adv_lambda)\n",
    "\n",
    "                # Weighted bond loss\n",
    "                if USE_CONFIDENCE_WEIGHTING:\n",
    "                    sample_weights = batch['sample_weights'].to(device)\n",
    "                    loss_bond = F.cross_entropy(out['bond_pred'], bond_labels, reduction='none')\n",
    "                    loss_bond = (loss_bond * sample_weights).mean()\n",
    "                else:\n",
    "                    loss_bond = F.cross_entropy(out['bond_pred'], bond_labels)\n",
    "\n",
    "                # Context auxiliary loss\n",
    "                if USE_CONTEXT_AUXILIARY:\n",
    "                    context_labels = batch['context_labels'].to(device)\n",
    "                    loss_context = F.cross_entropy(out['context_pred'], context_labels)\n",
    "                else:\n",
    "                    loss_context = 0\n",
    "\n",
    "                loss_lang = F.cross_entropy(out['language_pred'], language_labels)\n",
    "                loss_period = F.cross_entropy(out['period_pred'], period_labels)\n",
    "\n",
    "            loss = loss_bond + LANG_WEIGHT * loss_lang + PERIOD_WEIGHT * loss_period + CONTEXT_LOSS_WEIGHT * loss_context\n",
    "\n",
    "            if USE_AMP and scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / n_batches\n",
    "        print(f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f})\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), f'models/checkpoints/best_{split_name}.pt')\n",
    "            torch.save(model.state_dict(), f'{SAVE_DIR}/best_{split_name}.pt')\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.load_state_dict(torch.load(f'models/checkpoints/best_{split_name}.pt'))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = {'bond': [], 'lang': []}\n",
    "    all_labels = {'bond': [], 'lang': []}\n",
    "    all_languages = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n",
    "            all_preds['bond'].extend(out['bond_pred'].argmax(-1).cpu().tolist())\n",
    "            all_preds['lang'].extend(out['language_pred'].argmax(-1).cpu().tolist())\n",
    "            all_labels['bond'].extend(batch['bond_labels'].tolist())\n",
    "            all_labels['lang'].extend(batch['language_labels'].tolist())\n",
    "            all_languages.extend(batch['languages'])\n",
    "\n",
    "    bond_f1 = f1_score(all_labels['bond'], all_preds['bond'], average='macro', zero_division=0)\n",
    "    bond_acc = sum(p == l for p, l in zip(all_preds['bond'], all_labels['bond'])) / len(all_preds['bond'])\n",
    "    lang_acc = sum(p == l for p, l in zip(all_preds['lang'], all_labels['lang'])) / len(all_preds['lang'])\n",
    "\n",
    "    # Per-language F1\n",
    "    lang_f1 = {}\n",
    "    for lang in set(all_languages):\n",
    "        mask = [l == lang for l in all_languages]\n",
    "        if sum(mask) > 10:\n",
    "            preds = [p for p, m in zip(all_preds['bond'], mask) if m]\n",
    "            labels = [l for l, m in zip(all_labels['bond'], mask) if m]\n",
    "            lang_f1[lang] = {'f1': f1_score(labels, preds, average='macro', zero_division=0), 'n': sum(mask)}\n",
    "\n",
    "    all_results[split_name] = {\n",
    "        'bond_f1_macro': bond_f1,\n",
    "        'bond_acc': bond_acc,\n",
    "        'language_acc': lang_acc,\n",
    "        'per_language_f1': lang_f1,\n",
    "        'training_time': time.time() - split_start\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{split_name} RESULTS:\")\n",
    "    print(f\"  Bond F1 (macro): {bond_f1:.3f} ({bond_f1/0.1:.1f}x chance)\")\n",
    "    print(f\"  Bond accuracy:   {bond_acc:.1%}\")\n",
    "    print(f\"  Language acc:    {lang_acc:.1%} (want ~20% = invariant)\")\n",
    "    print(\"  Per-language:\")\n",
    "    for lang, m in sorted(lang_f1.items(), key=lambda x: -x[1]['n']):\n",
    "        print(f\"    {lang:20s}: F1={m['f1']:.3f} (n={m['n']:,})\")\n",
    "\n",
    "    # Context analysis\n",
    "    high_conf = sum(1 for c in test_dataset.data if c['confidence'] == 'high')\n",
    "    prescriptive = sum(1 for c in test_dataset.data if c['context'] == 'prescriptive')\n",
    "    print(f\"  Context: {prescriptive:,}/{len(test_dataset):,} prescriptive ({prescriptive/len(test_dataset)*100:.1f}%)\")\n",
    "    print(f\"  High confidence: {high_conf:,}/{len(test_dataset):,} ({high_conf/len(test_dataset)*100:.1f}%)\")\n",
    "\n",
    "    # GPU memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        mem = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"\\n  GPU memory: {mem:.1f} GB / {VRAM_GB:.1f} GB ({mem/VRAM_GB*100:.0f}%)\")\n",
    "\n",
    "    del model, train_dataset, test_dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8. Linear Probe Test { display-mode: \"form\" }\n",
    "#@markdown Tests if z_bond encodes language/period (should be low = invariant)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LINEAR PROBE TEST\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nIf probe accuracy is NEAR CHANCE, representation is INVARIANT\")\n",
    "print(\"(This is what we want for BIP)\")\n",
    "\n",
    "probe_results = {}\n",
    "\n",
    "for split_name in ['hebrew_to_others', 'semitic_to_non_semitic']:\n",
    "    model_path = f'{SAVE_DIR}/best_{split_name}.pt'\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"\\nSkipping {split_name} - no saved model\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PROBE: {split_name}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    model = BIPModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    test_ids = set(all_splits[split_name]['test_ids'][:5000])\n",
    "    test_dataset = NativeDataset(test_ids, 'data/processed/passages.jsonl',\n",
    "                                  'data/processed/bonds.jsonl', tokenizer)\n",
    "    \n",
    "    if len(test_dataset) < 50:\n",
    "        print(f\"  Skip - only {len(test_dataset)} samples\")\n",
    "        continue\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    all_z, all_lang, all_period = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Extract\"):\n",
    "            out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n",
    "            all_z.append(out['z'].cpu().numpy())\n",
    "            all_lang.extend(batch['language_labels'].tolist())\n",
    "            all_period.extend(batch['period_labels'].tolist())\n",
    "    \n",
    "    X = np.vstack(all_z)\n",
    "    y_lang = np.array(all_lang)\n",
    "    y_period = np.array(all_period)\n",
    "    \n",
    "    scaler_probe = StandardScaler()\n",
    "    X_scaled = scaler_probe.fit_transform(X)\n",
    "    \n",
    "    # Train/test split for probes\n",
    "    n = len(X)\n",
    "    idx = np.random.permutation(n)\n",
    "    train_idx, test_idx = idx[:int(0.7*n)], idx[int(0.7*n):]\n",
    "    \n",
    "    # Language probe - check for multiple classes\n",
    "    unique_langs = np.unique(y_lang[test_idx])\n",
    "    if len(unique_langs) < 2:\n",
    "        print(f\"  SKIP language probe - only {len(unique_langs)} class\")\n",
    "        lang_acc = 1.0 / max(1, len(np.unique(y_lang)))\n",
    "        lang_chance = lang_acc\n",
    "    else:\n",
    "        lang_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        lang_probe.fit(X_scaled[train_idx], y_lang[train_idx])\n",
    "        lang_acc = (lang_probe.predict(X_scaled[test_idx]) == y_lang[test_idx]).mean()\n",
    "        lang_chance = 1.0 / len(unique_langs)\n",
    "    \n",
    "    # Period probe - same check\n",
    "    unique_periods = np.unique(y_period[test_idx])\n",
    "    if len(unique_periods) < 2:\n",
    "        print(f\"  SKIP period probe - only {len(unique_periods)} class\")\n",
    "        period_acc = 1.0 / max(1, len(np.unique(y_period)))\n",
    "        period_chance = period_acc\n",
    "    else:\n",
    "        period_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        period_probe.fit(X_scaled[train_idx], y_period[train_idx])\n",
    "        period_acc = (period_probe.predict(X_scaled[test_idx]) == y_period[test_idx]).mean()\n",
    "        period_chance = 1.0 / len(unique_periods)\n",
    "    \n",
    "    lang_status = \"INVARIANT\" if lang_acc < lang_chance + 0.15 else \"NOT invariant\"\n",
    "    period_status = \"INVARIANT\" if period_acc < period_chance + 0.15 else \"NOT invariant\"\n",
    "    \n",
    "    probe_results[split_name] = {\n",
    "        'language_acc': lang_acc,\n",
    "        'language_chance': lang_chance,\n",
    "        'language_status': lang_status,\n",
    "        'period_acc': period_acc,\n",
    "        'period_chance': period_chance,\n",
    "        'period_status': period_status,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nRESULTS:\")\n",
    "    print(f\"  Language: {lang_acc:.1%} (chance: {lang_chance:.1%}) -> {lang_status}\")\n",
    "    print(f\"  Period:   {period_acc:.1%} (chance: {period_chance:.1%}) -> {period_status}\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Probe tests complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9. Final Results { display-mode: \"form\" }\n",
    "#@markdown Comprehensive summary with verdict\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL BIP EVALUATION (v10.2)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nHardware: {GPU_TIER} ({VRAM_GB:.0f}GB VRAM, {RAM_GB:.0f}GB RAM)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"CROSS-DOMAIN TRANSFER RESULTS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "successful_splits = []\n",
    "for name, r in all_results.items():\n",
    "    ratio = r['bond_f1_macro'] / 0.1\n",
    "    lang_acc = r['language_acc']\n",
    "    \n",
    "    transfer_ok = ratio > 1.3\n",
    "    invariant_ok = lang_acc < 0.35  # Near chance (20%)\n",
    "    \n",
    "    status = \"SUCCESS\" if (transfer_ok and invariant_ok) else \"PARTIAL\" if transfer_ok else \"FAIL\"\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Bond F1:     {r['bond_f1_macro']:.3f} ({ratio:.1f}x chance) {'OK' if transfer_ok else 'WEAK'}\")\n",
    "    print(f\"  Language:    {lang_acc:.1%} {'INVARIANT' if invariant_ok else 'LEAKING'}\")\n",
    "    print(f\"  -> {status}\")\n",
    "    \n",
    "    if transfer_ok and invariant_ok:\n",
    "        successful_splits.append(name)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"VERDICT\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "n_success = len(successful_splits)\n",
    "if n_success >= 2:\n",
    "    verdict = \"STRONGLY_SUPPORTED\"\n",
    "    msg = \"Multiple independent transfer paths demonstrate universal structure\"\n",
    "elif n_success >= 1:\n",
    "    verdict = \"SUPPORTED\"\n",
    "    msg = \"At least one transfer path works\"\n",
    "elif any(r['bond_f1_macro'] > 0.13 for r in all_results.values()):\n",
    "    verdict = \"PARTIAL\"\n",
    "    msg = \"Some transfer signal, but not robust\"\n",
    "else:\n",
    "    verdict = \"INCONCLUSIVE\"\n",
    "    msg = \"No clear transfer demonstrated\"\n",
    "\n",
    "print(f\"\\n  Successful transfers: {n_success}/{len(all_results)}\")\n",
    "print(f\"  Splits: {successful_splits if successful_splits else 'None'}\")\n",
    "print(f\"\\n  VERDICT: {verdict}\")\n",
    "print(f\"  {msg}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'all_results': all_results,\n",
    "    'probe_results': probe_results if 'probe_results' in dir() else {},\n",
    "    'successful_splits': successful_splits,\n",
    "    'verdict': verdict,\n",
    "    'hardware': {'gpu': GPU_TIER, 'vram_gb': VRAM_GB, 'ram_gb': RAM_GB},\n",
    "    'settings': {'batch_size': BATCH_SIZE, 'max_per_lang': MAX_PER_LANG, 'num_workers': NUM_WORKERS},\n",
    "    'experiment_time': time.time() - EXPERIMENT_START,\n",
    "}\n",
    "\n",
    "with open('results/final_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "shutil.copy('results/final_results.json', f'{SAVE_DIR}/final_results.json')\n",
    "\n",
    "print(f\"\\nTotal time: {(time.time() - EXPERIMENT_START)/60:.1f} minutes\")\n",
    "print(\"Results saved to Drive!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10. Download Results { display-mode: \"form\" }\n#@markdown Download all models and results\n\nimport zipfile\n\nzip_path = 'BIP_v10.8_results.zip'\nprint(\"Creating download package...\")\n\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n    # Results\n    if os.path.exists('results/final_results.json'):\n        zf.write('results/final_results.json')\n    \n    # Models (from Drive)\n    if SAVE_DIR and os.path.exists(SAVE_DIR):\n        for f in os.listdir(SAVE_DIR):\n            if f.endswith('.pt'):\n                zf.write(f'{SAVE_DIR}/{f}', f'models/{f}')\n    \n    # Config\n    if os.path.exists('data/splits/all_splits.json'):\n        zf.write('data/splits/all_splits.json')\n\nprint(f\"\\nDownload package ready: {zip_path}\")\n\n# Download in Colab, or show path otherwise\ntry:\n    from google.colab import files\n    files.download(zip_path)\nexcept ImportError:\n    print(f\"Not running in Colab. Results saved to: {os.path.abspath(zip_path)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}