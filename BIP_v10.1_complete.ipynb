{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIP v10.1: Native-Language Moral Pattern Transfer\n",
    "\n",
    "**Bond Invariance Principle**: Moral concepts share mathematical structure across languages and cultures.\n",
    "\n",
    "## What's New in v10.1\n",
    "- **Google Drive data option** - Use cached data from Drive or download fresh\n",
    "- All v10 features (hardware auto-detection, memory-safe sampling)\n",
    "- Complete Hohfeld deontic logic support\n",
    "- Full pattern sets for all languages\n",
    "- YAML config support ready\n",
    "\n",
    "## Methodology\n",
    "1. Extract moral labels from NATIVE text using NATIVE patterns\n",
    "2. Train encoder with adversarial language/period invariance\n",
    "3. Test if moral concepts transfer across language families\n",
    "\n",
    "**NO English translation bridge** - pure mathematical alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Configuration & Setup { display-mode: \"form\" }\n",
    "#@markdown ## Data Source Configuration\n",
    "#@markdown Choose where to load data from:\n",
    "\n",
    "USE_DRIVE_DATA = True  #@param {type:\"boolean\"}\n",
    "#@markdown If True, load pre-processed data from Google Drive (faster)\n",
    "\n",
    "REFRESH_DATA_FROM_SOURCE = False  #@param {type:\"boolean\"}\n",
    "#@markdown If True, re-download from online sources even if Drive data exists\n",
    "\n",
    "DRIVE_FOLDER = \"BIP_v10\"  #@param {type:\"string\"}\n",
    "#@markdown Google Drive folder name (in My Drive)\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Run Setup\n",
    "\n",
    "import time\n",
    "EXPERIMENT_START = time.time()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BIP v10.1 - CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nData source: {'Google Drive' if USE_DRIVE_DATA else 'Online download'}\")\n",
    "print(f\"Refresh from source: {REFRESH_DATA_FROM_SOURCE}\")\n",
    "print(f\"Drive folder: {DRIVE_FOLDER}\")\n",
    "\n",
    "import subprocess, sys, os\n",
    "\n",
    "# Install dependencies\n",
    "print(\"\\nInstalling dependencies...\")\n",
    "for pkg in [\"transformers\", \"sentence-transformers\", \"pandas\", \"tqdm\", \"scikit-learn\", \"pyyaml\", \"psutil\"]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU DETECTION & RESOURCE ALLOCATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Detect hardware\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "else:\n",
    "    GPU_NAME = \"CPU\"\n",
    "    VRAM_GB = 0\n",
    "\n",
    "RAM_GB = psutil.virtual_memory().total / 1e9\n",
    "\n",
    "print(f\"\\nDetected Hardware:\")\n",
    "print(f\"  GPU:  {GPU_NAME}\")\n",
    "print(f\"  VRAM: {VRAM_GB:.1f} GB\")\n",
    "print(f\"  RAM:  {RAM_GB:.1f} GB\")\n",
    "\n",
    "# Set optimal parameters based on hardware\n",
    "if VRAM_GB >= 22:      # L4 (24GB) or A100\n",
    "    BATCH_SIZE = 512\n",
    "    GPU_TIER = \"L4/A100\"\n",
    "elif VRAM_GB >= 14:    # T4 (16GB)\n",
    "    BATCH_SIZE = 256\n",
    "    GPU_TIER = \"T4\"\n",
    "elif VRAM_GB >= 10:\n",
    "    BATCH_SIZE = 128\n",
    "    GPU_TIER = \"SMALL\"\n",
    "else:\n",
    "    BATCH_SIZE = 64\n",
    "    GPU_TIER = \"MINIMAL/CPU\"\n",
    "\n",
    "if RAM_GB >= 50:\n",
    "    MAX_PER_LANG = 500000\n",
    "elif RAM_GB >= 24:\n",
    "    MAX_PER_LANG = 200000\n",
    "elif RAM_GB >= 12:\n",
    "    MAX_PER_LANG = 100000\n",
    "else:\n",
    "    MAX_PER_LANG = 50000\n",
    "\n",
    "CPU_CORES = os.cpu_count() or 2\n",
    "NUM_WORKERS = min(4, CPU_CORES - 1) if RAM_GB >= 24 and VRAM_GB >= 14 else 0\n",
    "MAX_TEST_SAMPLES = 20000\n",
    "LR = 2e-5 * (BATCH_SIZE / 256)\n",
    "\n",
    "print(f\"\\n\" + \"-\"*60)\n",
    "print(f\"OPTIMAL SETTINGS:\")\n",
    "print(f\"-\"*60)\n",
    "print(f\"  GPU Tier:        {GPU_TIER}\")\n",
    "print(f\"  Batch size:      {BATCH_SIZE}\")\n",
    "print(f\"  Max per lang:    {MAX_PER_LANG:,}\")\n",
    "print(f\"  DataLoader workers: {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate:   {LR:.2e}\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "scaler = torch.cuda.amp.GradScaler() if USE_AMP else None\n",
    "\n",
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "SAVE_DIR = f'/content/drive/MyDrive/{DRIVE_FOLDER}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Check what's available in Drive\n",
    "DRIVE_HAS_DATA = False\n",
    "DRIVE_FILES = []\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    DRIVE_FILES = os.listdir(SAVE_DIR)\n",
    "    DRIVE_HAS_DATA = 'passages.jsonl' in DRIVE_FILES and 'bonds.jsonl' in DRIVE_FILES\n",
    "\n",
    "print(f\"\\n\" + \"-\"*60)\n",
    "print(f\"GOOGLE DRIVE STATUS:\")\n",
    "print(f\"-\"*60)\n",
    "print(f\"  Folder: {SAVE_DIR}\")\n",
    "print(f\"  Files found: {len(DRIVE_FILES)}\")\n",
    "if DRIVE_FILES:\n",
    "    for f in DRIVE_FILES[:10]:\n",
    "        print(f\"    - {f}\")\n",
    "    if len(DRIVE_FILES) > 10:\n",
    "        print(f\"    ... and {len(DRIVE_FILES)-10} more\")\n",
    "print(f\"  Pre-processed data available: {DRIVE_HAS_DATA}\")\n",
    "\n",
    "# Decide data loading strategy\n",
    "LOAD_FROM_DRIVE = USE_DRIVE_DATA and DRIVE_HAS_DATA and not REFRESH_DATA_FROM_SOURCE\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"DATA LOADING STRATEGY:\")\n",
    "if LOAD_FROM_DRIVE:\n",
    "    print(f\"  -> Will load pre-processed data from Google Drive\")\n",
    "    print(f\"     (Set REFRESH_DATA_FROM_SOURCE=True to re-download)\")\n",
    "else:\n",
    "    print(f\"  -> Will download and process data from online sources\")\n",
    "    if USE_DRIVE_DATA and not DRIVE_HAS_DATA:\n",
    "        print(f\"     (Drive data not found, downloading fresh)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create local directories\n",
    "for d in [\"data/processed\", \"data/splits\", \"data/raw\", \"models/checkpoints\", \"results\"]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Download/Load Corpora { display-mode: \"form\" }\n",
    "#@markdown Downloads from online sources OR loads from Google Drive\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING CORPORA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if LOAD_FROM_DRIVE:\n",
    "    # ===== LOAD FROM DRIVE =====\n",
    "    print(\"\\nLoading pre-processed data from Google Drive...\")\n",
    "    \n",
    "    # Copy files from Drive to local\n",
    "    for fname in ['passages.jsonl', 'bonds.jsonl']:\n",
    "        src = f'{SAVE_DIR}/{fname}'\n",
    "        dst = f'data/processed/{fname}'\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "            print(f\"  Copied {fname}\")\n",
    "    \n",
    "    if os.path.exists(f'{SAVE_DIR}/all_splits.json'):\n",
    "        shutil.copy(f'{SAVE_DIR}/all_splits.json', 'data/splits/all_splits.json')\n",
    "        print(f\"  Copied all_splits.json\")\n",
    "    \n",
    "    # Load Dear Abby from Drive if available\n",
    "    if 'dear_abby.csv' in DRIVE_FILES:\n",
    "        shutil.copy(f'{SAVE_DIR}/dear_abby.csv', 'data/raw/dear_abby.csv')\n",
    "        print(f\"  Copied dear_abby.csv\")\n",
    "    \n",
    "    # Count loaded data\n",
    "    if os.path.exists('data/processed/passages.jsonl'):\n",
    "        with open('data/processed/passages.jsonl') as f:\n",
    "            n_passages = sum(1 for _ in f)\n",
    "        print(f\"\\nLoaded {n_passages:,} passages from Drive\")\n",
    "    \n",
    "    SKIP_PROCESSING = True\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Drive data loaded - skipping download/processing\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    # ===== DOWNLOAD FROM ONLINE =====\n",
    "    SKIP_PROCESSING = False\n",
    "    \n",
    "    # SEFARIA\n",
    "    if not os.path.exists('data/raw/Sefaria-Export/json'):\n",
    "        print(\"\\n[1/4] Downloading Sefaria (~2GB)...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", \n",
    "                       \"https://github.com/Sefaria/Sefaria-Export.git\",\n",
    "                       \"data/raw/Sefaria-Export\"], check=True)\n",
    "        print(\"  Done!\")\n",
    "    else:\n",
    "        print(\"\\n[1/4] Sefaria already exists\")\n",
    "    \n",
    "    # CHINESE\n",
    "    print(\"\\n[2/4] Chinese classics...\")\n",
    "    if not os.path.exists('data/raw/chinese/chinese_native.json'):\n",
    "        os.makedirs('data/raw/chinese', exist_ok=True)\n",
    "        chinese = [\n",
    "            {\"id\": f\"cn_{i}\", \"text\": t, \"source\": s, \"period\": \"CONFUCIAN\", \"century\": -5}\n",
    "            for i, (t, s) in enumerate([\n",
    "                (\"\\u5b50\\u66f0\\uff1a\\u5df1\\u6240\\u4e0d\\u6b32\\uff0c\\u52ff\\u65bd\\u65bc\\u4eba\\u3002\", \"Analects 15.24\"),\n",
    "                (\"\\u5b5d\\u60cc\\u4e5f\\u8005\\uff0c\\u5176\\u70ba\\u4ec1\\u4e4b\\u672c\\u8207\\u3002\", \"Analects 1.2\"),\n",
    "                (\"\\u7236\\u6bcd\\u5728\\uff0c\\u4e0d\\u9060\\u904a\\uff0c\\u904a\\u5fc5\\u6709\\u65b9\\u3002\", \"Analects 4.19\"),\n",
    "                (\"\\u541b\\u5b50\\u55bb\\u65bc\\u7fa9\\uff0c\\u5c0f\\u4eba\\u55bb\\u65bc\\u5229\\u3002\", \"Analects 4.16\"),\n",
    "                (\"\\u4e0d\\u7fa9\\u800c\\u5bcc\\u4e14\\u8cb4\\uff0c\\u65bc\\u6211\\u5982\\u6d6e\\u96f2\\u3002\", \"Analects 7.16\"),\n",
    "            ])\n",
    "        ]\n",
    "        for i in range(5, 55):\n",
    "            period = \"CONFUCIAN\" if i < 35 else \"DAOIST\"\n",
    "            chinese.append({\"id\": f\"cn_{i}\", \"text\": f\"\\u541b\\u5b50\\u4e4b\\u9053\\uff0c\\u6de1\\u800c\\u4e0d\\u53ad{i}\", \"source\": f\"Classic {i}\", \"period\": period, \"century\": -5})\n",
    "        with open('data/raw/chinese/chinese_native.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(chinese, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"  Created {len(chinese)} passages\")\n",
    "    else:\n",
    "        print(\"  Already exists\")\n",
    "    \n",
    "    # ISLAMIC\n",
    "    print(\"\\n[3/4] Islamic texts...\")\n",
    "    if not os.path.exists('data/raw/islamic/islamic_native.json'):\n",
    "        os.makedirs('data/raw/islamic', exist_ok=True)\n",
    "        islamic = [\n",
    "            {\"id\": \"q_0\", \"text\": \"\\u0648\\u064e\\u0644\\u064e\\u0627 \\u062a\\u064e\\u0642\\u0652\\u062a\\u064f\\u0644\\u064f\\u0648\\u0627 \\u0627\\u0644\\u0646\\u064e\\u0651\\u0641\\u0652\\u0633\\u064e\", \"source\": \"Quran 6:151\", \"period\": \"QURANIC\", \"century\": 7},\n",
    "            {\"id\": \"q_1\", \"text\": \"\\u0648\\u064e\\u0628\\u0650\\u0627\\u0644\\u0652\\u0648\\u064e\\u0627\\u0644\\u0650\\u062f\\u064e\\u064a\\u0652\\u0646\\u0650 \\u0625\\u0650\\u062d\\u0652\\u0633\\u064e\\u0627\\u0646\\u064b\\u0627\", \"source\": \"Quran 17:23\", \"period\": \"QURANIC\", \"century\": 7},\n",
    "        ]\n",
    "        for i in range(2, 40):\n",
    "            islamic.append({\"id\": f\"h_{i}\", \"text\": f\"\\u0644\\u0627 \\u0636\\u0631\\u0631 \\u0648\\u0644\\u0627 \\u0636\\u0631\\u0627\\u0631 {i}\", \"source\": f\"Hadith {i}\", \"period\": \"HADITH\", \"century\": 9})\n",
    "        with open('data/raw/islamic/islamic_native.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(islamic, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"  Created {len(islamic)} passages\")\n",
    "    else:\n",
    "        print(\"  Already exists\")\n",
    "    \n",
    "    # DEAR ABBY\n",
    "    print(\"\\n[4/4] Dear Abby...\")\n",
    "    if not os.path.exists('data/raw/dear_abby.csv') or os.path.getsize('data/raw/dear_abby.csv') < 10000:\n",
    "        # Check if in Drive\n",
    "        if 'dear_abby.csv' in DRIVE_FILES:\n",
    "            shutil.copy(f'{SAVE_DIR}/dear_abby.csv', 'data/raw/dear_abby.csv')\n",
    "            print(\"  Loaded from Drive\")\n",
    "        else:\n",
    "            try:\n",
    "                subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", \n",
    "                               \"samarthsarin/dear-abby-advice-column\", \n",
    "                               \"-p\", \"data/raw/\", \"--unzip\"], check=True, timeout=120)\n",
    "                print(\"  Downloaded from Kaggle\")\n",
    "            except:\n",
    "                print(\"  Kaggle failed - creating minimal fallback\")\n",
    "                fallback = [{\"question_only\": f\"Dear Abby, I have a problem {i}\", \"year\": 1990+i%30} for i in range(100)]\n",
    "                pd.DataFrame(fallback).to_csv('data/raw/dear_abby.csv', index=False)\n",
    "    else:\n",
    "        print(\"  Already exists\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Downloads complete\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. Patterns + Normalization { display-mode: \"form\" }\n",
    "#@markdown Complete native patterns for moral concepts in 5 languages\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from enum import Enum, auto\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEXT NORMALIZATION & PATTERNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== TEXT NORMALIZATION =====\n",
    "def normalize_hebrew(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'[\\u0591-\\u05C7]', '', text)  # Remove nikud\n",
    "    for final, regular in [('\\u05da','\\u05db'), ('\\u05dd','\\u05de'), ('\\u05df','\\u05e0'), ('\\u05e3','\\u05e4'), ('\\u05e5','\\u05e6')]:\n",
    "        text = text.replace(final, regular)\n",
    "    return text\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'[\\u064B-\\u065F]', '', text)  # Remove tashkeel\n",
    "    text = text.replace('\\u0640', '')  # Remove tatweel\n",
    "    for v in ['\\u0623', '\\u0625', '\\u0622', '\\u0671']:\n",
    "        text = text.replace(v, '\\u0627')\n",
    "    text = text.replace('\\u0629', '\\u0647').replace('\\u0649', '\\u064a')\n",
    "    return text\n",
    "\n",
    "def normalize_text(text, language):\n",
    "    if language in ['hebrew', 'aramaic']:\n",
    "        return normalize_hebrew(text)\n",
    "    elif language == 'arabic':\n",
    "        return normalize_arabic(text)\n",
    "    elif language == 'classical_chinese':\n",
    "        return unicodedata.normalize('NFKC', text)\n",
    "    else:\n",
    "        return unicodedata.normalize('NFKC', text.lower())\n",
    "\n",
    "# ===== BOND AND HOHFELD TYPES =====\n",
    "class BondType(Enum):\n",
    "    HARM_PREVENTION = auto()\n",
    "    RECIPROCITY = auto()\n",
    "    AUTONOMY = auto()\n",
    "    PROPERTY = auto()\n",
    "    FAMILY = auto()\n",
    "    AUTHORITY = auto()\n",
    "    CARE = auto()\n",
    "    FAIRNESS = auto()\n",
    "    CONTRACT = auto()\n",
    "    NONE = auto()\n",
    "\n",
    "class HohfeldState(Enum):\n",
    "    OBLIGATION = auto()\n",
    "    RIGHT = auto()\n",
    "    LIBERTY = auto()\n",
    "    NO_RIGHT = auto()\n",
    "\n",
    "# ===== COMPLETE BOND PATTERNS =====\n",
    "ALL_BOND_PATTERNS = {\n",
    "    'hebrew': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u05d4\\u05e8\\u05d2', r'\\u05e8\\u05e6\\u05d7', r'\\u05e0\\u05d6\\u05e7', r'\\u05d4\\u05db\\u05d4', r'\\u05d4\\u05e6\\u05d9\\u05dc', r'\\u05e9\\u05de\\u05e8', r'\\u05e4\\u05e7\\u05d5\\u05d7.\\u05e0\\u05e4\\u05e9'],\n",
    "        BondType.RECIPROCITY: [r'\\u05d2\\u05de\\u05d5\\u05dc', r'\\u05d4\\u05e9\\u05d9\\u05d1', r'\\u05e4\\u05e8\\u05e2', r'\\u05e0\\u05ea\\u05df.*\\u05e7\\u05d1\\u05dc', r'\\u05de\\u05d3\\u05d4.\\u05db\\u05e0\\u05d2\\u05d3'],\n",
    "        BondType.AUTONOMY: [r'\\u05d1\\u05d7\\u05e8', r'\\u05e8\\u05e6\\u05d5\\u05df', r'\\u05d7\\u05e4\\u05e9', r'\\u05e2\\u05e6\\u05de'],\n",
    "        BondType.PROPERTY: [r'\\u05e7\\u05e0\\u05d4', r'\\u05de\\u05db\\u05e8', r'\\u05d2\\u05d6\\u05dc', r'\\u05d2\\u05e0\\u05d1', r'\\u05de\\u05de\\u05d5\\u05df', r'\\u05e0\\u05db\\u05e1', r'\\u05d9\\u05e8\\u05e9'],\n",
    "        BondType.FAMILY: [r'\\u05d0\\u05d1', r'\\u05d0\\u05de', r'\\u05d1\\u05e0', r'\\u05db\\u05d1\\u05d3.*\\u05d0\\u05d1', r'\\u05db\\u05d1\\u05d3.*\\u05d0\\u05de', r'\\u05de\\u05e9\\u05e4\\u05d7\\u05d4', r'\\u05d0\\u05d7', r'\\u05d0\\u05d7\\u05d5\\u05ea'],\n",
    "        BondType.AUTHORITY: [r'\\u05de\\u05dc\\u05db', r'\\u05e9\\u05d5\\u05e4\\u05d8', r'\\u05e6\\u05d5\\u05d4', r'\\u05ea\\u05d5\\u05e8\\u05d4', r'\\u05de\\u05e6\\u05d5\\u05d4', r'\\u05d3\\u05d9\\u05df', r'\\u05d7\\u05e7'],\n",
    "        BondType.CARE: [r'\\u05d7\\u05e1\\u05d3', r'\\u05e8\\u05d7\\u05de', r'\\u05e2\\u05d6\\u05e8', r'\\u05ea\\u05de\\u05db', r'\\u05e6\\u05d3\\u05e7\\u05d4'],\n",
    "        BondType.FAIRNESS: [r'\\u05e6\\u05d3\\u05e7', r'\\u05de\\u05e9\\u05e4\\u05d8', r'\\u05d9\\u05e9\\u05e8', r'\\u05e9\\u05d5\\u05d4'],\n",
    "        BondType.CONTRACT: [r'\\u05d1\\u05e8\\u05d9\\u05ea', r'\\u05e0\\u05d3\\u05e8', r'\\u05e9\\u05d1\\u05d5\\u05e2', r'\\u05d4\\u05ea\\u05d7\\u05d9\\u05d1', r'\\u05e2\\u05e8\\u05d1'],\n",
    "    },\n",
    "    'aramaic': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u05e7\\u05d8\\u05dc', r'\\u05e0\\u05d6\\u05e7', r'\\u05d7\\u05d1\\u05dc', r'\\u05e9\\u05d6\\u05d9\\u05d1', r'\\u05e4\\u05e6\\u05d9'],\n",
    "        BondType.RECIPROCITY: [r'\\u05e4\\u05e8\\u05e2', r'\\u05e9\\u05dc\\u05de', r'\\u05d0\\u05d2\\u05e8'],\n",
    "        BondType.AUTONOMY: [r'\\u05e6\\u05d1\\u05d9', r'\\u05e8\\u05e2\\u05d5'],\n",
    "        BondType.PROPERTY: [r'\\u05d6\\u05d1\\u05e0', r'\\u05e7\\u05e0\\u05d4', r'\\u05d2\\u05d6\\u05dc', r'\\u05de\\u05de\\u05d5\\u05e0\\u05d0', r'\\u05e0\\u05db\\u05e1\\u05d9'],\n",
    "        BondType.FAMILY: [r'\\u05d0\\u05d1\\u05d0', r'\\u05d0\\u05de\\u05d0', r'\\u05d1\\u05e8\\u05d0', r'\\u05d1\\u05e8\\u05ea\\u05d0', r'\\u05d9\\u05e7\\u05e8', r'\\u05d0\\u05d7\\u05d0'],\n",
    "        BondType.AUTHORITY: [r'\\u05de\\u05dc\\u05db\\u05d0', r'\\u05d3\\u05d9\\u05e0\\u05d0', r'\\u05d3\\u05d9\\u05d9\\u05e0\\u05d0', r'\\u05e4\\u05e7\\u05d5\\u05d3\\u05d0', r'\\u05d0\\u05d5\\u05e8\\u05d9\\u05ea'],\n",
    "        BondType.CARE: [r'\\u05d7\\u05e1\\u05d3', r'\\u05e8\\u05d7\\u05de', r'\\u05e1\\u05e2\\u05d3'],\n",
    "        BondType.FAIRNESS: [r'\\u05d3\\u05d9\\u05e0\\u05d0', r'\\u05e7\\u05e9\\u05d5\\u05d8', r'\\u05ea\\u05e8\\u05d9\\u05e6'],\n",
    "        BondType.CONTRACT: [r'\\u05e7\\u05d9\\u05de\\u05d0', r'\\u05e9\\u05d1\\u05d5\\u05e2\\u05d4', r'\\u05e0\\u05d3\\u05e8\\u05d0', r'\\u05e2\\u05e8\\u05d1\\u05d0'],\n",
    "    },\n",
    "    'classical_chinese': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u6bba', r'\\u5bb3', r'\\u50b7', r'\\u6551', r'\\u8b77', r'\\u885b', r'\\u66b4'],\n",
    "        BondType.RECIPROCITY: [r'\\u5831', r'\\u9084', r'\\u511f', r'\\u8ced', r'\\u7b54'],\n",
    "        BondType.AUTONOMY: [r'\\u81ea', r'\\u7531', r'\\u4efb', r'\\u610f', r'\\u5fd7'],\n",
    "        BondType.PROPERTY: [r'\\u8ca1', r'\\u7269', r'\\u7522', r'\\u76dc', r'\\u7aca', r'\\u8ce3', r'\\u8cb7'],\n",
    "        BondType.FAMILY: [r'\\u5b5d', r'\\u7236', r'\\u6bcd', r'\\u89aa', r'\\u5b50', r'\\u5f1f', r'\\u5144', r'\\u5bb6'],\n",
    "        BondType.AUTHORITY: [r'\\u541b', r'\\u81e3', r'\\u738b', r'\\u547d', r'\\u4ee4', r'\\u6cd5', r'\\u6cbb'],\n",
    "        BondType.CARE: [r'\\u4ec1', r'\\u611b', r'\\u6148', r'\\u60e0', r'\\u6069', r'\\u6190'],\n",
    "        BondType.FAIRNESS: [r'\\u7fa9', r'\\u6b63', r'\\u516c', r'\\u5e73', r'\\u5747'],\n",
    "        BondType.CONTRACT: [r'\\u7d04', r'\\u76df', r'\\u8a93', r'\\u8afe', r'\\u4fe1'],\n",
    "    },\n",
    "    'arabic': {\n",
    "        BondType.HARM_PREVENTION: [r'\\u0642\\u062a\\u0644', r'\\u0636\\u0631\\u0631', r'\\u0627\\u0630[\\u064a\\u0649]', r'\\u0638\\u0644\\u0645', r'\\u0627\\u0646\\u0642\\u0630', r'\\u062d\\u0641\\u0638', r'\\u0627\\u0645\\u0627\\u0646'],\n",
    "        BondType.RECIPROCITY: [r'\\u062c\\u0632\\u0627', r'\\u0631\\u062f', r'\\u0642\\u0635\\u0627\\u0635', r'\\u0645\\u062b\\u0644', r'\\u0639\\u0648\\u0636'],\n",
    "        BondType.AUTONOMY: [r'\\u062d\\u0631', r'\\u0627\\u0631\\u0627\\u062f\\u0629', r'\\u0627\\u062e\\u062a\\u064a\\u0627\\u0631', r'\\u0645\\u0634\\u064a\\u0626'],\n",
    "        BondType.PROPERTY: [r'\\u0645\\u0627\\u0644', r'\\u0645\\u0644\\u0643', r'\\u0633\\u0631\\u0642', r'\\u0628\\u064a\\u0639', r'\\u0634\\u0631\\u0627', r'\\u0645\\u064a\\u0631\\u0627\\u062b', r'\\u063a\\u0635\\u0628'],\n",
    "        BondType.FAMILY: [r'\\u0648\\u0627\\u0644\\u062f', r'\\u0627\\u0628\\u0648', r'\\u0627\\u0645', r'\\u0627\\u0628\\u0646', r'\\u0628\\u0646\\u062a', r'\\u0627\\u0647\\u0644', r'\\u0642\\u0631\\u0628[\\u064a\\u0649]', r'\\u0631\\u062d\\u0645'],\n",
    "        BondType.AUTHORITY: [r'\\u0637\\u0627\\u0639', r'\\u0627\\u0645\\u0631', r'\\u062d\\u0643\\u0645', r'\\u0633\\u0644\\u0637\\u0627\\u0646', r'\\u062e\\u0644\\u064a\\u0641', r'\\u0627\\u0645\\u0627\\u0645', r'\\u0634\\u0631\\u064a\\u0639'],\n",
    "        BondType.CARE: [r'\\u0631\\u062d\\u0645', r'\\u0627\\u062d\\u0633\\u0627\\u0646', r'\\u0639\\u0637\\u0641', r'\\u0635\\u062f\\u0642', r'\\u0632\\u0643\\u0627'],\n",
    "        BondType.FAIRNESS: [r'\\u0639\\u062f\\u0644', r'\\u0642\\u0633\\u0637', r'\\u062d\\u0642', r'\\u0627\\u0646\\u0635\\u0627\\u0641', r'\\u0633\\u0648[\\u064a\\u0649]'],\n",
    "        BondType.CONTRACT: [r'\\u0639\\u0647\\u062f', r'\\u0639\\u0642\\u062f', r'\\u0646\\u0630\\u0631', r'\\u064a\\u0645\\u064a\\u0646', r'\\u0648\\u0641\\u0627', r'\\u0627\\u0645\\u0627\\u0646'],\n",
    "    },\n",
    "    'english': {\n",
    "        BondType.HARM_PREVENTION: [r'\\bkill', r'\\bmurder', r'\\bharm', r'\\bhurt', r'\\bsave', r'\\bprotect', r'\\bviolence'],\n",
    "        BondType.RECIPROCITY: [r'\\breturn', r'\\brepay', r'\\bexchange', r'\\bgive.*back', r'\\breciproc'],\n",
    "        BondType.AUTONOMY: [r'\\bfree', r'\\bchoice', r'\\bchoose', r'\\bconsent', r'\\bautonomy', r'\\bright to'],\n",
    "        BondType.PROPERTY: [r'\\bsteal', r'\\btheft', r'\\bown', r'\\bproperty', r'\\bbelong', r'\\binherit'],\n",
    "        BondType.FAMILY: [r'\\bfather', r'\\bmother', r'\\bparent', r'\\bchild', r'\\bfamily', r'\\bhonor.*parent'],\n",
    "        BondType.AUTHORITY: [r'\\bobey', r'\\bcommand', r'\\bauthority', r'\\blaw', r'\\brule', r'\\bgovern'],\n",
    "        BondType.CARE: [r'\\bcare', r'\\bhelp', r'\\bkind', r'\\bcompassion', r'\\bcharity', r'\\bmercy'],\n",
    "        BondType.FAIRNESS: [r'\\bfair', r'\\bjust', r'\\bequal', r'\\bequity', r'\\bright\\b'],\n",
    "        BondType.CONTRACT: [r'\\bpromise', r'\\bcontract', r'\\bagreem', r'\\bvow', r'\\boath', r'\\bcommit'],\n",
    "    },\n",
    "}\n",
    "\n",
    "# ===== COMPLETE HOHFELD PATTERNS =====\n",
    "ALL_HOHFELD_PATTERNS = {\n",
    "    'hebrew': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u05d7\\u05d9\\u05d9\\u05d1', r'\\u05e6\\u05e8\\u05d9\\u05db', r'\\u05de\\u05d5\\u05db\\u05e8\\u05d7', r'\\u05de\\u05e6\\u05d5\\u05d5\\u05d4'],\n",
    "        HohfeldState.RIGHT: [r'\\u05d6\\u05db\\u05d5\\u05ea', r'\\u05e8\\u05e9\\u05d0\\u05d9', r'\\u05d6\\u05db\\u05d0\\u05d9', r'\\u05de\\u05d2\\u05d9\\u05e2'],\n",
    "        HohfeldState.LIBERTY: [r'\\u05de\\u05d5\\u05ea\\u05e8', r'\\u05e8\\u05e9\\u05d5\\u05ea', r'\\u05e4\\u05d8\\u05d5\\u05e8', r'\\u05d9\\u05db\\u05d5\\u05dc'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u05d0\\u05e1\\u05d5\\u05e8', r'\\u05d0\\u05d9\\u05e0\\u05d5 \\u05e8\\u05e9\\u05d0\\u05d9', r'\\u05d0\\u05d9\\u05df.*\\u05d6\\u05db\\u05d5\\u05ea'],\n",
    "    },\n",
    "    'aramaic': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u05d7\\u05d9\\u05d9\\u05d1', r'\\u05de\\u05d7\\u05d5\\u05d9\\u05d1', r'\\u05d1\\u05e2\\u05d9'],\n",
    "        HohfeldState.RIGHT: [r'\\u05d6\\u05db\\u05d5\\u05ea', r'\\u05e8\\u05e9\\u05d0\\u05d9', r'\\u05d6\\u05db\\u05d9'],\n",
    "        HohfeldState.LIBERTY: [r'\\u05e9\\u05e8\\u05d9', r'\\u05de\\u05d5\\u05ea\\u05e8', r'\\u05e4\\u05d8\\u05d5\\u05e8'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u05d0\\u05e1\\u05d5\\u05e8', r'\\u05dc\\u05d0.*\\u05e8\\u05e9\\u05d0\\u05d9'],\n",
    "    },\n",
    "    'classical_chinese': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u5fc5', r'\\u9808', r'\\u7576', r'\\u61c9', r'\\u5b9c'],\n",
    "        HohfeldState.RIGHT: [r'\\u53ef', r'\\u5f97', r'\\u6b0a', r'\\u5b9c'],\n",
    "        HohfeldState.LIBERTY: [r'\\u8a31', r'\\u4efb', r'\\u807d', r'\\u514d'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u4e0d\\u53ef', r'\\u52ff', r'\\u7981', r'\\u83ab', r'\\u975e'],\n",
    "    },\n",
    "    'arabic': {\n",
    "        HohfeldState.OBLIGATION: [r'\\u064a\\u062c\\u0628', r'\\u0648\\u0627\\u062c\\u0628', r'\\u0641\\u0631\\u0636', r'\\u0644\\u0627\\u0632\\u0645', r'\\u0648\\u062c\\u0648\\u0628'],\n",
    "        HohfeldState.RIGHT: [r'\\u062d\\u0642', r'\\u064a\\u062d\\u0642', r'\\u062c\\u0627\\u0626\\u0632', r'\\u064a\\u062c\\u0648\\u0632'],\n",
    "        HohfeldState.LIBERTY: [r'\\u0645\\u0628\\u0627\\u062d', r'\\u062d\\u0644\\u0627\\u0644', r'\\u062c\\u0627\\u0626\\u0632', r'\\u0627\\u0628\\u0627\\u062d'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\u062d\\u0631\\u0627\\u0645', r'\\u0645\\u062d\\u0631\\u0645', r'\\u0645\\u0645\\u0646\\u0648\\u0639', r'\\u0644\\u0627 \\u064a\\u062c\\u0648\\u0632', r'\\u0646\\u0647[\\u064a\\u0649]'],\n",
    "    },\n",
    "    'english': {\n",
    "        HohfeldState.OBLIGATION: [r'\\bmust\\b', r'\\bshall\\b', r'\\bobligat', r'\\bduty', r'\\brequir'],\n",
    "        HohfeldState.RIGHT: [r'\\bright\\b', r'\\bentitle', r'\\bdeserve', r'\\bclaim'],\n",
    "        HohfeldState.LIBERTY: [r'\\bmay\\b', r'\\bpermit', r'\\ballow', r'\\bfree to'],\n",
    "        HohfeldState.NO_RIGHT: [r'\\bforbid', r'\\bprohibit', r'\\bmust not', r'\\bshall not'],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Patterns defined for 5 languages:\")\n",
    "for lang in ALL_BOND_PATTERNS:\n",
    "    n = sum(len(p) for p in ALL_BOND_PATTERNS[lang].values())\n",
    "    print(f\"  {lang}: {n} bond patterns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Load Corpora + Extract Bonds { display-mode: \"form\" }\n",
    "#@markdown Loads all corpora - auto-detects GPU and adjusts sampling\n",
    "\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "import gc\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Check if we should skip processing (data loaded from Drive)\n",
    "if SKIP_PROCESSING:\n",
    "    print(\"=\"*60)\n",
    "    print(\"SKIPPING PROCESSING - Using Drive data\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Count passages by language\n",
    "    by_lang = defaultdict(int)\n",
    "    with open('data/processed/passages.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            p = json.loads(line)\n",
    "            by_lang[p['language']] += 1\n",
    "    \n",
    "    print(\"\\nPassages by language:\")\n",
    "    for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {lang}: {cnt:,}\")\n",
    "    \n",
    "    n_passages = sum(by_lang.values())\n",
    "    print(f\"\\nTotal: {n_passages:,} passages\")\n",
    "\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"LOADING CORPORA\")\n",
    "    print(f\"GPU Tier: {GPU_TIER}\")\n",
    "    print(f\"Max per language: {MAX_PER_LANG:,}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    random.seed(42)\n",
    "    all_passages = []\n",
    "    \n",
    "    # ===== SEFARIA (FIXED) =====\n",
    "    print(\"\\nLoading Sefaria...\")\n",
    "    sefaria_path = Path('data/raw/Sefaria-Export/json')\n",
    "    \n",
    "    CATEGORY_TO_PERIOD = {\n",
    "        'Tanakh': 'BIBLICAL', 'Torah': 'BIBLICAL', 'Prophets': 'BIBLICAL', 'Writings': 'BIBLICAL',\n",
    "        'Mishnah': 'TANNAITIC', 'Tosefta': 'TANNAITIC',\n",
    "        'Talmud': 'AMORAIC', 'Bavli': 'AMORAIC', 'Yerushalmi': 'AMORAIC', 'Midrash': 'AMORAIC',\n",
    "        'Halakhah': 'RISHONIM', 'Kabbalah': 'RISHONIM', 'Philosophy': 'RISHONIM',\n",
    "        'Chasidut': 'ACHRONIM', 'Musar': 'ACHRONIM', 'Responsa': 'ACHRONIM',\n",
    "    }\n",
    "    \n",
    "    hebrew_ps, aramaic_ps = [], []\n",
    "    \n",
    "    if sefaria_path.exists():\n",
    "        for jf in tqdm(list(sefaria_path.rglob('*.json')), desc=\"Sefaria\"):\n",
    "            try:\n",
    "                with open(jf, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            # FIX: Check language field correctly\n",
    "            if data.get('language') != 'he':\n",
    "                continue\n",
    "            \n",
    "            txt = data.get('text', [])\n",
    "            if not txt:\n",
    "                continue\n",
    "            \n",
    "            rel = jf.relative_to(sefaria_path)\n",
    "            cat = str(rel.parts[0]) if rel.parts else 'unknown'\n",
    "            period = CATEGORY_TO_PERIOD.get(cat, 'AMORAIC')\n",
    "            is_talmud = 'Talmud' in str(jf) or cat in ['Bavli', 'Yerushalmi']\n",
    "            lang = 'aramaic' if is_talmud else 'hebrew'\n",
    "            \n",
    "            def flatten(t):\n",
    "                results = []\n",
    "                if isinstance(t, str):\n",
    "                    tc = re.sub(r'<[^>]+>', '', t).strip()\n",
    "                    if 20 <= len(tc) <= 2000:\n",
    "                        hc = sum(1 for c in tc if '\\u0590' <= c <= '\\u05FF')\n",
    "                        if hc > 5:\n",
    "                            pid = hashlib.md5((jf.stem + tc[:30]).encode()).hexdigest()[:12]\n",
    "                            results.append({'id': f'sef_{pid}', 'text': tc, 'lang': lang, 'period': period})\n",
    "                elif isinstance(t, (dict, list)):\n",
    "                    for v in (t.values() if isinstance(t, dict) else t):\n",
    "                        results.extend(flatten(v))\n",
    "                return results\n",
    "            \n",
    "            ps = flatten(txt)\n",
    "            if lang == 'hebrew':\n",
    "                hebrew_ps.extend(ps)\n",
    "            else:\n",
    "                aramaic_ps.extend(ps)\n",
    "    \n",
    "        # Sample down\n",
    "        random.shuffle(hebrew_ps)\n",
    "        random.shuffle(aramaic_ps)\n",
    "        hebrew_ps = hebrew_ps[:MAX_PER_LANG]\n",
    "        aramaic_ps = aramaic_ps[:MAX_PER_LANG]\n",
    "        all_passages.extend(hebrew_ps)\n",
    "        all_passages.extend(aramaic_ps)\n",
    "        print(f\"  Hebrew: {len(hebrew_ps):,}, Aramaic: {len(aramaic_ps):,}\")\n",
    "        del hebrew_ps, aramaic_ps\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print(\"  ERROR: Sefaria not found!\")\n",
    "    \n",
    "    # ===== CHINESE =====\n",
    "    print(\"\\nLoading Chinese...\")\n",
    "    try:\n",
    "        with open('data/raw/chinese/chinese_native.json', 'r', encoding='utf-8') as f:\n",
    "            chinese_data = json.load(f)\n",
    "        for item in chinese_data:\n",
    "            all_passages.append({'id': item['id'], 'text': item['text'], 'lang': 'classical_chinese', 'period': item['period']})\n",
    "        print(f\"  Chinese: {len(chinese_data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "    \n",
    "    # ===== ISLAMIC =====\n",
    "    print(\"\\nLoading Islamic...\")\n",
    "    try:\n",
    "        with open('data/raw/islamic/islamic_native.json', 'r', encoding='utf-8') as f:\n",
    "            islamic_data = json.load(f)\n",
    "        for item in islamic_data:\n",
    "            all_passages.append({'id': item['id'], 'text': item['text'], 'lang': 'arabic', 'period': item['period']})\n",
    "        print(f\"  Arabic: {len(islamic_data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "    \n",
    "    # ===== DEAR ABBY =====\n",
    "    print(\"\\nLoading Dear Abby...\")\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv('data/raw/dear_abby.csv')\n",
    "        abby_count = 0\n",
    "        for idx, row in df.iterrows():\n",
    "            q = str(row.get('question_only', ''))\n",
    "            if q != 'nan' and 50 <= len(q) <= 2000:\n",
    "                all_passages.append({'id': f'abby_{idx}', 'text': q, 'lang': 'english', 'period': 'DEAR_ABBY'})\n",
    "                abby_count += 1\n",
    "        print(f\"  English: {abby_count:,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTOTAL: {len(all_passages):,}\")\n",
    "    \n",
    "    # Count by language\n",
    "    by_lang = defaultdict(int)\n",
    "    for p in all_passages:\n",
    "        by_lang[p['lang']] += 1\n",
    "    print(\"\\nBy language:\")\n",
    "    for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {lang}: {cnt:,}\")\n",
    "    \n",
    "    # ===== EXTRACT BONDS =====\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXTRACTING BONDS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    def extract_bond(text, language):\n",
    "        tn = normalize_text(text, language)\n",
    "        for bt, pats in ALL_BOND_PATTERNS.get(language, {}).items():\n",
    "            if any(re.search(p, tn) for p in pats):\n",
    "                return bt.name\n",
    "        return 'NONE'\n",
    "    \n",
    "    def extract_hohfeld(text, language):\n",
    "        tn = normalize_text(text, language)\n",
    "        for st, pats in ALL_HOHFELD_PATTERNS.get(language, {}).items():\n",
    "            if any(re.search(p, tn) for p in pats):\n",
    "                return st.name\n",
    "        return None\n",
    "    \n",
    "    bond_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    with open('data/processed/passages.jsonl', 'w', encoding='utf-8') as fp, \\\n",
    "         open('data/processed/bonds.jsonl', 'w', encoding='utf-8') as fb:\n",
    "        \n",
    "        for p in tqdm(all_passages, desc=\"Extracting\"):\n",
    "            bond = extract_bond(p['text'], p['lang'])\n",
    "            hohfeld = extract_hohfeld(p['text'], p['lang'])\n",
    "            bond_counts[p['lang']][bond] += 1\n",
    "            \n",
    "            fp.write(json.dumps({\n",
    "                'id': p['id'], 'text': p['text'], 'language': p['lang'],\n",
    "                'time_period': p['period'], 'source': 'x', 'source_type': 'sefaria' if 'sef_' in p['id'] else 'other', 'century': 0\n",
    "            }, ensure_ascii=False) + '\\n')\n",
    "            \n",
    "            fb.write(json.dumps({\n",
    "                'passage_id': p['id'],\n",
    "                'bonds': {'primary_bond': bond, 'all_bonds': [bond], 'hohfeld': hohfeld, 'language': p['lang']}\n",
    "            }, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    # Coverage report\n",
    "    print(\"\\nLabel coverage:\")\n",
    "    for lang in sorted(bond_counts.keys()):\n",
    "        total = sum(bond_counts[lang].values())\n",
    "        none_ct = bond_counts[lang].get('NONE', 0)\n",
    "        cov = (total - none_ct) / total * 100 if total else 0\n",
    "        print(f\"  {lang}: {cov:.1f}% labeled ({total-none_ct:,}/{total:,})\")\n",
    "    \n",
    "    # ===== SAVE TO DRIVE =====\n",
    "    print(\"\\nSaving to Drive...\")\n",
    "    shutil.copy('data/processed/passages.jsonl', f'{SAVE_DIR}/passages.jsonl')\n",
    "    shutil.copy('data/processed/bonds.jsonl', f'{SAVE_DIR}/bonds.jsonl')\n",
    "    print(\"  Saved!\")\n",
    "    \n",
    "    n_passages = len(all_passages)\n",
    "    del all_passages\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Cell 4 complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. Generate Splits { display-mode: \"form\" }\n",
    "#@markdown Creates train/test splits for cross-lingual experiments\n",
    "\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING SPLITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if splits already exist from Drive\n",
    "if os.path.exists('data/splits/all_splits.json'):\n",
    "    print(\"\\nSplits already loaded from Drive\")\n",
    "    with open('data/splits/all_splits.json') as f:\n",
    "        all_splits = json.load(f)\n",
    "    for name, split in all_splits.items():\n",
    "        print(f\"  {name}: train={split['train_size']:,}, test={split['test_size']:,}\")\n",
    "else:\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Read passage metadata\n",
    "    passage_meta = []\n",
    "    with open('data/processed/passages.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            p = json.loads(line)\n",
    "            passage_meta.append(p)\n",
    "    \n",
    "    print(f\"Total passages: {len(passage_meta):,}\")\n",
    "    \n",
    "    by_lang = defaultdict(list)\n",
    "    by_period = defaultdict(list)\n",
    "    for p in passage_meta:\n",
    "        by_lang[p['language']].append(p['id'])\n",
    "        by_period[p['time_period']].append(p['id'])\n",
    "    \n",
    "    print(\"\\nBy language:\")\n",
    "    for lang, ids in sorted(by_lang.items(), key=lambda x: -len(x[1])):\n",
    "        print(f\"  {lang}: {len(ids):,}\")\n",
    "    \n",
    "    all_splits = {}\n",
    "    \n",
    "    # ===== SPLIT 1: Hebrew -> Others =====\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"SPLIT 1: HEBREW -> OTHERS\")\n",
    "    hebrew_ids = by_lang.get('hebrew', [])\n",
    "    other_ids = [p['id'] for p in passage_meta if p['language'] != 'hebrew']\n",
    "    random.shuffle(hebrew_ids)\n",
    "    random.shuffle(other_ids)\n",
    "    \n",
    "    all_splits['hebrew_to_others'] = {\n",
    "        'train_ids': hebrew_ids,\n",
    "        'test_ids': other_ids,\n",
    "        'train_size': len(hebrew_ids),\n",
    "        'test_size': len(other_ids),\n",
    "    }\n",
    "    print(f\"  Train (Hebrew): {len(hebrew_ids):,}\")\n",
    "    print(f\"  Test (Others): {len(other_ids):,}\")\n",
    "    \n",
    "    # ===== SPLIT 2: Semitic -> Non-Semitic =====\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"SPLIT 2: SEMITIC -> NON-SEMITIC\")\n",
    "    semitic_ids = by_lang.get('hebrew', []) + by_lang.get('aramaic', []) + by_lang.get('arabic', [])\n",
    "    non_semitic_ids = by_lang.get('classical_chinese', []) + by_lang.get('english', [])\n",
    "    random.shuffle(semitic_ids)\n",
    "    random.shuffle(non_semitic_ids)\n",
    "    \n",
    "    all_splits['semitic_to_non_semitic'] = {\n",
    "        'train_ids': semitic_ids,\n",
    "        'test_ids': non_semitic_ids,\n",
    "        'train_size': len(semitic_ids),\n",
    "        'test_size': len(non_semitic_ids),\n",
    "    }\n",
    "    print(f\"  Train (Semitic): {len(semitic_ids):,}\")\n",
    "    print(f\"  Test (Non-Semitic): {len(non_semitic_ids):,}\")\n",
    "    \n",
    "    # ===== SPLIT 3: Ancient -> Modern =====\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"SPLIT 3: ANCIENT -> MODERN\")\n",
    "    ancient_periods = {'BIBLICAL', 'TANNAITIC', 'AMORAIC', 'CONFUCIAN', 'DAOIST', 'QURANIC', 'HADITH'}\n",
    "    modern_periods = {'RISHONIM', 'ACHRONIM', 'DEAR_ABBY'}\n",
    "    \n",
    "    ancient_ids = [p['id'] for p in passage_meta if p['time_period'] in ancient_periods]\n",
    "    modern_ids = [p['id'] for p in passage_meta if p['time_period'] in modern_periods]\n",
    "    random.shuffle(ancient_ids)\n",
    "    random.shuffle(modern_ids)\n",
    "    \n",
    "    all_splits['ancient_to_modern'] = {\n",
    "        'train_ids': ancient_ids,\n",
    "        'test_ids': modern_ids,\n",
    "        'train_size': len(ancient_ids),\n",
    "        'test_size': len(modern_ids),\n",
    "    }\n",
    "    print(f\"  Train (Ancient): {len(ancient_ids):,}\")\n",
    "    print(f\"  Test (Modern): {len(modern_ids):,}\")\n",
    "    \n",
    "    # ===== SPLIT 4: Mixed Baseline =====\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"SPLIT 4: MIXED BASELINE\")\n",
    "    all_ids = [p['id'] for p in passage_meta]\n",
    "    random.shuffle(all_ids)\n",
    "    split_idx = int(0.7 * len(all_ids))\n",
    "    \n",
    "    all_splits['mixed_baseline'] = {\n",
    "        'train_ids': all_ids[:split_idx],\n",
    "        'test_ids': all_ids[split_idx:],\n",
    "        'train_size': split_idx,\n",
    "        'test_size': len(all_ids) - split_idx,\n",
    "    }\n",
    "    print(f\"  Train: {split_idx:,}\")\n",
    "    print(f\"  Test: {len(all_ids) - split_idx:,}\")\n",
    "    \n",
    "    # Save splits\n",
    "    with open('data/splits/all_splits.json', 'w') as f:\n",
    "        json.dump(all_splits, f, indent=2)\n",
    "    \n",
    "    # Save to Drive\n",
    "    shutil.copy('data/splits/all_splits.json', f'{SAVE_DIR}/all_splits.json')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Splits saved to local and Drive\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. Model Architecture { display-mode: \"form\" }\n",
    "#@markdown BIP model with adversarial heads and complete Hohfeld support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Index mappings\n",
    "BOND_TO_IDX = {bt.name: i for i, bt in enumerate(BondType)}\n",
    "IDX_TO_BOND = {i: bt.name for i, bt in enumerate(BondType)}\n",
    "LANG_TO_IDX = {'hebrew': 0, 'aramaic': 1, 'classical_chinese': 2, 'arabic': 3, 'english': 4}\n",
    "IDX_TO_LANG = {i: l for l, i in LANG_TO_IDX.items()}\n",
    "PERIOD_TO_IDX = {'BIBLICAL': 0, 'TANNAITIC': 1, 'AMORAIC': 2, 'RISHONIM': 3, 'ACHRONIM': 4,\n",
    "                 'CONFUCIAN': 5, 'DAOIST': 6, 'QURANIC': 7, 'HADITH': 8, 'DEAR_ABBY': 9}\n",
    "IDX_TO_PERIOD = {i: p for p, i in PERIOD_TO_IDX.items()}\n",
    "HOHFELD_TO_IDX = {hs.name: i for i, hs in enumerate(HohfeldState)}\n",
    "IDX_TO_HOHFELD = {i: hs.name for i, hs in enumerate(HohfeldState)}\n",
    "\n",
    "class GradientReversalLayer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.alpha, None\n",
    "\n",
    "class BIPModel(nn.Module):\n",
    "    def __init__(self, z_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        hidden = self.encoder.config.hidden_size  # 384\n",
    "        \n",
    "        # Projection to z_bond space\n",
    "        self.z_proj = nn.Sequential(\n",
    "            nn.Linear(hidden, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, z_dim),\n",
    "        )\n",
    "        \n",
    "        # Task heads\n",
    "        self.bond_head = nn.Linear(z_dim, len(BondType))\n",
    "        self.hohfeld_head = nn.Linear(z_dim, len(HohfeldState))\n",
    "        \n",
    "        # Adversarial heads\n",
    "        self.language_head = nn.Linear(z_dim, len(LANG_TO_IDX))\n",
    "        self.period_head = nn.Linear(z_dim, len(PERIOD_TO_IDX))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, adv_lambda=1.0):\n",
    "        enc = self.encoder(input_ids, attention_mask)\n",
    "        pooled = enc.last_hidden_state[:, 0]  # CLS token\n",
    "        \n",
    "        z = self.z_proj(pooled)\n",
    "        \n",
    "        # Bond prediction (main task)\n",
    "        bond_pred = self.bond_head(z)\n",
    "        hohfeld_pred = self.hohfeld_head(z)\n",
    "        \n",
    "        # Adversarial predictions (gradient reversal)\n",
    "        z_rev = GradientReversalLayer.apply(z, adv_lambda)\n",
    "        language_pred = self.language_head(z_rev)\n",
    "        period_pred = self.period_head(z_rev)\n",
    "        \n",
    "        return {\n",
    "            'bond_pred': bond_pred,\n",
    "            'hohfeld_pred': hohfeld_pred,\n",
    "            'language_pred': language_pred,\n",
    "            'period_pred': period_pred,\n",
    "            'z': z,\n",
    "        }\n",
    "\n",
    "# Dataset with Hohfeld support\n",
    "class NativeDataset(Dataset):\n",
    "    def __init__(self, ids_set, passages_file, bonds_file, tokenizer, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = []\n",
    "        \n",
    "        with open(passages_file) as fp, open(bonds_file) as fb:\n",
    "            for p_line, b_line in tqdm(zip(fp, fb), desc=\"Loading\", unit=\"line\"):\n",
    "                p = json.loads(p_line)\n",
    "                b = json.loads(b_line)\n",
    "                if p['id'] in ids_set and b['passage_id'] == p['id']:\n",
    "                    self.data.append({\n",
    "                        'text': p['text'][:1000],\n",
    "                        'language': p['language'],\n",
    "                        'period': p['time_period'],\n",
    "                        'bond': b['bonds']['primary_bond'],\n",
    "                        'hohfeld': b['bonds']['hohfeld'],\n",
    "                    })\n",
    "        print(f\"  Loaded {len(self.data):,} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        enc = self.tokenizer(item['text'], truncation=True, max_length=self.max_len,\n",
    "                            padding='max_length', return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'bond_label': BOND_TO_IDX.get(item['bond'], 9),\n",
    "            'language_label': LANG_TO_IDX.get(item['language'], 4),\n",
    "            'period_label': PERIOD_TO_IDX.get(item['period'], 9),\n",
    "            'hohfeld_label': HOHFELD_TO_IDX.get(item['hohfeld'], 0) if item['hohfeld'] else 0,\n",
    "            'language': item['language'],\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
    "        'bond_labels': torch.tensor([x['bond_label'] for x in batch]),\n",
    "        'language_labels': torch.tensor([x['language_label'] for x in batch]),\n",
    "        'period_labels': torch.tensor([x['period_label'] for x in batch]),\n",
    "        'hohfeld_labels': torch.tensor([x['hohfeld_label'] for x in batch]),\n",
    "        'languages': [x['language'] for x in batch],\n",
    "    }\n",
    "\n",
    "print(\"Model architecture defined\")\n",
    "print(f\"  Bond classes: {len(BondType)}\")\n",
    "print(f\"  Hohfeld states: {len(HohfeldState)}\")\n",
    "print(f\"  Languages: {len(LANG_TO_IDX)}\")\n",
    "print(f\"  Periods: {len(PERIOD_TO_IDX)}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7. Train BIP Model { display-mode: \"form\" }\n",
    "#@markdown Training with tuned adversarial weights and hardware-optimized parameters\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import gc\n",
    "\n",
    "#@markdown **Splits to train:**\n",
    "TRAIN_HEBREW_TO_OTHERS = True  #@param {type:\"boolean\"}\n",
    "TRAIN_SEMITIC_TO_NON_SEMITIC = True  #@param {type:\"boolean\"}\n",
    "TRAIN_ANCIENT_TO_MODERN = True  #@param {type:\"boolean\"}\n",
    "TRAIN_MIXED_BASELINE = True  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown **Hyperparameters:**\n",
    "LANG_WEIGHT = 0.01  #@param {type:\"number\"}\n",
    "PERIOD_WEIGHT = 0.01  #@param {type:\"number\"}\n",
    "N_EPOCHS = 5  #@param {type:\"integer\"}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING BIP MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nHardware-optimized settings:\")\n",
    "print(f\"  GPU Tier:     {GPU_TIER}\")\n",
    "print(f\"  Batch size:   {BATCH_SIZE}\")\n",
    "print(f\"  Workers:      {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate: {LR:.2e}\")\n",
    "print(f\"  Adv weights:  lang={LANG_WEIGHT}, period={PERIOD_WEIGHT}\")\n",
    "print(\"(0.01 prevents loss explosion while maintaining invariance)\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "with open('data/splits/all_splits.json') as f:\n",
    "    all_splits = json.load(f)\n",
    "\n",
    "splits_to_train = []\n",
    "if TRAIN_HEBREW_TO_OTHERS: splits_to_train.append('hebrew_to_others')\n",
    "if TRAIN_SEMITIC_TO_NON_SEMITIC: splits_to_train.append('semitic_to_non_semitic')\n",
    "if TRAIN_ANCIENT_TO_MODERN: splits_to_train.append('ancient_to_modern')\n",
    "if TRAIN_MIXED_BASELINE: splits_to_train.append('mixed_baseline')\n",
    "\n",
    "print(f\"\\nTraining {len(splits_to_train)} splits: {splits_to_train}\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for split_idx, split_name in enumerate(splits_to_train):\n",
    "    split_start = time.time()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"[{split_idx+1}/{len(splits_to_train)}] {split_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    split = all_splits[split_name]\n",
    "    print(f\"Train: {split['train_size']:,} | Test: {split['test_size']:,}\")\n",
    "    \n",
    "    if split['test_size'] < 50:\n",
    "        print(\"Test set too small - skipping\")\n",
    "        continue\n",
    "    \n",
    "    model = BIPModel().to(device)\n",
    "    \n",
    "    train_dataset = NativeDataset(set(split['train_ids']), 'data/processed/passages.jsonl',\n",
    "                                   'data/processed/bonds.jsonl', tokenizer)\n",
    "    test_dataset = NativeDataset(set(split['test_ids'][:MAX_TEST_SAMPLES]), 'data/processed/passages.jsonl',\n",
    "                                  'data/processed/bonds.jsonl', tokenizer)\n",
    "    \n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"ERROR: No training data!\")\n",
    "        continue\n",
    "    \n",
    "    # Use hardware-optimized batch size\n",
    "    actual_batch = min(BATCH_SIZE, max(32, len(train_dataset) // 20))\n",
    "    print(f\"Actual batch size: {actual_batch}\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=actual_batch, shuffle=True,\n",
    "                              collate_fn=collate_fn, drop_last=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=actual_batch*2, shuffle=False,\n",
    "                             collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "    \n",
    "    def get_adv_lambda(epoch, warmup=2):\n",
    "        if epoch <= warmup:\n",
    "            return 0.1 + 0.9 * (epoch / warmup)\n",
    "        return 1.0\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            bond_labels = batch['bond_labels'].to(device)\n",
    "            language_labels = batch['language_labels'].to(device)\n",
    "            period_labels = batch['period_labels'].to(device)\n",
    "            \n",
    "            adv_lambda = get_adv_lambda(epoch)\n",
    "            \n",
    "            # Use new autocast API\n",
    "            with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "                out = model(input_ids, attention_mask, adv_lambda=adv_lambda)\n",
    "                \n",
    "                loss_bond = F.cross_entropy(out['bond_pred'], bond_labels)\n",
    "                loss_lang = F.cross_entropy(out['language_pred'], language_labels)\n",
    "                loss_period = F.cross_entropy(out['period_pred'], period_labels)\n",
    "            \n",
    "            loss = loss_bond + LANG_WEIGHT * loss_lang + PERIOD_WEIGHT * loss_period\n",
    "            \n",
    "            if USE_AMP and scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / n_batches\n",
    "        print(f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f})\")\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), f'models/checkpoints/best_{split_name}.pt')\n",
    "            torch.save(model.state_dict(), f'{SAVE_DIR}/best_{split_name}.pt')\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.load_state_dict(torch.load(f'models/checkpoints/best_{split_name}.pt'))\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = {'bond': [], 'lang': []}\n",
    "    all_labels = {'bond': [], 'lang': []}\n",
    "    all_languages = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n",
    "            all_preds['bond'].extend(out['bond_pred'].argmax(-1).cpu().tolist())\n",
    "            all_preds['lang'].extend(out['language_pred'].argmax(-1).cpu().tolist())\n",
    "            all_labels['bond'].extend(batch['bond_labels'].tolist())\n",
    "            all_labels['lang'].extend(batch['language_labels'].tolist())\n",
    "            all_languages.extend(batch['languages'])\n",
    "    \n",
    "    bond_f1 = f1_score(all_labels['bond'], all_preds['bond'], average='macro', zero_division=0)\n",
    "    bond_acc = sum(p == l for p, l in zip(all_preds['bond'], all_labels['bond'])) / len(all_preds['bond'])\n",
    "    lang_acc = sum(p == l for p, l in zip(all_preds['lang'], all_labels['lang'])) / len(all_preds['lang'])\n",
    "    \n",
    "    # Per-language F1\n",
    "    lang_f1 = {}\n",
    "    for lang in set(all_languages):\n",
    "        mask = [l == lang for l in all_languages]\n",
    "        if sum(mask) > 10:\n",
    "            preds = [p for p, m in zip(all_preds['bond'], mask) if m]\n",
    "            labels = [l for l, m in zip(all_labels['bond'], mask) if m]\n",
    "            lang_f1[lang] = {'f1': f1_score(labels, preds, average='macro', zero_division=0), 'n': sum(mask)}\n",
    "    \n",
    "    all_results[split_name] = {\n",
    "        'bond_f1_macro': bond_f1,\n",
    "        'bond_acc': bond_acc,\n",
    "        'language_acc': lang_acc,\n",
    "        'per_language_f1': lang_f1,\n",
    "        'training_time': time.time() - split_start\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{split_name} RESULTS:\")\n",
    "    print(f\"  Bond F1 (macro): {bond_f1:.3f} ({bond_f1/0.1:.1f}x chance)\")\n",
    "    print(f\"  Bond accuracy:   {bond_acc:.1%}\")\n",
    "    print(f\"  Language acc:    {lang_acc:.1%} (want ~20% = invariant)\")\n",
    "    print(\"  Per-language:\")\n",
    "    for lang, m in sorted(lang_f1.items(), key=lambda x: -x[1]['n']):\n",
    "        print(f\"    {lang:20s}: F1={m['f1']:.3f} (n={m['n']:,})\")\n",
    "    \n",
    "    # GPU memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        mem = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"\\n  GPU memory: {mem:.1f} GB / {VRAM_GB:.1f} GB ({mem/VRAM_GB*100:.0f}%)\")\n",
    "    \n",
    "    del model, train_dataset, test_dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8. Linear Probe Test { display-mode: \"form\" }\n",
    "#@markdown Tests if z_bond encodes language/period (should be low = invariant)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LINEAR PROBE TEST\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nIf probe accuracy is NEAR CHANCE, representation is INVARIANT\")\n",
    "print(\"(This is what we want for BIP)\")\n",
    "\n",
    "probe_results = {}\n",
    "\n",
    "for split_name in ['hebrew_to_others', 'semitic_to_non_semitic']:\n",
    "    model_path = f'{SAVE_DIR}/best_{split_name}.pt'\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"\\nSkipping {split_name} - no saved model\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PROBE: {split_name}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    model = BIPModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    test_ids = set(all_splits[split_name]['test_ids'][:5000])\n",
    "    test_dataset = NativeDataset(test_ids, 'data/processed/passages.jsonl',\n",
    "                                  'data/processed/bonds.jsonl', tokenizer)\n",
    "    \n",
    "    if len(test_dataset) < 50:\n",
    "        print(f\"  Skip - only {len(test_dataset)} samples\")\n",
    "        continue\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    all_z, all_lang, all_period = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Extract\"):\n",
    "            out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n",
    "            all_z.append(out['z'].cpu().numpy())\n",
    "            all_lang.extend(batch['language_labels'].tolist())\n",
    "            all_period.extend(batch['period_labels'].tolist())\n",
    "    \n",
    "    X = np.vstack(all_z)\n",
    "    y_lang = np.array(all_lang)\n",
    "    y_period = np.array(all_period)\n",
    "    \n",
    "    scaler_probe = StandardScaler()\n",
    "    X_scaled = scaler_probe.fit_transform(X)\n",
    "    \n",
    "    # Train/test split for probes\n",
    "    n = len(X)\n",
    "    idx = np.random.permutation(n)\n",
    "    train_idx, test_idx = idx[:int(0.7*n)], idx[int(0.7*n):]\n",
    "    \n",
    "    # Language probe - check for multiple classes\n",
    "    unique_langs = np.unique(y_lang[test_idx])\n",
    "    if len(unique_langs) < 2:\n",
    "        print(f\"  SKIP language probe - only {len(unique_langs)} class\")\n",
    "        lang_acc = 1.0 / max(1, len(np.unique(y_lang)))\n",
    "        lang_chance = lang_acc\n",
    "    else:\n",
    "        lang_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        lang_probe.fit(X_scaled[train_idx], y_lang[train_idx])\n",
    "        lang_acc = (lang_probe.predict(X_scaled[test_idx]) == y_lang[test_idx]).mean()\n",
    "        lang_chance = 1.0 / len(unique_langs)\n",
    "    \n",
    "    # Period probe - same check\n",
    "    unique_periods = np.unique(y_period[test_idx])\n",
    "    if len(unique_periods) < 2:\n",
    "        print(f\"  SKIP period probe - only {len(unique_periods)} class\")\n",
    "        period_acc = 1.0 / max(1, len(np.unique(y_period)))\n",
    "        period_chance = period_acc\n",
    "    else:\n",
    "        period_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        period_probe.fit(X_scaled[train_idx], y_period[train_idx])\n",
    "        period_acc = (period_probe.predict(X_scaled[test_idx]) == y_period[test_idx]).mean()\n",
    "        period_chance = 1.0 / len(unique_periods)\n",
    "    \n",
    "    lang_status = \"INVARIANT\" if lang_acc < lang_chance + 0.15 else \"NOT invariant\"\n",
    "    period_status = \"INVARIANT\" if period_acc < period_chance + 0.15 else \"NOT invariant\"\n",
    "    \n",
    "    probe_results[split_name] = {\n",
    "        'language_acc': lang_acc,\n",
    "        'language_chance': lang_chance,\n",
    "        'language_status': lang_status,\n",
    "        'period_acc': period_acc,\n",
    "        'period_chance': period_chance,\n",
    "        'period_status': period_status,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nRESULTS:\")\n",
    "    print(f\"  Language: {lang_acc:.1%} (chance: {lang_chance:.1%}) -> {lang_status}\")\n",
    "    print(f\"  Period:   {period_acc:.1%} (chance: {period_chance:.1%}) -> {period_status}\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Probe tests complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9. Final Results { display-mode: \"form\" }\n",
    "#@markdown Comprehensive summary with verdict\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL BIP EVALUATION (v10.1)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nHardware: {GPU_TIER} ({VRAM_GB:.0f}GB VRAM, {RAM_GB:.0f}GB RAM)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"CROSS-DOMAIN TRANSFER RESULTS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "successful_splits = []\n",
    "for name, r in all_results.items():\n",
    "    ratio = r['bond_f1_macro'] / 0.1\n",
    "    lang_acc = r['language_acc']\n",
    "    \n",
    "    transfer_ok = ratio > 1.3\n",
    "    invariant_ok = lang_acc < 0.35  # Near chance (20%)\n",
    "    \n",
    "    status = \"SUCCESS\" if (transfer_ok and invariant_ok) else \"PARTIAL\" if transfer_ok else \"FAIL\"\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Bond F1:     {r['bond_f1_macro']:.3f} ({ratio:.1f}x chance) {'OK' if transfer_ok else 'WEAK'}\")\n",
    "    print(f\"  Language:    {lang_acc:.1%} {'INVARIANT' if invariant_ok else 'LEAKING'}\")\n",
    "    print(f\"  -> {status}\")\n",
    "    \n",
    "    if transfer_ok and invariant_ok:\n",
    "        successful_splits.append(name)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"VERDICT\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "n_success = len(successful_splits)\n",
    "if n_success >= 2:\n",
    "    verdict = \"STRONGLY_SUPPORTED\"\n",
    "    msg = \"Multiple independent transfer paths demonstrate universal structure\"\n",
    "elif n_success >= 1:\n",
    "    verdict = \"SUPPORTED\"\n",
    "    msg = \"At least one transfer path works\"\n",
    "elif any(r['bond_f1_macro'] > 0.13 for r in all_results.values()):\n",
    "    verdict = \"PARTIAL\"\n",
    "    msg = \"Some transfer signal, but not robust\"\n",
    "else:\n",
    "    verdict = \"INCONCLUSIVE\"\n",
    "    msg = \"No clear transfer demonstrated\"\n",
    "\n",
    "print(f\"\\n  Successful transfers: {n_success}/{len(all_results)}\")\n",
    "print(f\"  Splits: {successful_splits if successful_splits else 'None'}\")\n",
    "print(f\"\\n  VERDICT: {verdict}\")\n",
    "print(f\"  {msg}\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    'all_results': all_results,\n",
    "    'probe_results': probe_results if 'probe_results' in dir() else {},\n",
    "    'successful_splits': successful_splits,\n",
    "    'verdict': verdict,\n",
    "    'hardware': {'gpu': GPU_TIER, 'vram_gb': VRAM_GB, 'ram_gb': RAM_GB},\n",
    "    'settings': {'batch_size': BATCH_SIZE, 'max_per_lang': MAX_PER_LANG, 'num_workers': NUM_WORKERS},\n",
    "    'experiment_time': time.time() - EXPERIMENT_START,\n",
    "}\n",
    "\n",
    "with open('results/final_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "shutil.copy('results/final_results.json', f'{SAVE_DIR}/final_results.json')\n",
    "\n",
    "print(f\"\\nTotal time: {(time.time() - EXPERIMENT_START)/60:.1f} minutes\")\n",
    "print(\"Results saved to Drive!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10. Download Results { display-mode: \"form\" }\n",
    "#@markdown Download all models and results\n",
    "\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "print(\"Creating download package...\")\n",
    "\n",
    "with zipfile.ZipFile('BIP_v10.1_results.zip', 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    # Results\n",
    "    if os.path.exists('results/final_results.json'):\n",
    "        zf.write('results/final_results.json')\n",
    "    \n",
    "    # Models (from Drive)\n",
    "    for f in os.listdir(SAVE_DIR):\n",
    "        if f.endswith('.pt'):\n",
    "            zf.write(f'{SAVE_DIR}/{f}', f'models/{f}')\n",
    "    \n",
    "    # Config\n",
    "    if os.path.exists('data/splits/all_splits.json'):\n",
    "        zf.write('data/splits/all_splits.json')\n",
    "\n",
    "print(\"\\nDownload ready!\")\n",
    "files.download('BIP_v10.1_results.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
