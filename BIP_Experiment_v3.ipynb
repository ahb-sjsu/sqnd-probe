{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BIP Temporal Invariance Experiment\n",
        "\n",
        "**Testing the Bond Invariance Principle across 2000+ years of moral reasoning**\n",
        "\n",
        "This experiment tests whether moral cognition has invariant structure by:\n",
        "1. Training on ancient Hebrew ethical texts (Sefaria corpus, ~500 BCE - 1800 CE)\n",
        "2. Testing transfer to modern American advice columns (Dear Abby, 1956-2020)\n",
        "\n",
        "**Hypothesis**: If BIP holds, bond-level features should transfer across 2000 years with no accuracy drop.\n",
        "\n",
        "---\n",
        "\n",
        "## Setup Instructions\n",
        "1. **Runtime -> Change runtime type -> GPU (T4) or TPU (v5e)**\n",
        "2. Run cells in order - each shows progress in real-time\n",
        "3. Expected runtime: ~1-2 hours (TPU) or ~2-4 hours (GPU)\n",
        "\n",
        "**Supported Accelerators:**\n",
        "- NVIDIA GPU (T4, V100, A100)\n",
        "- Google TPU (v2, v3, v4, v5e)\n",
        "- CPU (slow, not recommended)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 1. Setup and Install Dependencies { display-mode: \"form\" }\n",
        "#@markdown Installs packages and detects GPU/TPU. Memory-optimized for Colab.\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BIP TEMPORAL INVARIANCE EXPERIMENT\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Progress tracker\n",
        "TASKS = [\n",
        "    \"Install dependencies\",\n",
        "    \"Clone Sefaria corpus (~8GB)\",\n",
        "    \"Clone sqnd-probe repo (Dear Abby data)\",\n",
        "    \"Preprocess corpora\",\n",
        "    \"Extract bond structures\",\n",
        "    \"Generate train/test splits\",\n",
        "    \"Train BIP model\",\n",
        "    \"Evaluate results\"\n",
        "]\n",
        "task_status = {task: \"pending\" for task in TASKS}\n",
        "\n",
        "def print_progress():\n",
        "    print()\n",
        "    print(\"-\" * 50)\n",
        "    print(\"EXPERIMENT PROGRESS:\")\n",
        "    print(\"-\" * 50)\n",
        "    for task in TASKS:\n",
        "        status = task_status[task]\n",
        "        if status == \"done\":\n",
        "            mark = \"[X]\"\n",
        "        elif status == \"running\":\n",
        "            mark = \"[>]\"\n",
        "        else:\n",
        "            mark = \"[ ]\"\n",
        "        print(f\"  {mark} {task}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(flush=True)\n",
        "\n",
        "def mark_task(task, status):\n",
        "    task_status[task] = status\n",
        "    print_progress()\n",
        "\n",
        "print_progress()\n",
        "\n",
        "mark_task(\"Install dependencies\", \"running\")\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Install dependencies - MINIMAL set to save memory\n",
        "print(\"Installing minimal dependencies...\")\n",
        "deps = [\n",
        "    \"transformers\",\n",
        "    \"torch\", \n",
        "    \"sentence-transformers\",\n",
        "    \"pandas\",\n",
        "    \"tqdm\",\n",
        "    \"psutil\"\n",
        "]\n",
        "\n",
        "for dep in deps:\n",
        "    print(f\"  Installing {dep}...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", dep])\n",
        "\n",
        "print()\n",
        "\n",
        "# Detect accelerator - WITHOUT importing tensorflow\n",
        "USE_TPU = False\n",
        "TPU_TYPE = None\n",
        "\n",
        "# Check for TPU\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    USE_TPU = True\n",
        "    TPU_TYPE = \"TPU (Colab)\"\n",
        "    print(\"TPU detected!\")\n",
        "\n",
        "# Check for GPU\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    ACCELERATOR = f\"GPU: {gpu_name} ({gpu_mem:.1f}GB)\"\n",
        "    device = torch.device(\"cuda\")\n",
        "elif USE_TPU:\n",
        "    ACCELERATOR = TPU_TYPE\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "else:\n",
        "    ACCELERATOR = \"CPU (slow!)\"\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Accelerator: {ACCELERATOR}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Memory status\n",
        "import psutil\n",
        "mem = psutil.virtual_memory()\n",
        "print(f\"System RAM: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU RAM: {torch.cuda.memory_allocated()/1e9:.1f}/{torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n",
        "\n",
        "mark_task(\"Install dependencies\", \"done\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 2. Download Sefaria Corpus (~8GB) { display-mode: \"form\" }\n",
        "#@markdown Downloads the complete Sefaria corpus with real-time git progress.\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "mark_task(\"Clone Sefaria corpus (~8GB)\", \"running\")\n",
        "\n",
        "sefaria_path = 'data/raw/Sefaria-Export'\n",
        "\n",
        "if not os.path.exists(sefaria_path) or not os.path.exists(f\"{sefaria_path}/json\"):\n",
        "    print(\"=\"*60)\n",
        "    print(\"CLONING SEFARIA CORPUS\")\n",
        "    print(\"=\"*60)\n",
        "    print()\n",
        "    print(\"This downloads ~3.5GB and takes 5-15 minutes.\")\n",
        "    print(\"Git's native progress will display below:\")\n",
        "    print(\"-\"*60)\n",
        "    print(flush=True)\n",
        "    \n",
        "    # Use subprocess.Popen for real-time output streaming\n",
        "    process = subprocess.Popen(\n",
        "        ['git', 'clone', '--depth', '1', '--progress',\n",
        "         'https://github.com/Sefaria/Sefaria-Export.git', sefaria_path],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,  # Git writes progress to stderr\n",
        "        text=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    \n",
        "    # Stream output in real-time\n",
        "    for line in process.stdout:\n",
        "        print(line, end='', flush=True)\n",
        "    \n",
        "    process.wait()\n",
        "    \n",
        "    print(\"-\"*60)\n",
        "    if process.returncode == 0:\n",
        "        print(\"\\nSefaria clone COMPLETE!\")\n",
        "    else:\n",
        "        print(f\"\\nERROR: Git clone failed with code {process.returncode}\")\n",
        "        print(\"Try running this cell again, or check your internet connection.\")\n",
        "else:\n",
        "    print(\"Sefaria already exists, skipping download.\")\n",
        "\n",
        "# Verify and count files\n",
        "print()\n",
        "print(\"Verifying download...\")\n",
        "!du -sh {sefaria_path} 2>/dev/null || echo \"Directory not found\"\n",
        "json_count = !find {sefaria_path}/json -name \"*.json\" 2>/dev/null | wc -l\n",
        "print(f\"Sefaria JSON files found: {json_count[0]}\")\n",
        "\n",
        "mark_task(\"Clone Sefaria corpus (~8GB)\", \"done\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 3. Download Dear Abby Dataset { display-mode: \"form\" }\n",
        "#@markdown Downloads the Dear Abby advice column dataset (68,330 entries).\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "mark_task(\"Clone sqnd-probe repo (Dear Abby data)\", \"running\")\n",
        "\n",
        "sqnd_path = 'sqnd-probe-data'\n",
        "if not os.path.exists(sqnd_path):\n",
        "    print(\"Cloning sqnd-probe repo...\")\n",
        "    process = subprocess.Popen(\n",
        "        ['git', 'clone', '--depth', '1', '--progress',\n",
        "         'https://github.com/ahb-sjsu/sqnd-probe.git', sqnd_path],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    for line in process.stdout:\n",
        "        print(line, end='', flush=True)\n",
        "    process.wait()\n",
        "else:\n",
        "    print(\"Repo already cloned.\")\n",
        "\n",
        "# Copy Dear Abby data\n",
        "dear_abby_source = Path('sqnd-probe-data/dear_abby_data/raw_da_qs.csv')\n",
        "dear_abby_path = Path('data/raw/dear_abby.csv')\n",
        "\n",
        "if dear_abby_source.exists():\n",
        "    !cp \"{dear_abby_source}\" \"{dear_abby_path}\"\n",
        "    print(f\"\\nCopied Dear Abby data\")\n",
        "elif not dear_abby_path.exists():\n",
        "    raise FileNotFoundError(\"Dear Abby dataset not found!\")\n",
        "\n",
        "# Verify\n",
        "df_check = pd.read_csv(dear_abby_path)\n",
        "print(f\"\\n\" + \"=\" * 50)\n",
        "print(f\"Dear Abby dataset: {len(df_check):,} entries\")\n",
        "print(f\"Columns: {list(df_check.columns)}\")\n",
        "print(f\"Year range: {df_check['year'].min():.0f} - {df_check['year'].max():.0f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "mark_task(\"Clone sqnd-probe repo (Dear Abby data)\", \"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 4. Define Data Classes and Loaders { display-mode: \"form\" }\n",
        "#@markdown Defines enums, dataclasses, and corpus loaders.\n",
        "\n",
        "import json\n",
        "import hashlib\n",
        "import re\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import List, Dict\n",
        "from enum import Enum\n",
        "from collections import defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"Defining data structures...\")\n",
        "\n",
        "class TimePeriod(Enum):\n",
        "    BIBLICAL = 0        # ~1000-500 BCE\n",
        "    SECOND_TEMPLE = 1   # ~500 BCE - 70 CE\n",
        "    TANNAITIC = 2       # ~70-200 CE\n",
        "    AMORAIC = 3         # ~200-500 CE\n",
        "    GEONIC = 4          # ~600-1000 CE\n",
        "    RISHONIM = 5        # ~1000-1500 CE\n",
        "    ACHRONIM = 6        # ~1500-1800 CE\n",
        "    MODERN_HEBREW = 7   # ~1800-present\n",
        "    DEAR_ABBY = 8       # 1956-2020\n",
        "\n",
        "class BondType(Enum):\n",
        "    HARM_PREVENTION = 0\n",
        "    RECIPROCITY = 1\n",
        "    AUTONOMY = 2\n",
        "    PROPERTY = 3\n",
        "    FAMILY = 4\n",
        "    AUTHORITY = 5\n",
        "    EMERGENCY = 6\n",
        "    CONTRACT = 7\n",
        "    CARE = 8\n",
        "    FAIRNESS = 9\n",
        "\n",
        "class HohfeldianState(Enum):\n",
        "    RIGHT = 0\n",
        "    OBLIGATION = 1\n",
        "    LIBERTY = 2\n",
        "    NO_RIGHT = 3\n",
        "\n",
        "@dataclass\n",
        "class Passage:\n",
        "    id: str\n",
        "    text_original: str\n",
        "    text_english: str\n",
        "    time_period: str\n",
        "    century: int\n",
        "    source: str\n",
        "    source_type: str\n",
        "    category: str\n",
        "    language: str = \"hebrew\"\n",
        "    word_count: int = 0\n",
        "    has_dispute: bool = False\n",
        "    consensus_tier: str = \"unknown\"\n",
        "    bond_types: List[str] = field(default_factory=list)\n",
        "    \n",
        "    def to_dict(self):\n",
        "        return asdict(self)\n",
        "\n",
        "CATEGORY_TO_PERIOD = {\n",
        "    'Tanakh': TimePeriod.BIBLICAL,\n",
        "    'Torah': TimePeriod.BIBLICAL,\n",
        "    'Mishnah': TimePeriod.TANNAITIC,\n",
        "    'Tosefta': TimePeriod.TANNAITIC,\n",
        "    'Talmud': TimePeriod.AMORAIC,\n",
        "    'Bavli': TimePeriod.AMORAIC,\n",
        "    'Midrash': TimePeriod.AMORAIC,\n",
        "    'Halakhah': TimePeriod.RISHONIM,\n",
        "    'Chasidut': TimePeriod.ACHRONIM,\n",
        "}\n",
        "\n",
        "PERIOD_TO_CENTURY = {\n",
        "    TimePeriod.BIBLICAL: -6,\n",
        "    TimePeriod.SECOND_TEMPLE: -2,\n",
        "    TimePeriod.TANNAITIC: 2,\n",
        "    TimePeriod.AMORAIC: 4,\n",
        "    TimePeriod.GEONIC: 8,\n",
        "    TimePeriod.RISHONIM: 12,\n",
        "    TimePeriod.ACHRONIM: 17,\n",
        "    TimePeriod.MODERN_HEBREW: 20,\n",
        "}\n",
        "\n",
        "def load_sefaria(base_path: str, max_passages: int = None) -> List[Passage]:\n",
        "    \"\"\"Load Sefaria corpus with progress bar.\"\"\"\n",
        "    passages = []\n",
        "    json_path = Path(base_path) / \"json\"\n",
        "    \n",
        "    if not json_path.exists():\n",
        "        print(f\"Warning: {json_path} not found\")\n",
        "        return []\n",
        "    \n",
        "    json_files = list(json_path.rglob(\"*.json\"))\n",
        "    print(f\"Processing {len(json_files):,} JSON files...\")\n",
        "    \n",
        "    for json_file in tqdm(json_files[:max_passages] if max_passages else json_files,\n",
        "                          desc=\"Loading Sefaria\", unit=\"file\"):\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "        except:\n",
        "            continue\n",
        "        \n",
        "        rel_path = json_file.relative_to(json_path)\n",
        "        category = str(rel_path.parts[0]) if rel_path.parts else \"unknown\"\n",
        "        time_period = CATEGORY_TO_PERIOD.get(category, TimePeriod.AMORAIC)\n",
        "        century = PERIOD_TO_CENTURY.get(time_period, 0)\n",
        "        \n",
        "        if isinstance(data, dict):\n",
        "            hebrew = data.get('he', data.get('text', []))\n",
        "            english = data.get('text', data.get('en', []))\n",
        "            \n",
        "            def flatten(h, e, ref=\"\"):\n",
        "                if isinstance(h, str) and isinstance(e, str):\n",
        "                    h_clean = re.sub(r'<[^>]+>', '', h).strip()\n",
        "                    e_clean = re.sub(r'<[^>]+>', '', e).strip()\n",
        "                    if 50 <= len(e_clean) <= 2000:\n",
        "                        pid = hashlib.md5(f\"{json_file.stem}:{ref}:{h_clean[:50]}\".encode()).hexdigest()[:12]\n",
        "                        return [Passage(\n",
        "                            id=f\"sefaria_{pid}\",\n",
        "                            text_original=h_clean,\n",
        "                            text_english=e_clean,\n",
        "                            time_period=time_period.name,\n",
        "                            century=century,\n",
        "                            source=f\"{json_file.stem} {ref}\".strip(),\n",
        "                            source_type=\"sefaria\",\n",
        "                            category=category,\n",
        "                            language=\"hebrew\",\n",
        "                            word_count=len(e_clean.split())\n",
        "                        )]\n",
        "                    return []\n",
        "                elif isinstance(h, list) and isinstance(e, list):\n",
        "                    result = []\n",
        "                    for i, (hh, ee) in enumerate(zip(h, e)):\n",
        "                        result.extend(flatten(hh, ee, f\"{ref}.{i+1}\" if ref else str(i+1)))\n",
        "                    return result\n",
        "                return []\n",
        "            \n",
        "            passages.extend(flatten(hebrew, english))\n",
        "    \n",
        "    return passages\n",
        "\n",
        "def load_dear_abby(path: str, max_passages: int = None) -> List[Passage]:\n",
        "    \"\"\"Load Dear Abby corpus with progress bar.\"\"\"\n",
        "    passages = []\n",
        "    df = pd.read_csv(path)\n",
        "    \n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading Dear Abby\", unit=\"row\"):\n",
        "        question = str(row.get('question_only', ''))\n",
        "        if not question or question == 'nan' or len(question) < 50 or len(question) > 2000:\n",
        "            continue\n",
        "        \n",
        "        year = int(row.get('year', 1990))\n",
        "        pid = hashlib.md5(f\"abby:{idx}:{question[:50]}\".encode()).hexdigest()[:12]\n",
        "        \n",
        "        passages.append(Passage(\n",
        "            id=f\"abby_{pid}\",\n",
        "            text_original=question,\n",
        "            text_english=question,\n",
        "            time_period=TimePeriod.DEAR_ABBY.name,\n",
        "            century=20 if year < 2000 else 21,\n",
        "            source=f\"Dear Abby {year}\",\n",
        "            source_type=\"dear_abby\",\n",
        "            category=\"general\",\n",
        "            language=\"english\",\n",
        "            word_count=len(question.split())\n",
        "        ))\n",
        "        \n",
        "        if max_passages and len(passages) >= max_passages:\n",
        "            break\n",
        "    \n",
        "    return passages\n",
        "\n",
        "print(\"Data structures defined!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 5. Load and Preprocess Corpora { display-mode: \"form\" }\n",
        "#@markdown Loads both corpora. Set MAX_SEFARIA_PASSAGES to limit memory usage.\n",
        "\n",
        "#@markdown **Memory Management:**\n",
        "MAX_SEFARIA_PASSAGES = 500000  #@param {type:\"integer\"}\n",
        "#@markdown Set to 0 for unlimited. Recommended: 500000 for Colab (12GB RAM)\n",
        "\n",
        "import gc\n",
        "\n",
        "mark_task(\"Preprocess corpora\", \"running\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING CORPORA\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "if MAX_SEFARIA_PASSAGES > 0:\n",
        "    print(f\"*** MEMORY MODE: Limited to {MAX_SEFARIA_PASSAGES:,} Sefaria passages ***\")\n",
        "    print()\n",
        "\n",
        "# Load Sefaria with optional limit\n",
        "limit = MAX_SEFARIA_PASSAGES if MAX_SEFARIA_PASSAGES > 0 else None\n",
        "sefaria_passages = load_sefaria(\"data/raw/Sefaria-Export\", max_passages=limit)\n",
        "print(f\"\\nSefaria passages loaded: {len(sefaria_passages):,}\")\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# Load Dear Abby\n",
        "print()\n",
        "abby_passages = load_dear_abby(\"data/raw/dear_abby.csv\")\n",
        "print(f\"\\nDear Abby passages loaded: {len(abby_passages):,}\")\n",
        "\n",
        "# Combine\n",
        "all_passages = sefaria_passages + abby_passages\n",
        "\n",
        "# Clear individual lists to save memory\n",
        "del sefaria_passages\n",
        "del abby_passages\n",
        "gc.collect()\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(f\"TOTAL PASSAGES: {len(all_passages):,}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Statistics\n",
        "by_period = defaultdict(int)\n",
        "by_source = defaultdict(int)\n",
        "for p in all_passages:\n",
        "    by_period[p.time_period] += 1\n",
        "    by_source[p.source_type] += 1\n",
        "\n",
        "print(\"\\nBy source:\")\n",
        "for source, count in sorted(by_source.items()):\n",
        "    print(f\"  {source}: {count:,}\")\n",
        "\n",
        "print(\"\\nBy time period:\")\n",
        "for period, count in sorted(by_period.items()):\n",
        "    pct = count / len(all_passages) * 100\n",
        "    bar = '#' * int(pct / 2)\n",
        "    print(f\"  {period:20s}: {count:6,} ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "# Memory status\n",
        "import psutil\n",
        "mem = psutil.virtual_memory()\n",
        "print(f\"\\nMemory: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n",
        "\n",
        "mark_task(\"Preprocess corpora\", \"done\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 6. Extract Bond Structures { display-mode: \"form\" }\n",
        "#@markdown Extracts moral bond structures. Streams to disk to save memory.\n",
        "\n",
        "import gc\n",
        "\n",
        "mark_task(\"Extract bond structures\", \"running\")\n",
        "\n",
        "RELATION_PATTERNS = {\n",
        "    BondType.HARM_PREVENTION: [r'\\b(kill|murder|harm|hurt|save|rescue|protect|danger)\\b'],\n",
        "    BondType.RECIPROCITY: [r'\\b(return|repay|owe|debt|mutual|exchange)\\b'],\n",
        "    BondType.AUTONOMY: [r'\\b(choose|decision|consent|agree|force|coerce|right)\\b'],\n",
        "    BondType.PROPERTY: [r'\\b(property|own|steal|theft|buy|sell|land)\\b'],\n",
        "    BondType.FAMILY: [r'\\b(honor|parent|marry|divorce|inherit|family)\\b'],\n",
        "    BondType.AUTHORITY: [r'\\b(obey|command|law|judge|rule|teach)\\b'],\n",
        "    BondType.CARE: [r'\\b(care|help|assist|feed|clothe|visit)\\b'],\n",
        "    BondType.FAIRNESS: [r'\\b(fair|just|equal|deserve|bias)\\b'],\n",
        "}\n",
        "\n",
        "HOHFELD_PATTERNS = {\n",
        "    HohfeldianState.OBLIGATION: [r'\\b(must|shall|duty|require|should)\\b'],\n",
        "    HohfeldianState.RIGHT: [r'\\b(right to|entitled|deserve)\\b'],\n",
        "    HohfeldianState.LIBERTY: [r'\\b(may|can|permitted|allowed)\\b'],\n",
        "}\n",
        "\n",
        "def extract_bond_structure(passage: Passage) -> Dict:\n",
        "    \"\"\"Extract bond structure from passage.\"\"\"\n",
        "    text = passage.text_english.lower()\n",
        "    \n",
        "    relations = []\n",
        "    for rel_type, patterns in RELATION_PATTERNS.items():\n",
        "        for pattern in patterns:\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                relations.append(rel_type.name)\n",
        "                break\n",
        "    \n",
        "    if not relations:\n",
        "        relations = ['CARE']\n",
        "    \n",
        "    hohfeld = None\n",
        "    for state, patterns in HOHFELD_PATTERNS.items():\n",
        "        for pattern in patterns:\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                hohfeld = state.name\n",
        "                break\n",
        "        if hohfeld:\n",
        "            break\n",
        "    \n",
        "    signature = \"|\".join(sorted(set(relations)))\n",
        "    \n",
        "    return {\n",
        "        'bonds': [{'relation': r} for r in relations],\n",
        "        'primary_relation': relations[0],\n",
        "        'hohfeld_state': hohfeld,\n",
        "        'signature': signature\n",
        "    }\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXTRACTING & SAVING (STREAMING)\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(\"Writing directly to disk to conserve memory...\")\n",
        "print()\n",
        "\n",
        "bond_counts = defaultdict(int)\n",
        "\n",
        "# Stream directly to files - don't accumulate in memory\n",
        "with open(\"data/processed/passages.jsonl\", 'w') as f_pass, \\\n",
        "     open(\"data/processed/bond_structures.jsonl\", 'w') as f_bond:\n",
        "    \n",
        "    for passage in tqdm(all_passages, desc=\"Processing\", unit=\"passage\"):\n",
        "        # Extract bonds\n",
        "        bond_struct = extract_bond_structure(passage)\n",
        "        passage.bond_types = [b['relation'] for b in bond_struct['bonds']]\n",
        "        \n",
        "        # Count for stats\n",
        "        for bond in bond_struct['bonds']:\n",
        "            bond_counts[bond['relation']] += 1\n",
        "        \n",
        "        # Write immediately (don't accumulate)\n",
        "        f_pass.write(json.dumps(passage.to_dict()) + '\\n')\n",
        "        f_bond.write(json.dumps({\n",
        "            'passage_id': passage.id,\n",
        "            'bond_structure': bond_struct\n",
        "        }) + '\\n')\n",
        "\n",
        "# Clear passages from memory - we've saved them to disk\n",
        "n_passages = len(all_passages)\n",
        "del all_passages\n",
        "gc.collect()\n",
        "\n",
        "print()\n",
        "print(f\"Saved {n_passages:,} passages to disk\")\n",
        "print(\"Cleared passages from memory\")\n",
        "\n",
        "# Memory status\n",
        "import psutil\n",
        "mem = psutil.virtual_memory()\n",
        "print(f\"Memory: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n",
        "\n",
        "print()\n",
        "print(\"Bond type distribution:\")\n",
        "for bond_type, count in sorted(bond_counts.items(), key=lambda x: -x[1]):\n",
        "    pct = count / sum(bond_counts.values()) * 100\n",
        "    bar = '#' * int(pct)\n",
        "    print(f\"  {bond_type:20s}: {count:6,} ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "mark_task(\"Extract bond structures\", \"done\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 7. Generate Train/Test Splits { display-mode: \"form\" }\n",
        "#@markdown Creates splits from saved files. Memory efficient - reads only IDs.\n",
        "\n",
        "import random\n",
        "import gc\n",
        "random.seed(42)\n",
        "\n",
        "mark_task(\"Generate train/test splits\", \"running\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GENERATING SPLITS (MEMORY EFFICIENT)\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Read only IDs and time periods from disk - don't load full passages\n",
        "print(\"Reading passage metadata from disk...\")\n",
        "passage_info = []  # List of (id, time_period) tuples - minimal memory\n",
        "\n",
        "with open(\"data/processed/passages.jsonl\", 'r') as f:\n",
        "    for line in tqdm(f, desc=\"Reading IDs\", unit=\"line\"):\n",
        "        p = json.loads(line)\n",
        "        passage_info.append((p['id'], p['time_period']))\n",
        "\n",
        "print(f\"Loaded {len(passage_info):,} passage IDs\")\n",
        "\n",
        "# Define time periods\n",
        "train_periods = {'BIBLICAL', 'SECOND_TEMPLE', 'TANNAITIC', 'AMORAIC', 'GEONIC', 'RISHONIM'}\n",
        "valid_periods = {'ACHRONIM'}\n",
        "test_periods = {'MODERN_HEBREW', 'DEAR_ABBY'}\n",
        "\n",
        "print()\n",
        "print(\"Filtering by time period...\")\n",
        "ancient_ids = [(pid, tp) for pid, tp in passage_info if tp in train_periods]\n",
        "early_modern_ids = [(pid, tp) for pid, tp in passage_info if tp in valid_periods]\n",
        "modern_ids = [(pid, tp) for pid, tp in passage_info if tp in test_periods]\n",
        "\n",
        "print(f\"  Ancient/Medieval: {len(ancient_ids):,}\")\n",
        "print(f\"  Early Modern:     {len(early_modern_ids):,}\")\n",
        "print(f\"  Modern:           {len(modern_ids):,}\")\n",
        "\n",
        "# Shuffle\n",
        "random.shuffle(ancient_ids)\n",
        "random.shuffle(early_modern_ids)\n",
        "random.shuffle(modern_ids)\n",
        "\n",
        "# ============================================================\n",
        "# SPLIT A: ANCIENT -> MODERN\n",
        "# ============================================================\n",
        "print()\n",
        "print(\"-\" * 60)\n",
        "print(\"SPLIT A: Train ANCIENT, Test MODERN\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "temporal_A = {\n",
        "    'name': 'ancient_to_modern',\n",
        "    'direction': 'A->M',\n",
        "    'train_ids': [pid for pid, _ in ancient_ids],\n",
        "    'valid_ids': [pid for pid, _ in early_modern_ids],\n",
        "    'test_ids': [pid for pid, _ in modern_ids],\n",
        "    'train_size': len(ancient_ids),\n",
        "    'valid_size': len(early_modern_ids),\n",
        "    'test_size': len(modern_ids)\n",
        "}\n",
        "print(f\"  Train: {temporal_A['train_size']:,}\")\n",
        "print(f\"  Valid: {temporal_A['valid_size']:,}\")\n",
        "print(f\"  Test:  {temporal_A['test_size']:,}\")\n",
        "\n",
        "# ============================================================\n",
        "# SPLIT B: MODERN -> ANCIENT\n",
        "# ============================================================\n",
        "print()\n",
        "print(\"-\" * 60)\n",
        "print(\"SPLIT B: Train MODERN, Test ANCIENT\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "n_modern = len(modern_ids)\n",
        "ancient_test = ancient_ids[n_modern:n_modern*2] if len(ancient_ids) >= n_modern*2 else ancient_ids[n_modern:]\n",
        "\n",
        "temporal_B = {\n",
        "    'name': 'modern_to_ancient',\n",
        "    'direction': 'M->A',\n",
        "    'train_ids': [pid for pid, _ in modern_ids],\n",
        "    'valid_ids': [pid for pid, _ in early_modern_ids[:len(early_modern_ids)//2]],\n",
        "    'test_ids': [pid for pid, _ in ancient_test],\n",
        "    'train_size': len(modern_ids),\n",
        "    'valid_size': len(early_modern_ids) // 2,\n",
        "    'test_size': len(ancient_test)\n",
        "}\n",
        "print(f\"  Train: {temporal_B['train_size']:,}\")\n",
        "print(f\"  Valid: {temporal_B['valid_size']:,}\")\n",
        "print(f\"  Test:  {temporal_B['test_size']:,}\")\n",
        "\n",
        "# ============================================================\n",
        "# SPLIT C: MIXED CONTROL\n",
        "# ============================================================\n",
        "print()\n",
        "print(\"-\" * 60)\n",
        "print(\"SPLIT C: MIXED (Control)\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "all_ids = ancient_ids + modern_ids\n",
        "random.shuffle(all_ids)\n",
        "n = len(all_ids)\n",
        "n_train = int(0.7 * n)\n",
        "n_valid = int(0.15 * n)\n",
        "\n",
        "temporal_C = {\n",
        "    'name': 'mixed_control',\n",
        "    'direction': 'MIXED',\n",
        "    'train_ids': [pid for pid, _ in all_ids[:n_train]],\n",
        "    'valid_ids': [pid for pid, _ in all_ids[n_train:n_train+n_valid]],\n",
        "    'test_ids': [pid for pid, _ in all_ids[n_train+n_valid:]],\n",
        "    'train_size': n_train,\n",
        "    'valid_size': n_valid,\n",
        "    'test_size': n - n_train - n_valid\n",
        "}\n",
        "print(f\"  Train: {temporal_C['train_size']:,}\")\n",
        "print(f\"  Valid: {temporal_C['valid_size']:,}\")\n",
        "print(f\"  Test:  {temporal_C['test_size']:,}\")\n",
        "\n",
        "# Clear temporary data\n",
        "del passage_info, ancient_ids, early_modern_ids, modern_ids, all_ids\n",
        "gc.collect()\n",
        "\n",
        "# Save\n",
        "print()\n",
        "print(\"Saving splits...\")\n",
        "splits = {\n",
        "    'ancient_to_modern': temporal_A,\n",
        "    'modern_to_ancient': temporal_B,\n",
        "    'mixed_control': temporal_C\n",
        "}\n",
        "\n",
        "with open(\"data/splits/all_splits.json\", 'w') as f:\n",
        "    json.dump(splits, f, indent=2)\n",
        "\n",
        "# Memory status\n",
        "import psutil\n",
        "mem = psutil.virtual_memory()\n",
        "print(f\"Memory: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n",
        "\n",
        "print()\n",
        "print(\"SPLITS SAVED:\")\n",
        "print(\"  - ancient_to_modern (A->M)\")\n",
        "print(\"  - modern_to_ancient (M->A)\")  \n",
        "print(\"  - mixed_control\")\n",
        "\n",
        "mark_task(\"Generate train/test splits\", \"done\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 8. Define BIP Model Architecture { display-mode: \"form\" }\n",
        "#@markdown Defines the model. Clears memory first to avoid OOM.\n",
        "\n",
        "import gc\n",
        "import psutil\n",
        "\n",
        "# CRITICAL: Clear memory before loading model\n",
        "print(\"Clearing memory before model load...\")\n",
        "gc.collect()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "mem = psutil.virtual_memory()\n",
        "print(f\"Memory before model: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n",
        "print()\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gc\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DEFINING MODEL ARCHITECTURE\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "class GradientReversal(torch.autograd.Function):\n",
        "    \"\"\"Gradient reversal layer for adversarial training.\"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambda_):\n",
        "        ctx.lambda_ = lambda_\n",
        "        return x.clone()\n",
        "    \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return -ctx.lambda_ * grad_output, None\n",
        "\n",
        "def gradient_reversal(x, lambda_=1.0):\n",
        "    return GradientReversal.apply(x, lambda_)\n",
        "\n",
        "class BIPEncoder(nn.Module):\n",
        "    \"\"\"Sentence encoder using pretrained transformer.\"\"\"\n",
        "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", d_model=384):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.d_model = d_model\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden = outputs.last_hidden_state\n",
        "        mask = attention_mask.unsqueeze(-1).float()\n",
        "        pooled = (hidden * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
        "        return pooled\n",
        "\n",
        "class BIPModel(nn.Module):\n",
        "    \"\"\"Bond Invariance Principle Model with adversarial disentanglement.\"\"\"\n",
        "    def __init__(self, d_model=384, d_bond=64, d_label=32, n_periods=9, n_hohfeld=4):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = BIPEncoder()\n",
        "        \n",
        "        self.bond_proj = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(d_model // 2, d_bond)\n",
        "        )\n",
        "        \n",
        "        self.label_proj = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(d_model // 2, d_label)\n",
        "        )\n",
        "        \n",
        "        self.time_classifier_bond = nn.Linear(d_bond, n_periods)\n",
        "        self.time_classifier_label = nn.Linear(d_label, n_periods)\n",
        "        self.hohfeld_classifier = nn.Linear(d_bond, n_hohfeld)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, adversarial_lambda=1.0):\n",
        "        h = self.encoder(input_ids, attention_mask)\n",
        "        \n",
        "        z_bond = self.bond_proj(h)\n",
        "        z_label = self.label_proj(h)\n",
        "        \n",
        "        z_bond_adv = gradient_reversal(z_bond, adversarial_lambda)\n",
        "        time_pred_bond = self.time_classifier_bond(z_bond_adv)\n",
        "        time_pred_label = self.time_classifier_label(z_label)\n",
        "        hohfeld_pred = self.hohfeld_classifier(z_bond)\n",
        "        \n",
        "        return {\n",
        "            'z_bond': z_bond,\n",
        "            'z_label': z_label,\n",
        "            'time_pred_bond': time_pred_bond,\n",
        "            'time_pred_label': time_pred_label,\n",
        "            'hohfeld_pred': hohfeld_pred\n",
        "        }\n",
        "\n",
        "# Time period mapping\n",
        "TIME_PERIOD_TO_IDX = {\n",
        "    'BIBLICAL': 0, 'SECOND_TEMPLE': 1, 'TANNAITIC': 2, 'AMORAIC': 3,\n",
        "    'GEONIC': 4, 'RISHONIM': 5, 'ACHRONIM': 6, 'MODERN_HEBREW': 7, 'DEAR_ABBY': 8\n",
        "}\n",
        "\n",
        "HOHFELD_TO_IDX = {\n",
        "    'OBLIGATION': 0, 'RIGHT': 1, 'LIBERTY': 2, None: 3\n",
        "}\n",
        "\n",
        "class MoralDataset(Dataset):\n",
        "    \"\"\"\n",
        "    MEMORY-EFFICIENT Dataset that reads from disk on demand.\n",
        "    Does NOT load all data into memory at once.\n",
        "    \"\"\"\n",
        "    def __init__(self, passage_ids: set, passages_file: str, bonds_file: str, tokenizer, max_len=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.passage_ids = passage_ids\n",
        "        \n",
        "        # Build index: passage_id -> (file_offset, line_length) for passages file\n",
        "        # This allows us to seek directly to the line we need\n",
        "        print(f\"  Indexing {len(passage_ids):,} passages...\")\n",
        "        \n",
        "        self.data = []  # Store minimal data: (text, time_period, hohfeld_state)\n",
        "        \n",
        "        # Load only the passages we need\n",
        "        with open(passages_file, 'r') as f_pass, open(bonds_file, 'r') as f_bond:\n",
        "            for p_line, b_line in tqdm(zip(f_pass, f_bond), desc=\"  Loading subset\", unit=\"line\", total=None):\n",
        "                p = json.loads(p_line)\n",
        "                if p['id'] in passage_ids:\n",
        "                    b = json.loads(b_line)\n",
        "                    self.data.append({\n",
        "                        'text': p['text_english'][:1000],  # Truncate long texts\n",
        "                        'time_period': p['time_period'],\n",
        "                        'hohfeld': b['bond_structure']['hohfeld_state']\n",
        "                    })\n",
        "        \n",
        "        print(f\"  Loaded {len(self.data):,} samples\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        \n",
        "        encoding = self.tokenizer(\n",
        "            item['text'],\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'time_label': TIME_PERIOD_TO_IDX.get(item['time_period'], 8),\n",
        "            'hohfeld_label': HOHFELD_TO_IDX.get(item['hohfeld'], 3)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
        "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
        "        'time_labels': torch.tensor([x['time_label'] for x in batch]),\n",
        "        'hohfeld_labels': torch.tensor([x['hohfeld_label'] for x in batch])\n",
        "    }\n",
        "\n",
        "# Memory cleanup\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Model architecture defined!\")\n",
        "print()\n",
        "\n",
        "# Memory status\n",
        "import psutil\n",
        "mem = psutil.virtual_memory()\n",
        "print(f\"Memory: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 9. Train BIP Model (GPU/TPU) { display-mode: \"form\" }\n",
        "#@markdown Trains the model with adversarial disentanglement. Runs on selected split.\n",
        "\n",
        "#@markdown **Select which split to train:**\n",
        "SPLIT_NAME = \"ancient_to_modern\"  #@param [\"ancient_to_modern\", \"modern_to_ancient\", \"mixed_control\"]\n",
        "\n",
        "mark_task(\"Train BIP model\", \"running\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING BIP MODEL\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(f\"Accelerator: {ACCELERATOR}\")\n",
        "print(f\"Device: {device}\")\n",
        "print()\n",
        "\n",
        "# Load the selected split\n",
        "with open(\"data/splits/all_splits.json\", 'r') as f:\n",
        "    all_splits = json.load(f)\n",
        "\n",
        "split = all_splits[SPLIT_NAME]\n",
        "print(f\"Using split: {SPLIT_NAME}\")\n",
        "print(f\"  Direction: {split.get('direction', 'N/A')}\")\n",
        "print(f\"  Train: {split['train_size']:,}\")\n",
        "print(f\"  Valid: {split['valid_size']:,}\")\n",
        "print(f\"  Test:  {split['test_size']:,}\")\n",
        "print()\n",
        "\n",
        "# For backward compatibility\n",
        "temporal_holdout = split\n",
        "\n",
        "# Initialize\n",
        "print(\"Loading tokenizer and model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model = BIPModel().to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print()\n",
        "\n",
        "# Create datasets\n",
        "print(\"Creating datasets...\")\n",
        "train_dataset = MoralDataset(\n",
        "    set(split['train_ids']),\n",
        "    \"data/processed/passages.jsonl\",\n",
        "    \"data/processed/bond_structures.jsonl\",\n",
        "    tokenizer\n",
        ")\n",
        "valid_dataset = MoralDataset(\n",
        "    set(split['valid_ids']),\n",
        "    \"data/processed/passages.jsonl\",\n",
        "    \"data/processed/bond_structures.jsonl\",\n",
        "    tokenizer\n",
        ")\n",
        "test_dataset = MoralDataset(\n",
        "    set(split['test_ids']),\n",
        "    \"data/processed/passages.jsonl\",\n",
        "    \"data/processed/bond_structures.jsonl\",\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset):,}\")\n",
        "print(f\"Valid samples: {len(valid_dataset):,}\")\n",
        "print(f\"Test samples:  {len(test_dataset):,}\")\n",
        "print()\n",
        "\n",
        "if len(train_dataset) == 0:\n",
        "    print(\"ERROR: No training data! Check if data loaded correctly.\")\n",
        "else:\n",
        "    batch_size = 64 if USE_TPU else 32\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
        "                              collate_fn=collate_fn, drop_last=True, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size*2, shuffle=False, \n",
        "                              collate_fn=collate_fn, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False, \n",
        "                             collate_fn=collate_fn, num_workers=2)\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "    \n",
        "    n_epochs = 10\n",
        "    best_valid_loss = float('inf')\n",
        "    \n",
        "    # Save model with split name\n",
        "    model_path = f\"models/checkpoints/best_model_{SPLIT_NAME}.pt\"\n",
        "    \n",
        "    print(f\"Training for {n_epochs} epochs (batch_size={batch_size})...\")\n",
        "    print(f\"Model will be saved to: {model_path}\")\n",
        "    print(\"=\" * 60)\n",
        "    print()\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        n_batches = 0\n",
        "        \n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{n_epochs}\", unit=\"batch\")\n",
        "        for batch in pbar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            time_labels = batch['time_labels'].to(device)\n",
        "            hohfeld_labels = batch['hohfeld_labels'].to(device)\n",
        "            \n",
        "            outputs = model(input_ids, attention_mask, adversarial_lambda=1.0)\n",
        "            \n",
        "            # Losses\n",
        "            time_probs = F.softmax(outputs['time_pred_bond'], dim=-1)\n",
        "            entropy = -torch.sum(time_probs * torch.log(time_probs + 1e-8), dim=-1)\n",
        "            loss_adv = -entropy.mean()\n",
        "            \n",
        "            loss_time = F.cross_entropy(outputs['time_pred_label'], time_labels)\n",
        "            loss_hohfeld = F.cross_entropy(outputs['hohfeld_pred'], hohfeld_labels)\n",
        "            \n",
        "            loss = loss_adv + loss_time + loss_hohfeld\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            \n",
        "            if USE_TPU:\n",
        "                xm.optimizer_step(optimizer)\n",
        "                xm.mark_step()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            n_batches += 1\n",
        "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'adv': f\"{loss_adv.item():.3f}\"})\n",
        "        \n",
        "        avg_train_loss = total_loss / n_batches\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        valid_batches = 0\n",
        "        time_correct = 0\n",
        "        time_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in valid_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                time_labels = batch['time_labels'].to(device)\n",
        "                hohfeld_labels = batch['hohfeld_labels'].to(device)\n",
        "                \n",
        "                outputs = model(input_ids, attention_mask, adversarial_lambda=0)\n",
        "                loss = F.cross_entropy(outputs['hohfeld_pred'], hohfeld_labels)\n",
        "                valid_loss += loss.item()\n",
        "                valid_batches += 1\n",
        "                \n",
        "                time_preds = outputs['time_pred_bond'].argmax(dim=-1)\n",
        "                time_correct += (time_preds == time_labels).sum().item()\n",
        "                time_total += len(time_labels)\n",
        "                \n",
        "                if USE_TPU:\n",
        "                    xm.mark_step()\n",
        "        \n",
        "        avg_valid_loss = valid_loss / valid_batches if valid_batches > 0 else 0\n",
        "        time_acc = time_correct / time_total if time_total > 0 else 0\n",
        "        \n",
        "        print(f\"\\nEpoch {epoch} Summary:\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"  Valid Loss: {avg_valid_loss:.4f}\")\n",
        "        print(f\"  Time Acc from z_bond: {time_acc:.1%} (target: ~11% = chance)\")\n",
        "        \n",
        "        if avg_valid_loss < best_valid_loss:\n",
        "            best_valid_loss = avg_valid_loss\n",
        "            if USE_TPU:\n",
        "                xm.save(model.state_dict(), model_path)\n",
        "            else:\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "            print(f\"  -> Saved best model!\")\n",
        "        print()\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(f\"TRAINING COMPLETE for {SPLIT_NAME}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "mark_task(\"Train BIP model\", \"done\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 10. Evaluate Results (Bidirectional) { display-mode: \"form\" }\n",
        "#@markdown Tests the BIP hypothesis with BIDIRECTIONAL transfer analysis.\n",
        "\n",
        "import gc\n",
        "\n",
        "mark_task(\"Evaluate results\", \"running\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BIP BIDIRECTIONAL EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "def evaluate_split(split_name, model_path, test_ids, tokenizer):\n",
        "    \"\"\"Evaluate a single split direction.\"\"\"\n",
        "    print(f\"\\nEvaluating: {split_name}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Create test dataset\n",
        "    test_ds = MoralDataset(\n",
        "        set(test_ids),\n",
        "        \"data/processed/passages.jsonl\",\n",
        "        \"data/processed/bond_structures.jsonl\",\n",
        "        tokenizer\n",
        "    )\n",
        "    test_ld = DataLoader(test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
        "    \n",
        "    # Load model\n",
        "    model_eval = BIPModel().to(device)\n",
        "    try:\n",
        "        if USE_TPU:\n",
        "            model_eval.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "            model_eval = model_eval.to(device)\n",
        "        else:\n",
        "            model_eval.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model_eval.eval()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  Model not found: {model_path}\")\n",
        "        return None\n",
        "    \n",
        "    all_time_preds = []\n",
        "    all_time_labels = []\n",
        "    all_hohfeld_preds = []\n",
        "    all_hohfeld_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_ld, desc=f\"Testing\", unit=\"batch\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            \n",
        "            outputs = model_eval(input_ids, attention_mask, adversarial_lambda=0)\n",
        "            \n",
        "            all_time_preds.extend(outputs['time_pred_bond'].argmax(dim=-1).cpu().tolist())\n",
        "            all_time_labels.extend(batch['time_labels'].tolist())\n",
        "            all_hohfeld_preds.extend(outputs['hohfeld_pred'].argmax(dim=-1).cpu().tolist())\n",
        "            all_hohfeld_labels.extend(batch['hohfeld_labels'].tolist())\n",
        "            \n",
        "            if USE_TPU:\n",
        "                xm.mark_step()\n",
        "    \n",
        "    # Cleanup\n",
        "    del model_eval, test_ds, test_ld\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    time_acc = sum(p == l for p, l in zip(all_time_preds, all_time_labels)) / len(all_time_preds) if all_time_preds else 0\n",
        "    hohfeld_acc = sum(p == l for p, l in zip(all_hohfeld_preds, all_hohfeld_labels)) / len(all_hohfeld_preds) if all_hohfeld_preds else 0\n",
        "    \n",
        "    return {'time_acc': time_acc, 'hohfeld_acc': hohfeld_acc}\n",
        "\n",
        "# Load splits\n",
        "with open(\"data/splits/all_splits.json\", 'r') as f:\n",
        "    all_splits = json.load(f)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "results = {}\n",
        "chance_level = 1/9\n",
        "\n",
        "# Evaluate each split that has a trained model\n",
        "for split_name in ['ancient_to_modern', 'modern_to_ancient', 'mixed_control']:\n",
        "    model_path = f\"models/checkpoints/best_model_{split_name}.pt\"\n",
        "    \n",
        "    if not os.path.exists(model_path):\n",
        "        if split_name == 'ancient_to_modern' and os.path.exists(\"models/checkpoints/best_model.pt\"):\n",
        "            model_path = \"models/checkpoints/best_model.pt\"\n",
        "        else:\n",
        "            print(f\"\\nSkipping {split_name}: No model found\")\n",
        "            continue\n",
        "    \n",
        "    split = all_splits.get(split_name)\n",
        "    if not split:\n",
        "        continue\n",
        "    \n",
        "    result = evaluate_split(split_name, model_path, split['test_ids'], tokenizer)\n",
        "    if result:\n",
        "        results[split_name] = result\n",
        "\n",
        "# Print results table\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(f\"{'Split':<25} {'Direction':<10} {'Time Acc':<12} {'Hohfeld Acc':<12}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for split_name, res in results.items():\n",
        "    split = all_splits.get(split_name, {})\n",
        "    direction = split.get('direction', '?')\n",
        "    print(f\"{split_name:<25} {direction:<10} {res['time_acc']:>10.1%} {res['hohfeld_acc']:>10.1%}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(f\"Chance level: {chance_level:.1%}\")\n",
        "\n",
        "# BIP Verdict\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(\"BIP VERDICT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "a2m = results.get('ancient_to_modern', {})\n",
        "m2a = results.get('modern_to_ancient', {})\n",
        "\n",
        "if a2m:\n",
        "    a2m_inv = abs(a2m.get('time_acc', 1) - chance_level) < 0.05\n",
        "    a2m_struct = a2m.get('hohfeld_acc', 0) > 0.35\n",
        "    \n",
        "    print()\n",
        "    if a2m_inv and a2m_struct:\n",
        "        print(\"*** BIP SUPPORTED ***\")\n",
        "        print()\n",
        "        print(\"Time invariance: YES (cannot predict era from z_bond)\")\n",
        "        print(\"Moral structure: YES (Hohfeld classification works)\")\n",
        "    elif a2m_struct:\n",
        "        print(\"BIP: PARTIAL SUPPORT\")\n",
        "        print()\n",
        "        print(\"Moral structure captured, but some temporal leakage.\")\n",
        "    else:\n",
        "        print(\"BIP: INCONCLUSIVE\")\n",
        "        print()\n",
        "        print(\"Weak moral structure encoding.\")\n",
        "\n",
        "    if m2a:\n",
        "        m2a_inv = abs(m2a.get('time_acc', 1) - chance_level) < 0.05\n",
        "        m2a_struct = m2a.get('hohfeld_acc', 0) > 0.35\n",
        "        \n",
        "        if a2m_inv and m2a_inv and a2m_struct and m2a_struct:\n",
        "            print()\n",
        "            print(\"*** BIDIRECTIONAL INVARIANCE CONFIRMED ***\")\n",
        "            print(\"Both A->M and M->A show temporal invariance!\")\n",
        "else:\n",
        "    print(\"No results to evaluate. Train at least one split first.\")\n",
        "\n",
        "mark_task(\"Evaluate results\", \"done\")\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(\"EXPERIMENT COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print_progress()\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Save Results\n",
        "\n",
        "Run the cell below to download your trained model and results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 11. Download Results (Optional) { display-mode: \"form\" }\n",
        "#@markdown Creates a zip file with model checkpoint and metrics.\n",
        "\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create results directory\n",
        "!mkdir -p results\n",
        "!cp models/checkpoints/best_model.pt results/\n",
        "!cp data/splits/all_splits.json results/\n",
        "\n",
        "# Save metrics\n",
        "if len(train_dataset) > 0:\n",
        "    metrics = {\n",
        "        'accelerator': ACCELERATOR,\n",
        "        'time_acc_from_bond': time_acc,\n",
        "        'hohfeld_acc': hohfeld_acc,\n",
        "        'chance_level': chance_level,\n",
        "        'time_invariant': time_invariant,\n",
        "        'moral_structure': moral_structure,\n",
        "        'bip_supported': time_invariant and moral_structure\n",
        "    }\n",
        "    with open('results/metrics.json', 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "# Zip\n",
        "shutil.make_archive('bip_results', 'zip', 'results')\n",
        "print(\"Results saved to bip_results.zip\")\n",
        "print()\n",
        "print(\"Contents:\")\n",
        "!ls -la results/\n",
        "\n",
        "# Download\n",
        "files.download('bip_results.zip')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}