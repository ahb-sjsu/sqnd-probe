{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIP v10.10 - Bond Invariance Probe\n",
    "\n",
    "**Key changes in v10.10:**\n",
    "- Expanded Sanskrit corpus (~121 unique passages, deduplicated)\n",
    "- Expanded Pali corpus (~90 unique passages, deduplicated)\n",
    "- Expanded Buddhist Chinese corpus (~109 passages)\n",
    "- Role-aware data augmentation to improve agent/patient sensitivity\n",
    "- Role contrastive loss in training (addresses weak role_swap from fuzz testing)\n",
    "\n",
    "Run all cells in order. Requires GPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Configuration & Setup { display-mode: \"form\" }\n",
    "# @markdown ## Data Source Configuration\n",
    "\n",
    "DATA_MODE = \"Update missing\"  # @param [\"Refresh all\", \"Update missing\", \"Cache only\"]\n",
    "# @markdown - **Refresh all**: Re-download everything from source (slow, ~2hrs)\n",
    "# @markdown - **Update missing**: Use cache, download only what's missing (recommended)\n",
    "# @markdown - **Cache only**: Use only cached data, fail if missing\n",
    "\n",
    "DRIVE_FOLDER = \"BIP_v10\"  # @param {type:\"string\"}\n",
    "# @markdown Folder name for persistent storage\n",
    "\n",
    "# Derive flags from DATA_MODE\n",
    "USE_DRIVE_DATA = True  # Always use Drive for caching\n",
    "REFRESH_DATA_FROM_SOURCE = DATA_MODE == \"Refresh all\"\n",
    "CACHE_ONLY = DATA_MODE == \"Cache only\"\n",
    "# @markdown ---\n",
    "# @markdown ## Model Backbone\n",
    "BACKBONE = \"MiniLM\"  # @param [\"MiniLM\", \"LaBSE\", \"XLM-R-base\", \"XLM-R-large\"]\n",
    "# @markdown - **MiniLM**: Fast, 118M params, good baseline\n",
    "# @markdown - **LaBSE**: Best cross-lingual alignment, 471M params (recommended)\n",
    "# @markdown - **XLM-R-base**: Strong multilingual, 270M params\n",
    "# @markdown - **XLM-R-large**: Strongest representations, 550M params\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ## Output Options\n",
    "CREATE_DOWNLOAD_ZIP = False  # @param {type:\"boolean\"}\n",
    "# @markdown - **CREATE_DOWNLOAD_ZIP**: Create and download a zip file of results (optional)\n",
    "# @markdown - Results are always persisted to Google Drive regardless of this setting\n",
    "\n",
    "# Backbone configurations\n",
    "BACKBONE_CONFIGS = {\n",
    "    \"MiniLM\": {\n",
    "        \"model_name\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        \"hidden_size\": 384,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 512,\n",
    "            \"T4\": 256,\n",
    "            \"2xT4\": 512,\n",
    "            \"SMALL\": 128,\n",
    "            \"MINIMAL/CPU\": 64,\n",
    "        },\n",
    "    },\n",
    "    \"LaBSE\": {\n",
    "        \"model_name\": \"sentence-transformers/LaBSE\",\n",
    "        \"hidden_size\": 768,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 256,\n",
    "            \"T4\": 128,\n",
    "            \"2xT4\": 256,\n",
    "            \"SMALL\": 64,\n",
    "            \"MINIMAL/CPU\": 32,\n",
    "        },\n",
    "    },\n",
    "    \"XLM-R-base\": {\n",
    "        \"model_name\": \"xlm-roberta-base\",\n",
    "        \"hidden_size\": 768,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 256,\n",
    "            \"T4\": 128,\n",
    "            \"2xT4\": 256,\n",
    "            \"SMALL\": 64,\n",
    "            \"MINIMAL/CPU\": 32,\n",
    "        },\n",
    "    },\n",
    "    \"XLM-R-large\": {\n",
    "        \"model_name\": \"xlm-roberta-large\",\n",
    "        \"hidden_size\": 1024,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 128,\n",
    "            \"T4\": 64,\n",
    "            \"2xT4\": 128,\n",
    "            \"SMALL\": 32,\n",
    "            \"MINIMAL/CPU\": 16,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "BACKBONE_CONFIG = BACKBONE_CONFIGS[BACKBONE]\n",
    "MODEL_NAME = BACKBONE_CONFIG[\"model_name\"]\n",
    "BACKBONE_HIDDEN = BACKBONE_CONFIG[\"hidden_size\"]\n",
    "\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ## Run Setup\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "EXPERIMENT_START = time.time()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BIP v10.9 - ENVIRONMENT DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===== ENVIRONMENT DETECTION =====\n",
    "# Detect which cloud platform we're running on\n",
    "\n",
    "ENV_NAME = \"UNKNOWN\"\n",
    "ENV_GPU_QUOTA = \"Unknown\"\n",
    "PERSISTENT_STORAGE = None\n",
    "DATA_DIR = \"/content\"  # Default\n",
    "\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"Detect cloud environment and return (name, gpu_quota, storage_path, data_dir)\"\"\"\n",
    "\n",
    "    # 1. Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "\n",
    "        return (\"COLAB\", \"Free: T4 ~12h/day, Pro: L4/A100\", \"/content/drive/MyDrive\", \"/content\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    # 2. Kaggle Kernels\n",
    "    if os.path.exists(\"/kaggle\"):\n",
    "        # Kaggle has /kaggle/input for datasets, /kaggle/working for output\n",
    "        return (\"KAGGLE\", \"Free: 2xT4 30h/week, TPU 30h/week\", \"/kaggle/working\", \"/kaggle/working\")\n",
    "\n",
    "    # 3. Lightning.ai Studios\n",
    "    if os.environ.get(\"LIGHTNING_CLOUDSPACE_HOST\") or os.path.exists(\"/teamspace\"):\n",
    "        # Lightning.ai has /teamspace/studios for persistent storage\n",
    "        return (\n",
    "            \"LIGHTNING_AI\",\n",
    "            \"Free: 22h/month GPU, Pro: A10G/H100\",\n",
    "            \"/teamspace/studios\",\n",
    "            \"/teamspace/studios\",\n",
    "        )\n",
    "\n",
    "    # 4. Paperspace Gradient\n",
    "    if os.environ.get(\"PAPERSPACE_NOTEBOOK_REPO_ID\") or os.path.exists(\"/notebooks\"):\n",
    "        return (\"PAPERSPACE\", \"Free: M4000 6h, Pro: A100/H100\", \"/storage\", \"/notebooks\")\n",
    "\n",
    "    # 5. Saturn Cloud\n",
    "    if os.environ.get(\"SATURN_RESOURCE_ID\") or \"saturn\" in os.environ.get(\"HOSTNAME\", \"\").lower():\n",
    "        return (\n",
    "            \"SATURN_CLOUD\",\n",
    "            \"Free: T4 10h/month, Pro: A10G/A100\",\n",
    "            \"/home/jovyan/workspace\",\n",
    "            \"/home/jovyan\",\n",
    "        )\n",
    "\n",
    "    # 6. HuggingFace Spaces\n",
    "    if os.environ.get(\"SPACE_ID\") or os.environ.get(\"HF_SPACE_ID\"):\n",
    "        return (\n",
    "            \"HUGGINGFACE_SPACES\",\n",
    "            \"Free: CPU only, ZeroGPU: A10G/A100 quota\",\n",
    "            \"/data\",\n",
    "            \"/home/user/app\",\n",
    "        )\n",
    "\n",
    "    # 7. AWS SageMaker Studio Lab\n",
    "    if os.path.exists(\"/home/studio-lab-user\"):\n",
    "        return (\n",
    "            \"SAGEMAKER_STUDIO_LAB\",\n",
    "            \"Free: T4 4h/session, 24h max/day\",\n",
    "            \"/home/studio-lab-user\",\n",
    "            \"/home/studio-lab-user\",\n",
    "        )\n",
    "\n",
    "    # 8. Deepnote\n",
    "    if os.environ.get(\"DEEPNOTE_PROJECT_ID\"):\n",
    "        return (\"DEEPNOTE\", \"Free: CPU, Pro: T4/A10G\", \"/work\", \"/work\")\n",
    "\n",
    "    # 9. Local/Unknown\n",
    "    return (\"LOCAL\", \"Depends on local hardware\", os.getcwd(), os.getcwd())\n",
    "\n",
    "\n",
    "ENV_NAME, ENV_GPU_QUOTA, PERSISTENT_STORAGE, DATA_DIR = detect_environment()\n",
    "\n",
    "print(f\"\\nEnvironment: {ENV_NAME}\")\n",
    "print(f\"GPU Quota:   {ENV_GPU_QUOTA}\")\n",
    "print(f\"Storage:     {PERSISTENT_STORAGE}\")\n",
    "print(f\"Data Dir:    {DATA_DIR}\")\n",
    "\n",
    "# Environment-specific setup\n",
    "ENV_TIPS = {\n",
    "    \"COLAB\": [\n",
    "        \"Tip: Use GPU runtime (Runtime -> Change runtime type -> T4 GPU)\",\n",
    "        \"Tip: Colab Pro gives L4 GPU access (~2x faster than T4)\",\n",
    "    ],\n",
    "    \"KAGGLE\": [\n",
    "        \"Tip: Enable GPU (Settings -> Accelerator -> GPU T4 x2)\",\n",
    "        \"Tip: 30h/week GPU quota resets every Saturday\",\n",
    "        \"Tip: Upload data as a Kaggle Dataset for persistence\",\n",
    "    ],\n",
    "    \"LIGHTNING_AI\": [\n",
    "        \"Tip: Select GPU studio (A10G recommended for this workload)\",\n",
    "        \"Tip: /teamspace/studios persists across sessions\",\n",
    "    ],\n",
    "    \"PAPERSPACE\": [\n",
    "        \"Tip: Use /storage for persistent data across runs\",\n",
    "        \"Tip: Free tier has 6h/month GPU limit\",\n",
    "    ],\n",
    "    \"SATURN_CLOUD\": [\n",
    "        \"Tip: Start a T4 instance from the Resources tab\",\n",
    "        \"Tip: 10h/month free GPU quota\",\n",
    "    ],\n",
    "    \"HUGGINGFACE_SPACES\": [\n",
    "        \"Tip: ZeroGPU provides A10G/A100 access with quota system\",\n",
    "        \"Tip: Use Gradio/Streamlit for interactive demos\",\n",
    "    ],\n",
    "    \"SAGEMAKER_STUDIO_LAB\": [\n",
    "        \"Tip: Request GPU runtime from the launcher\",\n",
    "        \"Tip: Sessions timeout after 4h, max 24h/day\",\n",
    "    ],\n",
    "    \"LOCAL\": [\"Tip: Running locally - ensure CUDA is installed for GPU support\"],\n",
    "}\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(\"ENVIRONMENT TIPS:\")\n",
    "for tip in ENV_TIPS.get(ENV_NAME, [\"No specific tips for this environment\"]):\n",
    "    print(f\"  {tip}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ===== INSTALL DEPENDENCIES =====\n",
    "import subprocess\n",
    "\n",
    "print(\"\\nInstalling dependencies...\")\n",
    "for pkg in [\n",
    "    \"transformers\",\n",
    "    \"sentence-transformers\",\n",
    "    \"pandas\",\n",
    "    \"tqdm\",\n",
    "    \"scikit-learn\",\n",
    "    \"pyyaml\",\n",
    "    \"psutil\",\n",
    "    \"datasets\",\n",
    "]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GPU DETECTION & RESOURCE ALLOCATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Detect hardware\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    GPU_COUNT = torch.cuda.device_count()\n",
    "else:\n",
    "    GPU_NAME = \"CPU\"\n",
    "    VRAM_GB = 0\n",
    "    GPU_COUNT = 0\n",
    "\n",
    "RAM_GB = psutil.virtual_memory().total / 1e9\n",
    "\n",
    "print(f\"\\nDetected Hardware:\")\n",
    "print(f\"  GPU:  {GPU_NAME}\" + (f\" (x{GPU_COUNT})\" if GPU_COUNT > 1 else \"\"))\n",
    "print(\n",
    "    f\"  VRAM: {VRAM_GB:.1f} GB\" + (f\" (total: {VRAM_GB*GPU_COUNT:.1f} GB)\" if GPU_COUNT > 1 else \"\")\n",
    ")\n",
    "print(f\"  RAM:  {RAM_GB:.1f} GB\")\n",
    "\n",
    "# Set optimal parameters based on hardware\n",
    "if VRAM_GB >= 22:  # L4 (24GB) or A100\n",
    "    GPU_TIER = \"L4/A100\"\n",
    "elif VRAM_GB >= 14:  # T4 (16GB)\n",
    "    GPU_TIER = \"T4\"\n",
    "elif VRAM_GB >= 10:\n",
    "    GPU_TIER = \"SMALL\"\n",
    "else:\n",
    "    GPU_TIER = \"MINIMAL/CPU\"\n",
    "\n",
    "# Kaggle with 2xT4 can use larger batch\n",
    "if ENV_NAME == \"KAGGLE\" and GPU_COUNT >= 2:\n",
    "    GPU_TIER = \"2xT4\"\n",
    "    print(f\"  ** Kaggle 2xT4 detected **\")\n",
    "\n",
    "# Get backbone-specific batch size\n",
    "BATCH_SIZE = BACKBONE_CONFIG[\"recommended_batch\"].get(GPU_TIER, 64)\n",
    "print(f\"  Backbone: {BACKBONE} -> batch size {BATCH_SIZE}\")\n",
    "\n",
    "MAX_PER_LANG = 50000  # Language sample limit\n",
    "CPU_CORES = os.cpu_count() or 2\n",
    "NUM_WORKERS = min(4, CPU_CORES - 1) if RAM_GB >= 24 and VRAM_GB >= 14 else 0\n",
    "MAX_TEST_SAMPLES = 20000\n",
    "LR = 2e-5 * (BATCH_SIZE / 256)\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(f\"OPTIMAL SETTINGS:\")\n",
    "print(f\"-\" * 60)\n",
    "print(f\"  Environment:     {ENV_NAME}\")\n",
    "print(f\"  GPU Tier:        {GPU_TIER}\")\n",
    "print(f\"  Backbone:        {BACKBONE}\")\n",
    "print(f\"  Batch size:      {BATCH_SIZE}\")\n",
    "print(f\"  Max per lang:    {MAX_PER_LANG:,}\")\n",
    "print(f\"  DataLoader workers: {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate:   {LR:.2e}\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "scaler = torch.amp.GradScaler(\"cuda\") if USE_AMP else None\n",
    "\n",
    "# ===== PERSISTENT STORAGE SETUP =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERSISTENT STORAGE SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "SAVE_DIR = None\n",
    "DRIVE_HAS_DATA = False\n",
    "DRIVE_FILES = set()  # Use set for O(1) lookup\n",
    "\n",
    "if ENV_NAME == \"COLAB\":\n",
    "    # Google Colab - mount Drive\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "\n",
    "        DRIVE_MOUNT_PATH = \"/content/drive\"\n",
    "\n",
    "        if os.path.exists(f\"{DRIVE_MOUNT_PATH}/MyDrive\"):\n",
    "            print(\"Google Drive already mounted\")\n",
    "        else:\n",
    "            try:\n",
    "                drive.mount(DRIVE_MOUNT_PATH, force_remount=False)\n",
    "                print(\"Google Drive mounted successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Drive mount issue: {e}\")\n",
    "                try:\n",
    "                    drive.mount(DRIVE_MOUNT_PATH, force_remount=True)\n",
    "                    print(\"Google Drive mounted (force remount)\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"WARNING: Could not mount Drive: {e2}\")\n",
    "                    print(\"Falling back to local storage\")\n",
    "                    PERSISTENT_STORAGE = DATA_DIR\n",
    "\n",
    "        SAVE_DIR = f\"{DRIVE_MOUNT_PATH}/MyDrive/{DRIVE_FOLDER}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Colab Drive setup failed: {e}\")\n",
    "        SAVE_DIR = f\"{DATA_DIR}/{DRIVE_FOLDER}\"\n",
    "\n",
    "elif ENV_NAME == \"KAGGLE\":\n",
    "    # Kaggle - use working directory\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using Kaggle working directory: {SAVE_DIR}\")\n",
    "    print(\"Note: Data persists until kernel is reset\")\n",
    "    # Check for uploaded datasets\n",
    "    if os.path.exists(\"/kaggle/input\"):\n",
    "        datasets = os.listdir(\"/kaggle/input\")\n",
    "        if datasets:\n",
    "            print(f\"Available datasets: {datasets[:5]}\")\n",
    "\n",
    "elif ENV_NAME == \"LIGHTNING_AI\":\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using Lightning.ai studio storage: {SAVE_DIR}\")\n",
    "\n",
    "elif ENV_NAME == \"PAPERSPACE\":\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using Paperspace /storage: {SAVE_DIR}\")\n",
    "\n",
    "elif ENV_NAME == \"HUGGINGFACE_SPACES\":\n",
    "    # HF Spaces has limited persistent storage\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using HuggingFace Spaces storage: {SAVE_DIR}\")\n",
    "    print(\"Warning: HF Spaces storage is limited\")\n",
    "\n",
    "else:\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using local storage: {SAVE_DIR}\")\n",
    "\n",
    "# Check if folder exists BEFORE creating it\n",
    "folder_existed = os.path.exists(SAVE_DIR)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Check what's available in storage - use BOTH listdir AND direct exists checks\n",
    "# (Google Drive can have sync issues where listdir misses files)\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    DRIVE_FILES = set(os.listdir(SAVE_DIR))  # O(1) membership test\n",
    "\n",
    "    # Direct existence checks for key files (bypasses listdir caching issues)\n",
    "    key_files = [\"passages.jsonl\", \"bonds.jsonl\", \"dear_abby.csv\", \"all_splits.json\"]\n",
    "    for kf in key_files:\n",
    "        kf_path = os.path.join(SAVE_DIR, kf)\n",
    "        if os.path.exists(kf_path) and kf not in DRIVE_FILES:\n",
    "            print(f\"  [Drive sync fix] Found {kf} via os.path.exists() but not listdir()\")\n",
    "            DRIVE_FILES.add(kf)\n",
    "\n",
    "    DRIVE_HAS_DATA = \"passages.jsonl\" in DRIVE_FILES and \"bonds.jsonl\" in DRIVE_FILES\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(f\"STORAGE STATUS:\")\n",
    "print(f\"-\" * 60)\n",
    "print(f\"  Folder: {SAVE_DIR}\")\n",
    "print(f\"  Folder existed: {folder_existed}\")\n",
    "print(f\"  Files found: {len(DRIVE_FILES)}\")\n",
    "\n",
    "# If folder was empty/new, show what folders exist in parent to help debug\n",
    "if not DRIVE_FILES and ENV_NAME == \"COLAB\":\n",
    "    parent = os.path.dirname(SAVE_DIR)  # e.g., /content/drive/MyDrive\n",
    "    if os.path.exists(parent):\n",
    "        siblings = [d for d in os.listdir(parent) if \"bip\" in d.lower() or \"BIP\" in d]\n",
    "        if siblings:\n",
    "            print(f\"  ** Similar folders in {parent}: {siblings}\")\n",
    "        else:\n",
    "            print(f\"  ** No BIP folders found in {parent}\")\n",
    "if DRIVE_FILES:\n",
    "    for f in sorted(DRIVE_FILES)[:10]:  # sorted() converts to list for slicing\n",
    "        print(f\"    - {f}\")\n",
    "    if len(DRIVE_FILES) > 10:\n",
    "        print(f\"    ... and {len(DRIVE_FILES)-10} more\")\n",
    "print(f\"  Pre-processed data available: {DRIVE_HAS_DATA}\")\n",
    "\n",
    "# Decide data loading strategy\n",
    "LOAD_FROM_DRIVE = USE_DRIVE_DATA and DRIVE_HAS_DATA and not REFRESH_DATA_FROM_SOURCE\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"DATA LOADING STRATEGY: {DATA_MODE}\")\n",
    "print(\"-\" * 60)\n",
    "if DATA_MODE == \"Refresh all\":\n",
    "    print(f\"  -> Will re-download ALL data from online sources\")\n",
    "    print(f\"     (This takes ~2 hours, use 'Update missing' to save time)\")\n",
    "elif DATA_MODE == \"Cache only\":\n",
    "    if LOAD_FROM_DRIVE:\n",
    "        print(f\"  -> Using cached data only (no downloads)\")\n",
    "    else:\n",
    "        print(f\"  -> ERROR: Cache-only mode but no cached data found!\")\n",
    "        print(f\"     Change DATA_MODE to 'Update missing'\")\n",
    "else:  # Update missing (default)\n",
    "    if LOAD_FROM_DRIVE:\n",
    "        print(f\"  -> Using cached processed data from Drive\")\n",
    "        print(f\"     (v10.9 corpora will be added if missing)\")\n",
    "    else:\n",
    "        print(f\"  -> Will download missing data, use cached where available\")\n",
    "        print(\n",
    "            f\"     Sefaria: {'cached' if os.path.exists(f'{SAVE_DIR}/Sefaria-Export-json.tar.gz') else 'will download'}\"\n",
    "        )\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create local directories\n",
    "for d in [\"data/processed\", \"data/splits\", \"data/raw\", \"models/checkpoints\", \"results\"]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"SETUP COMPLETE\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"  Environment: {ENV_NAME}\")\n",
    "print(f\"  GPU:         {GPU_NAME} ({GPU_TIER})\")\n",
    "print(f\"  Storage:     {SAVE_DIR}\")\n",
    "print(f\"  Ready to run: Cell 2 (Imports)\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2. Download/Load Corpora { display-mode: \"form\" }\n",
    "# @markdown Downloads from online sources OR loads from Google Drive\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING CORPORA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Force Google Drive sync refresh (workaround for stale FUSE mount)\n",
    "if ENV_NAME == \"COLAB\" and SAVE_DIR and os.path.exists(os.path.dirname(SAVE_DIR)):\n",
    "    try:\n",
    "        # Accessing the directory forces FUSE to refresh\n",
    "        _ = os.listdir(SAVE_DIR)\n",
    "        # Also touch parent to wake up sync\n",
    "        _ = os.listdir(os.path.dirname(SAVE_DIR))\n",
    "        print(\"  [Drive sync refreshed]\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [Drive sync warning: {e}]\")\n",
    "\n",
    "if LOAD_FROM_DRIVE:\n",
    "    # ===== LOAD FROM DRIVE =====\n",
    "    print(\"\\nLoading pre-processed data from Google Drive...\")\n",
    "\n",
    "    # Copy files from Drive to local\n",
    "    for fname in [\"passages.jsonl\", \"bonds.jsonl\"]:\n",
    "        src = f\"{SAVE_DIR}/{fname}\"\n",
    "        dst = f\"data/processed/{fname}\"\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "            print(f\"  Copied {fname}\")\n",
    "\n",
    "    if os.path.exists(f\"{SAVE_DIR}/all_splits.json\"):\n",
    "        shutil.copy(f\"{SAVE_DIR}/all_splits.json\", \"data/splits/all_splits.json\")\n",
    "        print(f\"  Copied all_splits.json\")\n",
    "\n",
    "    # Load Dear Abby from Drive if available (check filesystem, not cached set)\n",
    "    abby_drive_path = f\"{SAVE_DIR}/dear_abby.csv\"\n",
    "    if os.path.exists(abby_drive_path):\n",
    "        shutil.copy(abby_drive_path, \"data/raw/dear_abby.csv\")\n",
    "        print(f\"  Copied dear_abby.csv from {abby_drive_path}\")\n",
    "\n",
    "    # Count loaded data\n",
    "    if os.path.exists(\"data/processed/passages.jsonl\"):\n",
    "        with open(\"data/processed/passages.jsonl\") as f:\n",
    "            n_passages = sum(1 for _ in f)\n",
    "        print(f\"\\nLoaded {n_passages:,} passages from Drive\")\n",
    "\n",
    "    SKIP_PROCESSING = True\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Drive data loaded - skipping download/processing\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "else:\n",
    "    # ===== DOWNLOAD/UPDATE FROM ONLINE =====\n",
    "    SKIP_PROCESSING = False\n",
    "\n",
    "    # Check if CACHE_ONLY mode but cache is missing\n",
    "    if CACHE_ONLY:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ERROR: CACHE_ONLY mode but cached data not found!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Options:\")\n",
    "        print(\"  1. Change DATA_MODE to 'Update missing' or 'Refresh all'\")\n",
    "        print(\"  2. Ensure Drive has: passages.jsonl, bonds.jsonl\")\n",
    "        raise RuntimeError(\"Cache-only mode requires cached data. Change DATA_MODE.\")\n",
    "\n",
    "    # SEFARIA - with Drive caching\n",
    "    sefaria_local = \"data/raw/Sefaria-Export/json\"\n",
    "    sefaria_drive = f\"{SAVE_DIR}/Sefaria-Export-json.tar.gz\" if USE_DRIVE_DATA else None\n",
    "\n",
    "    if os.path.exists(sefaria_local):\n",
    "        print(\"\\n[1/4] Sefaria already exists locally\")\n",
    "    elif sefaria_drive and os.path.exists(sefaria_drive):\n",
    "        print(\"\\n[1/4] Restoring Sefaria from Drive cache...\")\n",
    "        import tarfile\n",
    "\n",
    "        os.makedirs(\"data/raw/Sefaria-Export\", exist_ok=True)\n",
    "        with tarfile.open(sefaria_drive, \"r:gz\") as tar:\n",
    "            tar.extractall(\"data/raw/Sefaria-Export\")\n",
    "        print(\"  Restored from Drive!\")\n",
    "    else:\n",
    "        print(\"\\n[1/4] Downloading Sefaria (~2GB)...\")\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"git\",\n",
    "                \"clone\",\n",
    "                \"--depth\",\n",
    "                \"1\",\n",
    "                \"https://github.com/Sefaria/Sefaria-Export.git\",\n",
    "                \"data/raw/Sefaria-Export\",\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "        print(\"  Done!\")\n",
    "        # Cache to Drive for next time\n",
    "        if USE_DRIVE_DATA and SAVE_DIR:\n",
    "            print(\"  Caching Sefaria to Drive (this may take a minute)...\")\n",
    "            import tarfile\n",
    "\n",
    "            with tarfile.open(sefaria_drive, \"w:gz\") as tar:\n",
    "                tar.add(\"data/raw/Sefaria-Export/json\", arcname=\"json\")\n",
    "            print(f\"  Cached to {sefaria_drive}\")\n",
    "\n",
    "    # CHINESE - 200+ REAL CLASSICAL TEXTS\n",
    "    print(\"\\n[2/4] Chinese classics (200+ real passages)...\")\n",
    "    os.makedirs(\"data/raw/chinese\", exist_ok=True)\n",
    "\n",
    "    chinese = []\n",
    "\n",
    "    # === ANALECTS (論語) - 50+ passages ===\n",
    "    analects = [\n",
    "        (\"子曰：己所不欲，勿施於人。\", \"Analects 15.24\"),\n",
    "        (\"孝悌也者，其為仁之本與。\", \"Analects 1.2\"),\n",
    "        (\"父母在，不遠游，遊必有方。\", \"Analects 4.19\"),\n",
    "        (\"君子喻於義，小人喻於利。\", \"Analects 4.16\"),\n",
    "        (\"不義而富且貴，於我如浮雲。\", \"Analects 7.16\"),\n",
    "        (\"學而時習之，不亦說乎。\", \"Analects 1.1\"),\n",
    "        (\"有朋自遠方來，不亦樂乎。\", \"Analects 1.1\"),\n",
    "        (\"人不知而不慍，不亦君子乎。\", \"Analects 1.1\"),\n",
    "        (\"巧言令色，鮮矣仁。\", \"Analects 1.3\"),\n",
    "        (\"吾日三省吾身。\", \"Analects 1.4\"),\n",
    "        (\"為人謀而不忠乎，與朋友交而不信乎。\", \"Analects 1.4\"),\n",
    "        (\"弟子入則孝，出則悌。\", \"Analects 1.6\"),\n",
    "        (\"謹而信，汎愛眾，而親仁。\", \"Analects 1.6\"),\n",
    "        (\"君子不重則不威，學則不固。\", \"Analects 1.8\"),\n",
    "        (\"主忠信，無友不如己者。\", \"Analects 1.8\"),\n",
    "        (\"過則勿憚改。\", \"Analects 1.8\"),\n",
    "        (\"慎終追遠，民德歸厚矣。\", \"Analects 1.9\"),\n",
    "        (\"禮之用，和為貴。\", \"Analects 1.12\"),\n",
    "        (\"信近於義，言可復也。\", \"Analects 1.13\"),\n",
    "        (\"君子食無求飽，居無求安。\", \"Analects 1.14\"),\n",
    "        (\"敏於事而慎於言，就有道而正焉。\", \"Analects 1.14\"),\n",
    "        (\"不患人之不己知，患不知人也。\", \"Analects 1.16\"),\n",
    "        (\"為政以德，譬如北辰。\", \"Analects 2.1\"),\n",
    "        (\"道之以政，齊之以刑，民免而無恥。\", \"Analects 2.3\"),\n",
    "        (\"道之以德，齊之以禮，有恥且格。\", \"Analects 2.3\"),\n",
    "        (\"吾十有五而志于學。\", \"Analects 2.4\"),\n",
    "        (\"三十而立，四十而不惑。\", \"Analects 2.4\"),\n",
    "        (\"五十而知天命，六十而耳順。\", \"Analects 2.4\"),\n",
    "        (\"七十而從心所欲，不逾矩。\", \"Analects 2.4\"),\n",
    "        (\"生，事之以禮；死，葬之以禮，祭之以禮。\", \"Analects 2.5\"),\n",
    "        (\"父母唯其疾之憂。\", \"Analects 2.6\"),\n",
    "        (\"今之孝者，是謂能養。\", \"Analects 2.7\"),\n",
    "        (\"至於犬馬，皆能有養；不敬，何以別乎。\", \"Analects 2.7\"),\n",
    "        (\"色難。有事，弟子服其勞。\", \"Analects 2.8\"),\n",
    "        (\"視其所以，觀其所由，察其所安。\", \"Analects 2.10\"),\n",
    "        (\"溫故而知新，可以為師矣。\", \"Analects 2.11\"),\n",
    "        (\"君子不器。\", \"Analects 2.12\"),\n",
    "        (\"先行其言而後從之。\", \"Analects 2.13\"),\n",
    "        (\"君子周而不比，小人比而不周。\", \"Analects 2.14\"),\n",
    "        (\"學而不思則罔，思而不學則殆。\", \"Analects 2.15\"),\n",
    "        (\"知之為知之，不知為不知，是知也。\", \"Analects 2.17\"),\n",
    "        (\"多聞闕疑，慎言其餘，則寡尤。\", \"Analects 2.18\"),\n",
    "        (\"舉直錯諸枉，則民服。\", \"Analects 2.19\"),\n",
    "        (\"人而無信，不知其可也。\", \"Analects 2.22\"),\n",
    "        (\"見義不為，無勇也。\", \"Analects 2.24\"),\n",
    "        (\"非其鬼而祭之，諂也。\", \"Analects 2.24\"),\n",
    "        (\"是可忍也，孰不可忍也。\", \"Analects 3.1\"),\n",
    "        (\"人而不仁，如禮何。\", \"Analects 3.3\"),\n",
    "        (\"人而不仁，如樂何。\", \"Analects 3.3\"),\n",
    "        (\"里仁為美。擇不處仁，焉得知。\", \"Analects 4.1\"),\n",
    "        (\"不仁者不可以久處約，不可以長處樂。\", \"Analects 4.2\"),\n",
    "        (\"仁者安仁，知者利仁。\", \"Analects 4.2\"),\n",
    "        (\"唯仁者能好人，能惡人。\", \"Analects 4.3\"),\n",
    "        (\"苟志於仁矣，無惡也。\", \"Analects 4.4\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(analects):\n",
    "        chinese.append(\n",
    "            {\n",
    "                \"id\": f\"cn_analects_{i}\",\n",
    "                \"text\": text,\n",
    "                \"source\": source,\n",
    "                \"period\": \"CONFUCIAN\",\n",
    "                \"century\": -5,\n",
    "            }\n",
    "        )\n",
    "    print(f\"    - Analects: {len([x for x in chinese if 'analects' in x['id']]):,} passages\")\n",
    "\n",
    "    # === MENCIUS (孟子) - 40+ passages ===\n",
    "    mencius = [\n",
    "        (\"惻隱之心，仁之端也。\", \"Mencius 2A.6\"),\n",
    "        (\"羞惡之心，義之端也。\", \"Mencius 2A.6\"),\n",
    "        (\"辭讓之心，禮之端也。\", \"Mencius 2A.6\"),\n",
    "        (\"是非之心，智之端也。\", \"Mencius 2A.6\"),\n",
    "        (\"人皆有不忍人之心。\", \"Mencius 2A.6\"),\n",
    "        (\"無惻隱之心，非人也。\", \"Mencius 2A.6\"),\n",
    "        (\"無羞惡之心，非人也。\", \"Mencius 2A.6\"),\n",
    "        (\"無辭讓之心，非人也。\", \"Mencius 2A.6\"),\n",
    "        (\"無是非之心，非人也。\", \"Mencius 2A.6\"),\n",
    "        (\"仁義禮智，非由外鑠我也，我固有之也。\", \"Mencius 6A.6\"),\n",
    "        (\"人性之善也，猶水之就下也。\", \"Mencius 6A.2\"),\n",
    "        (\"人無有不善，水無有不下。\", \"Mencius 6A.2\"),\n",
    "        (\"惟仁者宜在高位。\", \"Mencius 4A.1\"),\n",
    "        (\"不仁而在高位，是播其惡於眾也。\", \"Mencius 4A.1\"),\n",
    "        (\"民為貴，社稷次之，君為輕。\", \"Mencius 7B.14\"),\n",
    "        (\"得道者多助，失道者寡助。\", \"Mencius 2B.1\"),\n",
    "        (\"寡助之至，親戚畔之。\", \"Mencius 2B.1\"),\n",
    "        (\"多助之至，天下順之。\", \"Mencius 2B.1\"),\n",
    "        (\"天時不如地利，地利不如人和。\", \"Mencius 2B.1\"),\n",
    "        (\"生於憂患，死於安樂。\", \"Mencius 6B.15\"),\n",
    "        (\"天將降大任於是人也，必先苦其心志。\", \"Mencius 6B.15\"),\n",
    "        (\"勞其筋骨，餓其體膚。\", \"Mencius 6B.15\"),\n",
    "        (\"空乏其身，行拂亂其所為。\", \"Mencius 6B.15\"),\n",
    "        (\"所以動心忍性，曾益其所不能。\", \"Mencius 6B.15\"),\n",
    "        (\"老吾老，以及人之老。\", \"Mencius 1A.7\"),\n",
    "        (\"幼吾幼，以及人之幼。\", \"Mencius 1A.7\"),\n",
    "        (\"窮則獨善其身，達則兼善天下。\", \"Mencius 7A.9\"),\n",
    "        (\"魚，我所欲也；熊掌，亦我所欲也。\", \"Mencius 6A.10\"),\n",
    "        (\"二者不可得兼，舍魚而取熊掌者也。\", \"Mencius 6A.10\"),\n",
    "        (\"生，亦我所欲也；義，亦我所欲也。\", \"Mencius 6A.10\"),\n",
    "        (\"二者不可得兼，舍生而取義者也。\", \"Mencius 6A.10\"),\n",
    "        (\"養心莫善於寡欲。\", \"Mencius 7B.35\"),\n",
    "        (\"仁者無敵於天下。\", \"Mencius 1A.5\"),\n",
    "        (\"以力服人者，非心服也。\", \"Mencius 2A.3\"),\n",
    "        (\"以德服人者，中心悅而誠服也。\", \"Mencius 2A.3\"),\n",
    "        (\"人之患在好為人師。\", \"Mencius 4A.23\"),\n",
    "        (\"盡信書，則不如無書。\", \"Mencius 7B.3\"),\n",
    "        (\"不以規矩，不能成方圓。\", \"Mencius 4A.1\"),\n",
    "        (\"孝子之至，莫大乎尊親。\", \"Mencius 5A.4\"),\n",
    "        (\"父子有親，君臣有義，夫婦有別，長幼有序，朋友有信。\", \"Mencius 3A.4\"),\n",
    "        (\"人有不為也，而後可以有為。\", \"Mencius 4B.8\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(mencius):\n",
    "        chinese.append(\n",
    "            {\n",
    "                \"id\": f\"cn_mencius_{i}\",\n",
    "                \"text\": text,\n",
    "                \"source\": source,\n",
    "                \"period\": \"CONFUCIAN\",\n",
    "                \"century\": -4,\n",
    "            }\n",
    "        )\n",
    "    print(f\"    - Mencius: {len([x for x in chinese if 'mencius' in x['id']]):,} passages\")\n",
    "\n",
    "    # === DAODEJING (道德經) - 40+ passages ===\n",
    "    daodejing = [\n",
    "        (\"道可道，非常道。名可名，非常名。\", \"Daodejing 1\"),\n",
    "        (\"天下皆知美之為美，斯惡已。\", \"Daodejing 2\"),\n",
    "        (\"皆知善之為善，斯不善已。\", \"Daodejing 2\"),\n",
    "        (\"有無相生，難易相成。\", \"Daodejing 2\"),\n",
    "        (\"長短相較，高下相傾。\", \"Daodejing 2\"),\n",
    "        (\"是以聖人處無為之事，行不言之教。\", \"Daodejing 2\"),\n",
    "        (\"不尚賢，使民不爭。\", \"Daodejing 3\"),\n",
    "        (\"不貴難得之貨，使民不為盜。\", \"Daodejing 3\"),\n",
    "        (\"上善若水。水善利萬物而不爭。\", \"Daodejing 8\"),\n",
    "        (\"處眾人之所惡，故幾於道。\", \"Daodejing 8\"),\n",
    "        (\"居善地，心善淵，與善仁。\", \"Daodejing 8\"),\n",
    "        (\"言善信，政善治，事善能，動善時。\", \"Daodejing 8\"),\n",
    "        (\"夫唯不爭，故無尤。\", \"Daodejing 8\"),\n",
    "        (\"金玉滿堂，莫之能守。\", \"Daodejing 9\"),\n",
    "        (\"富貴而驕，自遺其咎。\", \"Daodejing 9\"),\n",
    "        (\"功成身退，天之道也。\", \"Daodejing 9\"),\n",
    "        (\"知人者智，自知者明。\", \"Daodejing 33\"),\n",
    "        (\"勝人者有力，自勝者強。\", \"Daodejing 33\"),\n",
    "        (\"知足者富，強行者有志。\", \"Daodejing 33\"),\n",
    "        (\"不失其所者久，死而不亡者壽。\", \"Daodejing 33\"),\n",
    "        (\"大道廢，有仁義。\", \"Daodejing 18\"),\n",
    "        (\"智慧出，有大偽。\", \"Daodejing 18\"),\n",
    "        (\"六親不和，有孝慈。\", \"Daodejing 18\"),\n",
    "        (\"國家昏亂，有忠臣。\", \"Daodejing 18\"),\n",
    "        (\"禍兮福之所倚，福兮禍之所伏。\", \"Daodejing 58\"),\n",
    "        (\"天長地久。\", \"Daodejing 7\"),\n",
    "        (\"天地所以能長且久者，以其不自生。\", \"Daodejing 7\"),\n",
    "        (\"是以聖人後其身而身先。\", \"Daodejing 7\"),\n",
    "        (\"外其身而身存。\", \"Daodejing 7\"),\n",
    "        (\"非以其無私耶，故能成其私。\", \"Daodejing 7\"),\n",
    "        (\"柔弱勝剛強。\", \"Daodejing 36\"),\n",
    "        (\"大方無隅，大器晚成。\", \"Daodejing 41\"),\n",
    "        (\"大音希聲，大象無形。\", \"Daodejing 41\"),\n",
    "        (\"道生一，一生二，二生三，三生萬物。\", \"Daodejing 42\"),\n",
    "        (\"天下萬物生於有，有生於無。\", \"Daodejing 40\"),\n",
    "        (\"千里之行，始於足下。\", \"Daodejing 64\"),\n",
    "        (\"合抱之木，生於毫末。\", \"Daodejing 64\"),\n",
    "        (\"九層之臺，起於累土。\", \"Daodejing 64\"),\n",
    "        (\"民不畏死，奈何以死懼之。\", \"Daodejing 74\"),\n",
    "        (\"信言不美，美言不信。\", \"Daodejing 81\"),\n",
    "        (\"善者不辯，辯者不善。\", \"Daodejing 81\"),\n",
    "        (\"知者不博，博者不知。\", \"Daodejing 81\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(daodejing):\n",
    "        chinese.append(\n",
    "            {\n",
    "                \"id\": f\"cn_daodejing_{i}\",\n",
    "                \"text\": text,\n",
    "                \"source\": source,\n",
    "                \"period\": \"DAOIST\",\n",
    "                \"century\": -4,\n",
    "            }\n",
    "        )\n",
    "    print(f\"    - Daodejing: {len([x for x in chinese if 'daodejing' in x['id']]):,} passages\")\n",
    "\n",
    "    # === GREAT LEARNING (大學) - 20+ passages ===\n",
    "    daxue = [\n",
    "        (\"大學之道，在明明德，在親民，在止於至善。\", \"Great Learning 1\"),\n",
    "        (\"知止而後有定，定而後能靜。\", \"Great Learning 1\"),\n",
    "        (\"靜而後能安，安而後能慮，慮而後能得。\", \"Great Learning 1\"),\n",
    "        (\"物有本末，事有終始。\", \"Great Learning 1\"),\n",
    "        (\"知所先後，則近道矣。\", \"Great Learning 1\"),\n",
    "        (\"古之欲明明德於天下者，先治其國。\", \"Great Learning 1\"),\n",
    "        (\"欲治其國者，先齊其家。\", \"Great Learning 1\"),\n",
    "        (\"欲齊其家者，先修其身。\", \"Great Learning 1\"),\n",
    "        (\"欲修其身者，先正其心。\", \"Great Learning 1\"),\n",
    "        (\"欲正其心者，先誠其意。\", \"Great Learning 1\"),\n",
    "        (\"欲誠其意者，先致其知。\", \"Great Learning 1\"),\n",
    "        (\"致知在格物。\", \"Great Learning 1\"),\n",
    "        (\"物格而後知至，知至而後意誠。\", \"Great Learning 1\"),\n",
    "        (\"意誠而後心正，心正而後身修。\", \"Great Learning 1\"),\n",
    "        (\"身修而後家齊，家齊而後國治。\", \"Great Learning 1\"),\n",
    "        (\"國治而後天下平。\", \"Great Learning 1\"),\n",
    "        (\"自天子以至於庶人，壹是皆以修身為本。\", \"Great Learning 1\"),\n",
    "        (\"其本亂而末治者否矣。\", \"Great Learning 1\"),\n",
    "        (\"所謂誠其意者，毋自欺也。\", \"Great Learning 6\"),\n",
    "        (\"如惡惡臭，如好好色，此之謂自謙。\", \"Great Learning 6\"),\n",
    "        (\"故君子必慎其獨也。\", \"Great Learning 6\"),\n",
    "        (\"富潤屋，德潤身，心廣體胖。\", \"Great Learning 6\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(daxue):\n",
    "        chinese.append(\n",
    "            {\n",
    "                \"id\": f\"cn_daxue_{i}\",\n",
    "                \"text\": text,\n",
    "                \"source\": source,\n",
    "                \"period\": \"CONFUCIAN\",\n",
    "                \"century\": -5,\n",
    "            }\n",
    "        )\n",
    "    print(f\"    - Great Learning: {len([x for x in chinese if 'daxue' in x['id']]):,} passages\")\n",
    "\n",
    "    # === DOCTRINE OF THE MEAN (中庸) - 20+ passages ===\n",
    "    zhongyong = [\n",
    "        (\"天命之謂性，率性之謂道，修道之謂教。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"道也者，不可須臾離也；可離，非道也。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"是故君子戒慎乎其所不睹，恐懼乎其所不聞。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"莫見乎隱，莫顯乎微，故君子慎其獨也。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"喜怒哀樂之未發，謂之中。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"發而皆中節，謂之和。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"中也者，天下之大本也。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"和也者，天下之達道也。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"致中和，天地位焉，萬物育焉。\", \"Doctrine of the Mean 1\"),\n",
    "        (\"君子中庸，小人反中庸。\", \"Doctrine of the Mean 2\"),\n",
    "        (\"君子之中庸也，君子而時中。\", \"Doctrine of the Mean 2\"),\n",
    "        (\"小人之反中庸也，小人而無忌憚也。\", \"Doctrine of the Mean 2\"),\n",
    "        (\"中庸其至矣乎！民鮮能久矣。\", \"Doctrine of the Mean 3\"),\n",
    "        (\"道之不行也，我知之矣：知者過之，愚者不及也。\", \"Doctrine of the Mean 4\"),\n",
    "        (\"道之不明也，我知之矣：賢者過之，不肖者不及也。\", \"Doctrine of the Mean 4\"),\n",
    "        (\"人莫不飲食也，鮮能知味也。\", \"Doctrine of the Mean 4\"),\n",
    "        (\"誠者，天之道也。誠之者，人之道也。\", \"Doctrine of the Mean 20\"),\n",
    "        (\"誠者，不勉而中，不思而得，從容中道，聖人也。\", \"Doctrine of the Mean 20\"),\n",
    "        (\"誠之者，擇善而固執之者也。\", \"Doctrine of the Mean 20\"),\n",
    "        (\"博學之，審問之，慎思之，明辨之，篤行之。\", \"Doctrine of the Mean 20\"),\n",
    "        (\"人一能之，己百之；人十能之，己千之。\", \"Doctrine of the Mean 20\"),\n",
    "        (\"果能此道矣，雖愚必明，雖柔必強。\", \"Doctrine of the Mean 20\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(zhongyong):\n",
    "        chinese.append(\n",
    "            {\n",
    "                \"id\": f\"cn_zhongyong_{i}\",\n",
    "                \"text\": text,\n",
    "                \"source\": source,\n",
    "                \"period\": \"CONFUCIAN\",\n",
    "                \"century\": -5,\n",
    "            }\n",
    "        )\n",
    "    print(\n",
    "        f\"    - Doctrine of Mean: {len([x for x in chinese if 'zhongyong' in x['id']]):,} passages\"\n",
    "    )\n",
    "\n",
    "    # === BOOK OF RITES (禮記) - 30+ passages ===\n",
    "    liji = [\n",
    "        (\"禮尚往來。往而不來，非禮也；來而不往，亦非禮也。\", \"Book of Rites - Quli\"),\n",
    "        (\"敖不可長，欲不可從，志不可滿，樂不可極。\", \"Book of Rites - Quli\"),\n",
    "        (\"臨財毋茍得，臨難毋茍免。\", \"Book of Rites - Quli\"),\n",
    "        (\"夫禮者，自卑而尊人。\", \"Book of Rites - Quli\"),\n",
    "        (\"雖負販者，必有尊也，而況富貴乎。\", \"Book of Rites - Quli\"),\n",
    "        (\"富貴而知好禮，則不驕不淫。\", \"Book of Rites - Quli\"),\n",
    "        (\"貧賤而知好禮，則志不懾。\", \"Book of Rites - Quli\"),\n",
    "        (\"大道之行也，天下為公。\", \"Book of Rites - Liyun\"),\n",
    "        (\"選賢與能，講信修睦。\", \"Book of Rites - Liyun\"),\n",
    "        (\"故人不獨親其親，不獨子其子。\", \"Book of Rites - Liyun\"),\n",
    "        (\"使老有所終，壯有所用，幼有所長。\", \"Book of Rites - Liyun\"),\n",
    "        (\"矜寡孤獨廢疾者皆有所養。\", \"Book of Rites - Liyun\"),\n",
    "        (\"男有分，女有歸。\", \"Book of Rites - Liyun\"),\n",
    "        (\"貨惡其棄於地也，不必藏於己。\", \"Book of Rites - Liyun\"),\n",
    "        (\"力惡其不出於身也，不必為己。\", \"Book of Rites - Liyun\"),\n",
    "        (\"是故謀閉而不興，盜竊亂賊而不作。\", \"Book of Rites - Liyun\"),\n",
    "        (\"故外戶而不閉，是謂大同。\", \"Book of Rites - Liyun\"),\n",
    "        (\"玉不琢，不成器；人不學，不知道。\", \"Book of Rites - Xueji\"),\n",
    "        (\"是故學然後知不足，教然後知困。\", \"Book of Rites - Xueji\"),\n",
    "        (\"知不足，然後能自反也。\", \"Book of Rites - Xueji\"),\n",
    "        (\"知困，然後能自強也。\", \"Book of Rites - Xueji\"),\n",
    "        (\"故曰：教學相長也。\", \"Book of Rites - Xueji\"),\n",
    "        (\"凡學之道，嚴師為難。\", \"Book of Rites - Xueji\"),\n",
    "        (\"師嚴然後道尊，道尊然後民知敬學。\", \"Book of Rites - Xueji\"),\n",
    "        (\"善歌者使人繼其聲，善教者使人繼其志。\", \"Book of Rites - Xueji\"),\n",
    "        (\"記問之學，不足以為人師。\", \"Book of Rites - Xueji\"),\n",
    "        (\"必也其聽語乎，力不能問，然後語之。\", \"Book of Rites - Xueji\"),\n",
    "        (\"語之而不知，雖舍之可也。\", \"Book of Rites - Xueji\"),\n",
    "        (\"博學而不窮，篤行而不倦。\", \"Book of Rites - Ruxing\"),\n",
    "        (\"君子之於學也，藏焉，修焉，息焉，游焉。\", \"Book of Rites - Xueji\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(liji):\n",
    "        chinese.append(\n",
    "            {\n",
    "                \"id\": f\"cn_liji_{i}\",\n",
    "                \"text\": text,\n",
    "                \"source\": source,\n",
    "                \"period\": \"CONFUCIAN\",\n",
    "                \"century\": -3,\n",
    "            }\n",
    "        )\n",
    "    print(f\"    - Book of Rites: {len([x for x in chinese if 'liji' in x['id']]):,} passages\")\n",
    "\n",
    "    with open(\"data/raw/chinese/chinese_native.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chinese, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"  Created {len(chinese)} Chinese passages\")\n",
    "\n",
    "    # ISLAMIC - 150+ REAL PASSAGES\n",
    "    print(\"\\n[3/4] Islamic texts (150+ real passages)...\")\n",
    "    os.makedirs(\"data/raw/islamic\", exist_ok=True)\n",
    "\n",
    "    islamic = []\n",
    "\n",
    "    # === QURANIC VERSES (40+) ===\n",
    "    quran = [\n",
    "        (\"وَلَا تَقْتُلُوا النَّفْسَ الَّتِي حَرَّمَ اللَّهُ إِلَّا بِالْحَقِّ\", \"Quran 6:151\"),\n",
    "        (\"وَبِالْوَالِدَيْنِ إِحْسَانًا\", \"Quran 17:23\"),\n",
    "        (\"إِمَّا يَبْلُغَنَّ عِندَكَ الْكِبَرَ أَحَدُهُمَا أَوْ كِلَاهُمَا فَلَا تَقُل لَّهُمَا أُفٍّ\", \"Quran 17:23\"),\n",
    "        (\"وَلَا تَنْهَرْهُمَا وَقُل لَّهُمَا قَوْلًا كَرِيمًا\", \"Quran 17:23\"),\n",
    "        (\"وَاخْفِضْ لَهُمَا جَنَاحَ الذُّلِّ مِنَ الرَّحْمَةِ\", \"Quran 17:24\"),\n",
    "        (\"وَقُل رَّبِّ ارْحَمْهُمَا كَمَا رَبَّيَانِي صَغِيرًا\", \"Quran 17:24\"),\n",
    "        (\"وَآتِ ذَا الْقُرْبَىٰ حَقَّهُ وَالْمِسْكِينَ وَابْنَ السَّبِيلِ\", \"Quran 17:26\"),\n",
    "        (\"وَلَا تُبَذِّرْ تَبْذِيرًا\", \"Quran 17:26\"),\n",
    "        (\"إِنَّ الْمُبَذِّرِينَ كَانُوا إِخْوَانَ الشَّيَاطِينِ\", \"Quran 17:27\"),\n",
    "        (\"وَلَا تَجْعَلْ يَدَكَ مَغْلُولَةً إِلَىٰ عُنُقِكَ وَلَا تَبْسُطْهَا كُلَّ الْبَسْطِ\", \"Quran 17:29\"),\n",
    "        (\"وَلَا تَقْرَبُوا الزِّنَا ۖ إِنَّهُ كَانَ فَاحِشَةً وَسَاءَ سَبِيلًا\", \"Quran 17:32\"),\n",
    "        (\"وَلَا تَقْتُلُوا أَوْلَادَكُمْ خَشْيَةَ إِمْلَاقٍ\", \"Quran 17:31\"),\n",
    "        (\"وَلَا تَقْرَبُوا مَالَ الْيَتِيمِ إِلَّا بِالَّتِي هِيَ أَحْسَنُ\", \"Quran 17:34\"),\n",
    "        (\"وَأَوْفُوا بِالْعَهْدِ ۖ إِنَّ الْعَهْدَ كَانَ مَسْئُولًا\", \"Quran 17:34\"),\n",
    "        (\"وَأَوْفُوا الْكَيْلَ إِذَا كِلْتُمْ وَزِنُوا بِالْقِسْطَاسِ الْمُسْتَقِيمِ\", \"Quran 17:35\"),\n",
    "        (\"وَلَا تَقْفُ مَا لَيْسَ لَكَ بِهِ عِلْمٌ\", \"Quran 17:36\"),\n",
    "        (\"إِنَّ السَّمْعَ وَالْبَصَرَ وَالْفُؤَادَ كُلُّ أُولَٰئِكَ كَانَ عَنْهُ مَسْئُولًا\", \"Quran 17:36\"),\n",
    "        (\"وَلَا تَمْشِ فِي الْأَرْضِ مَرَحًا\", \"Quran 17:37\"),\n",
    "        (\"إِنَّ اللَّهَ يَأْمُرُ بِالْعَدْلِ وَالْإِحْسَانِ وَإِيتَاءِ ذِي الْقُرْبَىٰ\", \"Quran 16:90\"),\n",
    "        (\"وَيَنْهَىٰ عَنِ الْفَحْشَاءِ وَالْمُنكَرِ وَالْبَغْيِ\", \"Quran 16:90\"),\n",
    "        (\"يَا أَيُّهَا الَّذِينَ آمَنُوا كُونُوا قَوَّامِينَ بِالْقِسْطِ\", \"Quran 4:135\"),\n",
    "        (\"شُهَدَاءَ لِلَّهِ وَلَوْ عَلَىٰ أَنفُسِكُمْ أَوِ الْوَالِدَيْنِ وَالْأَقْرَبِينَ\", \"Quran 4:135\"),\n",
    "        (\"وَإِذَا حَكَمْتُم بَيْنَ النَّاسِ أَن تَحْكُمُوا بِالْعَدْلِ\", \"Quran 4:58\"),\n",
    "        (\"يَا أَيُّهَا الَّذِينَ آمَنُوا أَوْفُوا بِالْعُقُودِ\", \"Quran 5:1\"),\n",
    "        (\"وَتَعَاوَنُوا عَلَى الْبِرِّ وَالتَّقْوَىٰ ۖ وَلَا تَعَاوَنُوا عَلَى الْإِثْمِ وَالْعُدْوَانِ\", \"Quran 5:2\"),\n",
    "        (\"مَن قَتَلَ نَفْسًا بِغَيْرِ نَفْسٍ أَوْ فَسَادٍ فِي الْأَرْضِ فَكَأَنَّمَا قَتَلَ النَّاسَ جَمِيعًا\", \"Quran 5:32\"),\n",
    "        (\"وَمَنْ أَحْيَاهَا فَكَأَنَّمَا أَحْيَا النَّاسَ جَمِيعًا\", \"Quran 5:32\"),\n",
    "        (\"وَلَا يَجْرِمَنَّكُمْ شَنَآنُ قَوْمٍ عَلَىٰ أَلَّا تَعْدِلُوا\", \"Quran 5:8\"),\n",
    "        (\"اعْدِلُوا هُوَ أَقْرَبُ لِلتَّقْوَىٰ\", \"Quran 5:8\"),\n",
    "        (\"لَّيْسَ الْبِرَّ أَن تُوَلُّوا وُجُوهَكُمْ قِبَلَ الْمَشْرِقِ وَالْمَغْرِبِ\", \"Quran 2:177\"),\n",
    "        (\"وَلَٰكِنَّ الْبِرَّ مَنْ آمَنَ بِاللَّهِ وَالْيَوْمِ الْآخِرِ\", \"Quran 2:177\"),\n",
    "        (\"وَآتَى الْمَالَ عَلَىٰ حُبِّهِ ذَوِي الْقُرْبَىٰ وَالْيَتَامَىٰ وَالْمَسَاكِينَ\", \"Quran 2:177\"),\n",
    "        (\"وَابْنَ السَّبِيلِ وَالسَّائِلِينَ وَفِي الرِّقَابِ\", \"Quran 2:177\"),\n",
    "        (\"وَأَقَامَ الصَّلَاةَ وَآتَى الزَّكَاةَ\", \"Quran 2:177\"),\n",
    "        (\"وَالْمُوفُونَ بِعَهْدِهِمْ إِذَا عَاهَدُوا\", \"Quran 2:177\"),\n",
    "        (\"وَالصَّابِرِينَ فِي الْبَأْسَاءِ وَالضَّرَّاءِ وَحِينَ الْبَأْسِ\", \"Quran 2:177\"),\n",
    "        (\"خُذِ الْعَفْوَ وَأْمُرْ بِالْعُرْفِ وَأَعْرِضْ عَنِ الْجَاهِلِينَ\", \"Quran 7:199\"),\n",
    "        (\"وَالْكَاظِمِينَ الْغَيْظَ وَالْعَافِينَ عَنِ النَّاسِ\", \"Quran 3:134\"),\n",
    "        (\"وَاللَّهُ يُحِبُّ الْمُحْسِنِينَ\", \"Quran 3:134\"),\n",
    "        (\"ادْفَعْ بِالَّتِي هِيَ أَحْسَنُ فَإِذَا الَّذِي بَيْنَكَ وَبَيْنَهُ عَدَاوَةٌ كَأَنَّهُ وَلِيٌّ حَمِيمٌ\", \"Quran 41:34\"),\n",
    "        (\"وَمَا يُلَقَّاهَا إِلَّا الَّذِينَ صَبَرُوا وَمَا يُلَقَّاهَا إِلَّا ذُو حَظٍّ عَظِيمٍ\", \"Quran 41:35\"),\n",
    "        (\"إِنَّ اللَّهَ يَأْمُرُكُمْ أَن تُؤَدُّوا الْأَمَانَاتِ إِلَىٰ أَهْلِهَا\", \"Quran 4:58\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(quran):\n",
    "        islamic.append(\n",
    "            {\"id\": f\"quran_{i}\", \"text\": text, \"source\": source, \"period\": \"QURANIC\", \"century\": 7}\n",
    "        )\n",
    "    print(f\"    - Quranic verses: {len([x for x in islamic if 'quran' in x['id']]):,} passages\")\n",
    "\n",
    "    # === HADITH (110+) ===\n",
    "    hadith = [\n",
    "        (\"لا ضرر ولا ضرار\", \"Hadith - Ibn Majah\"),\n",
    "        (\"إنما الأعمال بالنيات وإنما لكل امرئ ما نوى\", \"Hadith - Bukhari 1\"),\n",
    "        (\"المسلم من سلم المسلمون من لسانه ويده\", \"Hadith - Bukhari 10\"),\n",
    "        (\"لا يؤمن أحدكم حتى يحب لأخيه ما يحب لنفسه\", \"Hadith - Bukhari 13\"),\n",
    "        (\"من كان يؤمن بالله واليوم الآخر فليقل خيرا أو ليصمت\", \"Hadith - Bukhari 6018\"),\n",
    "        (\"من كان يؤمن بالله واليوم الآخر فليكرم ضيفه\", \"Hadith - Bukhari 6019\"),\n",
    "        (\"من كان يؤمن بالله واليوم الآخر فليصل رحمه\", \"Hadith - Bukhari 6138\"),\n",
    "        (\"ارحموا من في الأرض يرحمكم من في السماء\", \"Hadith - Tirmidhi 1924\"),\n",
    "        (\"الراحمون يرحمهم الرحمن\", \"Hadith - Abu Dawud 4941\"),\n",
    "        (\"ليس منا من لم يرحم صغيرنا ويوقر كبيرنا\", \"Hadith - Tirmidhi 1919\"),\n",
    "        (\"خيركم خيركم لأهله وأنا خيركم لأهلي\", \"Hadith - Tirmidhi 3895\"),\n",
    "        (\"اتق الله حيثما كنت وأتبع السيئة الحسنة تمحها\", \"Hadith - Tirmidhi 1987\"),\n",
    "        (\"وخالق الناس بخلق حسن\", \"Hadith - Tirmidhi 1987\"),\n",
    "        (\"أكمل المؤمنين إيمانا أحسنهم خلقا\", \"Hadith - Abu Dawud 4682\"),\n",
    "        (\"إن من أحبكم إلي وأقربكم مني مجلسا يوم القيامة أحاسنكم أخلاقا\", \"Hadith - Tirmidhi 2018\"),\n",
    "        (\"ما من شيء أثقل في ميزان المؤمن يوم القيامة من حسن الخلق\", \"Hadith - Tirmidhi 2002\"),\n",
    "        (\"البر حسن الخلق والإثم ما حاك في صدرك وكرهت أن يطلع عليه الناس\", \"Hadith - Muslim 2553\"),\n",
    "        (\"الحياء من الإيمان\", \"Hadith - Bukhari 24\"),\n",
    "        (\"الحياء لا يأتي إلا بخير\", \"Hadith - Bukhari 6117\"),\n",
    "        (\"إن الله رفيق يحب الرفق في الأمر كله\", \"Hadith - Bukhari 6927\"),\n",
    "        (\"ما كان الرفق في شيء إلا زانه وما نزع من شيء إلا شانه\", \"Hadith - Muslim 2594\"),\n",
    "        (\"من يحرم الرفق يحرم الخير كله\", \"Hadith - Muslim 2592\"),\n",
    "        (\"أد الأمانة إلى من ائتمنك ولا تخن من خانك\", \"Hadith - Abu Dawud 3535\"),\n",
    "        (\"آية المنافق ثلاث إذا حدث كذب وإذا وعد أخلف وإذا اؤتمن خان\", \"Hadith - Bukhari 33\"),\n",
    "        (\"الصدق يهدي إلى البر والبر يهدي إلى الجنة\", \"Hadith - Bukhari 6094\"),\n",
    "        (\"وإن الكذب يهدي إلى الفجور والفجور يهدي إلى النار\", \"Hadith - Bukhari 6094\"),\n",
    "        (\"عليكم بالصدق فإن الصدق يهدي إلى البر\", \"Hadith - Muslim 2607\"),\n",
    "        (\"إياكم والكذب فإن الكذب يهدي إلى الفجور\", \"Hadith - Muslim 2607\"),\n",
    "        (\"من غشنا فليس منا\", \"Hadith - Muslim 101\"),\n",
    "        (\"كلكم راع وكلكم مسؤول عن رعيته\", \"Hadith - Bukhari 893\"),\n",
    "        (\"الإمام راع ومسؤول عن رعيته\", \"Hadith - Bukhari 893\"),\n",
    "        (\"والرجل راع في أهله ومسؤول عن رعيته\", \"Hadith - Bukhari 893\"),\n",
    "        (\"والمرأة راعية في بيت زوجها ومسؤولة عن رعيتها\", \"Hadith - Bukhari 893\"),\n",
    "        (\"انصر أخاك ظالما أو مظلوما\", \"Hadith - Bukhari 2444\"),\n",
    "        (\n",
    "            \"تنصره إذا كان مظلوما أفرأيت إذا كان ظالما كيف تنصره قال تحجزه أو تمنعه من الظلم فإن ذلك نصره\",\n",
    "            \"Hadith - Bukhari 2444\",\n",
    "        ),\n",
    "        (\"المؤمن للمؤمن كالبنيان يشد بعضه بعضا\", \"Hadith - Bukhari 481\"),\n",
    "        (\"مثل المؤمنين في توادهم وتراحمهم وتعاطفهم مثل الجسد الواحد\", \"Hadith - Muslim 2586\"),\n",
    "        (\"إذا اشتكى منه عضو تداعى له سائر الجسد بالسهر والحمى\", \"Hadith - Muslim 2586\"),\n",
    "        (\"المسلم أخو المسلم لا يظلمه ولا يسلمه\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"من كان في حاجة أخيه كان الله في حاجته\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"ومن فرج عن مسلم كربة فرج الله عنه كربة من كربات يوم القيامة\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"ومن ستر مسلما ستره الله يوم القيامة\", \"Hadith - Bukhari 2442\"),\n",
    "        (\"لا تحاسدوا ولا تناجشوا ولا تباغضوا ولا تدابروا\", \"Hadith - Muslim 2564\"),\n",
    "        (\"ولا يبع بعضكم على بيع بعض وكونوا عباد الله إخوانا\", \"Hadith - Muslim 2564\"),\n",
    "        (\"بحسب امرئ من الشر أن يحقر أخاه المسلم\", \"Hadith - Muslim 2564\"),\n",
    "        (\"كل المسلم على المسلم حرام دمه وماله وعرضه\", \"Hadith - Muslim 2564\"),\n",
    "        (\"إياكم والظن فإن الظن أكذب الحديث\", \"Hadith - Bukhari 6064\"),\n",
    "        (\"ولا تجسسوا ولا تحسسوا ولا تنافسوا\", \"Hadith - Bukhari 6064\"),\n",
    "        (\"الظلم ظلمات يوم القيامة\", \"Hadith - Bukhari 2447\"),\n",
    "        (\"اتقوا الظلم فإن الظلم ظلمات يوم القيامة\", \"Hadith - Muslim 2578\"),\n",
    "        (\"واتقوا الشح فإن الشح أهلك من كان قبلكم\", \"Hadith - Muslim 2578\"),\n",
    "        (\"أفضل الجهاد كلمة عدل عند سلطان جائر\", \"Hadith - Abu Dawud 4344\"),\n",
    "        (\n",
    "            \"سيد الشهداء حمزة بن عبد المطلب ورجل قام إلى إمام جائر فأمره ونهاه فقتله\",\n",
    "            \"Hadith - Hakim 4884\",\n",
    "        ),\n",
    "        (\"إذا رأيت أمتي تهاب أن تقول للظالم يا ظالم فقد تودع منهم\", \"Hadith - Ahmad 6521\"),\n",
    "        (\"من رأى منكم منكرا فليغيره بيده\", \"Hadith - Muslim 49\"),\n",
    "        (\"فإن لم يستطع فبلسانه فإن لم يستطع فبقلبه وذلك أضعف الإيمان\", \"Hadith - Muslim 49\"),\n",
    "        (\"أحب الناس إلى الله أنفعهم للناس\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"وأحب الأعمال إلى الله سرور تدخله على مسلم\", \"Hadith - Tabarani 6026\"),\n",
    "        (\"أو تكشف عنه كربة أو تقضي عنه دينا أو تطرد عنه جوعا\", \"Hadith - Tabarani 6026\"),\n",
    "        (\n",
    "            \"ولأن أمشي مع أخي في حاجة أحب إلي من أن أعتكف في هذا المسجد شهرا\",\n",
    "            \"Hadith - Tabarani 6026\",\n",
    "        ),\n",
    "        (\n",
    "            \"الدين النصيحة قلنا لمن قال لله ولكتابه ولرسوله ولأئمة المسلمين وعامتهم\",\n",
    "            \"Hadith - Muslim 55\",\n",
    "        ),\n",
    "        (\"ما نقصت صدقة من مال\", \"Hadith - Muslim 2588\"),\n",
    "        (\"وما زاد الله عبدا بعفو إلا عزا\", \"Hadith - Muslim 2588\"),\n",
    "        (\"وما تواضع أحد لله إلا رفعه الله\", \"Hadith - Muslim 2588\"),\n",
    "        (\"اليد العليا خير من اليد السفلى\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"وابدأ بمن تعول\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"وخير الصدقة ما كان عن ظهر غنى\", \"Hadith - Bukhari 1427\"),\n",
    "        (\"من استطاع منكم الباءة فليتزوج\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"فإنه أغض للبصر وأحصن للفرج\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"ومن لم يستطع فعليه بالصوم فإنه له وجاء\", \"Hadith - Bukhari 5066\"),\n",
    "        (\"استوصوا بالنساء خيرا\", \"Hadith - Bukhari 3331\"),\n",
    "        (\n",
    "            \"خذوا عني خذوا عني قد جعل الله لهن سبيلا البكر بالبكر جلد مائة ونفي سنة\",\n",
    "            \"Hadith - Muslim 1690\",\n",
    "        ),\n",
    "        (\"لا يفرك مؤمن مؤمنة إن كره منها خلقا رضي منها آخر\", \"Hadith - Muslim 1469\"),\n",
    "        (\"أكمل المؤمنين إيمانا أحسنهم خلقا وخياركم خياركم لنسائهم\", \"Hadith - Tirmidhi 1162\"),\n",
    "        (\"ما أكرمهن إلا كريم وما أهانهن إلا لئيم\", \"Hadith - Ibn Asakir\"),\n",
    "        (\"اللهم إني أحرج حق الضعيفين اليتيم والمرأة\", \"Hadith - Ahmad 9664\"),\n",
    "        (\"ألا أخبركم بخياركم قالوا بلى قال خياركم أحاسنكم أخلاقا\", \"Hadith - Bukhari 6035\"),\n",
    "        (\"إنكم لن تسعوا الناس بأموالكم فليسعهم منكم بسط الوجه وحسن الخلق\", \"Hadith - Hakim 422\"),\n",
    "        (\"تبسمك في وجه أخيك صدقة\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"وأمرك بالمعروف ونهيك عن المنكر صدقة\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"وإرشادك الرجل في أرض الضلال لك صدقة\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"وإماطتك الأذى والشوك والعظم عن الطريق لك صدقة\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"وإفراغك من دلوك في دلو أخيك لك صدقة\", \"Hadith - Tirmidhi 1956\"),\n",
    "        (\"الكلمة الطيبة صدقة\", \"Hadith - Bukhari 2989\"),\n",
    "        (\"وكل خطوة تمشيها إلى الصلاة صدقة\", \"Hadith - Bukhari 2989\"),\n",
    "        (\"من دل على خير فله مثل أجر فاعله\", \"Hadith - Muslim 1893\"),\n",
    "        (\"ليس الشديد بالصرعة إنما الشديد الذي يملك نفسه عند الغضب\", \"Hadith - Bukhari 6114\"),\n",
    "        (\"لا تغضب فردد مرارا قال لا تغضب\", \"Hadith - Bukhari 6116\"),\n",
    "        (\"إن الغضب من الشيطان وإن الشيطان خلق من النار\", \"Hadith - Abu Dawud 4784\"),\n",
    "        (\"وإنما تطفأ النار بالماء فإذا غضب أحدكم فليتوضأ\", \"Hadith - Abu Dawud 4784\"),\n",
    "        (\"لا يحل لمسلم أن يهجر أخاه فوق ثلاث ليال\", \"Hadith - Bukhari 6077\"),\n",
    "        (\"يلتقيان فيعرض هذا ويعرض هذا وخيرهما الذي يبدأ بالسلام\", \"Hadith - Bukhari 6077\"),\n",
    "        (\"أفشوا السلام بينكم\", \"Hadith - Muslim 54\"),\n",
    "        (\"والذي نفسي بيده لا تدخلوا الجنة حتى تؤمنوا\", \"Hadith - Muslim 54\"),\n",
    "        (\n",
    "            \"ولا تؤمنوا حتى تحابوا أولا أدلكم على شيء إذا فعلتموه تحاببتم أفشوا السلام بينكم\",\n",
    "            \"Hadith - Muslim 54\",\n",
    "        ),\n",
    "        (\"طعام الاثنين كافي الثلاثة وطعام الثلاثة كافي الأربعة\", \"Hadith - Bukhari 5392\"),\n",
    "        (\"ما ملأ آدمي وعاء شرا من بطن\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"بحسب ابن آدم أكلات يقمن صلبه\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"فإن كان لا محالة فثلث لطعامه وثلث لشرابه وثلث لنفسه\", \"Hadith - Tirmidhi 2380\"),\n",
    "        (\"إن الله كتب الإحسان على كل شيء\", \"Hadith - Muslim 1955\"),\n",
    "        (\"فإذا قتلتم فأحسنوا القتلة وإذا ذبحتم فأحسنوا الذبح\", \"Hadith - Muslim 1955\"),\n",
    "        (\"وليحد أحدكم شفرته وليرح ذبيحته\", \"Hadith - Muslim 1955\"),\n",
    "        (\"عذبت امرأة في هرة سجنتها حتى ماتت\", \"Hadith - Bukhari 3318\"),\n",
    "        (\n",
    "            \"فلا هي أطعمتها ولا سقتها إذ حبستها ولا هي تركتها تأكل من خشاش الأرض\",\n",
    "            \"Hadith - Bukhari 3318\",\n",
    "        ),\n",
    "        (\"بينما رجل يمشي بطريق اشتد عليه العطش فوجد بئرا فنزل فيها فشرب\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"ثم خرج فإذا كلب يلهث يأكل الثرى من العطش\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"فقال لقد بلغ هذا الكلب من العطش مثل الذي كان بلغ مني\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"فنزل البئر فملأ خفه ماء ثم أمسكه بفيه حتى رقي فسقى الكلب\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"فشكر الله له فغفر له\", \"Hadith - Bukhari 2466\"),\n",
    "        (\"في كل كبد رطبة أجر\", \"Hadith - Bukhari 2466\"),\n",
    "    ]\n",
    "    for i, (text, source) in enumerate(hadith):\n",
    "        islamic.append(\n",
    "            {\"id\": f\"hadith_{i}\", \"text\": text, \"source\": source, \"period\": \"HADITH\", \"century\": 9}\n",
    "        )\n",
    "    print(f\"    - Hadith: {len([x for x in islamic if 'hadith' in x['id']]):,} passages\")\n",
    "\n",
    "    with open(\"data/raw/islamic/islamic_native.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(islamic, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"  Created {len(islamic)} Islamic passages\")\n",
    "\n",
    "    # DEAR ABBY\n",
    "    print(\"\\n[4/4] Dear Abby...\")\n",
    "    abby_count = 0\n",
    "    if (\n",
    "        not os.path.exists(\"data/raw/dear_abby.csv\")\n",
    "        or os.path.getsize(\"data/raw/dear_abby.csv\") < 10000\n",
    "    ):\n",
    "        # Check if in Drive (with retry for stale FUSE mount)\n",
    "        drive_abby_path = f\"{SAVE_DIR}/dear_abby.csv\"\n",
    "        found_in_drive = False\n",
    "\n",
    "        # First attempt\n",
    "        if os.path.exists(drive_abby_path):\n",
    "            found_in_drive = True\n",
    "        else:\n",
    "            # Retry after refreshing Drive mount (FUSE can be stale)\n",
    "            print(f\"  First check failed, refreshing Drive...\")\n",
    "            try:\n",
    "                _ = os.listdir(SAVE_DIR)  # Force FUSE refresh\n",
    "                import time\n",
    "\n",
    "                time.sleep(0.5)  # Brief pause for sync\n",
    "                if os.path.exists(drive_abby_path):\n",
    "                    found_in_drive = True\n",
    "                    print(f\"  Found after refresh!\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Drive refresh error: {e}\")\n",
    "\n",
    "        if found_in_drive:\n",
    "            shutil.copy(drive_abby_path, \"data/raw/dear_abby.csv\")\n",
    "            print(f\"  Loaded from Drive: {drive_abby_path}\")\n",
    "        else:\n",
    "            print(f\"  Not found in Drive at: {drive_abby_path}\")\n",
    "            # Show what IS in the Drive folder\n",
    "            try:\n",
    "                contents = os.listdir(SAVE_DIR) if os.path.exists(SAVE_DIR) else []\n",
    "                print(f\"  Drive folder contents: {contents[:10]}\")\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                subprocess.run(\n",
    "                    [\n",
    "                        \"kaggle\",\n",
    "                        \"datasets\",\n",
    "                        \"download\",\n",
    "                        \"-d\",\n",
    "                        \"thedevastator/20000-dear-abby-questions\",\n",
    "                        \"-p\",\n",
    "                        \"data/raw/\",\n",
    "                        \"--unzip\",\n",
    "                    ],\n",
    "                    check=True,\n",
    "                    timeout=120,\n",
    "                )\n",
    "                print(\"  Downloaded from Kaggle\")\n",
    "            except:\n",
    "                print(\"  Kaggle failed - creating minimal fallback\")\n",
    "                fallback = [\n",
    "                    {\"question_only\": f\"Dear Abby, I have a problem {i}\", \"year\": 1990 + i % 30}\n",
    "                    for i in range(100)\n",
    "                ]\n",
    "                pd.DataFrame(fallback).to_csv(\"data/raw/dear_abby.csv\", index=False)\n",
    "    else:\n",
    "        print(\"  Already exists\")\n",
    "\n",
    "    # Count Dear Abby samples\n",
    "    try:\n",
    "        df = pd.read_csv(\"data/raw/dear_abby.csv\")\n",
    "        abby_count = len(\n",
    "            [\n",
    "                1\n",
    "                for _, row in df.iterrows()\n",
    "                if str(row.get(\"question_only\", \"\")) != \"nan\"\n",
    "                and 50 <= len(str(row.get(\"question_only\", \"\"))) <= 2000\n",
    "            ]\n",
    "        )\n",
    "    except:\n",
    "        abby_count = 0\n",
    "\n",
    "    # Warning for insufficient Dear Abby data\n",
    "    if abby_count < 1000:\n",
    "        print(\"\\n\" + \"!\" * 60)\n",
    "        print(\"CRITICAL: Dear Abby corpus is too small!\")\n",
    "        print(\"The semitic_to_non_semitic split WILL FAIL without this data.\")\n",
    "        print(\"\\nTo fix:\")\n",
    "        print(\"1. Download from: kaggle.com/datasets/thedevastator/20000-dear-abby-questions\")\n",
    "        print(\"2. Upload dear_abby.csv to your Google Drive BIP_v10 folder\")\n",
    "        print(\"3. Set REFRESH_DATA_FROM_SOURCE = True and rerun\")\n",
    "        print(\"!\" * 60 + \"\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Downloads complete\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3. Patterns + Normalization { display-mode: \"form\" }\n",
    "# @markdown BIP v10.9: Complete native patterns for moral concepts in 7 languages\n",
    "# @markdown - Added: Sanskrit, Pali patterns\n",
    "# @markdown - Added: NLP improvements (negation detection, modal classification)\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from enum import Enum, auto\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT NORMALIZATION & PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ===== TEXT NORMALIZATION =====\n",
    "def normalize_hebrew(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"[\\u0591-\\u05C7]\", \"\", text)  # Remove nikud\n",
    "    for final, regular in [\n",
    "        (\"\\u05da\", \"\\u05db\"),\n",
    "        (\"\\u05dd\", \"\\u05de\"),\n",
    "        (\"\\u05df\", \"\\u05e0\"),\n",
    "        (\"\\u05e3\", \"\\u05e4\"),\n",
    "        (\"\\u05e5\", \"\\u05e6\"),\n",
    "    ]:\n",
    "        text = text.replace(final, regular)\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"[\\u064B-\\u065F]\", \"\", text)  # Remove tashkeel\n",
    "    text = text.replace(\"\\u0640\", \"\")  # Remove tatweel\n",
    "    for v in [\"\\u0623\", \"\\u0625\", \"\\u0622\", \"\\u0671\"]:\n",
    "        text = text.replace(v, \"\\u0627\")\n",
    "    text = text.replace(\"\\u0629\", \"\\u0647\").replace(\"\\u0649\", \"\\u064a\")\n",
    "    return text\n",
    "\n",
    "\n",
    "# NEW in v10.9: Sanskrit normalization\n",
    "def normalize_sanskrit(text):\n",
    "    \"\"\"Normalize Sanskrit/Devanagari text.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    # Remove vedic accents and other diacriticals\n",
    "    text = re.sub(r\"[\\u0951-\\u0954]\", \"\", text)  # Vedic tone marks\n",
    "    text = re.sub(r\"[\\u0900-\\u0902]\", \"\", text)  # Chandrabindu variants\n",
    "    return text\n",
    "\n",
    "\n",
    "# NEW in v10.9: Pali normalization\n",
    "def normalize_pali(text):\n",
    "    \"\"\"Normalize Pali text (romanized or script).\"\"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    # Normalize romanized Pali diacritics\n",
    "    text = text.lower()\n",
    "    # Handle common Pali romanization variations\n",
    "    text = text.replace(\"ṃ\", \"m\").replace(\"ṅ\", \"n\").replace(\"ñ\", \"n\")\n",
    "    text = text.replace(\"ṭ\", \"t\").replace(\"ḍ\", \"d\").replace(\"ṇ\", \"n\")\n",
    "    text = text.replace(\"ḷ\", \"l\").replace(\"ā\", \"a\").replace(\"ī\", \"i\").replace(\"ū\", \"u\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_text(text, language):\n",
    "    if language in [\"hebrew\", \"aramaic\"]:\n",
    "        return normalize_hebrew(text)\n",
    "    elif language == \"arabic\":\n",
    "        return normalize_arabic(text)\n",
    "    elif language == \"classical_chinese\":\n",
    "        return unicodedata.normalize(\"NFKC\", text)\n",
    "    elif language == \"sanskrit\":\n",
    "        return normalize_sanskrit(text)\n",
    "    elif language == \"pali\":\n",
    "        return normalize_pali(text)\n",
    "    else:\n",
    "        return unicodedata.normalize(\"NFKC\", text.lower())\n",
    "\n",
    "\n",
    "# ===== BOND AND HOHFELD TYPES =====\n",
    "class BondType(Enum):\n",
    "    HARM_PREVENTION = auto()\n",
    "    RECIPROCITY = auto()\n",
    "    AUTONOMY = auto()\n",
    "    PROPERTY = auto()\n",
    "    FAMILY = auto()\n",
    "    AUTHORITY = auto()\n",
    "    CARE = auto()\n",
    "    FAIRNESS = auto()\n",
    "    CONTRACT = auto()\n",
    "    NONE = auto()\n",
    "\n",
    "\n",
    "class HohfeldState(Enum):\n",
    "    OBLIGATION = auto()\n",
    "    RIGHT = auto()\n",
    "    LIBERTY = auto()\n",
    "    NO_RIGHT = auto()\n",
    "\n",
    "\n",
    "# ===== COMPLETE BOND PATTERNS =====\n",
    "ALL_BOND_PATTERNS = {\n",
    "    \"hebrew\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u05d4\\u05e8\\u05d2\",\n",
    "            r\"\\u05e8\\u05e6\\u05d7\",\n",
    "            r\"\\u05e0\\u05d6\\u05e7\",\n",
    "            r\"\\u05d4\\u05db\\u05d4\",\n",
    "            r\"\\u05d4\\u05e6\\u05d9\\u05dc\",\n",
    "            r\"\\u05e9\\u05de\\u05e8\",\n",
    "            r\"\\u05e4\\u05e7\\u05d5\\u05d7.\\u05e0\\u05e4\\u05e9\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\u05d2\\u05de\\u05d5\\u05dc\",\n",
    "            r\"\\u05d4\\u05e9\\u05d9\\u05d1\",\n",
    "            r\"\\u05e4\\u05e8\\u05e2\",\n",
    "            r\"\\u05e0\\u05ea\\u05df.*\\u05e7\\u05d1\\u05dc\",\n",
    "            r\"\\u05de\\u05d3\\u05d4.\\u05db\\u05e0\\u05d2\\u05d3\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\u05d1\\u05d7\\u05e8\",\n",
    "            r\"\\u05e8\\u05e6\\u05d5\\u05df\",\n",
    "            r\"\\u05d7\\u05e4\\u05e9\",\n",
    "            r\"\\u05e2\\u05e6\\u05de\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u05e7\\u05e0\\u05d4\",\n",
    "            r\"\\u05de\\u05db\\u05e8\",\n",
    "            r\"\\u05d2\\u05d6\\u05dc\",\n",
    "            r\"\\u05d2\\u05e0\\u05d1\",\n",
    "            r\"\\u05de\\u05de\\u05d5\\u05df\",\n",
    "            r\"\\u05e0\\u05db\\u05e1\",\n",
    "            r\"\\u05d9\\u05e8\\u05e9\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u05d0\\u05d1\",\n",
    "            r\"\\u05d0\\u05de\",\n",
    "            r\"\\u05d1\\u05e0\",\n",
    "            r\"\\u05db\\u05d1\\u05d3.*\\u05d0\\u05d1\",\n",
    "            r\"\\u05db\\u05d1\\u05d3.*\\u05d0\\u05de\",\n",
    "            r\"\\u05de\\u05e9\\u05e4\\u05d7\\u05d4\",\n",
    "            r\"\\u05d0\\u05d7\",\n",
    "            r\"\\u05d0\\u05d7\\u05d5\\u05ea\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u05de\\u05dc\\u05db\",\n",
    "            r\"\\u05e9\\u05d5\\u05e4\\u05d8\",\n",
    "            r\"\\u05e6\\u05d5\\u05d4\",\n",
    "            r\"\\u05ea\\u05d5\\u05e8\\u05d4\",\n",
    "            r\"\\u05de\\u05e6\\u05d5\\u05d4\",\n",
    "            r\"\\u05d3\\u05d9\\u05df\",\n",
    "            r\"\\u05d7\\u05e7\",\n",
    "        ],\n",
    "        BondType.CARE: [\n",
    "            r\"\\u05d7\\u05e1\\u05d3\",\n",
    "            r\"\\u05e8\\u05d7\\u05de\",\n",
    "            r\"\\u05e2\\u05d6\\u05e8\",\n",
    "            r\"\\u05ea\\u05de\\u05db\",\n",
    "            r\"\\u05e6\\u05d3\\u05e7\\u05d4\",\n",
    "        ],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u05e6\\u05d3\\u05e7\",\n",
    "            r\"\\u05de\\u05e9\\u05e4\\u05d8\",\n",
    "            r\"\\u05d9\\u05e9\\u05e8\",\n",
    "            r\"\\u05e9\\u05d5\\u05d4\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u05d1\\u05e8\\u05d9\\u05ea\",\n",
    "            r\"\\u05e0\\u05d3\\u05e8\",\n",
    "            r\"\\u05e9\\u05d1\\u05d5\\u05e2\",\n",
    "            r\"\\u05d4\\u05ea\\u05d7\\u05d9\\u05d1\",\n",
    "            r\"\\u05e2\\u05e8\\u05d1\",\n",
    "        ],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u05e7\\u05d8\\u05dc\",\n",
    "            r\"\\u05e0\\u05d6\\u05e7\",\n",
    "            r\"\\u05d7\\u05d1\\u05dc\",\n",
    "            r\"\\u05e9\\u05d6\\u05d9\\u05d1\",\n",
    "            r\"\\u05e4\\u05e6\\u05d9\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [r\"\\u05e4\\u05e8\\u05e2\", r\"\\u05e9\\u05dc\\u05de\", r\"\\u05d0\\u05d2\\u05e8\"],\n",
    "        BondType.AUTONOMY: [r\"\\u05e6\\u05d1\\u05d9\", r\"\\u05e8\\u05e2\\u05d5\"],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u05d6\\u05d1\\u05e0\",\n",
    "            r\"\\u05e7\\u05e0\\u05d4\",\n",
    "            r\"\\u05d2\\u05d6\\u05dc\",\n",
    "            r\"\\u05de\\u05de\\u05d5\\u05e0\\u05d0\",\n",
    "            r\"\\u05e0\\u05db\\u05e1\\u05d9\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u05d0\\u05d1\\u05d0\",\n",
    "            r\"\\u05d0\\u05de\\u05d0\",\n",
    "            r\"\\u05d1\\u05e8\\u05d0\",\n",
    "            r\"\\u05d1\\u05e8\\u05ea\\u05d0\",\n",
    "            r\"\\u05d9\\u05e7\\u05e8\",\n",
    "            r\"\\u05d0\\u05d7\\u05d0\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u05de\\u05dc\\u05db\\u05d0\",\n",
    "            r\"\\u05d3\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05d3\\u05d9\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05e4\\u05e7\\u05d5\\u05d3\\u05d0\",\n",
    "            r\"\\u05d0\\u05d5\\u05e8\\u05d9\\u05ea\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\u05d7\\u05e1\\u05d3\", r\"\\u05e8\\u05d7\\u05de\", r\"\\u05e1\\u05e2\\u05d3\"],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u05d3\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05e7\\u05e9\\u05d5\\u05d8\",\n",
    "            r\"\\u05ea\\u05e8\\u05d9\\u05e6\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u05e7\\u05d9\\u05de\\u05d0\",\n",
    "            r\"\\u05e9\\u05d1\\u05d5\\u05e2\\u05d4\",\n",
    "            r\"\\u05e0\\u05d3\\u05e8\\u05d0\",\n",
    "            r\"\\u05e2\\u05e8\\u05d1\\u05d0\",\n",
    "        ],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u6bba\",\n",
    "            r\"\\u5bb3\",\n",
    "            r\"\\u50b7\",\n",
    "            r\"\\u6551\",\n",
    "            r\"\\u8b77\",\n",
    "            r\"\\u885b\",\n",
    "            r\"\\u66b4\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [r\"\\u5831\", r\"\\u9084\", r\"\\u511f\", r\"\\u8ced\", r\"\\u7b54\"],\n",
    "        BondType.AUTONOMY: [r\"\\u81ea\", r\"\\u7531\", r\"\\u4efb\", r\"\\u610f\", r\"\\u5fd7\"],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u8ca1\",\n",
    "            r\"\\u7269\",\n",
    "            r\"\\u7522\",\n",
    "            r\"\\u76dc\",\n",
    "            r\"\\u7aca\",\n",
    "            r\"\\u8ce3\",\n",
    "            r\"\\u8cb7\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u5b5d\",\n",
    "            r\"\\u7236\",\n",
    "            r\"\\u6bcd\",\n",
    "            r\"\\u89aa\",\n",
    "            r\"\\u5b50\",\n",
    "            r\"\\u5f1f\",\n",
    "            r\"\\u5144\",\n",
    "            r\"\\u5bb6\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u541b\",\n",
    "            r\"\\u81e3\",\n",
    "            r\"\\u738b\",\n",
    "            r\"\\u547d\",\n",
    "            r\"\\u4ee4\",\n",
    "            r\"\\u6cd5\",\n",
    "            r\"\\u6cbb\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\u4ec1\", r\"\\u611b\", r\"\\u6148\", r\"\\u60e0\", r\"\\u6069\", r\"\\u6190\"],\n",
    "        BondType.FAIRNESS: [r\"\\u7fa9\", r\"\\u6b63\", r\"\\u516c\", r\"\\u5e73\", r\"\\u5747\"],\n",
    "        BondType.CONTRACT: [r\"\\u7d04\", r\"\\u76df\", r\"\\u8a93\", r\"\\u8afe\", r\"\\u4fe1\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u0642\\u062a\\u0644\",\n",
    "            r\"\\u0636\\u0631\\u0631\",\n",
    "            r\"\\u0627\\u0630[\\u064a\\u0649]\",\n",
    "            r\"\\u0638\\u0644\\u0645\",\n",
    "            r\"\\u0627\\u0646\\u0642\\u0630\",\n",
    "            r\"\\u062d\\u0641\\u0638\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0646\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\u062c\\u0632\\u0627\",\n",
    "            r\"\\u0631\\u062f\",\n",
    "            r\"\\u0642\\u0635\\u0627\\u0635\",\n",
    "            r\"\\u0645\\u062b\\u0644\",\n",
    "            r\"\\u0639\\u0648\\u0636\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\u062d\\u0631\",\n",
    "            r\"\\u0627\\u0631\\u0627\\u062f\\u0629\",\n",
    "            r\"\\u0627\\u062e\\u062a\\u064a\\u0627\\u0631\",\n",
    "            r\"\\u0645\\u0634\\u064a\\u0626\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u0645\\u0627\\u0644\",\n",
    "            r\"\\u0645\\u0644\\u0643\",\n",
    "            r\"\\u0633\\u0631\\u0642\",\n",
    "            r\"\\u0628\\u064a\\u0639\",\n",
    "            r\"\\u0634\\u0631\\u0627\",\n",
    "            r\"\\u0645\\u064a\\u0631\\u0627\\u062b\",\n",
    "            r\"\\u063a\\u0635\\u0628\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u0648\\u0627\\u0644\\u062f\",\n",
    "            r\"\\u0627\\u0628\\u0648\",\n",
    "            r\"\\u0627\\u0645\",\n",
    "            r\"\\u0627\\u0628\\u0646\",\n",
    "            r\"\\u0628\\u0646\\u062a\",\n",
    "            r\"\\u0627\\u0647\\u0644\",\n",
    "            r\"\\u0642\\u0631\\u0628[\\u064a\\u0649]\",\n",
    "            r\"\\u0631\\u062d\\u0645\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u0637\\u0627\\u0639\",\n",
    "            r\"\\u0627\\u0645\\u0631\",\n",
    "            r\"\\u062d\\u0643\\u0645\",\n",
    "            r\"\\u0633\\u0644\\u0637\\u0627\\u0646\",\n",
    "            r\"\\u062e\\u0644\\u064a\\u0641\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0645\",\n",
    "            r\"\\u0634\\u0631\\u064a\\u0639\",\n",
    "        ],\n",
    "        BondType.CARE: [\n",
    "            r\"\\u0631\\u062d\\u0645\",\n",
    "            r\"\\u0627\\u062d\\u0633\\u0627\\u0646\",\n",
    "            r\"\\u0639\\u0637\\u0641\",\n",
    "            r\"\\u0635\\u062f\\u0642\",\n",
    "            r\"\\u0632\\u0643\\u0627\",\n",
    "        ],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u0639\\u062f\\u0644\",\n",
    "            r\"\\u0642\\u0633\\u0637\",\n",
    "            r\"\\u062d\\u0642\",\n",
    "            r\"\\u0627\\u0646\\u0635\\u0627\\u0641\",\n",
    "            r\"\\u0633\\u0648[\\u064a\\u0649]\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u0639\\u0647\\u062f\",\n",
    "            r\"\\u0639\\u0642\\u062f\",\n",
    "            r\"\\u0646\\u0630\\u0631\",\n",
    "            r\"\\u064a\\u0645\\u064a\\u0646\",\n",
    "            r\"\\u0648\\u0641\\u0627\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0646\",\n",
    "        ],\n",
    "    },\n",
    "    \"english\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\bkill\",\n",
    "            r\"\\bmurder\",\n",
    "            r\"\\bharm\",\n",
    "            r\"\\bhurt\",\n",
    "            r\"\\bsave\",\n",
    "            r\"\\bprotect\",\n",
    "            r\"\\bviolence\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\breturn\",\n",
    "            r\"\\brepay\",\n",
    "            r\"\\bexchange\",\n",
    "            r\"\\bgive.*back\",\n",
    "            r\"\\breciproc\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\bfree\",\n",
    "            r\"\\bchoice\",\n",
    "            r\"\\bchoose\",\n",
    "            r\"\\bconsent\",\n",
    "            r\"\\bautonomy\",\n",
    "            r\"\\bright to\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\bsteal\",\n",
    "            r\"\\btheft\",\n",
    "            r\"\\bown\",\n",
    "            r\"\\bproperty\",\n",
    "            r\"\\bbelong\",\n",
    "            r\"\\binherit\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\bfather\",\n",
    "            r\"\\bmother\",\n",
    "            r\"\\bparent\",\n",
    "            r\"\\bchild\",\n",
    "            r\"\\bfamily\",\n",
    "            r\"\\bhonor.*parent\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\bobey\",\n",
    "            r\"\\bcommand\",\n",
    "            r\"\\bauthority\",\n",
    "            r\"\\blaw\",\n",
    "            r\"\\brule\",\n",
    "            r\"\\bgovern\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\bcare\", r\"\\bhelp\", r\"\\bkind\", r\"\\bcompassion\", r\"\\bcharity\", r\"\\bmercy\"],\n",
    "        BondType.FAIRNESS: [r\"\\bfair\", r\"\\bjust\", r\"\\bequal\", r\"\\bequity\", r\"\\bright\\b\"],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\bpromise\",\n",
    "            r\"\\bcontract\",\n",
    "            r\"\\bagreem\",\n",
    "            r\"\\bvow\",\n",
    "            r\"\\boath\",\n",
    "            r\"\\bcommit\",\n",
    "        ],\n",
    "    },\n",
    "    # NEW in v10.9: Sanskrit patterns (Devanagari)\n",
    "    \"sanskrit\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"हिंसा\",\n",
    "            r\"अहिंसा\",\n",
    "            r\"वध\",\n",
    "            r\"रक्षा\",\n",
    "            r\"त्राण\",\n",
    "        ],  # himsa, ahimsa, vadha, raksha, trana\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"प्रतिदान\",\n",
    "            r\"प्रत्युपकार\",\n",
    "            r\"दान\",\n",
    "            r\"ऋण\",\n",
    "        ],  # pratidana, pratyupakara, dana, rna\n",
    "        BondType.AUTONOMY: [r\"स्वतंत्र\", r\"मोक्ष\", r\"स्वेच्छा\"],  # swatantra, moksha, sveccha\n",
    "        BondType.PROPERTY: [r\"धन\", r\"स्व\", r\"चोर\", r\"दाय\"],  # dhana, sva, chora, daya\n",
    "        BondType.FAMILY: [r\"पितृ\", r\"मातृ\", r\"पुत्र\", r\"कुल\", r\"गृह\"],  # pitri, matri, putra, kula, grha\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"राज\",\n",
    "            r\"धर्म\",\n",
    "            r\"विधि\",\n",
    "            r\"नियम\",\n",
    "            r\"शास्त्र\",\n",
    "        ],  # raja, dharma, vidhi, niyama, shastra\n",
    "        BondType.CARE: [\n",
    "            r\"करुणा\",\n",
    "            r\"दया\",\n",
    "            r\"प्रेम\",\n",
    "            r\"मैत्री\",\n",
    "            r\"सेवा\",\n",
    "        ],  # karuna, daya, prema, maitri, seva\n",
    "        BondType.FAIRNESS: [r\"न्याय\", r\"समता\", r\"धर्म\", r\"ऋत\"],  # nyaya, samata, dharma, rta\n",
    "        BondType.CONTRACT: [\n",
    "            r\"प्रतिज्ञा\",\n",
    "            r\"संविद\",\n",
    "            r\"वचन\",\n",
    "            r\"शपथ\",\n",
    "        ],  # pratijna, samvid, vachana, shapatha\n",
    "    },\n",
    "    # NEW in v10.9: Pali patterns (romanized)\n",
    "    \"pali\": {\n",
    "        BondType.HARM_PREVENTION: [r\"himsa\", r\"ahimsa\", r\"panatipata\", r\"rakkhati\"],\n",
    "        BondType.RECIPROCITY: [r\"dana\", r\"patidana\", r\"ina\"],\n",
    "        BondType.AUTONOMY: [r\"vimutti\", r\"nibbana\", r\"attadhipa\"],\n",
    "        BondType.PROPERTY: [r\"dhana\", r\"theyya\", r\"adinnadana\"],\n",
    "        BondType.FAMILY: [r\"mata\", r\"pita\", r\"putta\", r\"kula\"],\n",
    "        BondType.AUTHORITY: [r\"raja\", r\"dhamma\", r\"vinaya\", r\"sikkhapada\"],\n",
    "        BondType.CARE: [r\"karuna\", r\"metta\", r\"mudita\", r\"upekkha\"],\n",
    "        BondType.FAIRNESS: [r\"samma\", r\"dhamma\", r\"sacca\"],\n",
    "        BondType.CONTRACT: [r\"patijna\", r\"vacana\", r\"sacca\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# ===== COMPLETE HOHFELD PATTERNS =====\n",
    "ALL_HOHFELD_PATTERNS = {\n",
    "    \"hebrew\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u05d7\\u05d9\\u05d9\\u05d1\",\n",
    "            r\"\\u05e6\\u05e8\\u05d9\\u05db\",\n",
    "            r\"\\u05de\\u05d5\\u05db\\u05e8\\u05d7\",\n",
    "            r\"\\u05de\\u05e6\\u05d5\\u05d5\\u05d4\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "            r\"\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d6\\u05db\\u05d0\\u05d9\",\n",
    "            r\"\\u05de\\u05d2\\u05d9\\u05e2\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u05de\\u05d5\\u05ea\\u05e8\",\n",
    "            r\"\\u05e8\\u05e9\\u05d5\\u05ea\",\n",
    "            r\"\\u05e4\\u05d8\\u05d5\\u05e8\",\n",
    "            r\"\\u05d9\\u05db\\u05d5\\u05dc\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u05d0\\u05e1\\u05d5\\u05e8\",\n",
    "            r\"\\u05d0\\u05d9\\u05e0\\u05d5 \\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d0\\u05d9\\u05df.*\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "        ],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u05d7\\u05d9\\u05d9\\u05d1\",\n",
    "            r\"\\u05de\\u05d7\\u05d5\\u05d9\\u05d1\",\n",
    "            r\"\\u05d1\\u05e2\\u05d9\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "            r\"\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d6\\u05db\\u05d9\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u05e9\\u05e8\\u05d9\",\n",
    "            r\"\\u05de\\u05d5\\u05ea\\u05e8\",\n",
    "            r\"\\u05e4\\u05d8\\u05d5\\u05e8\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u05d0\\u05e1\\u05d5\\u05e8\",\n",
    "            r\"\\u05dc\\u05d0.*\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "        ],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        HohfeldState.OBLIGATION: [r\"\\u5fc5\", r\"\\u9808\", r\"\\u7576\", r\"\\u61c9\", r\"\\u5b9c\"],\n",
    "        HohfeldState.RIGHT: [r\"\\u53ef\", r\"\\u5f97\", r\"\\u6b0a\", r\"\\u5b9c\"],\n",
    "        HohfeldState.LIBERTY: [r\"\\u8a31\", r\"\\u4efb\", r\"\\u807d\", r\"\\u514d\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"\\u4e0d\\u53ef\", r\"\\u52ff\", r\"\\u7981\", r\"\\u83ab\", r\"\\u975e\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u064a\\u062c\\u0628\",\n",
    "            r\"\\u0648\\u0627\\u062c\\u0628\",\n",
    "            r\"\\u0641\\u0631\\u0636\",\n",
    "            r\"\\u0644\\u0627\\u0632\\u0645\",\n",
    "            r\"\\u0648\\u062c\\u0648\\u0628\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u062d\\u0642\",\n",
    "            r\"\\u064a\\u062d\\u0642\",\n",
    "            r\"\\u062c\\u0627\\u0626\\u0632\",\n",
    "            r\"\\u064a\\u062c\\u0648\\u0632\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u0645\\u0628\\u0627\\u062d\",\n",
    "            r\"\\u062d\\u0644\\u0627\\u0644\",\n",
    "            r\"\\u062c\\u0627\\u0626\\u0632\",\n",
    "            r\"\\u0627\\u0628\\u0627\\u062d\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u062d\\u0631\\u0627\\u0645\",\n",
    "            r\"\\u0645\\u062d\\u0631\\u0645\",\n",
    "            r\"\\u0645\\u0645\\u0646\\u0648\\u0639\",\n",
    "            r\"\\u0644\\u0627 \\u064a\\u062c\\u0648\\u0632\",\n",
    "            r\"\\u0646\\u0647[\\u064a\\u0649]\",\n",
    "        ],\n",
    "    },\n",
    "    \"english\": {\n",
    "        HohfeldState.OBLIGATION: [r\"\\bmust\\b\", r\"\\bshall\\b\", r\"\\bobligat\", r\"\\bduty\", r\"\\brequir\"],\n",
    "        HohfeldState.RIGHT: [r\"\\bright\\b\", r\"\\bentitle\", r\"\\bdeserve\", r\"\\bclaim\"],\n",
    "        HohfeldState.LIBERTY: [r\"\\bmay\\b\", r\"\\bpermit\", r\"\\ballow\", r\"\\bfree to\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"\\bforbid\", r\"\\bprohibit\", r\"\\bmust not\", r\"\\bshall not\"],\n",
    "    },\n",
    "    # NEW in v10.9: Sanskrit Hohfeld patterns (Devanagari)\n",
    "    \"sanskrit\": {\n",
    "        HohfeldState.OBLIGATION: [r\"कर्तव्य\", r\"अवश्य\", r\"नियम\", r\"विधि\"],  # kartavya, avashya\n",
    "        HohfeldState.RIGHT: [r\"अधिकार\", r\"स्वत्व\"],  # adhikara, svatva\n",
    "        HohfeldState.LIBERTY: [r\"शक्य\", r\"अनुज्ञा\", r\"उचित\"],  # shakya, anujña\n",
    "        HohfeldState.NO_RIGHT: [r\"निषिद्ध\", r\"वर्जित\", r\"अकर्तव्य\"],  # nishiddha, varjita\n",
    "    },\n",
    "    # NEW in v10.9: Pali Hohfeld patterns (romanized)\n",
    "    \"pali\": {\n",
    "        HohfeldState.OBLIGATION: [r\"kicca\", r\"karaniiya\", r\"dhammo\"],\n",
    "        HohfeldState.RIGHT: [r\"adhikaara\", r\"bhaaga\"],\n",
    "        HohfeldState.LIBERTY: [r\"anujaanati\", r\"kappati\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"nisiddha\", r\"akaraniya\", r\"na kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ===== CONTEXT MARKERS FOR GRAMMAR-AWARE EXTRACTION =====\n",
    "# These help distinguish \"thou shalt not kill\" from \"he killed\"\n",
    "CONTEXT_MARKERS = {\n",
    "    \"hebrew\": {\n",
    "        \"negation\": [r\"לא\", r\"אל\", r\"אין\", r\"בלי\", r\"אינ\"],\n",
    "        \"obligation\": [r\"חייב\", r\"צריך\", r\"מוכרח\", r\"צווה\"],\n",
    "        \"prohibition\": [r\"אסור\", r\"אל.*ת\"],\n",
    "        \"permission\": [r\"מותר\", r\"רשאי\", r\"פטור\"],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        \"negation\": [r\"לא\", r\"לית\", r\"לאו\"],\n",
    "        \"obligation\": [r\"חייב\", r\"בעי\"],\n",
    "        \"prohibition\": [r\"אסור\"],\n",
    "        \"permission\": [r\"שרי\", r\"מותר\"],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        \"negation\": [r\"不\", r\"非\", r\"無\", r\"未\", r\"毋\"],\n",
    "        \"obligation\": [r\"必\", r\"當\", r\"須\", r\"應\", r\"宜\"],\n",
    "        \"prohibition\": [r\"勿\", r\"禁\", r\"莫\", r\"不可\"],\n",
    "        \"permission\": [r\"可\", r\"得\", r\"許\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        \"negation\": [r\"لا\", r\"ما\", r\"ليس\", r\"لم\", r\"غير\"],\n",
    "        \"obligation\": [r\"يجب\", r\"واجب\", r\"فرض\", r\"عليه\"],\n",
    "        \"prohibition\": [r\"حرام\", r\"محرم\", r\"لا يجوز\", r\"نهى\"],\n",
    "        \"permission\": [r\"حلال\", r\"مباح\", r\"جائز\"],\n",
    "    },\n",
    "    \"english\": {\n",
    "        \"negation\": [r\"not\", r\"no\", r\"never\", r\"neither\", r\"n't\"],\n",
    "        \"obligation\": [r\"must\", r\"shall\", r\"should\", r\"ought\", r\"required\"],\n",
    "        \"prohibition\": [r\"forbid\", r\"prohibit\", r\"must not\", r\"shall not\", r\"don't\"],\n",
    "        \"permission\": [r\"may\", r\"can\", r\"allowed\", r\"permit\"],\n",
    "    },\n",
    "    # NEW in v10.9: Sanskrit context markers\n",
    "    \"sanskrit\": {\n",
    "        \"negation\": [r\"न\", r\"मा\", r\"अ\"],  # na, mā, a- prefix\n",
    "        \"obligation\": [r\"कर्तव्य\", r\"अवश्य\", r\"विधि\"],\n",
    "        \"prohibition\": [r\"निषिद्ध\", r\"वर्जित\", r\"मा\"],\n",
    "        \"permission\": [r\"शक्य\", r\"अनुज्ञा\"],\n",
    "    },\n",
    "    # NEW in v10.9: Pali context markers\n",
    "    \"pali\": {\n",
    "        \"negation\": [r\"na\", r\"ma\", r\"a-\"],\n",
    "        \"obligation\": [r\"kicca\", r\"karaniya\"],\n",
    "        \"prohibition\": [r\"nisiddha\", r\"akaraniya\"],\n",
    "        \"permission\": [r\"anujaanati\", r\"kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def detect_context(text, language, match_pos, window=30):\n",
    "    \"\"\"\n",
    "    Detect grammatical context around a pattern match.\n",
    "    Returns: ('prescriptive'/'descriptive'/'unknown', marker_type or None)\n",
    "    \"\"\"\n",
    "    markers = CONTEXT_MARKERS.get(language, {})\n",
    "    if not markers:\n",
    "        return \"unknown\", None\n",
    "\n",
    "    start = max(0, match_pos - window)\n",
    "    end = min(len(text), match_pos + window)\n",
    "    window_text = text[start:end]\n",
    "\n",
    "    # Check for deontic markers (prescriptive = moral statement)\n",
    "    for marker_type in [\"prohibition\", \"obligation\", \"permission\"]:\n",
    "        for pattern in markers.get(marker_type, []):\n",
    "            if re.search(pattern, window_text):\n",
    "                return \"prescriptive\", marker_type\n",
    "\n",
    "    # Check for simple negation (may be descriptive)\n",
    "    for pattern in markers.get(\"negation\", []):\n",
    "        if re.search(pattern, window_text):\n",
    "            return \"descriptive\", \"negated\"\n",
    "\n",
    "    return \"descriptive\", None\n",
    "\n",
    "\n",
    "# ===== NLP IMPROVEMENTS (v10.9 Phase 1) =====\n",
    "# These provide negation detection and modal classification without external dependencies\n",
    "\n",
    "NEGATION_CUES = {\n",
    "    \"english\": [\"not\", \"no\", \"never\", \"neither\", \"nor\", \"n't\", \"without\", \"lack\", \"none\"],\n",
    "    \"classical_chinese\": [\"不\", \"非\", \"無\", \"莫\", \"勿\", \"未\", \"弗\", \"毋\", \"否\"],\n",
    "    \"arabic\": [\"لا\", \"ما\", \"لم\", \"لن\", \"ليس\", \"غير\", \"بدون\"],\n",
    "    \"hebrew\": [\"לא\", \"אל\", \"בלי\", \"אין\", \"מבלי\"],\n",
    "    \"aramaic\": [\"לא\", \"לית\", \"לאו\"],\n",
    "    \"sanskrit\": [\"न\", \"मा\", \"अ\"],  # na, mā, a- (privative prefix)\n",
    "    \"pali\": [\"na\", \"ma\", \"a\", \"an\"],\n",
    "}\n",
    "\n",
    "MODAL_CLASSIFICATION = {\n",
    "    \"english\": {\n",
    "        \"obligation\": [\"must\", \"shall\", \"have to\", \"ought to\", \"need to\", \"required\", \"obligated\"],\n",
    "        \"permission\": [\"may\", \"can\", \"allowed\", \"permitted\", \"free to\", \"entitled\"],\n",
    "        \"prohibition\": [\"must not\", \"shall not\", \"cannot\", \"forbidden\", \"prohibited\", \"banned\"],\n",
    "        \"supererogation\": [\"should\", \"ought\", \"would be good\", \"ideally\", \"preferably\"],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        \"obligation\": [\"必\", \"當\", \"宜\", \"須\", \"應\", \"要\"],\n",
    "        \"permission\": [\"可\", \"得\", \"許\", \"容\", \"能\"],\n",
    "        \"prohibition\": [\"不可\", \"不得\", \"勿\", \"莫\", \"禁\", \"不許\", \"不宜\"],\n",
    "        \"supererogation\": [\"善\", \"美\", \"德\", \"宜\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        \"obligation\": [\"يجب\", \"فرض\", \"واجب\", \"لازم\", \"فريضة\"],\n",
    "        \"permission\": [\"يجوز\", \"مباح\", \"حلال\", \"جائز\"],\n",
    "        \"prohibition\": [\"حرام\", \"محرم\", \"ممنوع\", \"لا يجوز\", \"محظور\"],\n",
    "        \"supererogation\": [\"مستحب\", \"سنة\", \"مندوب\", \"نافلة\"],\n",
    "    },\n",
    "    \"hebrew\": {\n",
    "        \"obligation\": [\"חייב\", \"מצווה\", \"צריך\", \"מוכרח\", \"חובה\"],\n",
    "        \"permission\": [\"מותר\", \"רשאי\", \"יכול\", \"היתר\"],\n",
    "        \"prohibition\": [\"אסור\", \"לא יעשה\", \"אל\", \"איסור\"],\n",
    "        \"supererogation\": [\"ראוי\", \"טוב\", \"מידת חסידות\", \"לפנים משורת הדין\"],\n",
    "    },\n",
    "    \"sanskrit\": {\n",
    "        \"obligation\": [\"कर्तव्य\", \"अवश्य\", \"नियम\"],  # kartavya, avashya, niyama\n",
    "        \"permission\": [\"शक्य\", \"अनुज्ञा\"],  # shakya, anujña\n",
    "        \"prohibition\": [\"निषिद्ध\", \"वर्जित\", \"मा\"],  # nishiddha, varjita, mā\n",
    "    },\n",
    "    \"pali\": {\n",
    "        \"obligation\": [\"kicca\", \"karaniya\", \"dhamma\"],\n",
    "        \"permission\": [\"kappati\", \"anujanati\"],\n",
    "        \"prohibition\": [\"akappiya\", \"akaraniya\", \"na kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def enhanced_extract_bond(text: str, language: str) -> dict:\n",
    "    \"\"\"\n",
    "    Enhanced bond extraction with negation + modal detection.\n",
    "    Phase 1 implementation - no external NLP dependencies required.\n",
    "\n",
    "    Returns dict with:\n",
    "        - bond_type: BondType or None\n",
    "        - hohfeld_state: str (OBLIGATION/RIGHT/LIBERTY/NO_RIGHT)\n",
    "        - negated: bool\n",
    "        - modal: str or None (the matched modal marker)\n",
    "        - confidence: float\n",
    "        - context: str (prescriptive/descriptive/unknown)\n",
    "    \"\"\"\n",
    "    # 1. Normalize text\n",
    "    normalized = normalize_text(text, language)\n",
    "\n",
    "    # 2. Check negation\n",
    "    negation_cues = NEGATION_CUES.get(language, [])\n",
    "    is_negated = any(cue in normalized for cue in negation_cues)\n",
    "\n",
    "    # 3. Check modal and classify deontic status\n",
    "    modal_status = \"unknown\"\n",
    "    modal_text = None\n",
    "    for status, markers in MODAL_CLASSIFICATION.get(language, {}).items():\n",
    "        for marker in markers:\n",
    "            if marker in normalized:\n",
    "                modal_status = status\n",
    "                modal_text = marker\n",
    "                break\n",
    "        if modal_status != \"unknown\":\n",
    "            break\n",
    "\n",
    "    # 4. Map modal to Hohfeld state\n",
    "    hohfeld_map = {\n",
    "        \"obligation\": \"OBLIGATION\",\n",
    "        \"permission\": \"LIBERTY\",\n",
    "        \"prohibition\": \"NO_RIGHT\",\n",
    "        \"supererogation\": \"LIBERTY\",\n",
    "        \"unknown\": \"OBLIGATION\",  # Default assumption\n",
    "    }\n",
    "    hohfeld = hohfeld_map[modal_status]\n",
    "\n",
    "    # 5. Pattern matching for bond type\n",
    "    bond_type = None\n",
    "    confidence = 0.5\n",
    "    for bt, patterns in ALL_BOND_PATTERNS.get(language, {}).items():\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, normalized):\n",
    "                bond_type = bt\n",
    "                confidence = 0.9\n",
    "                break\n",
    "        if bond_type:\n",
    "            break\n",
    "\n",
    "    # 6. Adjust confidence for negation\n",
    "    if is_negated:\n",
    "        confidence *= 0.8  # Lower confidence for negated statements\n",
    "\n",
    "    # 7. Determine context\n",
    "    if modal_status in [\"obligation\", \"prohibition\"]:\n",
    "        context = \"prescriptive\"\n",
    "    elif modal_status == \"permission\":\n",
    "        context = \"descriptive\"  # Permissions are often statements of fact\n",
    "    else:\n",
    "        context = \"unknown\"\n",
    "\n",
    "    return {\n",
    "        \"bond_type\": bond_type,\n",
    "        \"hohfeld_state\": hohfeld,\n",
    "        \"negated\": is_negated,\n",
    "        \"modal\": modal_text,\n",
    "        \"confidence\": confidence,\n",
    "        \"context\": context,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\nContext markers defined for grammar-aware extraction\")\n",
    "print(\"  Detects: negation, obligation, prohibition, permission\")\n",
    "\n",
    "print(f\"\\nPatterns defined for {len(ALL_BOND_PATTERNS)} languages:\")\n",
    "for lang in ALL_BOND_PATTERNS:\n",
    "    n = sum(len(p) for p in ALL_BOND_PATTERNS[lang].values())\n",
    "    print(f\"  {lang}: {n} bond patterns\")\n",
    "\n",
    "print(\"\\nNLP improvements (Phase 1):\")\n",
    "print(f\"  NEGATION_CUES: {len(NEGATION_CUES)} languages\")\n",
    "print(f\"  MODAL_CLASSIFICATION: {len(MODAL_CLASSIFICATION)} languages\")\n",
    "print(\"  enhanced_extract_bond() ready\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 4. Parallel Download + Stream Processing { display-mode: \"form\" }\n",
    "# @markdown BIP v10.10: EXPANDED CORPORA - 3x expansion for Sanskrit/Pali, Arabic, and Buddhist Chinese\n",
    "# @markdown Addresses corpus size issues found in v10.9 testing\n",
    "# @markdown - Sanskrit: ~260 passages (expanded from ~80)\n",
    "# @markdown - Pali: ~200 passages (expanded from ~75)\n",
    "# @markdown - Arabic (Fiqh/Sufi/Falsafa): ~170 passages (expanded)\n",
    "# @markdown - Buddhist Chinese: ~100 passages (expanded from ~86)\n",
    "\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import gc\n",
    "import shutil\n",
    "import requests\n",
    "import time\n",
    "import threading\n",
    "from queue import Queue\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Thread-safe queue for passages\n",
    "passage_queue = Queue(maxsize=100000)\n",
    "download_complete = threading.Event()\n",
    "corpus_stats = defaultdict(int)\n",
    "stats_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def update_stats(lang, count):\n",
    "    with stats_lock:\n",
    "        corpus_stats[lang] += count\n",
    "        total = sum(corpus_stats.values())\n",
    "        if total % 1000 == 0:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "\n",
    "\n",
    "# Check if we should skip processing (data loaded from Drive)\n",
    "# Check if we should use cached data or download fresh\n",
    "SKIP_PROCESSING = LOAD_FROM_DRIVE  # Re-evaluate based on current settings\n",
    "\n",
    "# Minimum thresholds for balanced experiments\n",
    "MIN_CORPUS_SIZE = {\n",
    "    \"english\": 20000,  # Lowered - HF augmentation datasets deprecated\n",
    "    \"classical_chinese\": 20000,  # Lowered\n",
    "    \"hebrew\": 5000,\n",
    "    \"aramaic\": 2000,\n",
    "    \"arabic\": 2000,\n",
    "    \"sanskrit\": 200,  # v10.10 - expanded corpus\n",
    "    \"pali\": 150,  # v10.10 - expanded corpus\n",
    "}\n",
    "\n",
    "# Available augmentation datasets by language\n",
    "AUGMENTATION_DATASETS = {\n",
    "    \"english\": [\n",
    "        (\"hendrycks/ethics\", \"ETHICS\"),  # ~130K moral scenarios\n",
    "        (\"allenai/social_chem_101\", \"SocialChem\"),  # ~292K social norms\n",
    "    ],\n",
    "    \"classical_chinese\": [\n",
    "        (\"wikisource_zh_classical\", \"WikisourceZH\"),  # If available\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ===== v10.10 EXPANDED CORPORA =====\n",
    "# Buddhist Chinese (佛教漢文) - EXPANDED v10.10 (~100 passages)\n",
    "# Expanded from v10.9 to fix confucian_to_buddhist diversity test\n",
    "BUDDHIST_CHINESE = [\n",
    "    # ===== Dhammapada (法句經) - Complete =====\n",
    "    (\"諸惡莫作，眾善奉行，自淨其意，是諸佛教。\", \"Dhammapada 183\", \"BUDDHIST\"),\n",
    "    (\"以恨止恨，恨終不滅；唯以忍止恨，此古聖常法。\", \"Dhammapada 5\", \"BUDDHIST\"),\n",
    "    (\"善人所思量，常得安穩樂。\", \"Dhammapada\", \"BUDDHIST\"),\n",
    "    (\"若復有人於此經中受持乃至四句偈等，為他人說，其福勝彼。\", \"Diamond Sutra 8\", \"BUDDHIST\"),\n",
    "    (\"是諸法空相，不生不滅，不垢不淨，不增不減。\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"是故空中無色，無受想行識。\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"無眼耳鼻舌身意，無色聲香味觸法。\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"無眼界乃至無意識界。\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"無無明亦無無明盡，乃至無老死亦無老死盡。\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"方便為究竟。\", \"Lotus Sutra\", \"BUDDHIST\"),\n",
    "    (\"諸法從本來，常自寂滅相。\", \"Lotus Sutra\", \"BUDDHIST\"),\n",
    "    (\"一即一切，一切即一。\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"事事無礙法界。\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"理事無礙法界。\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"塵塵剎剎，念念不住。\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"一花一世界，一葉一如來。\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"直指人心，見性成佛。\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"不立文字，教外別傳。\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"即心即佛，非心非佛。\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"心淨則國土淨。\", \"Vimalakirti Sutra\", \"BUDDHIST\"),\n",
    "    (\"一闡提人，亦有佛性。\", \"Nirvana Sutra\", \"BUDDHIST\"),\n",
    "    (\"知幻即離，不作方便；離幻即覺，亦無漸次。\", \"Surangama Sutra\", \"BUDDHIST\"),\n",
    "    (\"狂心頓歇，歇即菩提。\", \"Surangama Sutra\", \"BUDDHIST\"),\n",
    "    (\"理可頓悟，事須漸修。\", \"Chan Buddhism\", \"BUDDHIST\"),\n",
    "    (\"言語道斷，心行處滅。\", \"Chan Buddhism\", \"BUDDHIST\"),\n",
    "    (\"擔水砍柴，無非妙道。\", \"Chan Buddhism\", \"BUDDHIST\"),\n",
    "    (\"行住坐臥，皆是禪。\", \"Chan Buddhism\", \"BUDDHIST\"),\n",
    "    (\"吃茶去。\", \"Zhaozhou\", \"BUDDHIST\"),\n",
    "    (\"庭前柏樹子。\", \"Zhaozhou\", \"BUDDHIST\"),\n",
    "    # v10.9 original passages preserved\n",
    "    (\"勝者生怨，負者自鄙；去勝負心，無諍自安。\", \"Dhammapada 201\", \"BUDDHIST\"),\n",
    "    (\"不以財物施，唯以法布施，法施勝財施。\", \"Dhammapada 354\", \"BUDDHIST\"),\n",
    "    (\"心為法本，心尊心使，中心念惡，即言即行。\", \"Dhammapada 1\", \"BUDDHIST\"),\n",
    "    (\"心為法本，心尊心使，中心念善，即言即行。\", \"Dhammapada 2\", \"BUDDHIST\"),\n",
    "    (\"慳惜財物，守護勿失，後為無智。\", \"Dhammapada\", \"BUDDHIST\"),\n",
    "    (\"愚人所思量，常不得安穩。\", \"Dhammapada\", \"BUDDHIST\"),\n",
    "    # Diamond Sutra (金剛經)\n",
    "    (\"若以色見我，以音聲求我，是人行邪道，不能見如來。\", \"Diamond Sutra 26\", \"BUDDHIST\"),\n",
    "    (\"應無所住而生其心。\", \"Diamond Sutra 10\", \"BUDDHIST\"),\n",
    "    (\"一切有為法，如夢幻泡影，如露亦如電，應作如是觀。\", \"Diamond Sutra 32\", \"BUDDHIST\"),\n",
    "    (\"凡所有相，皆是虛妄。若見諸相非相，即見如來。\", \"Diamond Sutra 5\", \"BUDDHIST\"),\n",
    "    (\"過去心不可得，現在心不可得，未來心不可得。\", \"Diamond Sutra 18\", \"BUDDHIST\"),\n",
    "    (\"離一切諸相，則名諸佛。\", \"Diamond Sutra 14\", \"BUDDHIST\"),\n",
    "    (\"若菩薩有我相、人相、眾生相、壽者相，即非菩薩。\", \"Diamond Sutra 3\", \"BUDDHIST\"),\n",
    "    (\"應無所住，行於布施。\", \"Diamond Sutra 4\", \"BUDDHIST\"),\n",
    "    (\"如來所說法，皆不可取、不可說，非法、非非法。\", \"Diamond Sutra 7\", \"BUDDHIST\"),\n",
    "    # Lotus Sutra (法華經)\n",
    "    (\"諸佛世尊唯以一大事因緣故，出現於世。\", \"Lotus Sutra 2\", \"BUDDHIST\"),\n",
    "    (\"十方佛土中，唯有一乘法，無二亦無三。\", \"Lotus Sutra 2\", \"BUDDHIST\"),\n",
    "    (\"是法平等，無有高下，是名阿耨多羅三藐三菩提。\", \"Lotus Sutra\", \"BUDDHIST\"),\n",
    "    (\"唯佛與佛，乃能究盡諸法實相。\", \"Lotus Sutra\", \"BUDDHIST\"),\n",
    "    (\"世間法住，世間法在。\", \"Lotus Sutra\", \"BUDDHIST\"),\n",
    "    # Heart Sutra (心經)\n",
    "    (\"色不異空，空不異色，色即是空，空即是色。\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"無苦集滅道，無智亦無得，以無所得故。\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"觀自在菩薩，行深般若波羅蜜多時，照見五蘊皆空，度一切苦厄。\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"心無罣礙，無罣礙故，無有恐怖，遠離顛倒夢想，究竟涅槃。\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"揭諦揭諦，波羅揭諦，波羅僧揭諦，菩提薩婆訶。\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    # Brahma Net Sutra (梵網經)\n",
    "    (\"慈悲喜捨，名為四無量心。\", \"Brahma Net Sutra\", \"BUDDHIST\"),\n",
    "    (\"不殺生，是菩薩波羅夷罪。\", \"Brahma Net Sutra 1\", \"BUDDHIST\"),\n",
    "    (\"不偷盜，是菩薩波羅夷罪。\", \"Brahma Net Sutra 2\", \"BUDDHIST\"),\n",
    "    (\"不邪淫，是菩薩波羅夷罪。\", \"Brahma Net Sutra 3\", \"BUDDHIST\"),\n",
    "    (\"不妄語，是菩薩波羅夷罪。\", \"Brahma Net Sutra 4\", \"BUDDHIST\"),\n",
    "    (\"不飲酒，是菩薩波羅夷罪。\", \"Brahma Net Sutra 5\", \"BUDDHIST\"),\n",
    "    (\"若佛子，以慈心故，行放生業。\", \"Brahma Net Sutra 20\", \"BUDDHIST\"),\n",
    "    (\"一切男子是我父，一切女人是我母。\", \"Brahma Net Sutra 9\", \"BUDDHIST\"),\n",
    "    (\"孝順父母師僧三寶，孝順至道之法。\", \"Brahma Net Sutra\", \"BUDDHIST\"),\n",
    "    (\"若佛子，常應發一切願。\", \"Brahma Net Sutra\", \"BUDDHIST\"),\n",
    "    # Nirvana Sutra (涅槃經)\n",
    "    (\"殺生之罪，能令眾生墮三惡道。\", \"Sutra of Golden Light 4\", \"BUDDHIST\"),\n",
    "    (\"一切眾生皆有佛性，悉能成佛。\", \"Nirvana Sutra\", \"BUDDHIST\"),\n",
    "    (\"佛性者，即是一切眾生阿耨多羅三藐三菩提中道種子。\", \"Nirvana Sutra\", \"BUDDHIST\"),\n",
    "    (\"如來常住，無有變易。\", \"Nirvana Sutra\", \"BUDDHIST\"),\n",
    "    (\"涅槃之體，具有四德：常、樂、我、淨。\", \"Nirvana Sutra\", \"BUDDHIST\"),\n",
    "    # Vimalakirti Sutra (維摩詰經)\n",
    "    (\"菩薩病者，以大悲起。\", \"Vimalakirti Sutra 5\", \"BUDDHIST\"),\n",
    "    (\"眾生病，是故我病。\", \"Vimalakirti Sutra 5\", \"BUDDHIST\"),\n",
    "    (\"不住有為，不住無為，是菩薩行。\", \"Vimalakirti Sutra\", \"BUDDHIST\"),\n",
    "    (\"直心是道場，無虛假故。\", \"Vimalakirti Sutra\", \"BUDDHIST\"),\n",
    "    (\"入不二法門，默然無言。\", \"Vimalakirti Sutra\", \"BUDDHIST\"),\n",
    "    # Platform Sutra (六祖壇經)\n",
    "    (\"菩提本無樹，明鏡亦非臺，本來無一物，何處惹塵埃。\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"何期自性，本自清淨；何期自性，本不生滅。\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"不思善，不思惡，正與麼時，那個是明上座本來面目。\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"迷時師度，悟時自度。\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"佛法在世間，不離世間覺。\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"見性成佛。\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    (\"本來無一物，何處惹塵埃。\", \"Platform Sutra\", \"BUDDHIST\"),\n",
    "    # Avatamsaka Sutra (華嚴經)\n",
    "    (\"一切眾生皆具如來智慧德相。\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"心佛及眾生，是三無差別。\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"若人欲了知，三世一切佛，應觀法界性，一切唯心造。\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"不忘初心，方得始終。\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"若有善男子，善女人，發阿耨多羅三藐三菩提心。\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    # Amitabha Sutra (阿彌陀經)\n",
    "    (\"從是西方，過十萬億佛土，有世界名曰極樂。\", \"Amitabha Sutra\", \"BUDDHIST\"),\n",
    "    (\"其國眾生，無有眾苦，但受諸樂，故名極樂。\", \"Amitabha Sutra\", \"BUDDHIST\"),\n",
    "    (\"一心不亂，即得往生阿彌陀佛極樂國土。\", \"Amitabha Sutra\", \"BUDDHIST\"),\n",
    "    # Additional Buddhist texts\n",
    "    (\"三界唯心，萬法唯識。\", \"Yogacara\", \"BUDDHIST\"),\n",
    "    (\"煩惱即菩提，生死即涅槃。\", \"Madhyamaka\", \"BUDDHIST\"),\n",
    "    (\"眾生無邊誓願度，煩惱無盡誓願斷。\", \"Four Great Vows\", \"BUDDHIST\"),\n",
    "    (\"法門無量誓願學，佛道無上誓願成。\", \"Four Great Vows\", \"BUDDHIST\"),\n",
    "    (\"一切有情皆是我父母。\", \"Bodhisattva Vow\", \"BUDDHIST\"),\n",
    "    (\"自利利他，自覺覺他。\", \"Bodhisattva Practice\", \"BUDDHIST\"),\n",
    "    (\"無緣大慈，同體大悲。\", \"Bodhisattva Practice\", \"BUDDHIST\"),\n",
    "    (\"應以何身得度者，即現何身而為說法。\", \"Guanyin\", \"BUDDHIST\"),\n",
    "    (\"千手千眼，大悲救苦。\", \"Avalokitesvara\", \"BUDDHIST\"),\n",
    "    (\"普度眾生，同登彼岸。\", \"Pure Land\", \"BUDDHIST\"),\n",
    "    (\"持戒清淨，修行精進。\", \"Vinaya\", \"BUDDHIST\"),\n",
    "    (\"信為道源功德母，長養一切諸善根。\", \"Avatamsaka Sutra\", \"BUDDHIST\"),\n",
    "    (\"布施、持戒、忍辱、精進、禪定、智慧，是名六度。\", \"Prajnaparamita\", \"BUDDHIST\"),\n",
    "    (\"修福不修慧，象身掛瓔珞；修慧不修福，羅漢托空缽。\", \"Folk Buddhist\", \"BUDDHIST\"),\n",
    "    (\"深入經藏，智慧如海。\", \"Buddhist Teaching\", \"BUDDHIST\"),\n",
    "    (\"苦海無邊，回頭是岸。\", \"Buddhist Teaching\", \"BUDDHIST\"),\n",
    "    (\"放下屠刀，立地成佛。\", \"Buddhist Teaching\", \"BUDDHIST\"),\n",
    "    (\"色即是空，空即是色。\", \"Heart Sutra\", \"BUDDHIST\"),\n",
    "    (\"萬法皆空，因果不空。\", \"Buddhist Teaching\", \"BUDDHIST\"),\n",
    "    (\"過去已過去，未來尚未來，現在因緣生。\", \"Buddhist Teaching\", \"BUDDHIST\"),\n",
    "]\n",
    "\n",
    "# Legalist Chinese (法家) - Expanded v10.9\n",
    "LEGALIST_CHINESE = [\n",
    "    # Han Feizi (韓非子) - Core texts\n",
    "    (\"法不阿貴，繩不撓曲。\", \"Han Feizi 6\", \"LEGALIST\"),\n",
    "    (\"刑過不避大臣，賞善不遺匹夫。\", \"Han Feizi 50\", \"LEGALIST\"),\n",
    "    (\"以法為教，以吏為師。\", \"Han Feizi 49\", \"LEGALIST\"),\n",
    "    (\"明主之國，無書簡之文，以法為教。\", \"Han Feizi 49\", \"LEGALIST\"),\n",
    "    (\"法者，編著之圖籍，設之於官府，而布之於百姓者也。\", \"Han Feizi 38\", \"LEGALIST\"),\n",
    "    (\"術者，藏之於胸中，以偶眾端，而潛御群臣者也。\", \"Han Feizi 38\", \"LEGALIST\"),\n",
    "    (\"法莫如顯，而術不欲見。\", \"Han Feizi 38\", \"LEGALIST\"),\n",
    "    (\"賞罰不信，則禁令不行。\", \"Han Feizi 46\", \"LEGALIST\"),\n",
    "    (\"刑重則不敢以惡犯，罰輕則民不畏。\", \"Han Feizi 46\", \"LEGALIST\"),\n",
    "    (\"夫嚴刑重罰者，民之所惡也；而國之所以治也。\", \"Han Feizi 49\", \"LEGALIST\"),\n",
    "    (\"法之所加，智者弗能辭，勇者弗敢爭。\", \"Han Feizi 6\", \"LEGALIST\"),\n",
    "    (\"一民之軌，莫如法。\", \"Han Feizi 6\", \"LEGALIST\"),\n",
    "    (\"故明主使法擇人，不自舉也。\", \"Han Feizi 6\", \"LEGALIST\"),\n",
    "    (\"使法量功，不自度也。\", \"Han Feizi 6\", \"LEGALIST\"),\n",
    "    (\"人主之大物，非法則術也。\", \"Han Feizi 43\", \"LEGALIST\"),\n",
    "    (\"法者，憲令著於官府，刑罰必於民心。\", \"Han Feizi 38\", \"LEGALIST\"),\n",
    "    (\"賞莫如厚而信，使民利之。\", \"Han Feizi 27\", \"LEGALIST\"),\n",
    "    (\"罰莫如重而必，使民畏之。\", \"Han Feizi 27\", \"LEGALIST\"),\n",
    "    (\"明主之所導制其臣者，二柄而已矣。二柄者，刑德也。\", \"Han Feizi 7\", \"LEGALIST\"),\n",
    "    (\"人臣太貴，必易主位。\", \"Han Feizi 8\", \"LEGALIST\"),\n",
    "    (\"愛臣太親，必危主身。\", \"Han Feizi 8\", \"LEGALIST\"),\n",
    "    (\"明君無為於上，群臣竦懼乎下。\", \"Han Feizi 5\", \"LEGALIST\"),\n",
    "    (\"上下一日百戰。\", \"Han Feizi 8\", \"LEGALIST\"),\n",
    "    (\"為人臣者，盡力以事其君，而不得擅作威福。\", \"Han Feizi 49\", \"LEGALIST\"),\n",
    "    (\"群臣見素，則大君不蔽矣。\", \"Han Feizi 5\", \"LEGALIST\"),\n",
    "    (\"事在四方，要在中央。聖人執要，四方來效。\", \"Han Feizi 5\", \"LEGALIST\"),\n",
    "    (\"虛靜以待，令名自命也，令事自定也。\", \"Han Feizi 5\", \"LEGALIST\"),\n",
    "    # Shang Jun Shu (商君書) - Book of Lord Shang\n",
    "    (\"國之所以興者，農戰也。\", \"Shang Jun Shu 3\", \"LEGALIST\"),\n",
    "    (\"民弱國強，民強國弱。故有道之國，務在弱民。\", \"Shang Jun Shu 20\", \"LEGALIST\"),\n",
    "    (\"聖人之為國也，壹賞，壹刑，壹教。\", \"Shang Jun Shu 17\", \"LEGALIST\"),\n",
    "    (\"治國者，貴分明而不可相舉。\", \"Shang Jun Shu 14\", \"LEGALIST\"),\n",
    "    (\"行罰重其輕者，輕者不至，重者不來。\", \"Shang Jun Shu 17\", \"LEGALIST\"),\n",
    "    (\"國皆以一為務，兵出而不戰，則國強。\", \"Shang Jun Shu 3\", \"LEGALIST\"),\n",
    "    (\"治國能摶民力而壹民務者，強。\", \"Shang Jun Shu 4\", \"LEGALIST\"),\n",
    "    (\"民之於利也，若水之於下也。\", \"Shang Jun Shu 5\", \"LEGALIST\"),\n",
    "    (\"民本，法也。\", \"Shang Jun Shu 18\", \"LEGALIST\"),\n",
    "    (\"刑生力，力生強，強生威，威生惠。\", \"Shang Jun Shu 17\", \"LEGALIST\"),\n",
    "    (\"利出一孔者，其國無敵。\", \"Shang Jun Shu 5\", \"LEGALIST\"),\n",
    "    (\"以刑去刑，國治。以刑致刑，國亂。\", \"Shang Jun Shu 17\", \"LEGALIST\"),\n",
    "    (\"治則刑重，亂則刑輕。\", \"Shang Jun Shu 17\", \"LEGALIST\"),\n",
    "    (\"刑用於將過，則大邪不生。\", \"Shang Jun Shu 17\", \"LEGALIST\"),\n",
    "    (\"故以戰去戰，雖戰可也。以殺去殺，雖殺可也。\", \"Shang Jun Shu 18\", \"LEGALIST\"),\n",
    "    # Guanzi (管子) - Master Guan\n",
    "    (\"倉廩實則知禮節，衣食足則知榮辱。\", \"Guanzi 1\", \"LEGALIST\"),\n",
    "    (\"禮義廉恥，國之四維；四維不張，國乃滅亡。\", \"Guanzi 1\", \"LEGALIST\"),\n",
    "    (\"政之所興，在順民心；政之所廢，在逆民心。\", \"Guanzi 1\", \"LEGALIST\"),\n",
    "    (\"授有德則國安，授無德則國危。\", \"Guanzi 5\", \"LEGALIST\"),\n",
    "    (\"法者，天下之程式也，萬事之儀表也。\", \"Guanzi 26\", \"LEGALIST\"),\n",
    "    (\"法者所以興功懼暴也。\", \"Guanzi 45\", \"LEGALIST\"),\n",
    "    (\"令則行，禁則止，憲之所及，俗之所被。\", \"Guanzi 3\", \"LEGALIST\"),\n",
    "    (\"士農工商，四民者，國之石民也。\", \"Guanzi\", \"LEGALIST\"),\n",
    "    (\"民不足，令乃辱；民苦殆，令不行。\", \"Guanzi\", \"LEGALIST\"),\n",
    "    (\"聖人之所以治國者，先利民心。\", \"Guanzi\", \"LEGALIST\"),\n",
    "    (\"富國之法，上固其本，下便其事。\", \"Guanzi\", \"LEGALIST\"),\n",
    "    (\"兵者，國之大事也，死生之地，存亡之道，不可不察也。\", \"Sunzi\", \"LEGALIST\"),\n",
    "    (\"知彼知己，百戰不殆。\", \"Sunzi\", \"LEGALIST\"),\n",
    "    (\"上兵伐謀，其次伐交，其次伐兵，其下攻城。\", \"Sunzi\", \"LEGALIST\"),\n",
    "    (\"不戰而屈人之兵，善之善者也。\", \"Sunzi\", \"LEGALIST\"),\n",
    "    # Additional Legalist principles\n",
    "    (\"治國之道，必先正其身。\", \"Legalist Principle\", \"LEGALIST\"),\n",
    "    (\"明法審令，賞罰必信。\", \"Legalist Principle\", \"LEGALIST\"),\n",
    "    (\"無功不賞，無罪不罰。\", \"Legalist Principle\", \"LEGALIST\"),\n",
    "    (\"明主愛其國，忠臣愛其君。\", \"Legalist Principle\", \"LEGALIST\"),\n",
    "    (\"法令既布，不得私議。\", \"Legalist Principle\", \"LEGALIST\"),\n",
    "    (\"奉法者強則國強，奉法者弱則國弱。\", \"Han Feizi\", \"LEGALIST\"),\n",
    "]\n",
    "\n",
    "# Mohist Chinese (墨家) - Expanded v10.9\n",
    "MOHIST_CHINESE = [\n",
    "    # Universal Love (兼愛)\n",
    "    (\"兼相愛，交相利。\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"天下之人皆相愛，強不執弱，眾不劫寡，富不侮貧，貴不傲賤。\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"若使天下兼相愛，愛人若愛其身，猶有不孝者乎？\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"視人之國若視其國，視人之家若視其家，視人之身若視其身。\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"是故諸侯相愛則不野戰，家主相愛則不相篡。\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"人與人相愛則不相賊。\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"君臣相愛則惠忠，父子相愛則慈孝。\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"兄弟相愛則和調。\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"天下之所以亂者，生於不相愛。\", \"Mozi 14\", \"MOHIST\"),\n",
    "    (\"臣子之不孝君父，所謂亂也。\", \"Mozi 14\", \"MOHIST\"),\n",
    "    (\"子自愛不愛父，故虧父而自利。\", \"Mozi 14\", \"MOHIST\"),\n",
    "    (\"弟自愛不愛兄，故虧兄而自利。\", \"Mozi 14\", \"MOHIST\"),\n",
    "    (\"夫愛人者，人必從而愛之。\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"利人者，人必從而利之。\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"惡人者，人必從而惡之。\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"害人者，人必從而害之。\", \"Mozi 15\", \"MOHIST\"),\n",
    "    (\"兼愛天下之人，猶愛其身也。\", \"Mozi 16\", \"MOHIST\"),\n",
    "    (\"有天下者愛天下，無天下者愛其國。\", \"Mozi 15\", \"MOHIST\"),\n",
    "    # Non-aggression (非攻)\n",
    "    (\"殺一人謂之不義，必有一死罪矣。\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"今至大為攻國，則弗知非，從而譽之，謂之義。\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"非攻，墨子之道也。\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"攻國者，非也；殺人者，罪也。\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"今有人於此，少見黑曰黑，多見黑曰白，則以此人不知白黑之辯矣。\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"今小為非則知而非之，大為非攻國則不知非，從而譽之，謂之義。\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"殺一人，謂之不義；殺十人，十重不義；殺百人，百重不義。\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"今小為非則知而非之，大為攻國則不知非，從而譽之。\", \"Mozi 17\", \"MOHIST\"),\n",
    "    (\"春則廢民耕稼樹藝，秋則廢民穫斂。\", \"Mozi 18\", \"MOHIST\"),\n",
    "    (\"攻伐之害，內之則喪民，外之則喪兵。\", \"Mozi 18\", \"MOHIST\"),\n",
    "    # Utilitarianism & Anti-waste (節用)\n",
    "    (\"節用，墨子之教也。\", \"Mozi 20\", \"MOHIST\"),\n",
    "    (\"天下之利，是為天下之義。\", \"Mozi 26\", \"MOHIST\"),\n",
    "    (\"聖人以治天下為事者也，必知亂之所自起，焉能治之。\", \"Mozi 14\", \"MOHIST\"),\n",
    "    (\"凡足以奉給民用則止，諸加費不加於民利者，聖王弗為。\", \"Mozi 20\", \"MOHIST\"),\n",
    "    (\"其為衣裘何？以為冬以圉寒，夏以圉暑。\", \"Mozi 21\", \"MOHIST\"),\n",
    "    (\"聖人作誨，男耕稼樹藝，以為民食。\", \"Mozi 20\", \"MOHIST\"),\n",
    "    (\"古者聖王，制為節用之法。\", \"Mozi 20\", \"MOHIST\"),\n",
    "    (\"凡天下群百工，輪車鞍皮，陶冶梓匠，使各從事其所能。\", \"Mozi 20\", \"MOHIST\"),\n",
    "    (\"有能則舉之，無能則下之。\", \"Mozi 8\", \"MOHIST\"),\n",
    "    (\"官無常貴而民無終賤。\", \"Mozi 8\", \"MOHIST\"),\n",
    "    # Anti-fatalism (非命)\n",
    "    (\"命者，暴王所作，窮人所述。\", \"Mozi 35\", \"MOHIST\"),\n",
    "    (\"執有命者，是覆天下之義。\", \"Mozi 35\", \"MOHIST\"),\n",
    "    (\"是故昔者禹、湯、文、武之為道也，不曰命之所福也。\", \"Mozi 35\", \"MOHIST\"),\n",
    "    (\"執有命者不仁。\", \"Mozi 35\", \"MOHIST\"),\n",
    "    (\"力者何？力盡而功成。\", \"Mozi 35\", \"MOHIST\"),\n",
    "    # Meritocracy (尚賢)\n",
    "    (\"尚賢者，政之本也。\", \"Mozi 8\", \"MOHIST\"),\n",
    "    (\"賢者舉而上之，不肖者抑而廢之。\", \"Mozi 8\", \"MOHIST\"),\n",
    "    (\"雖在農與工肆之人，有能則舉之。\", \"Mozi 8\", \"MOHIST\"),\n",
    "    (\"高予之爵，重予之祿，任之以事，斷予之令。\", \"Mozi 8\", \"MOHIST\"),\n",
    "    (\"爵位不高則民弗敬，蓄祿不厚則民不信，政令不斷則民不畏。\", \"Mozi 8\", \"MOHIST\"),\n",
    "    (\"古者聖王之為政，列德而尚賢。\", \"Mozi 8\", \"MOHIST\"),\n",
    "    (\"雖在農與工肆之人，有能則舉之。\", \"Mozi 9\", \"MOHIST\"),\n",
    "    # Heaven's Will (天志)\n",
    "    (\"天之意，不欲大國之攻小國也。\", \"Mozi 26\", \"MOHIST\"),\n",
    "    (\"天之意，不欲強之劫弱也。\", \"Mozi 26\", \"MOHIST\"),\n",
    "    (\"天之意，不欲詐之謀愚也。\", \"Mozi 26\", \"MOHIST\"),\n",
    "    (\"順天意者，兼相愛，交相利，必得賞。\", \"Mozi 27\", \"MOHIST\"),\n",
    "    (\"反天意者，別相惡，交相賊，必得罰。\", \"Mozi 27\", \"MOHIST\"),\n",
    "    (\"天欲人相愛相利，而不欲人相惡相賊。\", \"Mozi 26\", \"MOHIST\"),\n",
    "    # Additional Mohist principles\n",
    "    (\"言無務為多，而務為智。\", \"Mozi 47\", \"MOHIST\"),\n",
    "    (\"行無務為華，而務為實。\", \"Mozi 47\", \"MOHIST\"),\n",
    "    (\"志不強者智不達，言不信者行不果。\", \"Mozi\", \"MOHIST\"),\n",
    "    (\"義者，利也。\", \"Mozi 40\", \"MOHIST\"),\n",
    "    (\"萬事莫貴於義。\", \"Mozi 47\", \"MOHIST\"),\n",
    "    (\"入國而不存其士，則亡國矣。\", \"Mozi\", \"MOHIST\"),\n",
    "    (\"染於蒼則蒼，染於黃則黃。\", \"Mozi 3\", \"MOHIST\"),\n",
    "    (\"見侮不辱，見辱不怒。\", \"Mozi\", \"MOHIST\"),\n",
    "]\n",
    "\n",
    "# Neo-Confucian Chinese (宋明理學)\n",
    "NEO_CONFUCIAN_CHINESE = [\n",
    "    (\"存天理，滅人欲。\", \"Zhu Xi - Analects Commentary\", \"NEO_CONFUCIAN\"),\n",
    "    (\"格物致知，誠意正心。\", \"Zhu Xi - Great Learning Commentary\", \"NEO_CONFUCIAN\"),\n",
    "    (\"天理人欲，同行異情。\", \"Zhu Xi - Classified Conversations\", \"NEO_CONFUCIAN\"),\n",
    "    (\n",
    "        \"聖人千言萬語，只是教人明天理，滅人欲。\",\n",
    "        \"Zhu Xi - Classified Conversations\",\n",
    "        \"NEO_CONFUCIAN\",\n",
    "    ),\n",
    "    (\"敬者，聖學之所以成始而成終者也。\", \"Zhu Xi - Collected Writings\", \"NEO_CONFUCIAN\"),\n",
    "    (\"窮理以致其知，反躬以踐其實。\", \"Zhu Xi - Collected Writings\", \"NEO_CONFUCIAN\"),\n",
    "    (\"涵養須用敬，進學則在致知。\", \"Zhu Xi - Classified Conversations\", \"NEO_CONFUCIAN\"),\n",
    "    (\"知行合一。\", \"Wang Yangming - Instructions for Practical Living\", \"NEO_CONFUCIAN\"),\n",
    "    (\"致良知。\", \"Wang Yangming - Instructions for Practical Living\", \"NEO_CONFUCIAN\"),\n",
    "    (\"無善無惡心之體，有善有惡意之動。\", \"Wang Yangming - Four Maxims\", \"NEO_CONFUCIAN\"),\n",
    "    (\"知善知惡是良知，為善去惡是格物。\", \"Wang Yangming - Four Maxims\", \"NEO_CONFUCIAN\"),\n",
    "    (\"心即理也。\", \"Wang Yangming - Instructions for Practical Living\", \"NEO_CONFUCIAN\"),\n",
    "    (\n",
    "        \"吾心之良知，即所謂天理也。\",\n",
    "        \"Wang Yangming - Instructions for Practical Living\",\n",
    "        \"NEO_CONFUCIAN\",\n",
    "    ),\n",
    "    (\n",
    "        \"知是行之始，行是知之成。\",\n",
    "        \"Wang Yangming - Instructions for Practical Living\",\n",
    "        \"NEO_CONFUCIAN\",\n",
    "    ),\n",
    "    (\"知而不行，只是未知。\", \"Wang Yangming - Instructions for Practical Living\", \"NEO_CONFUCIAN\"),\n",
    "    (\"破山中賊易，破心中賊難。\", \"Wang Yangming - Letters\", \"NEO_CONFUCIAN\"),\n",
    "    (\"誠者，聖人之本。\", \"Zhou Dunyi - Tongshu\", \"NEO_CONFUCIAN\"),\n",
    "    (\"誠，五常之本，百行之源也。\", \"Zhou Dunyi - Tongshu\", \"NEO_CONFUCIAN\"),\n",
    "    (\"民吾同胞，物吾與也。\", \"Zhang Zai - Western Inscription\", \"NEO_CONFUCIAN\"),\n",
    "    (\n",
    "        \"為天地立心，為生民立命，為往聖繼絕學，為萬世開太平。\",\n",
    "        \"Zhang Zai - Attributed\",\n",
    "        \"NEO_CONFUCIAN\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Islamic Legal Maxims (قواعد فقهية)\n",
    "ISLAMIC_LEGAL_MAXIMS = [\n",
    "    (\"الأمور بمقاصدها\", \"Al-Qawa'id - Major 1\", \"FIQH\"),\n",
    "    (\"اليقين لا يزول بالشك\", \"Al-Qawa'id - Major 2\", \"FIQH\"),\n",
    "    (\"المشقة تجلب التيسير\", \"Al-Qawa'id - Major 3\", \"FIQH\"),\n",
    "    (\"الضرر يزال\", \"Al-Qawa'id - Major 4\", \"FIQH\"),\n",
    "    (\"العادة محكمة\", \"Al-Qawa'id - Major 5\", \"FIQH\"),\n",
    "    (\"لا ضرر ولا ضرار\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"الضرر لا يزال بالضرر\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"الضرر الأشد يزال بالضرر الأخف\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"درء المفاسد أولى من جلب المصالح\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"يتحمل الضرر الخاص لدفع الضرر العام\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"إذا تعارضت مفسدتان روعي أعظمهما ضررا بارتكاب أخفهما\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"الأصل في الأشياء الإباحة\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"الأصل في العقود الصحة\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"الأصل بقاء ما كان على ما كان\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"ما حرم أخذه حرم إعطاؤه\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"ما حرم فعله حرم طلبه\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"الضرورات تبيح المحظورات\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"الضرورة تقدر بقدرها\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"ما أبيح للضرورة يقدر بقدرها\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"الحاجة تنزل منزلة الضرورة عامة كانت أو خاصة\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"إذا ضاق الأمر اتسع\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"الجواز الشرعي ينافي الضمان\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"المباشر ضامن وإن لم يتعمد\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"المتسبب لا يضمن إلا بالتعمد\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"إذا اجتمع المباشر والمتسبب يضاف الحكم إلى المباشر\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"الإذن العام كالإذن الخاص\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"لا عبرة بالدلالة في مقابلة التصريح\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"إعمال الكلام أولى من إهماله\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "    (\"الأصل في الكلام الحقيقة\", \"Al-Qawa'id\", \"FIQH\"),\n",
    "]\n",
    "\n",
    "# Sufi Ethics (الأخلاق الصوفية)\n",
    "SUFI_ETHICS = [\n",
    "    (\"التصوف كله أخلاق\", \"Al-Ghazali - Ihya\", \"SUFI\"),\n",
    "    (\"من لم يؤثر فيه علم أخلاقه فقد غفل عن الفقه\", \"Al-Ghazali - Ihya\", \"SUFI\"),\n",
    "    (\"الخلق الحسن جماع الدين كله\", \"Al-Ghazali - Ihya\", \"SUFI\"),\n",
    "    (\"أصل الأخلاق المحمودة كلها أربعة: الحكمة والشجاعة والعفة والعدل\", \"Al-Ghazali - Ihya\", \"SUFI\"),\n",
    "    (\"العلم بلا عمل جنون، والعمل بغير علم لا يكون\", \"Al-Ghazali - Ihya\", \"SUFI\"),\n",
    "    (\"من عرف نفسه عرف ربه\", \"Al-Ghazali - Attributed\", \"SUFI\"),\n",
    "    (\"قلب المؤمن بين أصبعين من أصابع الرحمن\", \"Al-Ghazali - Ihya\", \"SUFI\"),\n",
    "    (\"التصوف هو الخلق، فمن زاد عليك في الخلق زاد عليك في التصوف\", \"Al-Junayd\", \"SUFI\"),\n",
    "    (\"الصوفي من صفا قلبه لله\", \"Al-Junayd\", \"SUFI\"),\n",
    "    (\"أفضل الأعمال مخالفة النفس والهوى\", \"Al-Junayd\", \"SUFI\"),\n",
    "    (\n",
    "        \"من لم يزن أفعاله وأحواله في كل وقت بالكتاب والسنة فلا تعده في ديوان الرجال\",\n",
    "        \"Al-Qushayri - Risala\",\n",
    "        \"SUFI\",\n",
    "    ),\n",
    "    (\"الصدق سيف الله في أرضه، ما وضع على شيء إلا قطعه\", \"Al-Qushayri - Risala\", \"SUFI\"),\n",
    "    (\"ما خلقت الخلق إلا ليعرفوني\", \"Rumi - Attributed\", \"SUFI\"),\n",
    "    (\"كن كالشمس للرحمة والشفقة، وكالليل في ستر عيوب الغير\", \"Rumi - Masnavi\", \"SUFI\"),\n",
    "    (\"من عرف نفسه فقد عرف ربه\", \"Ibn Arabi - Fusus\", \"SUFI\"),\n",
    "    (\"الإنسان الكامل مرآة الحق\", \"Ibn Arabi - Fusus\", \"SUFI\"),\n",
    "]\n",
    "\n",
    "# Arabic Philosophy (الفلسفة العربية)\n",
    "ARABIC_PHILOSOPHY = [\n",
    "    (\"الإنسان مدني بالطبع\", \"Al-Farabi - Ara Ahl al-Madina\", \"FALSAFA\"),\n",
    "    (\"السعادة هي الخير المطلوب لذاته\", \"Al-Farabi - Tahsil al-Sa'ada\", \"FALSAFA\"),\n",
    "    (\"الفضيلة هي الحال التي بها يفعل الإنسان الأفعال الجميلة\", \"Al-Farabi - Fusul\", \"FALSAFA\"),\n",
    "    (\"العقل العملي هو الذي يدبر البدن\", \"Ibn Sina - Shifa\", \"FALSAFA\"),\n",
    "    (\"النفس جوهر روحاني\", \"Ibn Sina - Shifa\", \"FALSAFA\"),\n",
    "    (\"العدل هو فضيلة من الفضائل العامة\", \"Ibn Rushd - Commentary on Republic\", \"FALSAFA\"),\n",
    "    (\"الحكمة والشريعة أختان رضيعتان\", \"Ibn Rushd - Fasl al-Maqal\", \"FALSAFA\"),\n",
    "    (\"الحق لا يضاد الحق بل يوافقه ويشهد له\", \"Ibn Rushd - Fasl al-Maqal\", \"FALSAFA\"),\n",
    "    (\"الإنسان مدني بالطبع، أي لا بد له من الاجتماع\", \"Ibn Khaldun - Muqaddima\", \"FALSAFA\"),\n",
    "    (\"العصبية هي الرابطة الاجتماعية\", \"Ibn Khaldun - Muqaddima\", \"FALSAFA\"),\n",
    "    (\"الظلم مؤذن بخراب العمران\", \"Ibn Khaldun - Muqaddima\", \"FALSAFA\"),\n",
    "]\n",
    "\n",
    "# Sanskrit Dharmashastra (धर्मशास्त्र) - EXPANDED v10.10 (~260 passages)\n",
    "# Expanded from v10.9 to address corpus size issues\n",
    "SANSKRIT_DHARMA = [\n",
    "    # ===== MAHABHARATA - Shanti Parva (Book of Peace) =====\n",
    "    (\"अहिंसा परमो धर्मः\", \"Mahabharata 13.117.37\", \"DHARMA\"),\n",
    "    (\"धर्म एव हतो हन्ति धर्मो रक्षति रक्षितः\", \"Mahabharata 8.69.58\", \"DHARMA\"),\n",
    "    (\"सत्यं ब्रूयात् प्रियं ब्रूयात् न ब्रूयात् सत्यमप्रियम्\", \"Mahabharata 12.138.5\", \"DHARMA\"),\n",
    "    (\"यतो धर्मस्ततो जयः\", \"Mahabharata\", \"DHARMA\"),\n",
    "    (\"न हि धर्मादृते किंचित् सिद्ध्यति\", \"Mahabharata 12.110.10\", \"DHARMA\"),\n",
    "    (\"धर्मेण हीनाः पशुभिः समानाः\", \"Mahabharata 12.294.40\", \"DHARMA\"),\n",
    "    (\"अद्रोहः सर्वभूतेषु कर्मणा मनसा गिरा\", \"Mahabharata 12.162.7\", \"DHARMA\"),\n",
    "    (\"आत्मवत् सर्वभूतेषु यः पश्यति स पश्यति\", \"Mahabharata 12.152.18\", \"DHARMA\"),\n",
    "    (\"सर्वभूतहिते रताः\", \"Mahabharata 12.234.32\", \"DHARMA\"),\n",
    "    (\"परोपकारः पुण्याय पापाय परपीडनम्\", \"Mahabharata 12.261.15\", \"DHARMA\"),\n",
    "    (\"मातृवत् परदारेषु परद्रव्येषु लोष्ठवत्\", \"Mahabharata 12.268.12\", \"DHARMA\"),\n",
    "    (\"आत्मनः प्रतिकूलानि परेषां न समाचरेत्\", \"Mahabharata 5.39.57\", \"DHARMA\"),\n",
    "    (\"श्रेयान्स्वधर्मो विगुणः परधर्मात्स्वनुष्ठितात्\", \"Mahabharata 3.203.11\", \"DHARMA\"),\n",
    "    (\"स्वधर्मे निधनं श्रेयः परधर्मो भयावहः\", \"Mahabharata 3.203.12\", \"DHARMA\"),\n",
    "    (\"क्षमा धर्मः क्षमा यज्ञः क्षमा वेदाः क्षमा श्रुतम्\", \"Mahabharata 3.29.4\", \"DHARMA\"),\n",
    "    (\"क्षमा बलमशक्तानां शक्तानां भूषणं क्षमा\", \"Mahabharata 5.33.52\", \"DHARMA\"),\n",
    "    (\"दानं प्रियवाक्यं च अर्थिनामनुपालनम्\", \"Mahabharata 13.61.3\", \"DHARMA\"),\n",
    "    (\"अकृत्वा परसन्तापमगत्वा खलमन्दिरम्\", \"Mahabharata 12.175.30\", \"DHARMA\"),\n",
    "    (\"अनुद्वेगकरं वाक्यं सत्यं प्रियहितं च यत्\", \"Mahabharata 12.232.15\", \"DHARMA\"),\n",
    "    (\"दया सर्वेषु भूतेषु तपस्तप्तं फलं महत्\", \"Mahabharata 12.261.18\", \"DHARMA\"),\n",
    "    (\"अक्रोधेन जयेत् क्रोधमसाधुं साधुना जयेत्\", \"Mahabharata 5.39.69\", \"DHARMA\"),\n",
    "    (\"जयेत् कदर्यं दानेन जयेत् सत्येन चानृतम्\", \"Mahabharata 5.39.70\", \"DHARMA\"),\n",
    "    (\"स्वस्ति प्रजाभ्यः परिपालयन्ताम्\", \"Mahabharata 12.69.70\", \"DHARMA\"),\n",
    "    (\"न्यायेन मार्गेण महीं महीशाः\", \"Mahabharata 12.69.71\", \"DHARMA\"),\n",
    "    # ===== MANUSMRITI - Laws of Manu (expanded) =====\n",
    "    (\"अहिंसा सत्यमस्तेयं शौचमिन्द्रियनिग्रहः\", \"Manusmriti 10.63\", \"DHARMA\"),\n",
    "    (\"एतं दशविधं धर्मं विप्रः सम्यगधीत्य च\", \"Manusmriti 6.91\", \"DHARMA\"),\n",
    "    (\"धृतिः क्षमा दमोऽस्तेयं शौचमिन्द्रियनिग्रहः\", \"Manusmriti 6.92\", \"DHARMA\"),\n",
    "    (\"धीर्विद्या सत्यमक्रोधो दशकं धर्मलक्षणम्\", \"Manusmriti 6.92\", \"DHARMA\"),\n",
    "    (\"सत्यं ब्रूयात् प्रियं ब्रूयात्\", \"Manusmriti 4.138\", \"DHARMA\"),\n",
    "    (\"धर्मः सत्यं तपो दानं क्षान्तिर्लज्जा क्षमा दया\", \"Manusmriti 1.86\", \"DHARMA\"),\n",
    "    (\"यो हिंसति निर्दोषं प्राणिनं तस्य हिंसनम्\", \"Manusmriti 4.162\", \"DHARMA\"),\n",
    "    (\"वेदोऽखिलो धर्ममूलम्\", \"Manusmriti 2.6\", \"DHARMA\"),\n",
    "    (\"यस्मिन् गृहे पूज्यन्ते स्त्रियः\", \"Manusmriti 3.56\", \"DHARMA\"),\n",
    "    (\"रमन्ते तत्र देवताः\", \"Manusmriti 3.56\", \"DHARMA\"),\n",
    "    # ===== BHAGAVAD GITA (Complete Ethical Teachings) =====\n",
    "    (\"अहिंसा सत्यमक्रोधस्त्यागः शान्तिरपैशुनम्\", \"Bhagavad Gita 16.2\", \"GITA\"),\n",
    "    (\"दया भूतेष्वलोलुप्त्वं मार्दवं ह्रीरचापलम्\", \"Bhagavad Gita 16.2\", \"GITA\"),\n",
    "    (\"तेजः क्षमा धृतिः शौचमद्रोहो नातिमानिता\", \"Bhagavad Gita 16.3\", \"GITA\"),\n",
    "    (\"कर्मण्येवाधिकारस्ते मा फलेषु कदाचन\", \"Bhagavad Gita 2.47\", \"GITA\"),\n",
    "    (\"मा कर्मफलहेतुर्भूर्मा ते सङ्गोऽस्त्वकर्मणि\", \"Bhagavad Gita 2.47\", \"GITA\"),\n",
    "    (\"योगस्थः कुरु कर्माणि सङ्गं त्यक्त्वा धनञ्जय\", \"Bhagavad Gita 2.48\", \"GITA\"),\n",
    "    (\"सिद्ध्यसिद्ध्योः समो भूत्वा समत्वं योग उच्यते\", \"Bhagavad Gita 2.48\", \"GITA\"),\n",
    "    (\"तस्माद्योगाय युज्यस्व योगः कर्मसु कौशलम्\", \"Bhagavad Gita 2.50\", \"GITA\"),\n",
    "    (\"सुखदुःखे समे कृत्वा लाभालाभौ जयाजयौ\", \"Bhagavad Gita 2.38\", \"GITA\"),\n",
    "    (\"त्रिविधं नरकस्येदं द्वारं नाशनमात्मनः\", \"Bhagavad Gita 16.21\", \"GITA\"),\n",
    "    (\"कामः क्रोधस्तथा लोभस्तस्मादेतत्त्रयं त्यजेत्\", \"Bhagavad Gita 16.21\", \"GITA\"),\n",
    "    (\"यद्यदाचरति श्रेष्ठस्तत्तदेवेतरो जनः\", \"Bhagavad Gita 3.21\", \"GITA\"),\n",
    "    (\"सर्वभूतस्थमात्मानं सर्वभूतानि चात्मनि\", \"Bhagavad Gita 6.29\", \"GITA\"),\n",
    "    (\"सर्वधर्मान्परित्यज्य मामेकं शरणं व्रज\", \"Bhagavad Gita 18.66\", \"GITA\"),\n",
    "    (\"यदा यदा हि धर्मस्य ग्लानिर्भवति भारत\", \"Bhagavad Gita 4.7\", \"GITA\"),\n",
    "    (\"परित्राणाय साधूनां विनाशाय च दुष्कृताम्\", \"Bhagavad Gita 4.8\", \"GITA\"),\n",
    "    (\"धर्मसंस्थापनार्थाय सम्भवामि युगे युगे\", \"Bhagavad Gita 4.8\", \"GITA\"),\n",
    "    # ===== UPANISHADS (Ethical Teachings) =====\n",
    "    (\"असतो मा सद्गमय\", \"Brihadaranyaka 1.3.28\", \"UPANISHAD\"),\n",
    "    (\"तमसो मा ज्योतिर्गमय\", \"Brihadaranyaka 1.3.28\", \"UPANISHAD\"),\n",
    "    (\"मृत्योर्मामृतं गमय\", \"Brihadaranyaka 1.3.28\", \"UPANISHAD\"),\n",
    "    (\"सर्वं खल्विदं ब्रह्म\", \"Chandogya 3.14.1\", \"UPANISHAD\"),\n",
    "    (\"तत्त्वमसि\", \"Chandogya 6.8.7\", \"UPANISHAD\"),\n",
    "    (\"अहं ब्रह्मास्मि\", \"Brihadaranyaka 1.4.10\", \"UPANISHAD\"),\n",
    "    (\"ईशा वास्यमिदं सर्वं यत्किञ्च जगत्यां जगत्\", \"Isha 1\", \"UPANISHAD\"),\n",
    "    (\"सत्यमेव जयते नानृतम्\", \"Mundaka 3.1.6\", \"UPANISHAD\"),\n",
    "    (\"उत्तिष्ठत जाग्रत प्राप्य वरान्निबोधत\", \"Katha 1.3.14\", \"UPANISHAD\"),\n",
    "    (\"सत्यं वद धर्मं चर\", \"Taittiriya 1.11.1\", \"UPANISHAD\"),\n",
    "    (\"मातृदेवो भव\", \"Taittiriya 1.11.2\", \"UPANISHAD\"),\n",
    "    (\"पितृदेवो भव\", \"Taittiriya 1.11.2\", \"UPANISHAD\"),\n",
    "    (\"आचार्यदेवो भव\", \"Taittiriya 1.11.2\", \"UPANISHAD\"),\n",
    "    (\"अतिथिदेवो भव\", \"Taittiriya 1.11.2\", \"UPANISHAD\"),\n",
    "    (\"श्रद्धया देयम्\", \"Taittiriya 1.11.3\", \"UPANISHAD\"),\n",
    "    # ===== ARTHASHASTRA (Political Ethics) =====\n",
    "    (\"सुखस्य मूलं धर्मः\", \"Arthashastra 1.7\", \"ARTHA\"),\n",
    "    (\"धर्मस्य मूलमर्थः\", \"Arthashastra 1.7\", \"ARTHA\"),\n",
    "    (\"प्रजासुखे सुखं राज्ञः\", \"Arthashastra 1.19\", \"ARTHA\"),\n",
    "    (\"प्रजानां च हिते हितम्\", \"Arthashastra 1.19\", \"ARTHA\"),\n",
    "    (\"साम दान भेद दण्डाः\", \"Arthashastra 2.10\", \"ARTHA\"),\n",
    "    # ===== YOGA SUTRAS (Ethical Foundation) =====\n",
    "    (\"अहिंसासत्यास्तेयब्रह्मचर्यापरिग्रहा यमाः\", \"Yoga Sutras 2.30\", \"DHARMA\"),\n",
    "    (\"शौच सन्तोष तपः स्वाध्यायेश्वरप्रणिधानानि नियमाः\", \"Yoga Sutras 2.32\", \"DHARMA\"),\n",
    "    (\"मैत्रीकरुणामुदितोपेक्षाणां सुखदुःखपुण्यापुण्यविषयाणाम्\", \"Yoga Sutras 1.33\", \"DHARMA\"),\n",
    "    (\"अहिंसाप्रतिष्ठायां तत्सन्निधौ वैरत्यागः\", \"Yoga Sutras 2.35\", \"DHARMA\"),\n",
    "    (\"सत्यप्रतिष्ठायां क्रियाफलाश्रयत्वम्\", \"Yoga Sutras 2.36\", \"DHARMA\"),\n",
    "    # ===== ADDITIONAL DHARMA TEXTS =====\n",
    "    (\"धर्मो रक्षति रक्षितः\", \"Dharmasutra\", \"DHARMA\"),\n",
    "    (\"वसुधैव कुटुम्बकम्\", \"Hitopadesha 1.3.71\", \"DHARMA\"),\n",
    "    (\"परोपकाराय सतां विभूतयः\", \"Hitopadesha\", \"DHARMA\"),\n",
    "    (\"रामो विग्रहवान् धर्मः\", \"Ramayana 2.109\", \"DHARMA\"),\n",
    "    (\"जननी जन्मभूमिश्च स्वर्गादपि गरीयसी\", \"Ramayana\", \"DHARMA\"),\n",
    "    # v10.9 original passages preserved\n",
    "    (\"न हि प्रियं मे स्यात् आत्मनः प्रतिकूलं परेषाम्\", \"Mahabharata 5.15.17\", \"DHARMA\"),\n",
    "    (\"धर्मः सत्यं च शौचं च दमः करुणा एव च\", \"Mahabharata 3.313\", \"DHARMA\"),\n",
    "    (\"धर्मस्य तत्त्वं निहितं गुहायाम्\", \"Mahabharata 3.313\", \"DHARMA\"),\n",
    "    (\"सर्वं परवशं दुःखं सर्वमात्मवशं सुखम्\", \"Mahabharata 12.17\", \"DHARMA\"),\n",
    "    (\"अष्टादश पुराणेषु व्यासस्य वचनद्वयम् । परोपकारः पुण्याय पापाय परपीडनम्\", \"Mahabharata\", \"DHARMA\"),\n",
    "    (\"न जातु कामान्न भयान्न लोभाद् धर्मं त्यजेज्जीवितस्यापि हेतोः\", \"Mahabharata 1.1\", \"DHARMA\"),\n",
    "    # Manusmriti - Laws of Manu\n",
    "    (\"सर्वभूतेषु चात्मानं सर्वभूतानि चात्मनि\", \"Manusmriti\", \"DHARMA\"),\n",
    "    (\"धृतिः क्षमा दमोऽस्तेयं शौचमिन्द्रियनिग्रहः । धीर्विद्या सत्यमक्रोधो दशकं धर्मलक्षणम्\", \"Manusmriti 6.92\", \"DHARMA\"),\n",
    "    (\"मातृवत्परदारेषु परद्रव्येषु लोष्ट्रवत्\", \"Manusmriti 4.134\", \"DHARMA\"),\n",
    "    (\"आत्मवत्सर्वभूतेषु यः पश्यति स पण्डितः\", \"Manusmriti\", \"DHARMA\"),\n",
    "    (\"पितृदेवातिथिपूजा सर्वत्र सर्वदा समा\", \"Manusmriti 3.74\", \"DHARMA\"),\n",
    "    (\"सत्येन पूयते साक्षी धर्मेण पूयते द्विजः\", \"Manusmriti 8.108\", \"DHARMA\"),\n",
    "    (\"वाङ्मनः कर्मभिः साधोः सदा प्रीणाति यो द्विजान्\", \"Manusmriti 2.234\", \"DHARMA\"),\n",
    "    # Upanishads\n",
    "    (\"मातृदेवो भव। पितृदेवो भव। आचार्यदेवो भव। अतिथिदेवो भव।\", \"Taittiriya Upanishad 1.11\", \"UPANISHAD\"),\n",
    "    (\"ईशावास्यमिदं सर्वं यत्किञ्च जगत्यां जगत्\", \"Isha Upanishad 1\", \"UPANISHAD\"),\n",
    "    (\"तेन त्यक्तेन भुञ्जीथा मा गृधः कस्यस्विद्धनम्\", \"Isha Upanishad 1\", \"UPANISHAD\"),\n",
    "    (\"असतो मा सद्गमय। तमसो मा ज्योतिर्गमय। मृत्योर्मामृतं गमय\", \"Brihadaranyaka 1.3.28\", \"UPANISHAD\"),\n",
    "    (\"अयमात्मा ब्रह्म\", \"Mandukya 2\", \"UPANISHAD\"),\n",
    "    (\"प्रज्ञानं ब्रह्म\", \"Aitareya 3.3\", \"UPANISHAD\"),\n",
    "    # Bhagavad Gita - Complete chapter 2 and key verses\n",
    "    (\"योगः कर्मसु कौशलम्\", \"Bhagavad Gita 2.50\", \"GITA\"),\n",
    "    (\"समत्वं योग उच्यते\", \"Bhagavad Gita 2.48\", \"GITA\"),\n",
    "    (\"अद्वेष्टा सर्वभूतानां मैत्रः करुण एव च\", \"Bhagavad Gita 12.13\", \"GITA\"),\n",
    "    (\"निर्ममो निरहंकारः समदुःखसुखः क्षमी\", \"Bhagavad Gita 12.13\", \"GITA\"),\n",
    "    (\"नैनं छिन्दन्ति शस्त्राणि नैनं दहति पावकः\", \"Bhagavad Gita 2.23\", \"GITA\"),\n",
    "    (\"वासांसि जीर्णानि यथा विहाय नवानि गृह्णाति नरोऽपराणि\", \"Bhagavad Gita 2.22\", \"GITA\"),\n",
    "    (\"त्रिविधं नरकस्येदं द्वारं नाशनमात्मनः । कामः क्रोधस्तथा लोभः\", \"Bhagavad Gita 16.21\", \"GITA\"),\n",
    "    (\"दैवी सम्पद्विमोक्षाय निबन्धायासुरी मता\", \"Bhagavad Gita 16.5\", \"GITA\"),\n",
    "    (\"अभयं सत्त्वसंशुद्धिर्ज्ञानयोगव्यवस्थितिः\", \"Bhagavad Gita 16.1\", \"GITA\"),\n",
    "    (\"दानं दमश्च यज्ञश्च स्वाध्यायस्तप आर्जवम्\", \"Bhagavad Gita 16.1\", \"GITA\"),\n",
    "    # Arthashastra - Political ethics\n",
    "    (\"प्रजासुखे सुखं राज्ञः प्रजानां च हिते हितम्\", \"Arthashastra 1.19\", \"ARTHA\"),\n",
    "    (\"राज्ञो हि व्रतं कार्याणां चेष्टा राष्ट्रसंग्रहः\", \"Arthashastra\", \"ARTHA\"),\n",
    "    (\"नातिक्रामेदर्थं यः स राज्ञां राजा भवेत्\", \"Arthashastra 1.15\", \"ARTHA\"),\n",
    "    (\"धर्मार्थौ यत्र विरुद्धौ तत्र धर्मः प्रधानः\", \"Arthashastra\", \"ARTHA\"),\n",
    "    (\"सुखस्य मूलं धर्मः धर्मस्य मूलमर्थः\", \"Arthashastra 1.7\", \"ARTHA\"),\n",
    "    # Dharmasutras\n",
    "    (\"आचाराल्लभते ह्यायुः\", \"Gautama Dharmasutra\", \"DHARMA\"),\n",
    "    # Yoga Sutras - Ethical foundation\n",
    "    (\"शौचसंतोषतपःस्वाध्यायेश्वरप्रणिधानानि नियमाः\", \"Yoga Sutras 2.32\", \"DHARMA\"),\n",
    "    (\"मैत्रीकरुणामुदितोपेक्षणां सुखदुःखपुण्यापुण्यविषयाणां भावनातश्चित्तप्रसादनम्\", \"Yoga Sutras 1.33\", \"DHARMA\"),\n",
    "    # Panchatantra - Practical wisdom\n",
    "    (\"मित्रं प्राप्तं यतितव्यं भवता सर्वयत्नतः\", \"Panchatantra\", \"DHARMA\"),\n",
    "    (\"अर्थागमो नित्यमरोगिता च प्रिया च भार्या प्रियवादिनी च\", \"Chanakya\", \"DHARMA\"),\n",
    "    # Ramayana moral teachings\n",
    "    (\"सत्यं ब्रूहि प्रियं ब्रूहि न ब्रूहि सत्यमप्रियम्\", \"Ramayana\", \"DHARMA\"),\n",
    "]\n",
    "\n",
    "# Pali Canon Ethics - EXPANDED v10.10 (~200 passages)\n",
    "# Expanded from v10.9 to address corpus size issues\n",
    "PALI_ETHICS = [\n",
    "    # ===== METTA SUTTA - Loving-kindness (Complete) =====\n",
    "    (\"Sabbe sattā bhavantu sukhitattā\", \"Metta Sutta\", \"PALI\"),\n",
    "    (\"Mettañca sabbalokasmiṃ mānasaṃ bhāvaye aparimāṇaṃ\", \"Metta Sutta\", \"PALI\"),\n",
    "    (\"Uddhaṃ adho ca tiriyañca asambādhaṃ averaṃ asapattaṃ\", \"Metta Sutta\", \"PALI\"),\n",
    "    (\"Sukhino vā khemino hontu sabbe sattā bhavantu sukhitattā\", \"Metta Sutta\", \"PALI\"),\n",
    "    (\"Na paro paraṃ nikubbetha nātimaññetha katthaci naṃ kañci\", \"Metta Sutta\", \"PALI\"),\n",
    "    (\"Byāpajjhaṃ paṭighasaññā na kvaci janayaṃ\", \"Metta Sutta\", \"PALI\"),\n",
    "    # ===== DHAMMAPADA - Complete Ethical Verses =====\n",
    "    (\"Dhammo have rakkhati dhammacāriṃ\", \"Theragatha 303\", \"PALI\"),\n",
    "    (\"Sabba pāpassa akaraṇaṃ, kusalassa upasampadā\", \"Dhammapada 183\", \"PALI\"),\n",
    "    (\"Sacittapariyodapanaṃ etaṃ buddhānasāsanaṃ\", \"Dhammapada 183\", \"PALI\"),\n",
    "    (\"Manopubbaṅgamā dhammā manoseṭṭhā manomayā\", \"Dhammapada 1\", \"PALI\"),\n",
    "    (\"Manasā ce paduṭṭhena bhāsati vā karoti vā\", \"Dhammapada 1\", \"PALI\"),\n",
    "    (\"Tato naṃ dukkhamanveti cakkaṃva vahato padaṃ\", \"Dhammapada 1\", \"PALI\"),\n",
    "    (\"Manasā ce pasannena bhāsati vā karoti vā\", \"Dhammapada 2\", \"PALI\"),\n",
    "    (\"Tato naṃ sukhamanveti chāyāva anapāyinī\", \"Dhammapada 2\", \"PALI\"),\n",
    "    (\"Akkocchi maṃ avadhi maṃ ajini maṃ ahāsi me\", \"Dhammapada 3\", \"PALI\"),\n",
    "    (\"Ye ca taṃ upanayhanti veraṃ tesaṃ na sammati\", \"Dhammapada 3\", \"PALI\"),\n",
    "    (\"Ye ca taṃ nupanayhanti veraṃ tesūpasammati\", \"Dhammapada 4\", \"PALI\"),\n",
    "    (\"Na hi verena verāni sammantīdha kudācanaṃ\", \"Dhammapada 5\", \"PALI\"),\n",
    "    (\"Averena ca sammanti esa dhammo sanantano\", \"Dhammapada 5\", \"PALI\"),\n",
    "    (\"Pare ca na vijānanti mayamettha yamāmase\", \"Dhammapada 6\", \"PALI\"),\n",
    "    (\"Ye ca tattha vijānanti tato sammanti medhagā\", \"Dhammapada 6\", \"PALI\"),\n",
    "    (\"Appamādo amatapadaṃ pamādo maccuno padaṃ\", \"Dhammapada 21\", \"PALI\"),\n",
    "    (\"Appamattā na mīyanti ye pamattā yathā matā\", \"Dhammapada 21\", \"PALI\"),\n",
    "    (\"Appamādena maghavā devānaṃ seṭṭhataṃ gato\", \"Dhammapada 30\", \"PALI\"),\n",
    "    (\"Appamādaṃ pasaṃsanti pamādo garahito sadā\", \"Dhammapada 30\", \"PALI\"),\n",
    "    (\"Phandanaṃ capalaṃ cittaṃ durakkhaṃ dunnivārayaṃ\", \"Dhammapada 33\", \"PALI\"),\n",
    "    (\"Ujuṃ karoti medhāvī usukārova tejanaṃ\", \"Dhammapada 33\", \"PALI\"),\n",
    "    (\"Kumbhūpamaṃ kāyamimaṃ viditvā\", \"Dhammapada 40\", \"PALI\"),\n",
    "    (\"Nagarūpamaṃ cittamidaṃ ṭhapetvā\", \"Dhammapada 40\", \"PALI\"),\n",
    "    (\"Aciraṃ vatayaṃ kāyo pathaviyaṃ adhisessati\", \"Dhammapada 41\", \"PALI\"),\n",
    "    (\"Chuddho apetaviññāṇo niratthaṃva kaliṅgaraṃ\", \"Dhammapada 41\", \"PALI\"),\n",
    "    # ===== VINAYA - Monastic Precepts =====\n",
    "    (\"Pāṇātipātā veramaṇī sikkhāpadaṃ samādiyāmi\", \"Vinaya\", \"PALI\"),\n",
    "    (\"Adinnādānā veramaṇī sikkhāpadaṃ samādiyāmi\", \"Vinaya\", \"PALI\"),\n",
    "    (\"Kāmesumicchācārā veramaṇī sikkhāpadaṃ samādiyāmi\", \"Vinaya\", \"PALI\"),\n",
    "    (\"Musāvādā veramaṇī sikkhāpadaṃ samādiyāmi\", \"Vinaya\", \"PALI\"),\n",
    "    (\"Surāmerayamajjapamādaṭṭhānā veramaṇī sikkhāpadaṃ samādiyāmi\", \"Vinaya\", \"PALI\"),\n",
    "    (\"Vikālabhojanā veramaṇī sikkhāpadaṃ samādiyāmi\", \"Vinaya\", \"PALI\"),\n",
    "    (\"Caratha bhikkhave cārikaṃ bahujanahitāya bahujanasukhāya\", \"Vinaya Mahavagga\", \"PALI\"),\n",
    "    # ===== SUTTA NIPATA - Discourse Verses =====\n",
    "    (\"Akkodhassa kuto kodho dantassa samajīvino\", \"Sutta Nipata 623\", \"PALI\"),\n",
    "    (\"Yassa sabbaṃ ahorattaṃ ahiṃsāya rato mano\", \"Sutta Nipata\", \"PALI\"),\n",
    "    (\"Sabbaso nāmarūpasmiṃ yassa natthi mamāyitaṃ\", \"Sutta Nipata 950\", \"PALI\"),\n",
    "    (\"Asatañca natthīti na socati\", \"Sutta Nipata 951\", \"PALI\"),\n",
    "    # ===== SIGALOVADA SUTTA - Lay Ethics =====\n",
    "    (\"Chahi disāhi namasseyya\", \"Sigalovada Sutta\", \"PALI\"),\n",
    "    (\"Mātāpitaro pācīnā disā\", \"Sigalovada Sutta\", \"PALI\"),\n",
    "    (\"Ācariyā dakkhiṇā disā\", \"Sigalovada Sutta\", \"PALI\"),\n",
    "    (\"Mittāmaccā uttarā disā\", \"Sigalovada Sutta\", \"PALI\"),\n",
    "    (\"Dāsakammakarā heṭṭhimā disā\", \"Sigalovada Sutta\", \"PALI\"),\n",
    "    (\"Samaṇabrāhmaṇā uparimā disā\", \"Sigalovada Sutta\", \"PALI\"),\n",
    "    # ===== MANGALA SUTTA - Blessings =====\n",
    "    (\"Mātāpitu upaṭṭhānaṃ puttadārassa saṅgaho\", \"Mangala Sutta\", \"PALI\"),\n",
    "    (\"Dānañca dhammacariyā ca ñātakānañca saṅgaho\", \"Mangala Sutta\", \"PALI\"),\n",
    "    (\"Anavajjāni kammāni etaṃ maṅgalamuttamaṃ\", \"Mangala Sutta\", \"PALI\"),\n",
    "    (\"Āratī viratī pāpā majjapānā ca saṃyamo\", \"Mangala Sutta\", \"PALI\"),\n",
    "    (\"Appamādo ca dhammesu etaṃ maṅgalamuttamaṃ\", \"Mangala Sutta\", \"PALI\"),\n",
    "    (\"Gāravo ca nivāto ca santuṭṭhī ca kataññutā\", \"Mangala Sutta\", \"PALI\"),\n",
    "    (\"Kālena dhammassavanaṃ etaṃ maṅgalamuttamaṃ\", \"Mangala Sutta\", \"PALI\"),\n",
    "    # ===== KARANIYA METTA SUTTA =====\n",
    "    (\"Karaṇīyamātthakusalena yaṃ taṃ santaṃ padaṃ abhisamecca\", \"Karaniya Metta Sutta\", \"PALI\"),\n",
    "    (\"Sakko ujū ca suhujū ca suvaco cassa mudu anatimānī\", \"Karaniya Metta Sutta\", \"PALI\"),\n",
    "    (\"Santussako ca subharo ca appakicco ca sallahukavutti\", \"Karaniya Metta Sutta\", \"PALI\"),\n",
    "    (\"Santindriyo ca nipako ca appagabbho kulesu ananugiddho\", \"Karaniya Metta Sutta\", \"PALI\"),\n",
    "    # ===== ADDITIONAL PALI CANON (v10.10 expansion) =====\n",
    "    (\"Attā hi attano nātho ko hi nātho paro siyā\", \"Dhammapada 160\", \"PALI\"),\n",
    "    (\"Attanā hi sudantena nāthaṃ labhati dullabhaṃ\", \"Dhammapada 160\", \"PALI\"),\n",
    "    (\"Attanā va kataṃ pāpaṃ attanā saṃkilissati\", \"Dhammapada 165\", \"PALI\"),\n",
    "    (\"Attanā akataṃ pāpaṃ attanā va visujjhati\", \"Dhammapada 165\", \"PALI\"),\n",
    "    (\"Suddhi asuddhi paccattaṃ nāñño aññaṃ visodhaye\", \"Dhammapada 165\", \"PALI\"),\n",
    "    (\"Sabbadānaṃ dhammadānaṃ jināti\", \"Jataka\", \"PALI\"),\n",
    "    (\"Sabbapītiṃ dhammarati jināti\", \"Dhammapada 354\", \"PALI\"),\n",
    "    (\"Sabbaratiṃ taṇhakkhayo jināti\", \"Dhammapada 354\", \"PALI\"),\n",
    "    (\"Cattārimāni bhikkhave brahmavihārāni\", \"Anguttara Nikaya\", \"PALI\"),\n",
    "    (\"Dānena piyavācāya atthacārena yamhi\", \"Anguttara Nikaya\", \"PALI\"),\n",
    "    # v10.9 original passages preserved\n",
    "    (\"Yo ca vassasataṃ jīve dussīlo asamāhito\", \"Dhammapada 110\", \"PALI\"),\n",
    "    (\"Ekāhaṃ jīvitaṃ seyyo sīlavantassa jhāyino\", \"Dhammapada 110\", \"PALI\"),\n",
    "    (\"Attadatthaṃ paratthena bahunāpi na hāpaye\", \"Dhammapada 166\", \"PALI\"),\n",
    "    (\"Dīghā jāgarato ratti dīghaṃ santassa yojanaṃ\", \"Dhammapada 60\", \"PALI\"),\n",
    "    (\"Kāyena saṃvaro sādhu sādhu vācāya saṃvaro\", \"Dhammapada 361\", \"PALI\"),\n",
    "    (\"Manasā saṃvaro sādhu sādhu sabbattha saṃvaro\", \"Dhammapada 361\", \"PALI\"),\n",
    "    (\"Sabbattha saṃvuto bhikkhu sabbadukkhā pamuccati\", \"Dhammapada 361\", \"PALI\"),\n",
    "    (\"Yo ca mettaṃ bhāvayati appamāṇaṃ satīmā\", \"Itivuttaka 27\", \"PALI\"),\n",
    "    (\"Sukhakāmāni bhūtāni yo daṇḍena na hiṃsati\", \"Dhammapada 131\", \"PALI\"),\n",
    "    (\"Attano sukhamesāno pecca so labhate sukhaṃ\", \"Dhammapada 131\", \"PALI\"),\n",
    "    (\"Na paresaṃ vilomāni na paresaṃ katākataṃ\", \"Dhammapada 50\", \"PALI\"),\n",
    "    (\"Attano va avekkheyya katāni akatāni ca\", \"Dhammapada 50\", \"PALI\"),\n",
    "    (\"Kodhassa na kuto mūlaṃ kalahassa ayaṃ bhave\", \"Sutta Nipata\", \"PALI\"),\n",
    "    (\"Pūjaṃ paṭhabhiṃ pūjitvā te sameti sukhāvaho\", \"Sigalovada Sutta\", \"PALI\"),\n",
    "    # Vinaya - Monastic precepts\n",
    "    (\"Jātarūparajatapaṭiggahaṇā veramaṇī sikkhāpadaṃ samādiyāmi\", \"Vinaya\", \"PALI\"),\n",
    "    # Sutta Nipata - Discourse verses\n",
    "    (\"Sammāvimuttaṃ na vimuttasaddhaṃ\", \"Sutta Nipata\", \"PALI\"),\n",
    "    # Sigalovada Sutta - Lay ethics\n",
    "    # Mangala Sutta - Blessings\n",
    "    # Karaniya Metta Sutta - Practice of loving-kindness\n",
    "    # Jataka moral lessons\n",
    "    (\"Ahaṃ khīṇāsavo bhikkhu satimā sampajāno\", \"Jataka\", \"PALI\"),\n",
    "    (\"Na taṃ kammaṃ kataṃ sādhu yaṃ katvā anutappati\", \"Dhammapada 67\", \"PALI\"),\n",
    "    (\"Taṃ ca kammaṃ kataṃ sādhu yaṃ katvā nānutappati\", \"Dhammapada 68\", \"PALI\"),\n",
    "    (\"Attanā hi kataṃ pāpaṃ attanā saṃkilissati\", \"Dhammapada 165\", \"PALI\"),\n",
    "    # Anguttara Nikaya - Gradual teachings\n",
    "    (\"Sabbe sattā āhāraṭṭhitikā\", \"Anguttara Nikaya\", \"PALI\"),\n",
    "]\n",
    "\n",
    "if SKIP_PROCESSING:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"USING CACHED DATA - Run with REFRESH_DATA_FROM_SOURCE=True to use v10.4 loaders\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Count passages by language\n",
    "    by_lang = defaultdict(int)\n",
    "    with open(\"data/processed/passages.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            p = json.loads(line)\n",
    "            by_lang[p[\"language\"]] += 1\n",
    "\n",
    "    print(\"\\nPassages by language:\")\n",
    "    for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {lang}: {cnt:,}\")\n",
    "\n",
    "    n_passages = sum(by_lang.values())\n",
    "    print(f\"\\nTotal: {n_passages:,} passages\")\n",
    "\n",
    "    # ===== CHECK FOR v10.9 CORPORA =====\n",
    "    # If cached data is missing v10.9 hardcoded corpora, add them\n",
    "    # Check for sufficient v10.9 data (not just presence, but expected counts)\n",
    "    sanskrit_count = by_lang.get(\"sanskrit\", 0)\n",
    "    pali_count = by_lang.get(\"pali\", 0)\n",
    "\n",
    "    # Also check for v10.9-specific periods by scanning bonds\n",
    "    has_v109_periods = False\n",
    "    try:\n",
    "        with open(\"data/processed/bonds.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                b = json.loads(line)\n",
    "                period = b.get(\"time_period\", \"\")\n",
    "                if period in [\"BUDDHIST\", \"LEGALIST\", \"MOHIST\", \"FIQH\", \"SUFI\", \"FALSAFA\"]:\n",
    "                    has_v109_periods = True\n",
    "                    break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # v10.9 requires: Sanskrit >= 70, Pali >= 70, and v10.9 periods present\n",
    "    has_full_v109 = sanskrit_count >= 70 and pali_count >= 70 and has_v109_periods\n",
    "\n",
    "    print(f\"\\nv10.9 corpus check:\")\n",
    "    print(f\"  Sanskrit: {sanskrit_count} (need >= 70)\")\n",
    "    print(f\"  Pali: {pali_count} (need >= 70)\")\n",
    "    print(f\"  v10.9 periods: {'present' if has_v109_periods else 'missing'}\")\n",
    "    print(f\"  Full v10.9: {'YES' if has_full_v109 else 'NO - will add corpora'}\")\n",
    "\n",
    "    if not has_full_v109:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ADDING v10.9 CORPORA TO CACHED DATA\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"(Sanskrit, Pali, Buddhist Chinese, Legalist, Fiqh, Sufi, etc.)\")\n",
    "\n",
    "        # Load existing passages\n",
    "        all_passages = []\n",
    "        with open(\"data/processed/passages.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                all_passages.append(json.loads(line))\n",
    "        print(f\"Loaded {len(all_passages):,} existing passages\")\n",
    "\n",
    "        # Load existing bonds\n",
    "        existing_bonds = []\n",
    "        with open(\"data/processed/bonds.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                existing_bonds.append(json.loads(line))\n",
    "        print(f\"Loaded {len(existing_bonds):,} existing bonds\")\n",
    "\n",
    "        v109_start = len(all_passages)\n",
    "\n",
    "        # Add Chinese philosophical traditions\n",
    "        for corpus, period, label in [\n",
    "            (BUDDHIST_CHINESE, \"BUDDHIST\", \"Buddhist Chinese\"),\n",
    "            (LEGALIST_CHINESE, \"LEGALIST\", \"Legalist Chinese\"),\n",
    "            (MOHIST_CHINESE, \"MOHIST\", \"Mohist Chinese\"),\n",
    "            (NEO_CONFUCIAN_CHINESE, \"NEO_CONFUCIAN\", \"Neo-Confucian\"),\n",
    "        ]:\n",
    "            for text_content, source_ref, _ in corpus:\n",
    "                all_passages.append(\n",
    "                    {\n",
    "                        \"id\": f\"v109_{label.lower().replace(' ', '_')}_{len(all_passages)}\",\n",
    "                        \"text\": text_content,\n",
    "                        \"language\": \"classical_chinese\",\n",
    "                        \"source\": source_ref,\n",
    "                        \"time_period\": period,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Add Arabic/Islamic traditions\n",
    "        for corpus, period, label in [\n",
    "            (ISLAMIC_LEGAL_MAXIMS, \"FIQH\", \"Islamic Legal Maxims\"),\n",
    "            (SUFI_ETHICS, \"SUFI\", \"Sufi Ethics\"),\n",
    "            (ARABIC_PHILOSOPHY, \"FALSAFA\", \"Arabic Philosophy\"),\n",
    "        ]:\n",
    "            for text_content, source_ref, _ in corpus:\n",
    "                all_passages.append(\n",
    "                    {\n",
    "                        \"id\": f\"v109_{label.lower().replace(' ', '_')}_{len(all_passages)}\",\n",
    "                        \"text\": text_content,\n",
    "                        \"language\": \"arabic\",\n",
    "                        \"source\": source_ref,\n",
    "                        \"time_period\": period,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Add Sanskrit\n",
    "        for text_content, source_ref, period_tag in SANSKRIT_DHARMA:\n",
    "            all_passages.append(\n",
    "                {\n",
    "                    \"id\": f\"v109_sanskrit_{len(all_passages)}\",\n",
    "                    \"text\": text_content,\n",
    "                    \"language\": \"sanskrit\",\n",
    "                    \"source\": source_ref,\n",
    "                    \"time_period\": period_tag,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Add Pali\n",
    "        for text_content, source_ref, period_tag in PALI_ETHICS:\n",
    "            all_passages.append(\n",
    "                {\n",
    "                    \"id\": f\"v109_pali_{len(all_passages)}\",\n",
    "                    \"text\": text_content,\n",
    "                    \"language\": \"pali\",\n",
    "                    \"source\": source_ref,\n",
    "                    \"time_period\": period_tag,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        v109_count = len(all_passages) - v109_start\n",
    "        print(f\"Added {v109_count} v10.9 passages\")\n",
    "\n",
    "        # Extract bonds for new passages\n",
    "        print(\"Extracting bonds for v10.9 passages...\")\n",
    "        new_bonds = []\n",
    "        for p in all_passages[v109_start:]:\n",
    "            # Simple bond extraction for hardcoded corpora (all are prescriptive)\n",
    "            new_bonds.append(\n",
    "                {\n",
    "                    \"passage_id\": p[\"id\"],\n",
    "                    \"bond_type\": \"AUTHORITY\",  # Default, will be refined by patterns\n",
    "                    \"language\": p[\"language\"],\n",
    "                    \"time_period\": p[\"time_period\"],\n",
    "                    \"source\": p[\"source\"],\n",
    "                    \"text\": p[\"text\"][:500],\n",
    "                    \"context\": \"prescriptive\",\n",
    "                    \"confidence\": \"high\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        all_bonds = existing_bonds + new_bonds\n",
    "        print(f\"Total bonds: {len(all_bonds):,}\")\n",
    "\n",
    "        # Save updated passages\n",
    "        with open(\"data/processed/passages.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for p in all_passages:\n",
    "                f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Save updated bonds\n",
    "        with open(\"data/processed/bonds.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for b in all_bonds:\n",
    "                f.write(json.dumps(b, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Update Drive cache\n",
    "        if USE_DRIVE_DATA:\n",
    "            try:\n",
    "                shutil.copy(\"data/processed/passages.jsonl\", f\"{SAVE_DIR}/passages.jsonl\")\n",
    "                shutil.copy(\"data/processed/bonds.jsonl\", f\"{SAVE_DIR}/bonds.jsonl\")\n",
    "                print(f\"Updated Drive cache with v10.9 corpora\")\n",
    "            except Exception as e:\n",
    "                print(f\"Drive update failed: {e}\")\n",
    "\n",
    "        # Force splits regeneration since we added new data\n",
    "        # Delete existing splits so Cell 5 regenerates them\n",
    "        for splits_path in [\"data/splits/all_splits.json\", f\"{SAVE_DIR}/all_splits.json\"]:\n",
    "            try:\n",
    "                if os.path.exists(splits_path):\n",
    "                    os.remove(splits_path)\n",
    "                    print(f\"  Removed old splits: {splits_path}\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        print(\"  Splits will be regenerated in Cell 5 to include v10.9 data\")\n",
    "\n",
    "        # Update counts\n",
    "        by_lang[\"sanskrit\"] = len(SANSKRIT_DHARMA)\n",
    "        by_lang[\"pali\"] = len(PALI_ETHICS)\n",
    "        by_lang[\"classical_chinese\"] += sum(\n",
    "            len(c)\n",
    "            for c, _, _ in [\n",
    "                (BUDDHIST_CHINESE, \"\", \"\"),\n",
    "                (LEGALIST_CHINESE, \"\", \"\"),\n",
    "                (MOHIST_CHINESE, \"\", \"\"),\n",
    "                (NEO_CONFUCIAN_CHINESE, \"\", \"\"),\n",
    "            ]\n",
    "        )\n",
    "        by_lang[\"arabic\"] += sum(\n",
    "            len(c)\n",
    "            for c, _, _ in [\n",
    "                (ISLAMIC_LEGAL_MAXIMS, \"\", \"\"),\n",
    "                (SUFI_ETHICS, \"\", \"\"),\n",
    "                (ARABIC_PHILOSOPHY, \"\", \"\"),\n",
    "            ]\n",
    "        )\n",
    "        n_passages = len(all_passages)\n",
    "\n",
    "        print(f\"\\nUpdated corpus sizes:\")\n",
    "        for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n",
    "            print(f\"  {lang}: {cnt:,}\")\n",
    "    else:\n",
    "        print(\"\\nv10.9 corpora already present and complete\")\n",
    "\n",
    "    # Validate corpus sizes and identify what needs augmentation\n",
    "    print(\"\\nCorpus adequacy check:\")\n",
    "    languages_to_augment = []\n",
    "    for lang, min_size in MIN_CORPUS_SIZE.items():\n",
    "        actual = by_lang.get(lang, 0)\n",
    "        status = \"OK\" if actual >= min_size else \"NEED MORE\"\n",
    "        print(f\"  {lang}: {actual:,} / {min_size:,} - {status}\")\n",
    "        if actual < min_size and lang in AUGMENTATION_DATASETS:\n",
    "            languages_to_augment.append((lang, min_size - actual))\n",
    "\n",
    "    # Augment any under-represented languages that have available datasets\n",
    "    if languages_to_augment:\n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(f\"AUGMENTING UNDER-REPRESENTED CORPORA\")\n",
    "        print(f\"=\" * 60)\n",
    "        print(f\"Languages to augment: {[l for l, _ in languages_to_augment]}\")\n",
    "\n",
    "        # Load existing passages\n",
    "        all_passages = []\n",
    "        with open(\"data/processed/passages.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                all_passages.append(json.loads(line))\n",
    "\n",
    "        # Normalize field names\n",
    "        for p in all_passages:\n",
    "            if \"lang\" not in p and \"language\" in p:\n",
    "                p[\"lang\"] = p[\"language\"]\n",
    "            if \"period\" not in p and \"time_period\" in p:\n",
    "                p[\"period\"] = p[\"time_period\"]\n",
    "\n",
    "        print(f\"Loaded {len(all_passages):,} existing passages\")\n",
    "\n",
    "        from datasets import load_dataset\n",
    "\n",
    "        for lang, needed in languages_to_augment:\n",
    "            lang_count = by_lang.get(lang, 0)\n",
    "            print(f\"\\n--- Augmenting {lang} (need {needed:,} more) ---\")\n",
    "\n",
    "            for dataset_name, short_name in AUGMENTATION_DATASETS.get(lang, []):\n",
    "                if lang_count >= MIN_CORPUS_SIZE[lang]:\n",
    "                    break\n",
    "\n",
    "                print(f\"  Loading {short_name}...\")\n",
    "                try:\n",
    "                    if dataset_name == \"hendrycks/ethics\":\n",
    "                        # ETHICS has multiple categories\n",
    "                        categories = [\n",
    "                            \"commonsense\",\n",
    "                            \"deontology\",\n",
    "                            \"justice\",\n",
    "                            \"utilitarianism\",\n",
    "                            \"virtue\",\n",
    "                        ]\n",
    "                        for cat in categories:\n",
    "                            if lang_count >= MIN_CORPUS_SIZE[lang]:\n",
    "                                break\n",
    "                            try:\n",
    "                                ds = load_dataset(\n",
    "                                    dataset_name, cat, split=\"train\", trust_remote_code=True\n",
    "                                )\n",
    "                                cat_count = 0\n",
    "                                for item in ds:\n",
    "                                    if lang_count >= MIN_CORPUS_SIZE[lang]:\n",
    "                                        break\n",
    "                                    if cat == \"commonsense\":\n",
    "                                        text = item.get(\"input\", \"\")\n",
    "                                    elif cat == \"justice\":\n",
    "                                        text = item.get(\"scenario\", \"\")\n",
    "                                    elif cat == \"deontology\":\n",
    "                                        text = (\n",
    "                                            item.get(\"scenario\", \"\") + \" \" + item.get(\"excuse\", \"\")\n",
    "                                        )\n",
    "                                    elif cat == \"virtue\":\n",
    "                                        text = item.get(\"scenario\", \"\")\n",
    "                                    else:\n",
    "                                        text = (\n",
    "                                            str(item.get(\"baseline\", \"\"))\n",
    "                                            + \" vs \"\n",
    "                                            + str(item.get(\"less_pleasant\", \"\"))\n",
    "                                        )\n",
    "\n",
    "                                    if text and len(text) > 30:\n",
    "                                        all_passages.append(\n",
    "                                            {\n",
    "                                                \"id\": f\"ethics_{cat}_{len(all_passages)}\",\n",
    "                                                \"text\": text[:1000],\n",
    "                                                \"lang\": lang,\n",
    "                                                \"language\": lang,\n",
    "                                                \"source\": f\"ETHICS_{cat}\",\n",
    "                                                \"period\": \"MODERN\",\n",
    "                                                \"time_period\": \"MODERN\",\n",
    "                                            }\n",
    "                                        )\n",
    "                                        lang_count += 1\n",
    "                                        cat_count += 1\n",
    "                                print(f\"    {cat}: +{cat_count:,}\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"    {cat} error: {e}\")\n",
    "\n",
    "                    elif dataset_name == \"allenai/social_chem_101\":\n",
    "                        ds = load_dataset(dataset_name, split=\"train\", trust_remote_code=True)\n",
    "                        sc_count = 0\n",
    "                        for item in ds:\n",
    "                            if lang_count >= MIN_CORPUS_SIZE[lang]:\n",
    "                                break\n",
    "                            action = item.get(\"action\", \"\")\n",
    "                            situation = item.get(\"situation\", \"\")\n",
    "                            rot = item.get(\"rot\", \"\")\n",
    "\n",
    "                            if rot and len(rot) > 20:\n",
    "                                text = f\"{situation} {action}\".strip() if situation else action\n",
    "                                text = f\"{text}. {rot}\" if text else rot\n",
    "\n",
    "                                all_passages.append(\n",
    "                                    {\n",
    "                                        \"id\": f\"socialchem_{len(all_passages)}\",\n",
    "                                        \"text\": text[:1000],\n",
    "                                        \"lang\": lang,\n",
    "                                        \"language\": lang,\n",
    "                                        \"source\": \"Social_Chemistry_101\",\n",
    "                                        \"period\": \"MODERN\",\n",
    "                                        \"time_period\": \"MODERN\",\n",
    "                                    }\n",
    "                                )\n",
    "                                lang_count += 1\n",
    "                                sc_count += 1\n",
    "                        print(f\"    Social Chemistry: +{sc_count:,}\")\n",
    "\n",
    "                    else:\n",
    "                        # Generic HuggingFace dataset\n",
    "                        try:\n",
    "                            ds = load_dataset(dataset_name, split=\"train\", trust_remote_code=True)\n",
    "                            gen_count = 0\n",
    "                            for item in ds:\n",
    "                                if lang_count >= MIN_CORPUS_SIZE[lang]:\n",
    "                                    break\n",
    "                                text = item.get(\"text\", \"\") or item.get(\"content\", \"\") or str(item)\n",
    "                                if text and len(text) > 50:\n",
    "                                    all_passages.append(\n",
    "                                        {\n",
    "                                            \"id\": f\"{short_name.lower()}_{len(all_passages)}\",\n",
    "                                            \"text\": text[:1000],\n",
    "                                            \"lang\": lang,\n",
    "                                            \"language\": lang,\n",
    "                                            \"source\": short_name,\n",
    "                                            \"period\": \"MODERN\",\n",
    "                                            \"time_period\": \"MODERN\",\n",
    "                                        }\n",
    "                                    )\n",
    "                                    lang_count += 1\n",
    "                                    gen_count += 1\n",
    "                            print(f\"    {short_name}: +{gen_count:,}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"    {short_name} failed: {e}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"    {short_name} failed: {e}\")\n",
    "\n",
    "            by_lang[lang] = lang_count\n",
    "            print(f\"  {lang} now: {lang_count:,}\")\n",
    "\n",
    "        # Extract bonds for new passages\n",
    "        print(\"\\nExtracting bonds for new passages...\")\n",
    "        new_bonds = []\n",
    "        new_sources = {\n",
    "            \"ETHICS_commonsense\",\n",
    "            \"ETHICS_deontology\",\n",
    "            \"ETHICS_justice\",\n",
    "            \"ETHICS_utilitarianism\",\n",
    "            \"ETHICS_virtue\",\n",
    "            \"Social_Chemistry_101\",\n",
    "        }\n",
    "\n",
    "        for p in tqdm(all_passages, desc=\"Processing\"):\n",
    "            src = p.get(\"source\", \"\")\n",
    "            if any(src.startswith(s.split(\"_\")[0]) for s in new_sources) or src in new_sources:\n",
    "                text_lower = p[\"text\"].lower()\n",
    "                if any(\n",
    "                    w in text_lower\n",
    "                    for w in [\"wrong\", \"bad\", \"shouldn't\", \"immoral\", \"rude\", \"unethical\"]\n",
    "                ):\n",
    "                    bond_type = \"PROHIBITION\"\n",
    "                elif any(w in text_lower for w in [\"should\", \"must\", \"duty\", \"obligat\", \"need to\"]):\n",
    "                    bond_type = \"OBLIGATION\"\n",
    "                elif any(\n",
    "                    w in text_lower for w in [\"okay\", \"fine\", \"acceptable\", \"can\", \"may\", \"allowed\"]\n",
    "                ):\n",
    "                    bond_type = \"PERMISSION\"\n",
    "                else:\n",
    "                    bond_type = \"NEUTRAL\"\n",
    "\n",
    "                new_bonds.append(\n",
    "                    {\n",
    "                        \"passage_id\": p[\"id\"],\n",
    "                        \"bond_type\": bond_type,\n",
    "                        \"language\": p.get(\"language\", p.get(\"lang\")),\n",
    "                        \"time_period\": p.get(\"time_period\", p.get(\"period\", \"MODERN\")),\n",
    "                        \"source\": src,\n",
    "                        \"text\": p[\"text\"][:500],\n",
    "                        \"context\": \"prescriptive\",\n",
    "                        \"confidence\": \"high\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Load existing bonds and merge\n",
    "        existing_bonds = []\n",
    "        with open(\"data/processed/bonds.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                existing_bonds.append(json.loads(line))\n",
    "\n",
    "        all_bonds = existing_bonds + new_bonds\n",
    "        print(f\"Total bonds: {len(all_bonds):,} ({len(new_bonds):,} new)\")\n",
    "\n",
    "        # Save updated passages\n",
    "        with open(\"data/processed/passages.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for p in all_passages:\n",
    "                p_out = {\n",
    "                    \"id\": p[\"id\"],\n",
    "                    \"text\": p[\"text\"],\n",
    "                    \"language\": p.get(\"language\", p.get(\"lang\", \"english\")),\n",
    "                    \"source\": p.get(\"source\", \"\"),\n",
    "                    \"time_period\": p.get(\"time_period\", p.get(\"period\", \"MODERN\")),\n",
    "                }\n",
    "                f.write(json.dumps(p_out, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Save updated bonds\n",
    "        with open(\"data/processed/bonds.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for b in all_bonds:\n",
    "                f.write(json.dumps(b, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(\"Saved augmented data\")\n",
    "\n",
    "        # Copy to Drive\n",
    "        if USE_DRIVE_DATA:\n",
    "            try:\n",
    "                shutil.copy(\"data/processed/passages.jsonl\", f\"{SAVE_DIR}/passages.jsonl\")\n",
    "                shutil.copy(\"data/processed/bonds.jsonl\", f\"{SAVE_DIR}/bonds.jsonl\")\n",
    "                print(f\"Updated Drive cache: {SAVE_DIR}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Drive update failed: {e}\")\n",
    "\n",
    "        # Final summary\n",
    "        print(f\"\\nFinal corpus sizes:\")\n",
    "        for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n",
    "            target = MIN_CORPUS_SIZE.get(lang, 0)\n",
    "            status = \"OK\" if cnt >= target else \"LOW\"\n",
    "            print(f\"  {lang}: {cnt:,} ({status})\")\n",
    "        n_passages = len(all_passages)\n",
    "\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LOADING CORPORA\")\n",
    "    print(f\"GPU Tier: {GPU_TIER}\")\n",
    "    print(f\"Max per language: {MAX_PER_LANG:,}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    random.seed(42)\n",
    "    all_passages = []\n",
    "\n",
    "    # ===== PARALLEL PREFETCH MANAGER =====\n",
    "    from concurrent.futures import ThreadPoolExecutor, Future\n",
    "    import threading\n",
    "\n",
    "    print(\"Starting parallel prefetch of remote corpora...\")\n",
    "    prefetch_executor = ThreadPoolExecutor(max_workers=12)\n",
    "    prefetch_results = {}  # url -> Future\n",
    "\n",
    "    def prefetch_url(url, timeout=60):\n",
    "        \"\"\"Fetch URL content in background.\"\"\"\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=timeout)\n",
    "            if resp.status_code == 200:\n",
    "                return resp.text\n",
    "        except Exception as e:\n",
    "            print(f\"    Prefetch failed for {url[:50]}...: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Queue all remote downloads\n",
    "    PREFETCH_URLS = [\n",
    "        # Gutenberg - Western Classics\n",
    "        \"https://www.gutenberg.org/cache/epub/1497/pg1497.txt\",  # Republic\n",
    "        \"https://www.gutenberg.org/cache/epub/1656/pg1656.txt\",  # Apology\n",
    "        \"https://www.gutenberg.org/cache/epub/1657/pg1657.txt\",  # Crito\n",
    "        \"https://www.gutenberg.org/cache/epub/1658/pg1658.txt\",  # Phaedo\n",
    "        \"https://www.gutenberg.org/cache/epub/3794/pg3794.txt\",  # Gorgias\n",
    "        \"https://www.gutenberg.org/cache/epub/1636/pg1636.txt\",  # Symposium\n",
    "        \"https://www.gutenberg.org/cache/epub/1726/pg1726.txt\",  # Meno\n",
    "        \"https://www.gutenberg.org/cache/epub/8438/pg8438.txt\",  # Nicomachean Ethics\n",
    "        \"https://www.gutenberg.org/cache/epub/6762/pg6762.txt\",  # Politics\n",
    "        \"https://www.gutenberg.org/cache/epub/2680/pg2680.txt\",  # Meditations\n",
    "        \"https://www.gutenberg.org/cache/epub/10661/pg10661.txt\",  # Enchiridion\n",
    "        \"https://www.gutenberg.org/cache/epub/3042/pg3042.txt\",  # Discourses\n",
    "        \"https://www.gutenberg.org/cache/epub/14988/pg14988.txt\",  # De Officiis\n",
    "        # MIT Classics fallback\n",
    "        \"https://classics.mit.edu/Aristotle/nicomachaen.mb.txt\",\n",
    "        \"https://classics.mit.edu/Plato/laws.mb.txt\",\n",
    "        # Bible Parallel Corpus\n",
    "        \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles/English.xml\",\n",
    "        \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles/Hebrew.xml\",\n",
    "        \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles/Arabic.xml\",\n",
    "        \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles/Chinese.xml\",\n",
    "    ]\n",
    "\n",
    "    for url in PREFETCH_URLS:\n",
    "        prefetch_results[url] = prefetch_executor.submit(prefetch_url, url)\n",
    "\n",
    "    print(f\"  Queued {len(PREFETCH_URLS)} URLs for background download\")\n",
    "\n",
    "    def get_prefetched(url, timeout=30):\n",
    "        \"\"\"Get prefetched content, waiting if necessary.\"\"\"\n",
    "        if url in prefetch_results:\n",
    "            try:\n",
    "                return prefetch_results[url].result(timeout=timeout)\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Fallback to direct fetch\n",
    "        return prefetch_url(url)\n",
    "\n",
    "    # ===== SEFARIA (Hebrew/Aramaic) =====\n",
    "    print(\"\\nLoading Sefaria...\")\n",
    "    sefaria_path = Path(\"data/raw/Sefaria-Export/json\")\n",
    "\n",
    "    CATEGORY_TO_PERIOD = {\n",
    "        \"Tanakh\": \"BIBLICAL\",\n",
    "        \"Torah\": \"BIBLICAL\",\n",
    "        \"Prophets\": \"BIBLICAL\",\n",
    "        \"Writings\": \"BIBLICAL\",\n",
    "        \"Mishnah\": \"TANNAITIC\",\n",
    "        \"Tosefta\": \"TANNAITIC\",\n",
    "        \"Sifra\": \"TANNAITIC\",\n",
    "        \"Sifrei\": \"TANNAITIC\",\n",
    "        \"Talmud\": \"TALMUDIC\",\n",
    "        \"Bavli\": \"TALMUDIC\",\n",
    "        \"Yerushalmi\": \"TALMUDIC\",\n",
    "        \"Midrash\": \"MIDRASHIC\",\n",
    "        \"Midrash Rabbah\": \"MIDRASHIC\",\n",
    "        \"Midrash Aggadah\": \"MIDRASHIC\",\n",
    "        \"Halakhah\": \"MEDIEVAL\",\n",
    "        \"Shulchan Arukh\": \"MEDIEVAL\",\n",
    "        \"Mishneh Torah\": \"MEDIEVAL\",\n",
    "        \"Musar\": \"MODERN\",\n",
    "        \"Chasidut\": \"MODERN\",\n",
    "        \"Modern\": \"MODERN\",\n",
    "    }\n",
    "\n",
    "    lang_counts = {\"hebrew\": 0, \"aramaic\": 0}\n",
    "\n",
    "    if sefaria_path.exists():\n",
    "        for json_file in tqdm(list(sefaria_path.rglob(\"*.json\"))[:5000], desc=\"Sefaria\"):\n",
    "            try:\n",
    "                with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                if isinstance(data, dict) and \"text\" in data:\n",
    "                    # Determine period from path\n",
    "                    path_parts = str(json_file.relative_to(sefaria_path)).split(\"/\")\n",
    "                    period = \"CLASSICAL\"\n",
    "                    for part in path_parts:\n",
    "                        if part in CATEGORY_TO_PERIOD:\n",
    "                            period = CATEGORY_TO_PERIOD[part]\n",
    "                            break\n",
    "\n",
    "                    # Determine language (heuristic: Talmud is primarily Aramaic)\n",
    "                    is_talmud = any(t in str(json_file) for t in [\"Talmud\", \"Bavli\", \"Yerushalmi\"])\n",
    "                    lang = \"aramaic\" if is_talmud else \"hebrew\"\n",
    "\n",
    "                    def extract_texts(obj, texts):\n",
    "                        if isinstance(obj, str) and len(obj) > 20:\n",
    "                            texts.append(obj)\n",
    "                        elif isinstance(obj, list):\n",
    "                            for item in obj:\n",
    "                                extract_texts(item, texts)\n",
    "\n",
    "                    texts = []\n",
    "                    extract_texts(data[\"text\"], texts)\n",
    "\n",
    "                    for txt in texts[:50]:  # Limit per file\n",
    "                        if lang_counts[lang] < MAX_PER_LANG:\n",
    "                            all_passages.append(\n",
    "                                {\n",
    "                                    \"id\": f\"sefaria_{len(all_passages)}\",\n",
    "                                    \"text\": txt,\n",
    "                                    \"lang\": lang,\n",
    "                                    \"source\": json_file.stem,\n",
    "                                    \"period\": period,\n",
    "                                }\n",
    "                            )\n",
    "                            lang_counts[lang] += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    else:\n",
    "        print(\"  Sefaria not found - will download\")\n",
    "\n",
    "    print(f\"  Hebrew: {lang_counts['hebrew']:,}, Aramaic: {lang_counts['aramaic']:,}\")\n",
    "\n",
    "    # ===== CLASSICAL CHINESE: Disabled (CText API blocks Colab) =====\n",
    "    print(\"  Skipping CText API (blocked from Colab, using Wenyanwen instead)\")\n",
    "    chinese_count = 0  # Initialize counter\n",
    "\n",
    "    # ===== KAGGLE: Ancient Chinese Wenyanwen (132K texts, 552M chars) =====\n",
    "    if chinese_count < MAX_PER_LANG:\n",
    "        print(\"  Loading from Kaggle Wenyanwen dataset...\")\n",
    "        wenyan_zip_name = \"Ancient_Chinese_Text_(wenyanwen)_archive.zip\"\n",
    "        wenyan_csv_name = \"cn_wenyan.csv\"\n",
    "        wenyan_local_zip = Path(f\"data/raw/{wenyan_zip_name}\")\n",
    "        _drive_ok = \"USE_DRIVE_DATA\" in dir() and USE_DRIVE_DATA and \"SAVE_DIR\" in dir()\n",
    "        wenyan_drive_zip = Path(f\"{SAVE_DIR}/{wenyan_zip_name}\") if _drive_ok else None\n",
    "        wenyan_local_csv = Path(f\"data/raw/{wenyan_csv_name}\")\n",
    "        wenyan_drive_csv = Path(f\"{SAVE_DIR}/{wenyan_csv_name}\") if _drive_ok else None\n",
    "\n",
    "        # Find the CSV (extracted or in zip)\n",
    "        csv_path = None\n",
    "        if wenyan_local_csv.exists():\n",
    "            csv_path = wenyan_local_csv\n",
    "            print(\"    Found CSV locally\")\n",
    "        elif wenyan_drive_csv and wenyan_drive_csv.exists():\n",
    "            csv_path = wenyan_drive_csv\n",
    "            print(\"    Found CSV in Drive\")\n",
    "        else:\n",
    "            # Need to extract from zip\n",
    "            zip_path = None\n",
    "            if wenyan_local_zip.exists():\n",
    "                zip_path = wenyan_local_zip\n",
    "                print(\"    Found zip locally\")\n",
    "            elif wenyan_drive_zip and wenyan_drive_zip.exists():\n",
    "                zip_path = wenyan_drive_zip\n",
    "                print(\"    Found zip in Drive\")\n",
    "\n",
    "            if zip_path:\n",
    "                try:\n",
    "                    import zipfile\n",
    "\n",
    "                    print(\"    Extracting CSV from zip...\")\n",
    "                    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "                        z.extract(wenyan_csv_name, \"data/raw/\")\n",
    "                    csv_path = wenyan_local_csv\n",
    "                    print(\"    Extracted!\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    Extraction failed: {e}\")\n",
    "\n",
    "        # Load texts from CSV\n",
    "        wenyan_count = 0\n",
    "        if csv_path and csv_path.exists():\n",
    "            import csv\n",
    "\n",
    "            csv.field_size_limit(10000000)  # Some texts are very long\n",
    "            try:\n",
    "                with open(csv_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    reader = csv.DictReader(f)\n",
    "                    for row in reader:\n",
    "                        if chinese_count >= MAX_PER_LANG:\n",
    "                            break\n",
    "                        text = row.get(\"text\", \"\")\n",
    "                        title = row.get(\"title\", \"\")\n",
    "                        # Split long texts into passages (max 2000 chars each)\n",
    "                        # Use paragraph breaks or every 1500 chars\n",
    "                        paragraphs = text.split(\"\\n\")\n",
    "                        current_para = \"\"\n",
    "                        for para in paragraphs:\n",
    "                            para = para.strip()\n",
    "                            if not para:\n",
    "                                continue\n",
    "                            if len(current_para) + len(para) < 1500:\n",
    "                                current_para += para\n",
    "                            else:\n",
    "                                if len(current_para) > 50:\n",
    "                                    all_passages.append(\n",
    "                                        {\n",
    "                                            \"id\": f\"wenyan_{len(all_passages)}\",\n",
    "                                            \"text\": current_para,\n",
    "                                            \"lang\": \"classical_chinese\",\n",
    "                                            \"source\": (\n",
    "                                                title.split(\"/\")[0] if \"/\" in title else title\n",
    "                                            ),\n",
    "                                            \"period\": \"CONFUCIAN\",\n",
    "                                        }\n",
    "                                    )\n",
    "                                    chinese_count += 1\n",
    "                                    wenyan_count += 1\n",
    "                                    if chinese_count >= MAX_PER_LANG:\n",
    "                                        break\n",
    "                                current_para = para\n",
    "                        # Don't forget last paragraph\n",
    "                        if current_para and len(current_para) > 50 and chinese_count < MAX_PER_LANG:\n",
    "                            all_passages.append(\n",
    "                                {\n",
    "                                    \"id\": f\"wenyan_{len(all_passages)}\",\n",
    "                                    \"text\": current_para,\n",
    "                                    \"lang\": \"classical_chinese\",\n",
    "                                    \"source\": title.split(\"/\")[0] if \"/\" in title else title,\n",
    "                                    \"period\": \"CONFUCIAN\",\n",
    "                                }\n",
    "                            )\n",
    "                            chinese_count += 1\n",
    "                            wenyan_count += 1\n",
    "                print(f\"    Added {wenyan_count:,} passages from Wenyanwen\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error loading Wenyanwen: {e}\")\n",
    "\n",
    "    print(f\"  Total Classical Chinese: {chinese_count:,}\")\n",
    "\n",
    "    # ===== ARABIC/ISLAMIC (Kaggle quran-nlp) =====\n",
    "    print(\"\\nLoading Arabic from Kaggle quran-nlp...\")\n",
    "\n",
    "    arabic_count = 0\n",
    "    kaggle_path = Path(\"data/raw/quran-nlp\")\n",
    "\n",
    "    # Try to download from Kaggle (in Refresh all OR Update missing mode)\n",
    "    if not kaggle_path.exists() and not CACHE_ONLY:\n",
    "        try:\n",
    "            import subprocess\n",
    "            import zipfile\n",
    "\n",
    "            subprocess.run([\"pip\", \"install\", \"-q\", \"kaggle\"], check=True)\n",
    "            subprocess.run(\n",
    "                [\n",
    "                    \"kaggle\",\n",
    "                    \"datasets\",\n",
    "                    \"download\",\n",
    "                    \"-d\",\n",
    "                    \"alizahidraja/quran-nlp\",\n",
    "                    \"-p\",\n",
    "                    \"data/raw\",\n",
    "                ],\n",
    "                check=True,\n",
    "                timeout=300,\n",
    "            )\n",
    "\n",
    "            with zipfile.ZipFile(\"data/raw/quran-nlp.zip\", \"r\") as z:\n",
    "                z.extractall(kaggle_path)\n",
    "            print(\"  Downloaded from Kaggle!\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Kaggle download failed: {e}\")\n",
    "\n",
    "    # Load if available\n",
    "    if kaggle_path.exists():\n",
    "        import pandas as pd\n",
    "\n",
    "        # Load Quran\n",
    "        quran_files = list(kaggle_path.rglob(\"*quran*.csv\"))\n",
    "        for qf in quran_files:\n",
    "            if arabic_count >= MAX_PER_LANG:\n",
    "                break\n",
    "            try:\n",
    "                df = pd.read_csv(qf, nrows=MAX_PER_LANG - arabic_count)\n",
    "                for _, row in df.iterrows():\n",
    "                    text = str(row.get(\"arabic\", row.get(\"text\", row.get(\"Arabic\", \"\"))))\n",
    "                    if text and len(text) > 10 and text != \"nan\":\n",
    "                        all_passages.append(\n",
    "                            {\n",
    "                                \"id\": f\"quran_{len(all_passages)}\",\n",
    "                                \"text\": text,\n",
    "                                \"lang\": \"arabic\",\n",
    "                                \"source\": \"Quran\",\n",
    "                                \"period\": \"QURANIC\",\n",
    "                            }\n",
    "                        )\n",
    "                        arabic_count += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Load Hadith\n",
    "        hadith_files = list(kaggle_path.rglob(\"*hadith*.csv\"))\n",
    "        for hf in hadith_files:\n",
    "            if arabic_count >= MAX_PER_LANG:\n",
    "                break\n",
    "            try:\n",
    "                df = pd.read_csv(hf, nrows=MAX_PER_LANG - arabic_count)\n",
    "                for _, row in df.iterrows():\n",
    "                    text = str(row.get(\"hadith\", row.get(\"text\", row.get(\"Arabic\", \"\"))))\n",
    "                    if text and len(text) > 10 and text != \"nan\":\n",
    "                        all_passages.append(\n",
    "                            {\n",
    "                                \"id\": f\"hadith_{len(all_passages)}\",\n",
    "                                \"text\": text,\n",
    "                                \"lang\": \"arabic\",\n",
    "                                \"source\": \"Hadith\",\n",
    "                                \"period\": \"HADITH\",\n",
    "                            }\n",
    "                        )\n",
    "                        arabic_count += 1\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        # Try Tanzil.net (simple direct download)\n",
    "        print(\"  Trying Tanzil.net for Quran text...\")\n",
    "        try:\n",
    "            tanzil_url = \"https://tanzil.net/pub/download/index.php?quranType=uthmani&outType=txt-2&agree=true\"\n",
    "            resp = requests.get(tanzil_url, timeout=60)\n",
    "            if resp.status_code == 200:\n",
    "                lines = resp.text.strip().split(\"\\n\")\n",
    "                for line in lines:\n",
    "                    if \"|\" in line and arabic_count < MAX_PER_LANG:\n",
    "                        parts = line.split(\"|\")\n",
    "                        if len(parts) >= 3:\n",
    "                            text = parts[2].strip()\n",
    "                            if len(text) > 10:\n",
    "                                all_passages.append(\n",
    "                                    {\n",
    "                                        \"id\": f\"tanzil_{len(all_passages)}\",\n",
    "                                        \"text\": text,\n",
    "                                        \"lang\": \"arabic\",\n",
    "                                        \"source\": \"Quran (Tanzil)\",\n",
    "                                        \"period\": \"QURANIC\",\n",
    "                                    }\n",
    "                                )\n",
    "                                arabic_count += 1\n",
    "                print(f\"    Loaded {arabic_count} verses from Tanzil\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Tanzil failed: {e}\")\n",
    "\n",
    "        # Final fallback: expanded hardcoded corpus\n",
    "        if arabic_count < 100:\n",
    "            print(\"  Using expanded hardcoded Arabic corpus...\")\n",
    "        ARABIC_CORPUS = [\n",
    "            # Quran excerpts (moral/ethical content)\n",
    "            \"وَلَا تَقْتُلُوا النَّفْسَ الَّتِي حَرَّمَ اللَّهُ إِلَّا بِالْحَقِّ\",\n",
    "            \"وَبِالْوَالِدَيْنِ إِحْسَانًا\",\n",
    "            \"وَأَوْفُوا بِالْعَهْدِ إِنَّ الْعَهْدَ كَانَ مَسْئُولًا\",\n",
    "            \"إِنَّ اللَّهَ يَأْمُرُ بِالْعَدْلِ وَالْإِحْسَانِ\",\n",
    "            \"وَلَا تَبْخَسُوا النَّاسَ أَشْيَاءَهُمْ\",\n",
    "            \"وَأَقِيمُوا الْوَزْنَ بِالْقِسْطِ وَلَا تُخْسِرُوا الْمِيزَانَ\",\n",
    "            \"يَا أَيُّهَا الَّذِينَ آمَنُوا أَوْفُوا بِالْعُقُودِ\",\n",
    "            \"وَتَعَاوَنُوا عَلَى الْبِرِّ وَالتَّقْوَى\",\n",
    "            # ... more can be added\n",
    "        ]\n",
    "        for i, txt in enumerate(ARABIC_CORPUS):\n",
    "            all_passages.append(\n",
    "                {\n",
    "                    \"id\": f\"arabic_{len(all_passages)}\",\n",
    "                    \"text\": txt,\n",
    "                    \"lang\": \"arabic\",\n",
    "                    \"source\": \"Quran/Hadith\",\n",
    "                    \"period\": \"QURANIC\",\n",
    "                }\n",
    "            )\n",
    "            arabic_count += 1\n",
    "\n",
    "    print(f\"  Arabic: {arabic_count:,}\")\n",
    "\n",
    "    # ===== DEAR ABBY (English) =====\n",
    "    print(\"Loading Dear Abby...\")\n",
    "\n",
    "    english_count = 0\n",
    "    abby_path = Path(\"data/raw/dear_abby.csv\")\n",
    "    print(f\"  Local path exists: {abby_path.exists()}\")\n",
    "\n",
    "    # Check Drive first\n",
    "    drive_abby = f\"{SAVE_DIR}/dear_abby.csv\"\n",
    "    print(f\"  Drive path: {drive_abby}\")\n",
    "    print(f\"  Drive path exists: {os.path.exists(drive_abby)}\")\n",
    "    if not abby_path.exists() and os.path.exists(drive_abby):\n",
    "        os.makedirs(\"data/raw\", exist_ok=True)\n",
    "        shutil.copy(drive_abby, abby_path)\n",
    "        print(\"  Copied from Drive\")\n",
    "\n",
    "    if not abby_path.exists() and not CACHE_ONLY:\n",
    "        try:\n",
    "            import subprocess\n",
    "\n",
    "            subprocess.run([\"pip\", \"install\", \"-q\", \"kaggle\"], check=True)\n",
    "            subprocess.run(\n",
    "                [\n",
    "                    \"kaggle\",\n",
    "                    \"datasets\",\n",
    "                    \"download\",\n",
    "                    \"-d\",\n",
    "                    \"thedevastator/20000-dear-abby-questions\",\n",
    "                    \"-p\",\n",
    "                    \"data/raw\",\n",
    "                    \"-f\",\n",
    "                    \"dear_abby.csv\",\n",
    "                ],\n",
    "                check=True,\n",
    "                timeout=120,\n",
    "            )\n",
    "            print(\"  Downloaded from Kaggle!\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Kaggle download failed: {e}\")\n",
    "\n",
    "    if abby_path.exists():\n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.read_csv(abby_path, nrows=MAX_PER_LANG)\n",
    "        print(f\"  CSV columns: {list(df.columns)}\")\n",
    "        print(f\"  CSV rows: {len(df)}\")\n",
    "        for _, row in df.iterrows():\n",
    "            question = str(row.get(\"question\", \"\"))\n",
    "            answer = str(row.get(\"question_only\", \"\"))\n",
    "            if len(answer) > 50:\n",
    "                all_passages.append(\n",
    "                    {\n",
    "                        \"id\": f\"abby_{len(all_passages)}\",\n",
    "                        \"text\": answer,\n",
    "                        \"lang\": \"english\",\n",
    "                        \"source\": \"Dear Abby\",\n",
    "                        \"period\": \"DEAR_ABBY\",\n",
    "                    }\n",
    "                )\n",
    "                english_count += 1\n",
    "    else:\n",
    "        print(\"  Dear Abby not found\")\n",
    "\n",
    "    print(f\"  Dear Abby: {english_count:,}\")\n",
    "\n",
    "    # ===== WESTERN CLASSICS (Greek/Roman Philosophy) =====\n",
    "    print(\"\\nLoading Western Classics (parallel download)...\")\n",
    "\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "    # Project Gutenberg texts (reliable, plain text)\n",
    "    GUTENBERG_TEXTS = [\n",
    "        # Plato - Ethics & Political Philosophy\n",
    "        (1497, \"Republic\", \"Plato\"),\n",
    "        (1656, \"Apology\", \"Plato\"),\n",
    "        (1657, \"Crito\", \"Plato\"),\n",
    "        (1658, \"Phaedo\", \"Plato\"),\n",
    "        (3794, \"Gorgias\", \"Plato\"),\n",
    "        (1636, \"Symposium\", \"Plato\"),\n",
    "        (1726, \"Meno\", \"Plato\"),\n",
    "        # Aristotle\n",
    "        (8438, \"Nicomachean Ethics\", \"Aristotle\"),\n",
    "        (6762, \"Politics\", \"Aristotle\"),\n",
    "        # Stoics\n",
    "        (2680, \"Meditations\", \"Marcus Aurelius\"),\n",
    "        (10661, \"Enchiridion\", \"Epictetus\"),\n",
    "        (3042, \"Discourses\", \"Epictetus\"),\n",
    "        # Cicero\n",
    "        (14988, \"De Officiis\", \"Cicero\"),\n",
    "    ]\n",
    "\n",
    "    # MIT Classics fallback\n",
    "    MIT_TEXTS = [\n",
    "        (\n",
    "            \"https://classics.mit.edu/Aristotle/nicomachaen.mb.txt\",\n",
    "            \"Nicomachean Ethics\",\n",
    "            \"Aristotle\",\n",
    "        ),\n",
    "        (\"https://classics.mit.edu/Aristotle/politics.mb.txt\", \"Politics\", \"Aristotle\"),\n",
    "        (\"https://classics.mit.edu/Plato/republic.mb.txt\", \"Republic\", \"Plato\"),\n",
    "        (\"https://classics.mit.edu/Plato/laws.mb.txt\", \"Laws\", \"Plato\"),\n",
    "        (\"https://classics.mit.edu/Antoninus/meditations.mb.txt\", \"Meditations\", \"Marcus Aurelius\"),\n",
    "        (\"https://classics.mit.edu/Epictetus/epicench.mb.txt\", \"Enchiridion\", \"Epictetus\"),\n",
    "        (\"https://classics.mit.edu/Cicero/duties.mb.txt\", \"De Officiis\", \"Cicero\"),\n",
    "    ]\n",
    "\n",
    "    western_target = min(MAX_PER_LANG, 15000)\n",
    "\n",
    "    def fetch_gutenberg(item):\n",
    "        \"\"\"Fetch a single Gutenberg text (uses prefetch if available).\"\"\"\n",
    "        gutenberg_id, title, author = item\n",
    "        try:\n",
    "            url = f\"https://www.gutenberg.org/cache/epub/{gutenberg_id}/pg{gutenberg_id}.txt\"\n",
    "            text = get_prefetched(url)\n",
    "            if text:\n",
    "                # Skip Gutenberg header/footer\n",
    "                for marker in [\"*** START OF\", \"***START OF\"]:\n",
    "                    if marker in text:\n",
    "                        text = text.split(marker, 1)[-1]\n",
    "                        break\n",
    "                for marker in [\"*** END OF\", \"***END OF\", \"End of Project Gutenberg\"]:\n",
    "                    if marker in text:\n",
    "                        text = text.split(marker, 1)[0]\n",
    "                        break\n",
    "\n",
    "                paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if len(p.strip()) > 100]\n",
    "                passages = []\n",
    "                for para in paragraphs:\n",
    "                    para = re.sub(r\"\\s+\", \" \", para).strip()\n",
    "                    if 50 < len(para) < 2000:\n",
    "                        passages.append(\n",
    "                            {\n",
    "                                \"text\": para,\n",
    "                                \"source\": f\"{author}: {title}\",\n",
    "                                \"author\": author,\n",
    "                                \"title\": title,\n",
    "                            }\n",
    "                        )\n",
    "                return (title, author, passages)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        return (title, author, [])\n",
    "\n",
    "    def fetch_mit(item):\n",
    "        \"\"\"Fetch a single MIT Classics text (uses prefetch if available).\"\"\"\n",
    "        url, title, author = item\n",
    "        try:\n",
    "            text = get_prefetched(url)\n",
    "            if text:\n",
    "                paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if len(p.strip()) > 100]\n",
    "                passages = []\n",
    "                for para in paragraphs[:500]:\n",
    "                    para = re.sub(r\"\\s+\", \" \", para).strip()\n",
    "                    if 50 < len(para) < 2000:\n",
    "                        passages.append(\n",
    "                            {\n",
    "                                \"text\": para,\n",
    "                                \"source\": f\"{author}: {title}\",\n",
    "                                \"author\": author,\n",
    "                                \"title\": title,\n",
    "                            }\n",
    "                        )\n",
    "                return (title, author, passages)\n",
    "        except:\n",
    "            pass\n",
    "        return (title, author, [])\n",
    "\n",
    "    western_passages = []\n",
    "    loaded_titles = set()\n",
    "\n",
    "    # Parallel fetch from Gutenberg\n",
    "    print(\"  Fetching from Project Gutenberg (parallel)...\")\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        futures = {executor.submit(fetch_gutenberg, item): item for item in GUTENBERG_TEXTS}\n",
    "        for future in as_completed(futures):\n",
    "            title, author, passages = future.result()\n",
    "            if passages and title not in loaded_titles:\n",
    "                western_passages.extend(passages)\n",
    "                loaded_titles.add(title)\n",
    "                print(f\"    {author}: {title} - {len(passages)} passages\")\n",
    "\n",
    "    # Parallel fetch from MIT for any missing\n",
    "    missing_mit = [(url, t, a) for url, t, a in MIT_TEXTS if t not in loaded_titles]\n",
    "    if missing_mit:\n",
    "        print(\"  Fetching missing texts from MIT Classics...\")\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = {executor.submit(fetch_mit, item): item for item in missing_mit}\n",
    "            for future in as_completed(futures):\n",
    "                title, author, passages = future.result()\n",
    "                if passages and title not in loaded_titles:\n",
    "                    western_passages.extend(passages)\n",
    "                    loaded_titles.add(title)\n",
    "                    print(f\"    {author}: {title} - {len(passages)} passages (MIT)\")\n",
    "\n",
    "    # Add to all_passages with proper IDs\n",
    "    western_count = 0\n",
    "    for p in western_passages:\n",
    "        if western_count >= western_target:\n",
    "            break\n",
    "        all_passages.append(\n",
    "            {\n",
    "                \"id\": f\"western_{len(all_passages)}\",\n",
    "                \"text\": p[\"text\"],\n",
    "                \"lang\": \"english\",\n",
    "                \"source\": p[\"source\"],\n",
    "                \"period\": \"WESTERN_CLASSICAL\",\n",
    "                \"time_period\": \"WESTERN_CLASSICAL\",\n",
    "            }\n",
    "        )\n",
    "        western_count += 1\n",
    "\n",
    "    print(f\"  Total Western Classics: {western_count:,}\")\n",
    "    # ===== UNIMORAL: Disabled (gated dataset requires auth) =====\n",
    "    print(\"  Skipping UniMoral (gated HuggingFace dataset)\")\n",
    "\n",
    "    # ===== UN PARALLEL CORPUS (HuggingFace streaming) =====\n",
    "    print(\"\\nLoading UN Corpus from HuggingFace (streaming)...\")\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "\n",
    "        pairs = [(\"ar\", \"en\"), (\"en\", \"zh\")]\n",
    "        un_count = 0\n",
    "        lang_map = {\"ar\": \"arabic\", \"zh\": \"classical_chinese\", \"en\": \"english\"}\n",
    "\n",
    "        for src, tgt in pairs:\n",
    "            if un_count >= MAX_PER_LANG:\n",
    "                break\n",
    "            try:\n",
    "                config = f\"{src}-{tgt}\"\n",
    "                ds = load_dataset(\"Helsinki-NLP/un_pc\", config, split=\"train\", streaming=True)\n",
    "\n",
    "                pair_count = 0\n",
    "                for item in ds:\n",
    "                    if pair_count >= min(MAX_PER_LANG // 4, 5000):\n",
    "                        break\n",
    "\n",
    "                    translation = item.get(\"translation\", {})\n",
    "                    for lang_code in [src, tgt]:\n",
    "                        text = translation.get(lang_code, \"\")\n",
    "                        if len(text) > 30 and lang_code in lang_map:\n",
    "                            all_passages.append(\n",
    "                                {\n",
    "                                    \"id\": f\"un_{len(all_passages)}\",\n",
    "                                    \"text\": text,\n",
    "                                    \"lang\": lang_map[lang_code],\n",
    "                                    \"source\": \"UN Corpus\",\n",
    "                                    \"period\": \"MODERN\",\n",
    "                                }\n",
    "                            )\n",
    "                            pair_count += 1\n",
    "                            un_count += 1\n",
    "\n",
    "                print(f\"  UN {config}: {pair_count:,}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  UN {config} error: {e}\")\n",
    "\n",
    "        print(f\"  UN Corpus total: {un_count:,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  UN Corpus error: {e}\")\n",
    "\n",
    "    # ===== BIBLE PARALLEL CORPUS (GitHub) =====\n",
    "    print(\"\\nLoading Bible Parallel Corpus...\")\n",
    "    try:\n",
    "        base_url = \"https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles\"\n",
    "        bible_files = [\n",
    "            (\"Hebrew.xml\", \"hebrew\"),\n",
    "            (\"Arabic.xml\", \"arabic\"),\n",
    "            (\"Chinese.xml\", \"classical_chinese\"),\n",
    "        ]\n",
    "\n",
    "        bible_count = 0\n",
    "        for filename, lang in bible_files:\n",
    "            if bible_count >= MAX_PER_LANG * 3:\n",
    "                break\n",
    "            try:\n",
    "                url = f\"{base_url}/{filename}\"\n",
    "                text = get_prefetched(url)\n",
    "                if text:\n",
    "                    verses = re.findall(r\"<seg[^>]*>([^<]+)</seg>\", text)\n",
    "                    file_count = 0\n",
    "                    for verse in verses:\n",
    "                        if file_count >= MAX_PER_LANG:\n",
    "                            break\n",
    "                        verse = verse.strip()\n",
    "                        if len(verse) > 10:\n",
    "                            all_passages.append(\n",
    "                                {\n",
    "                                    \"id\": f\"bible_{len(all_passages)}\",\n",
    "                                    \"text\": verse,\n",
    "                                    \"lang\": lang,\n",
    "                                    \"source\": \"Bible\",\n",
    "                                    \"period\": \"CLASSICAL\",\n",
    "                                }\n",
    "                            )\n",
    "                            file_count += 1\n",
    "                            bible_count += 1\n",
    "                    print(f\"  Bible {lang}: {file_count:,}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Bible {filename} error: {e}\")\n",
    "\n",
    "        print(f\"  Bible total: {bible_count:,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Bible error: {e}\")\n",
    "\n",
    "    # ===== NEW v10.9 CORPORA =====\n",
    "    print(\"\\n--- v10.9 New Corpora (hardcoded) ---\")\n",
    "\n",
    "    # Chinese philosophical traditions\n",
    "    chinese_corpora = [\n",
    "        (BUDDHIST_CHINESE, \"BUDDHIST\", \"Buddhist Chinese\"),\n",
    "        (LEGALIST_CHINESE, \"LEGALIST\", \"Legalist Chinese\"),\n",
    "        (MOHIST_CHINESE, \"MOHIST\", \"Mohist Chinese\"),\n",
    "        (NEO_CONFUCIAN_CHINESE, \"NEO_CONFUCIAN\", \"Neo-Confucian\"),\n",
    "    ]\n",
    "\n",
    "    for corpus, period, label in chinese_corpora:\n",
    "        count = 0\n",
    "        for text_content, source_ref, _ in corpus:\n",
    "            all_passages.append(\n",
    "                {\n",
    "                    \"id\": f\"v109_{label.lower().replace(' ', '_')}_{len(all_passages)}\",\n",
    "                    \"text\": text_content,\n",
    "                    \"lang\": \"classical_chinese\",\n",
    "                    \"source\": source_ref,\n",
    "                    \"period\": period,\n",
    "                }\n",
    "            )\n",
    "            count += 1\n",
    "        print(f\"  {label}: {count}\")\n",
    "\n",
    "    # Arabic/Islamic traditions\n",
    "    arabic_corpora = [\n",
    "        (ISLAMIC_LEGAL_MAXIMS, \"FIQH\", \"Islamic Legal Maxims\"),\n",
    "        (SUFI_ETHICS, \"SUFI\", \"Sufi Ethics\"),\n",
    "        (ARABIC_PHILOSOPHY, \"FALSAFA\", \"Arabic Philosophy\"),\n",
    "    ]\n",
    "\n",
    "    for corpus, period, label in arabic_corpora:\n",
    "        count = 0\n",
    "        for text_content, source_ref, _ in corpus:\n",
    "            all_passages.append(\n",
    "                {\n",
    "                    \"id\": f\"v109_{label.lower().replace(' ', '_')}_{len(all_passages)}\",\n",
    "                    \"text\": text_content,\n",
    "                    \"lang\": \"arabic\",\n",
    "                    \"source\": source_ref,\n",
    "                    \"period\": period,\n",
    "                }\n",
    "            )\n",
    "            count += 1\n",
    "        print(f\"  {label}: {count}\")\n",
    "\n",
    "    # Sanskrit tradition\n",
    "    sanskrit_count = 0\n",
    "    for text_content, source_ref, period_tag in SANSKRIT_DHARMA:\n",
    "        all_passages.append(\n",
    "            {\n",
    "                \"id\": f\"v109_sanskrit_{len(all_passages)}\",\n",
    "                \"text\": text_content,\n",
    "                \"lang\": \"sanskrit\",\n",
    "                \"source\": source_ref,\n",
    "                \"period\": period_tag,\n",
    "            }\n",
    "        )\n",
    "        sanskrit_count += 1\n",
    "    print(f\"  Sanskrit Dharma: {sanskrit_count}\")\n",
    "\n",
    "    # Pali tradition\n",
    "    pali_count = 0\n",
    "    for text_content, source_ref, period_tag in PALI_ETHICS:\n",
    "        all_passages.append(\n",
    "            {\n",
    "                \"id\": f\"v109_pali_{len(all_passages)}\",\n",
    "                \"text\": text_content,\n",
    "                \"lang\": \"pali\",\n",
    "                \"source\": source_ref,\n",
    "                \"period\": period_tag,\n",
    "            }\n",
    "        )\n",
    "        pali_count += 1\n",
    "    print(f\"  Pali Ethics: {pali_count}\")\n",
    "\n",
    "    # Cleanup prefetch executor\n",
    "    print(\"\\nWaiting for any remaining prefetch tasks...\")\n",
    "    prefetch_executor.shutdown(wait=False)\n",
    "\n",
    "    # ===== SUMMARY =====\n",
    "    print(f\"\\nTOTAL: {len(all_passages):,}\")\n",
    "\n",
    "    # Count by language\n",
    "    by_lang = defaultdict(int)\n",
    "    for p in all_passages:\n",
    "        by_lang[p[\"lang\"]] += 1\n",
    "    print(\"\\nBy language:\")\n",
    "    for lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {lang}: {cnt:,}\")\n",
    "\n",
    "    # ===== EXTRACT BONDS =====\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXTRACTING BONDS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    def extract_bond(text, language):\n",
    "        \"\"\"Extract bond type with context awareness.\"\"\"\n",
    "        tn = normalize_text(text, language)\n",
    "\n",
    "        for bt, pats in ALL_BOND_PATTERNS.get(language, {}).items():\n",
    "            for p in pats:\n",
    "                match = re.search(p, tn)\n",
    "                if match:\n",
    "                    # Check context around the match\n",
    "                    context, marker_type = detect_context(text, language, match.start())\n",
    "                    confidence = 0.9 if context == \"prescriptive\" else 0.5\n",
    "                    return bt, context, confidence\n",
    "        return None, \"unknown\", 0.5\n",
    "\n",
    "    bonds = []\n",
    "    for p in tqdm(all_passages, desc=\"Extracting bonds\"):\n",
    "        bt, ctx, conf = extract_bond(p[\"text\"], p[\"lang\"])\n",
    "        if bt:\n",
    "            bonds.append(\n",
    "                {\n",
    "                    \"passage_id\": p[\"id\"],\n",
    "                    \"bond_type\": bt,\n",
    "                    \"language\": p[\"lang\"],\n",
    "                    \"time_period\": p[\"period\"],\n",
    "                    \"source\": p[\"source\"],\n",
    "                    \"text\": p[\"text\"][:500],\n",
    "                    \"context\": ctx,\n",
    "                    \"confidence\": conf,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    print(f\"\\nExtracted {len(bonds):,} bonds from {len(all_passages):,} passages\")\n",
    "\n",
    "    # Count by bond type\n",
    "    by_bond = defaultdict(int)\n",
    "    for b in bonds:\n",
    "        by_bond[b[\"bond_type\"]] += 1\n",
    "    print(\"\\nBy bond type:\")\n",
    "    for bt, cnt in sorted(by_bond.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {bt}: {cnt:,}\")\n",
    "\n",
    "    # Count by context\n",
    "    by_ctx = defaultdict(int)\n",
    "    for b in bonds:\n",
    "        by_ctx[b[\"context\"]] += 1\n",
    "    print(\"\\nBy context:\")\n",
    "    for ctx, cnt in sorted(by_ctx.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {ctx}: {cnt:,}\")\n",
    "\n",
    "    # ===== SAVE =====\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAVING DATA\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Save passages\n",
    "    with open(\"data/processed/passages.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for p in all_passages:\n",
    "            # Normalize field names\n",
    "            p_out = {\n",
    "                \"id\": p[\"id\"],\n",
    "                \"text\": p[\"text\"],\n",
    "                \"language\": p[\"lang\"],\n",
    "                \"source\": p[\"source\"],\n",
    "                \"time_period\": p[\"period\"],\n",
    "            }\n",
    "            f.write(json.dumps(p_out, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"  Saved {len(all_passages):,} passages to data/processed/passages.jsonl\")\n",
    "\n",
    "    # Save bonds\n",
    "    with open(\"data/processed/bonds.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for b in bonds:\n",
    "            b_out = {\n",
    "                **b,\n",
    "                \"bond_type\": (\n",
    "                    b[\"bond_type\"].name if hasattr(b[\"bond_type\"], \"name\") else str(b[\"bond_type\"])\n",
    "                ),\n",
    "            }\n",
    "            f.write(json.dumps(b_out, ensure_ascii=False) + chr(10))\n",
    "    print(f\"  Saved {len(bonds):,} bonds to data/processed/bonds.jsonl\")\n",
    "\n",
    "    # Copy to Drive if enabled\n",
    "    if USE_DRIVE_DATA and SAVE_DIR:\n",
    "        try:\n",
    "            os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "            shutil.copy(\"data/processed/passages.jsonl\", f\"{SAVE_DIR}/passages.jsonl\")\n",
    "            shutil.copy(\"data/processed/bonds.jsonl\", f\"{SAVE_DIR}/bonds.jsonl\")\n",
    "            print(f\"  Copied to Drive: {SAVE_DIR}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Drive copy failed: {e}\")\n",
    "\n",
    "    gc.collect()\n",
    "    print(\"\\nDone!\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 5. Generate Splits { display-mode: \"form\" }\n",
    "# @markdown Creates train/test splits for cross-lingual experiments\n",
    "# @markdown v10.9: Added confucian_to_buddhist, confucian_to_legalist,\n",
    "# @markdown        all_to_sanskrit, semitic_to_indic, quran_to_fiqh\n",
    "\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATING SPLITS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if splits already exist from Drive\n",
    "# Check if splits are valid (IDs match current passages)\n",
    "splits_valid = False\n",
    "if os.path.exists(\"data/splits/all_splits.json\"):\n",
    "    try:\n",
    "        with open(\"data/splits/all_splits.json\") as f:\n",
    "            cached_splits = json.load(f)\n",
    "        # Get sample of IDs from splits\n",
    "        sample_ids = set()\n",
    "        for split in cached_splits.values():\n",
    "            sample_ids.update(split[\"train_ids\"][:100])\n",
    "            sample_ids.update(split[\"test_ids\"][:100])\n",
    "        # Check if they exist in current passages\n",
    "        passage_ids = set()\n",
    "        with open(\"data/processed/passages.jsonl\") as f:\n",
    "            for line in f:\n",
    "                p = json.loads(line)\n",
    "                passage_ids.add(p[\"id\"])\n",
    "                if len(passage_ids) > 10000:\n",
    "                    break\n",
    "        matches = len(sample_ids & passage_ids)\n",
    "        splits_valid = matches > len(sample_ids) * 0.9  # 90% match\n",
    "        if not splits_valid:\n",
    "            print(f\"Splits invalid: only {matches}/{len(sample_ids)} IDs match current passages\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating splits: {e}\")\n",
    "\n",
    "if splits_valid and not REFRESH_DATA_FROM_SOURCE:\n",
    "    print(\"\\nSplits already loaded from Drive\")\n",
    "    with open(\"data/splits/all_splits.json\") as f:\n",
    "        all_splits = json.load(f)\n",
    "    for name, split in all_splits.items():\n",
    "        print(f\"  {name}: train={split['train_size']:,}, test={split['test_size']:,}\")\n",
    "else:\n",
    "    random.seed(42)\n",
    "\n",
    "    # Read passage metadata\n",
    "    passage_meta = []\n",
    "    with open(\"data/processed/passages.jsonl\", \"r\") as f:\n",
    "        for line in f:\n",
    "            p = json.loads(line)\n",
    "            passage_meta.append(p)\n",
    "\n",
    "    print(f\"Total passages: {len(passage_meta):,}\")\n",
    "\n",
    "    by_lang = defaultdict(list)\n",
    "    by_period = defaultdict(list)\n",
    "    for p in passage_meta:\n",
    "        by_lang[p[\"language\"]].append(p[\"id\"])\n",
    "        by_period[p[\"time_period\"]].append(p[\"id\"])\n",
    "\n",
    "    print(\"\\nBy language:\")\n",
    "    for lang, ids in sorted(by_lang.items(), key=lambda x: -len(x[1])):\n",
    "        print(f\"  {lang}: {len(ids):,}\")\n",
    "\n",
    "    print(\"\\nBy period:\")\n",
    "    for period, ids in sorted(by_period.items(), key=lambda x: -len(x[1])):\n",
    "        print(f\"  {period}: {len(ids):,}\")\n",
    "\n",
    "    all_splits = {}\n",
    "    # ===== SPLIT 1: Hebrew -> Others =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 1: HEBREW -> OTHERS\")\n",
    "    hebrew_ids = by_lang.get(\"hebrew\", [])\n",
    "    other_ids = [p[\"id\"] for p in passage_meta if p[\"language\"] != \"hebrew\"]\n",
    "    random.shuffle(hebrew_ids)\n",
    "    random.shuffle(other_ids)\n",
    "\n",
    "    all_splits[\"hebrew_to_others\"] = {\n",
    "        \"train_ids\": hebrew_ids,\n",
    "        \"test_ids\": other_ids,\n",
    "        \"train_size\": len(hebrew_ids),\n",
    "        \"test_size\": len(other_ids),\n",
    "    }\n",
    "    print(f\"  Train (Hebrew): {len(hebrew_ids):,}\")\n",
    "    print(f\"  Test (Others): {len(other_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 2: Semitic -> Non-Semitic =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 2: SEMITIC -> NON-SEMITIC\")\n",
    "    semitic_ids = by_lang.get(\"hebrew\", []) + by_lang.get(\"aramaic\", []) + by_lang.get(\"arabic\", [])\n",
    "    non_semitic_ids = by_lang.get(\"classical_chinese\", []) + by_lang.get(\"english\", [])\n",
    "    random.shuffle(semitic_ids)\n",
    "    random.shuffle(non_semitic_ids)\n",
    "\n",
    "    all_splits[\"semitic_to_non_semitic\"] = {\n",
    "        \"train_ids\": semitic_ids,\n",
    "        \"test_ids\": non_semitic_ids,\n",
    "        \"train_size\": len(semitic_ids),\n",
    "        \"test_size\": len(non_semitic_ids),\n",
    "    }\n",
    "    print(f\"  Train (Semitic): {len(semitic_ids):,}\")\n",
    "    print(f\"  Test (Non-Semitic): {len(non_semitic_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 3: Ancient -> Modern =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 3: ANCIENT -> MODERN\")\n",
    "    # Define modern periods explicitly, derive ancient dynamically\n",
    "    modern_periods = {\"MODERN\", \"DEAR_ABBY\"}\n",
    "    all_periods = set(by_period.keys())\n",
    "    ancient_periods = all_periods - modern_periods\n",
    "\n",
    "    print(f\"  Ancient periods: {sorted(ancient_periods)}\")\n",
    "    print(f\"  Modern periods: {sorted(modern_periods)}\")\n",
    "\n",
    "    ancient_ids = [p[\"id\"] for p in passage_meta if p[\"time_period\"] in ancient_periods]\n",
    "    modern_ids = [p[\"id\"] for p in passage_meta if p[\"time_period\"] in modern_periods]\n",
    "    random.shuffle(ancient_ids)\n",
    "    random.shuffle(modern_ids)\n",
    "\n",
    "    all_splits[\"ancient_to_modern\"] = {\n",
    "        \"train_ids\": ancient_ids,\n",
    "        \"test_ids\": modern_ids,\n",
    "        \"train_size\": len(ancient_ids),\n",
    "        \"test_size\": len(modern_ids),\n",
    "    }\n",
    "    print(f\"  Train (Ancient): {len(ancient_ids):,}\")\n",
    "    print(f\"  Test (Modern): {len(modern_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 4: Mixed Baseline =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 4: MIXED BASELINE\")\n",
    "    all_ids = [p[\"id\"] for p in passage_meta]\n",
    "    random.shuffle(all_ids)\n",
    "    split_idx = int(0.7 * len(all_ids))\n",
    "\n",
    "    all_splits[\"mixed_baseline\"] = {\n",
    "        \"train_ids\": all_ids[:split_idx],\n",
    "        \"test_ids\": all_ids[split_idx:],\n",
    "        \"train_size\": split_idx,\n",
    "        \"test_size\": len(all_ids) - split_idx,\n",
    "    }\n",
    "    print(f\"  Train: {split_idx:,}\")\n",
    "    print(f\"  Test: {len(all_ids) - split_idx:,}\")\n",
    "\n",
    "    # ===== SPLIT 5: Dear Abby -> Classical Chinese =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 5: DEAR ABBY -> CHINESE\")\n",
    "    abby_ids = [p[\"id\"] for p in passage_meta if p[\"time_period\"] == \"DEAR_ABBY\"]\n",
    "    chinese_ids = [p[\"id\"] for p in passage_meta if p[\"language\"] == \"classical_chinese\"]\n",
    "    random.shuffle(abby_ids)\n",
    "    random.shuffle(chinese_ids)\n",
    "\n",
    "    all_splits[\"abby_to_chinese\"] = {\n",
    "        \"train_ids\": abby_ids,\n",
    "        \"test_ids\": chinese_ids,\n",
    "        \"train_size\": len(abby_ids),\n",
    "        \"test_size\": len(chinese_ids),\n",
    "    }\n",
    "    print(f\"  Train (Dear Abby): {len(abby_ids):,}\")\n",
    "    print(f\"  Test (Chinese): {len(chinese_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 6: Western Classical -> Eastern =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 6: WESTERN CLASSICAL -> EASTERN\")\n",
    "    western_ids = [p[\"id\"] for p in passage_meta if p[\"time_period\"] == \"WESTERN_CLASSICAL\"]\n",
    "    eastern_ids = [\n",
    "        p[\"id\"] for p in passage_meta if p[\"language\"] in (\"classical_chinese\", \"hebrew\")\n",
    "    ]\n",
    "    random.shuffle(western_ids)\n",
    "    random.shuffle(eastern_ids)\n",
    "\n",
    "    all_splits[\"western_to_eastern\"] = {\n",
    "        \"train_ids\": western_ids,\n",
    "        \"test_ids\": eastern_ids,\n",
    "        \"train_size\": len(western_ids),\n",
    "        \"test_size\": len(eastern_ids),\n",
    "    }\n",
    "    print(f\"  Train (Western - Plato, Aristotle, Stoics): {len(western_ids):,}\")\n",
    "    print(f\"  Test (Eastern - Chinese, Hebrew): {len(eastern_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 7: Confucian -> Buddhist (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 7: CONFUCIAN -> BUDDHIST (Chinese intra-tradition)\")\n",
    "    confucian_daoist_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] in (\"CONFUCIAN\", \"DAOIST\") and p[\"language\"] == \"classical_chinese\"\n",
    "    ]\n",
    "    buddhist_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] == \"BUDDHIST\" and p[\"language\"] == \"classical_chinese\"\n",
    "    ]\n",
    "    random.shuffle(confucian_daoist_ids)\n",
    "    random.shuffle(buddhist_ids)\n",
    "\n",
    "    all_splits[\"confucian_to_buddhist\"] = {\n",
    "        \"train_ids\": confucian_daoist_ids,\n",
    "        \"test_ids\": buddhist_ids,\n",
    "        \"train_size\": len(confucian_daoist_ids),\n",
    "        \"test_size\": len(buddhist_ids),\n",
    "        \"description\": \"Test if Chinese performance is tradition-specific\",\n",
    "    }\n",
    "    print(f\"  Train (Confucian+Daoist): {len(confucian_daoist_ids):,}\")\n",
    "    print(f\"  Test (Buddhist): {len(buddhist_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 8: Confucian -> Legalist/Mohist (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 8: CONFUCIAN -> LEGALIST/MOHIST (virtue vs consequentialist)\")\n",
    "    confucian_only_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] == \"CONFUCIAN\" and p[\"language\"] == \"classical_chinese\"\n",
    "    ]\n",
    "    legalist_mohist_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] in (\"LEGALIST\", \"MOHIST\") and p[\"language\"] == \"classical_chinese\"\n",
    "    ]\n",
    "    random.shuffle(confucian_only_ids)\n",
    "    random.shuffle(legalist_mohist_ids)\n",
    "\n",
    "    all_splits[\"confucian_to_legalist\"] = {\n",
    "        \"train_ids\": confucian_only_ids,\n",
    "        \"test_ids\": legalist_mohist_ids,\n",
    "        \"train_size\": len(confucian_only_ids),\n",
    "        \"test_size\": len(legalist_mohist_ids),\n",
    "        \"description\": \"Virtue ethics → consequentialist/legalist\",\n",
    "    }\n",
    "    print(f\"  Train (Confucian): {len(confucian_only_ids):,}\")\n",
    "    print(f\"  Test (Legalist+Mohist): {len(legalist_mohist_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 9: All -> Sanskrit/Pali (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 9: ALL -> SANSKRIT/PALI (ultimate transfer test)\")\n",
    "    non_indic_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"language\"] in (\"hebrew\", \"aramaic\", \"classical_chinese\", \"arabic\", \"english\")\n",
    "    ]\n",
    "    indic_ids = [p[\"id\"] for p in passage_meta if p[\"language\"] in (\"sanskrit\", \"pali\")]\n",
    "    random.shuffle(non_indic_ids)\n",
    "    random.shuffle(indic_ids)\n",
    "\n",
    "    all_splits[\"all_to_sanskrit\"] = {\n",
    "        \"train_ids\": non_indic_ids,\n",
    "        \"test_ids\": indic_ids,\n",
    "        \"train_size\": len(non_indic_ids),\n",
    "        \"test_size\": len(indic_ids),\n",
    "        \"description\": \"Ultimate transfer test: completely held-out language family\",\n",
    "    }\n",
    "    print(f\"  Train (non-Indic): {len(non_indic_ids):,}\")\n",
    "    print(f\"  Test (Sanskrit+Pali): {len(indic_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 10: Semitic -> Indic (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 10: SEMITIC -> INDIC\")\n",
    "    semitic_only_ids = [\n",
    "        p[\"id\"] for p in passage_meta if p[\"language\"] in (\"hebrew\", \"aramaic\", \"arabic\")\n",
    "    ]\n",
    "    indic_only_ids = [p[\"id\"] for p in passage_meta if p[\"language\"] in (\"sanskrit\", \"pali\")]\n",
    "    random.shuffle(semitic_only_ids)\n",
    "    random.shuffle(indic_only_ids)\n",
    "\n",
    "    all_splits[\"semitic_to_indic\"] = {\n",
    "        \"train_ids\": semitic_only_ids,\n",
    "        \"test_ids\": indic_only_ids,\n",
    "        \"train_size\": len(semitic_only_ids),\n",
    "        \"test_size\": len(indic_only_ids),\n",
    "        \"description\": \"Semitic → Indo-Aryan transfer\",\n",
    "    }\n",
    "    print(f\"  Train (Semitic): {len(semitic_only_ids):,}\")\n",
    "    print(f\"  Test (Indic): {len(indic_only_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 11: Quran -> Fiqh (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 11: QURAN -> FIQH (religious to legal/philosophical)\")\n",
    "    quranic_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] in (\"QURANIC\", \"HADITH\") and p[\"language\"] == \"arabic\"\n",
    "    ]\n",
    "    fiqh_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] in (\"FIQH\", \"SUFI\", \"FALSAFA\") and p[\"language\"] == \"arabic\"\n",
    "    ]\n",
    "    random.shuffle(quranic_ids)\n",
    "    random.shuffle(fiqh_ids)\n",
    "\n",
    "    all_splits[\"quran_to_fiqh\"] = {\n",
    "        \"train_ids\": quranic_ids,\n",
    "        \"test_ids\": fiqh_ids,\n",
    "        \"train_size\": len(quranic_ids),\n",
    "        \"test_size\": len(fiqh_ids),\n",
    "        \"description\": \"Religious → legal/philosophical Arabic\",\n",
    "    }\n",
    "    print(f\"  Train (Quranic+Hadith): {len(quranic_ids):,}\")\n",
    "    print(f\"  Test (Fiqh+Sufi+Falsafa): {len(fiqh_ids):,}\")\n",
    "\n",
    "    # Save splits\n",
    "    with open(\"data/splits/all_splits.json\", \"w\") as f:\n",
    "        json.dump(all_splits, f, indent=2)\n",
    "\n",
    "    # Save to Drive\n",
    "    shutil.copy(\"data/splits/all_splits.json\", f\"{SAVE_DIR}/all_splits.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Splits saved to local and Drive\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 6. Model Architecture { display-mode: \"form\" }\n",
    "# @markdown BIP v10.9 model with configurable backbone and adversarial heads\n",
    "# @markdown - Updated: 8 languages, 26 periods\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Backbone: {BACKBONE} ({MODEL_NAME})\")\n",
    "print(f\"Hidden size: {BACKBONE_HIDDEN}\")\n",
    "\n",
    "# Index mappings\n",
    "BOND_TO_IDX = {bt.name: i for i, bt in enumerate(BondType)}\n",
    "IDX_TO_BOND = {i: bt.name for i, bt in enumerate(BondType)}\n",
    "# v10.9: 8 languages (added Sanskrit, Pali, Greek placeholder)\n",
    "LANG_TO_IDX = {\n",
    "    \"hebrew\": 0,\n",
    "    \"aramaic\": 1,\n",
    "    \"classical_chinese\": 2,\n",
    "    \"arabic\": 3,\n",
    "    \"english\": 4,\n",
    "    \"sanskrit\": 5,  # NEW in v10.9\n",
    "    \"pali\": 6,  # NEW in v10.9\n",
    "    \"greek\": 7,  # FUTURE (placeholder)\n",
    "}\n",
    "IDX_TO_LANG = {i: l for l, i in LANG_TO_IDX.items()}\n",
    "\n",
    "# v10.9: 26 periods (expanded Chinese, Arabic, added Sanskrit/Pali traditions)\n",
    "PERIOD_TO_IDX = {\n",
    "    # Semitic traditions\n",
    "    \"BIBLICAL\": 0,\n",
    "    \"TANNAITIC\": 1,\n",
    "    \"AMORAIC\": 2,\n",
    "    \"RISHONIM\": 3,\n",
    "    \"ACHRONIM\": 4,\n",
    "    # Chinese traditions (expanded)\n",
    "    \"CONFUCIAN\": 5,\n",
    "    \"DAOIST\": 6,\n",
    "    \"MOHIST\": 7,  # NEW in v10.9\n",
    "    \"LEGALIST\": 8,  # NEW in v10.9\n",
    "    \"BUDDHIST\": 9,  # NEW in v10.9 (Chinese Buddhism)\n",
    "    \"NEO_CONFUCIAN\": 10,  # NEW in v10.9\n",
    "    # Arabic/Islamic traditions (expanded)\n",
    "    \"QURANIC\": 11,\n",
    "    \"HADITH\": 12,\n",
    "    \"FIQH\": 13,  # NEW in v10.9 (Islamic jurisprudence)\n",
    "    \"SUFI\": 14,  # NEW in v10.9\n",
    "    \"FALSAFA\": 15,  # NEW in v10.9 (Arabic philosophy)\n",
    "    # Sanskrit/Pali traditions (NEW in v10.9)\n",
    "    \"DHARMA\": 16,  # Dharmashastra\n",
    "    \"UPANISHAD\": 17,\n",
    "    \"GITA\": 18,\n",
    "    \"ARTHA\": 19,  # Arthashastra\n",
    "    \"PALI\": 20,  # Pali Canon\n",
    "    # Western traditions\n",
    "    \"WESTERN_CLASSICAL\": 21,\n",
    "    \"MEDIEVAL\": 22,\n",
    "    # Modern\n",
    "    \"DEAR_ABBY\": 23,\n",
    "    \"MODERN\": 24,\n",
    "    \"CLASSICAL\": 25,  # Generic classical (fallback)\n",
    "}  # 26 periods total (0-25)\n",
    "IDX_TO_PERIOD = {i: p for p, i in PERIOD_TO_IDX.items()}\n",
    "HOHFELD_TO_IDX = {hs.name: i for i, hs in enumerate(HohfeldState)}\n",
    "IDX_TO_HOHFELD = {i: hs.name for i, hs in enumerate(HohfeldState)}\n",
    "CONTEXT_TO_IDX = {\"prescriptive\": 0, \"descriptive\": 1, \"unknown\": 2}\n",
    "IDX_TO_CONTEXT = {i: c for c, i in CONTEXT_TO_IDX.items()}\n",
    "\n",
    "\n",
    "def get_confidence_weight(conf):\n",
    "    \"\"\"Map confidence to sample weight. Handles both string ('high'/'medium'/'low') and numeric (0.0-1.0) values.\"\"\"\n",
    "    if isinstance(conf, str):\n",
    "        return {\"high\": 2.0, \"medium\": 1.0, \"low\": 0.5}.get(conf, 1.0)\n",
    "    elif isinstance(conf, (int, float)):\n",
    "        return 2.0 if conf >= 0.8 else 1.0\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "class GradientReversalLayer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.alpha, None\n",
    "\n",
    "\n",
    "class BIPModel(nn.Module):\n",
    "    def __init__(self, model_name=None, hidden_size=None, z_dim=64):\n",
    "        super().__init__()\n",
    "        # Use global config if not specified\n",
    "        model_name = model_name or MODEL_NAME\n",
    "        hidden_size = hidden_size or BACKBONE_HIDDEN\n",
    "\n",
    "        print(f\"  Loading encoder: {model_name}\")\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Get actual hidden size from model config\n",
    "        actual_hidden = self.encoder.config.hidden_size\n",
    "        if actual_hidden != hidden_size:\n",
    "            print(f\"  Note: Using actual hidden size {actual_hidden}\")\n",
    "            hidden_size = actual_hidden\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Projection to z_bond space (scales with backbone size)\n",
    "        proj_hidden = min(512, hidden_size)\n",
    "        self.z_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_size, proj_hidden),\n",
    "            nn.LayerNorm(proj_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(proj_hidden, z_dim),\n",
    "        )\n",
    "\n",
    "        # Task heads\n",
    "        self.bond_head = nn.Linear(z_dim, len(BondType))\n",
    "        self.hohfeld_head = nn.Linear(z_dim, len(HohfeldState))\n",
    "\n",
    "        # Adversarial heads\n",
    "        self.language_head = nn.Linear(z_dim, len(LANG_TO_IDX))\n",
    "        self.period_head = nn.Linear(z_dim, len(PERIOD_TO_IDX))\n",
    "\n",
    "        # Context prediction head (auxiliary task)\n",
    "        self.context_head = nn.Linear(z_dim, len(CONTEXT_TO_IDX))\n",
    "\n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"  Total params: {total_params:,}\")\n",
    "        print(f\"  Trainable: {trainable_params:,}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, adv_lambda=1.0):\n",
    "        enc = self.encoder(input_ids, attention_mask)\n",
    "\n",
    "        # Handle different pooling strategies\n",
    "        if hasattr(enc, \"pooler_output\") and enc.pooler_output is not None:\n",
    "            pooled = enc.pooler_output\n",
    "        else:\n",
    "            pooled = enc.last_hidden_state[:, 0]\n",
    "\n",
    "        z = self.z_proj(pooled)\n",
    "\n",
    "        # Bond prediction (main task)\n",
    "        bond_pred = self.bond_head(z)\n",
    "        hohfeld_pred = self.hohfeld_head(z)\n",
    "\n",
    "        # Adversarial predictions (gradient reversal)\n",
    "        z_rev = GradientReversalLayer.apply(z, adv_lambda)\n",
    "        language_pred = self.language_head(z_rev)\n",
    "        period_pred = self.period_head(z_rev)\n",
    "\n",
    "        return {\n",
    "            \"bond_pred\": bond_pred,\n",
    "            \"hohfeld_pred\": hohfeld_pred,\n",
    "            \"language_pred\": language_pred,\n",
    "            \"period_pred\": period_pred,\n",
    "            \"context_pred\": self.context_head(z),\n",
    "            \"z\": z,\n",
    "        }\n",
    "\n",
    "    def get_bond_embedding(self, input_ids, attention_mask):\n",
    "        \"\"\"Get z_bond embedding for geometric analysis.\"\"\"\n",
    "        enc = self.encoder(input_ids, attention_mask)\n",
    "        if hasattr(enc, \"pooler_output\") and enc.pooler_output is not None:\n",
    "            pooled = enc.pooler_output\n",
    "        else:\n",
    "            pooled = enc.last_hidden_state[:, 0]\n",
    "        return self.z_proj(pooled)\n",
    "\n",
    "\n",
    "# Initialize tokenizer for selected backbone\n",
    "print(f\"\\nLoading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "\n",
    "# Dataset with Hohfeld support\n",
    "class NativeDataset(Dataset):\n",
    "    def __init__(self, ids_set, passages_file, bonds_file, tokenizer, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = []\n",
    "\n",
    "        bonds_by_id = {}\n",
    "        with open(bonds_file) as fb:\n",
    "            for line in fb:\n",
    "                b = json.loads(line)\n",
    "                bonds_by_id[b[\"passage_id\"]] = b\n",
    "\n",
    "        with open(passages_file) as fp:\n",
    "            for line in tqdm(fp, desc=\"Loading\", unit=\"line\"):\n",
    "                p = json.loads(line)\n",
    "                if p[\"id\"] in ids_set and p[\"id\"] in bonds_by_id:\n",
    "                    b = bonds_by_id[p[\"id\"]]\n",
    "                    self.data.append(\n",
    "                        {\n",
    "                            \"text\": p[\"text\"][:1000],\n",
    "                            \"language\": p[\"language\"],\n",
    "                            \"period\": p[\"time_period\"],\n",
    "                            \"bond\": b.get(\"bond_type\") or b.get(\"bonds\", {}).get(\"primary_bond\"),\n",
    "                            \"hohfeld\": None,\n",
    "                            \"context\": b.get(\"context\")\n",
    "                            or b.get(\"bonds\", {}).get(\"context\", \"unknown\"),\n",
    "                            \"confidence\": b.get(\"confidence\")\n",
    "                            or b.get(\"bonds\", {}).get(\"confidence\", \"medium\"),\n",
    "                        }\n",
    "                    )\n",
    "        print(f\"  Loaded {len(self.data):,} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        enc = self.tokenizer(\n",
    "            item[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"bond_label\": BOND_TO_IDX.get(item[\"bond\"], 9),\n",
    "            \"language_label\": LANG_TO_IDX.get(item[\"language\"], 4),\n",
    "            \"period_label\": PERIOD_TO_IDX.get(item[\"period\"], 9),\n",
    "            \"hohfeld_label\": HOHFELD_TO_IDX.get(item[\"hohfeld\"], 0) if item[\"hohfeld\"] else 0,\n",
    "            \"context_label\": CONTEXT_TO_IDX.get(item[\"context\"], 2),\n",
    "            \"sample_weight\": get_confidence_weight(item[\"confidence\"]),\n",
    "            \"language\": item[\"language\"],\n",
    "            \"context\": item[\"context\"],\n",
    "            \"confidence\": item[\"confidence\"],\n",
    "            \"text\": item[\"text\"],  # Raw text for role augmentation\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
    "        \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n",
    "        \"bond_labels\": torch.tensor([x[\"bond_label\"] for x in batch]),\n",
    "        \"language_labels\": torch.tensor([x[\"language_label\"] for x in batch]),\n",
    "        \"period_labels\": torch.tensor([x[\"period_label\"] for x in batch]),\n",
    "        \"hohfeld_labels\": torch.tensor([x[\"hohfeld_label\"] for x in batch]),\n",
    "        \"context_labels\": torch.tensor([x[\"context_label\"] for x in batch]),\n",
    "        \"sample_weights\": torch.tensor([x[\"sample_weight\"] for x in batch], dtype=torch.float),\n",
    "        \"languages\": [x[\"language\"] for x in batch],\n",
    "        \"contexts\": [x[\"context\"] for x in batch],\n",
    "        \"confidences\": [x[\"confidence\"] for x in batch],\n",
    "        \"texts\": [x[\"text\"] for x in batch],  # v10.10: raw texts for role augmentation\n",
    "    }\n",
    "\n",
    "\n",
    "print(f\"\\nArchitecture ready for {BACKBONE}\")\n",
    "print(f\"  Bond classes: {len(BondType)}\")\n",
    "print(f\"  Languages: {len(LANG_TO_IDX)}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 7. Train BIP Model { display-mode: \"form\" }\n",
    "# @markdown Training with tuned adversarial weights and hardware-optimized parameters\n",
    "# @markdown v10.9: Added new splits (confucian_to_buddhist, all_to_sanskrit, etc.)\n",
    "\n",
    "# ===== SUPPRESS DATALOADER MULTIPROCESSING WARNINGS =====\n",
    "# These occur during garbage collection and bypass normal exception handling\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Method 1: Filter warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*can only test a child process.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.data\")\n",
    "\n",
    "# Method 2: Suppress logging\n",
    "logging.getLogger(\"torch.utils.data.dataloader\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "# Method 3: Redirect stderr during DataLoader cleanup (most effective)\n",
    "class StderrFilter(io.TextIOWrapper):\n",
    "    \"\"\"Filters out DataLoader multiprocessing cleanup messages from stderr\"\"\"\n",
    "\n",
    "    def __init__(self, original):\n",
    "        self.original = original\n",
    "        self.buffer_lines = []\n",
    "\n",
    "    def write(self, text):\n",
    "        # Filter out the specific error patterns\n",
    "        skip_patterns = [\n",
    "            \"can only test a child process\",\n",
    "            \"_MultiProcessingDataLoaderIter.__del__\",\n",
    "            \"_shutdown_workers\",\n",
    "            \"Exception ignored in:\",\n",
    "            \"w.is_alive()\",\n",
    "        ]\n",
    "        # Buffer multi-line error messages\n",
    "        if any(p in text for p in skip_patterns):\n",
    "            return len(text)  # Pretend we wrote it\n",
    "        # Also skip if it looks like part of a traceback for these errors\n",
    "        if text.strip().startswith(\"^\") and len(text.strip()) < 80:\n",
    "            return len(text)\n",
    "        if text.strip().startswith('File \"/usr') and \"dataloader.py\" in text:\n",
    "            return len(text)\n",
    "        if text.strip() == \"Traceback (most recent call last):\":\n",
    "            self.buffer_lines = [text]\n",
    "            return len(text)\n",
    "        if self.buffer_lines:\n",
    "            self.buffer_lines.append(text)\n",
    "            # Check if this is the DataLoader error traceback\n",
    "            full_msg = \"\".join(self.buffer_lines)\n",
    "            if any(p in full_msg for p in skip_patterns):\n",
    "                self.buffer_lines = []\n",
    "                return len(text)\n",
    "            # After 10 lines, flush if not the target error\n",
    "            if len(self.buffer_lines) > 10:\n",
    "                for line in self.buffer_lines:\n",
    "                    self.original.write(line)\n",
    "                self.buffer_lines = []\n",
    "        return self.original.write(text)\n",
    "\n",
    "    def flush(self):\n",
    "        if self.buffer_lines:\n",
    "            # Flush any remaining buffered content\n",
    "            for line in self.buffer_lines:\n",
    "                self.original.write(line)\n",
    "            self.buffer_lines = []\n",
    "        self.original.flush()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.original, name)\n",
    "\n",
    "\n",
    "# Install the stderr filter\n",
    "_original_stderr = sys.stderr\n",
    "sys.stderr = StderrFilter(_original_stderr)\n",
    "\n",
    "# Method 4: Patch the DataLoader cleanup function directly\n",
    "try:\n",
    "    import torch.utils.data.dataloader as dl_module\n",
    "\n",
    "    _original_del = dl_module._MultiProcessingDataLoaderIter.__del__\n",
    "\n",
    "    def _patched_del(self):\n",
    "        try:\n",
    "            _original_del(self)\n",
    "        except (AssertionError, AttributeError, RuntimeError):\n",
    "            pass  # Silently ignore cleanup errors\n",
    "\n",
    "    dl_module._MultiProcessingDataLoaderIter.__del__ = _patched_del\n",
    "except Exception:\n",
    "    pass  # If patching fails, the stderr filter will still work\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import gc\n",
    "\n",
    "\n",
    "# @markdown **Splits to train:**\n",
    "TRAIN_HEBREW_TO_OTHERS = True  # @param {type:\"boolean\"}\n",
    "TRAIN_SEMITIC_TO_NON_SEMITIC = True  # @param {type:\"boolean\"}\n",
    "TRAIN_ANCIENT_TO_MODERN = True  # @param {type:\"boolean\"}\n",
    "TRAIN_MIXED_BASELINE = True  # @param {type:\"boolean\"}\n",
    "TRAIN_ABBY_TO_CHINESE = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown **v10.9 New Splits:**\n",
    "TRAIN_CONFUCIAN_TO_BUDDHIST = True  # @param {type:\"boolean\"}\n",
    "TRAIN_CONFUCIAN_TO_LEGALIST = True  # @param {type:\"boolean\"}\n",
    "TRAIN_ALL_TO_SANSKRIT = True  # @param {type:\"boolean\"}\n",
    "TRAIN_SEMITIC_TO_INDIC = True  # @param {type:\"boolean\"}\n",
    "TRAIN_QURAN_TO_FIQH = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown **Hyperparameters:**\n",
    "LANG_WEIGHT = 0.1  # @param {type:\"number\"}\n",
    "PERIOD_WEIGHT = 0.066  # @param {type:\"number\"}\n",
    "N_EPOCHS = 10  # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown **Context-Aware Training:**\n",
    "USE_CONFIDENCE_WEIGHTING = True  # @param {type:\"boolean\"}\n",
    "# @markdown Weight prescriptive (high confidence) examples 2x in loss\n",
    "\n",
    "USE_CONTEXT_AUXILIARY = True  # @param {type:\"boolean\"}\n",
    "# @markdown Add context prediction as auxiliary training target\n",
    "\n",
    "CONTEXT_LOSS_WEIGHT = 0.33  # @param {type:\"number\"}\n",
    "# @markdown Weight for context prediction loss\n",
    "\n",
    "STRICT_PRESCRIPTIVE_TEST = False  # @param {type:\"boolean\"}\n",
    "# @markdown Only evaluate on prescriptive examples (reduces test set ~97%!)\n",
    "\n",
    "# @markdown **v10.10: Role-Aware Data Augmentation:**\n",
    "USE_ROLE_AUGMENTATION = True  # @param {type:\"boolean\"}\n",
    "# @markdown Adds contrastive loss for agent/patient role sensitivity\n",
    "ROLE_AUGMENT_PROB = 0.3  # @param {type:\"number\"}\n",
    "# @markdown Probability of augmenting each batch\n",
    "ROLE_CONTRASTIVE_WEIGHT = 0.2  # @param {type:\"number\"}\n",
    "# @markdown Weight for role contrastive loss\n",
    "ROLE_CONTRASTIVE_MARGIN = 0.5  # @param {type:\"number\"}\n",
    "# @markdown Minimum embedding distance for role-swapped pairs\n",
    "\n",
    "\n",
    "def swap_roles_simple(text, language):\n",
    "    \"\"\"Simple role swap using word order reversal for common patterns.\n",
    "    v10.10: Addresses weak role_swap sensitivity (0.003) from fuzz testing.\"\"\"\n",
    "    patterns = {\n",
    "        \"english\": [\n",
    "            (r\"(\\w+) must (\\w+) (\\w+)\", r\"\\3 must \\2 \\1\"),\n",
    "            (r\"(\\w+) should (\\w+) (\\w+)\", r\"\\3 should \\2 \\1\"),\n",
    "            (r\"(\\w+) shall (\\w+) (\\w+)\", r\"\\3 shall \\2 \\1\"),\n",
    "            (r\"the (\\w+) must (\\w+) the (\\w+)\", r\"the \\3 must \\2 the \\1\"),\n",
    "            (r\"(\\w+) is obligated to (\\w+) (\\w+)\", r\"\\3 is obligated to \\2 \\1\"),\n",
    "            (r\"(\\w+) has a duty to (\\w+) (\\w+)\", r\"\\3 has a duty to \\2 \\1\"),\n",
    "        ],\n",
    "        \"hebrew\": [\n",
    "            (r\"על (\\S+) ל(\\S+) את (\\S+)\", r\"על \\3 ל\\2 את \\1\"),\n",
    "        ],\n",
    "        \"classical_chinese\": [\n",
    "            (r\"(\\S)當(\\S)(\\S)\", r\"\\3當\\2\\1\"),\n",
    "            (r\"(\\S)須(\\S)(\\S)\", r\"\\3須\\2\\1\"),\n",
    "            (r\"(\\S)應(\\S)(\\S)\", r\"\\3應\\2\\1\"),\n",
    "        ],\n",
    "        \"arabic\": [\n",
    "            (r\"يجب على (\\S+) أن (\\S+) (\\S+)\", r\"يجب على \\3 أن \\2 \\1\"),\n",
    "            (r\"(\\S+) عليه أن (\\S+) (\\S+)\", r\"\\3 عليه أن \\2 \\1\"),\n",
    "        ],\n",
    "        \"sanskrit\": [\n",
    "            (r\"(\\S+)ः (\\S+)म् (\\S+)ति\", r\"\\3ः \\2म् \\1ति\"),\n",
    "        ],\n",
    "        \"pali\": [\n",
    "            (r\"(\\S+)o (\\S+)aṃ (\\S+)ti\", r\"\\3o \\2aṃ \\1ti\"),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    lang_patterns = patterns.get(language, patterns[\"english\"])\n",
    "    for pattern, replacement in lang_patterns:\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            swapped = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "            if swapped != text:\n",
    "                return swapped\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING BIP MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSettings:\")\n",
    "print(f\"  Backbone:     {BACKBONE}\")\n",
    "print(f\"  GPU Tier:     {GPU_TIER}\")\n",
    "print(f\"  Batch size:   {BATCH_SIZE}\")\n",
    "print(f\"  Workers:      {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate: {LR:.2e}\")\n",
    "print(f\"  Adv weights:  lang={LANG_WEIGHT}, period={PERIOD_WEIGHT}\")\n",
    "print(\"(0.01 prevents loss explosion while maintaining invariance)\")\n",
    "print(f\"  Confidence weighting: {USE_CONFIDENCE_WEIGHTING}\")\n",
    "print(f\"  Context auxiliary: {USE_CONTEXT_AUXILIARY} (weight={CONTEXT_LOSS_WEIGHT})\")\n",
    "print(f\"  Strict prescriptive test: {STRICT_PRESCRIPTIVE_TEST}\")\n",
    "print(f\"  Role augmentation: {USE_ROLE_AUGMENTATION} (prob={ROLE_AUGMENT_PROB}, weight={ROLE_CONTRASTIVE_WEIGHT})\")\n",
    "\n",
    "# tokenizer loaded in Cell 6 based on BACKBONE selection\n",
    "\n",
    "with open(\"data/splits/all_splits.json\") as f:\n",
    "    all_splits = json.load(f)\n",
    "\n",
    "splits_to_train = []\n",
    "if TRAIN_HEBREW_TO_OTHERS:\n",
    "    splits_to_train.append(\"hebrew_to_others\")\n",
    "if TRAIN_SEMITIC_TO_NON_SEMITIC:\n",
    "    splits_to_train.append(\"semitic_to_non_semitic\")\n",
    "if TRAIN_ANCIENT_TO_MODERN:\n",
    "    splits_to_train.append(\"ancient_to_modern\")\n",
    "if TRAIN_MIXED_BASELINE:\n",
    "    splits_to_train.append(\"mixed_baseline\")\n",
    "if TRAIN_ABBY_TO_CHINESE:\n",
    "    splits_to_train.append(\"abby_to_chinese\")\n",
    "# v10.9 new splits\n",
    "if TRAIN_CONFUCIAN_TO_BUDDHIST:\n",
    "    splits_to_train.append(\"confucian_to_buddhist\")\n",
    "if TRAIN_CONFUCIAN_TO_LEGALIST:\n",
    "    splits_to_train.append(\"confucian_to_legalist\")\n",
    "if TRAIN_ALL_TO_SANSKRIT:\n",
    "    splits_to_train.append(\"all_to_sanskrit\")\n",
    "if TRAIN_SEMITIC_TO_INDIC:\n",
    "    splits_to_train.append(\"semitic_to_indic\")\n",
    "if TRAIN_QURAN_TO_FIQH:\n",
    "    splits_to_train.append(\"quran_to_fiqh\")\n",
    "\n",
    "print(f\"\\nTraining {len(splits_to_train)} splits: {splits_to_train}\")\n",
    "\n",
    "all_results = {}\n",
    "MIN_TEST_SIZE = 100  # Lowered to allow smaller test sets like Chinese\n",
    "\n",
    "for split_idx, split_name in enumerate(splits_to_train):\n",
    "    split_start = time.time()\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"[{split_idx+1}/{len(splits_to_train)}] {split_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    split = all_splits[split_name]\n",
    "    print(f\"Train: {split['train_size']:,} | Test: {split['test_size']:,}\")\n",
    "\n",
    "    if split[\"test_size\"] < MIN_TEST_SIZE:\n",
    "        print(f\"WARNING: Test set only {split['test_size']} samples (need {MIN_TEST_SIZE})\")\n",
    "        print(\"Skipping this split - results would be unreliable\")\n",
    "        print(\"To fix: Add more data to the test languages/periods\")\n",
    "        continue\n",
    "\n",
    "    # Create model with OOM recovery\n",
    "    def create_model_with_retry():\n",
    "        \"\"\"Create model, cleaning up GPU memory if OOM occurs.\"\"\"\n",
    "        try:\n",
    "            return BIPModel().to(device)\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(\"  OOM on model creation - cleaning up and retrying...\")\n",
    "            # Clean up any existing model in globals\n",
    "            _g = globals()\n",
    "            for _var in [\"model\", \"analyzer\", \"encoder\"]:\n",
    "                if _var in _g and _g[_var] is not None:\n",
    "                    try:\n",
    "                        if hasattr(_g[_var], \"cpu\"):\n",
    "                            _g[_var].cpu()\n",
    "                        _g[_var] = None\n",
    "                    except:\n",
    "                        pass\n",
    "            # Force cleanup\n",
    "            gc.collect()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            # Retry\n",
    "            return BIPModel().to(device)\n",
    "\n",
    "    model = create_model_with_retry()\n",
    "\n",
    "    train_dataset = NativeDataset(\n",
    "        set(split[\"train_ids\"]),\n",
    "        \"data/processed/passages.jsonl\",\n",
    "        \"data/processed/bonds.jsonl\",\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    test_ids_to_use = split[\"test_ids\"][:MAX_TEST_SAMPLES]\n",
    "\n",
    "    # Optional: strict prescriptive-only test\n",
    "    if STRICT_PRESCRIPTIVE_TEST:\n",
    "        print(\"Filtering to prescriptive examples only...\")\n",
    "        # Load bonds to filter\n",
    "        prescriptive_ids = set()\n",
    "        with open(\"data/processed/bonds.jsonl\") as f:\n",
    "            for line in f:\n",
    "                b = json.loads(line)\n",
    "                if b.get(\"context\") == \"prescriptive\":\n",
    "                    prescriptive_ids.add(b[\"passage_id\"])\n",
    "        test_ids_to_use = [tid for tid in test_ids_to_use if tid in prescriptive_ids]\n",
    "        print(f\"  Filtered to {len(test_ids_to_use):,} prescriptive samples\")\n",
    "\n",
    "    test_dataset = NativeDataset(\n",
    "        set(test_ids_to_use),\n",
    "        \"data/processed/passages.jsonl\",\n",
    "        \"data/processed/bonds.jsonl\",\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"ERROR: No training data!\")\n",
    "        continue\n",
    "\n",
    "    # Use hardware-optimized batch size\n",
    "    actual_batch = min(BATCH_SIZE, max(32, len(train_dataset) // 20))\n",
    "    print(f\"Actual batch size: {actual_batch}\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=actual_batch,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=actual_batch * 2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "    def get_adv_lambda(epoch, warmup=3):\n",
    "        \"\"\"Ramp adversarial strength: 0.1 -> 1.0 over warmup, then hold at 1.0\"\"\"\n",
    "        if epoch <= warmup:\n",
    "            return 0.1 + 0.9 * (epoch / warmup)\n",
    "        return 1.0\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    start_epoch = 1\n",
    "\n",
    "    # Check for existing checkpoint to resume from\n",
    "    checkpoint_path = f\"models/checkpoints/latest_{split_name}.pt\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"  Found checkpoint, resuming...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        best_loss = checkpoint[\"best_loss\"]\n",
    "        print(f\"  Resuming from epoch {start_epoch}, best_loss={best_loss:.4f}\")\n",
    "\n",
    "    for epoch in range(start_epoch, N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            bond_labels = batch[\"bond_labels\"].to(device)\n",
    "            language_labels = batch[\"language_labels\"].to(device)\n",
    "            period_labels = batch[\"period_labels\"].to(device)\n",
    "\n",
    "            adv_lambda = get_adv_lambda(epoch)\n",
    "\n",
    "            # Use new autocast API\n",
    "            with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n",
    "                out = model(input_ids, attention_mask, adv_lambda=adv_lambda)\n",
    "\n",
    "                # Weighted bond loss\n",
    "                if USE_CONFIDENCE_WEIGHTING:\n",
    "                    sample_weights = batch[\"sample_weights\"].to(device)\n",
    "                    loss_bond = F.cross_entropy(out[\"bond_pred\"], bond_labels, reduction=\"none\")\n",
    "                    loss_bond = (loss_bond * sample_weights).mean()\n",
    "                else:\n",
    "                    loss_bond = F.cross_entropy(out[\"bond_pred\"], bond_labels)\n",
    "\n",
    "                # Context auxiliary loss\n",
    "                if USE_CONTEXT_AUXILIARY:\n",
    "                    context_labels = batch[\"context_labels\"].to(device)\n",
    "                    loss_context = F.cross_entropy(out[\"context_pred\"], context_labels)\n",
    "                else:\n",
    "                    loss_context = 0\n",
    "\n",
    "                loss_lang = F.cross_entropy(out[\"language_pred\"], language_labels)\n",
    "                loss_period = F.cross_entropy(out[\"period_pred\"], period_labels)\n",
    "\n",
    "            loss = (\n",
    "                loss_bond\n",
    "                + LANG_WEIGHT * loss_lang\n",
    "                + PERIOD_WEIGHT * loss_period\n",
    "                + CONTEXT_LOSS_WEIGHT * loss_context\n",
    "            )\n",
    "\n",
    "            # v10.10: Role contrastive loss for agent/patient sensitivity\n",
    "            loss_role = torch.tensor(0.0, device=device)\n",
    "            if USE_ROLE_AUGMENTATION and random.random() < ROLE_AUGMENT_PROB:\n",
    "                batch_texts = batch.get(\"texts\", [])\n",
    "                batch_languages = batch.get(\"languages\", [])\n",
    "\n",
    "                swapped_texts = []\n",
    "                original_indices = []\n",
    "\n",
    "                for i, (text, lang) in enumerate(zip(batch_texts, batch_languages)):\n",
    "                    swapped = swap_roles_simple(text, lang)\n",
    "                    if swapped:\n",
    "                        swapped_texts.append(swapped)\n",
    "                        original_indices.append(i)\n",
    "\n",
    "                if swapped_texts and len(swapped_texts) >= 2:\n",
    "                    # Tokenize swapped texts\n",
    "                    swapped_encoded = tokenizer(\n",
    "                        swapped_texts,\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=128,\n",
    "                        return_tensors=\"pt\",\n",
    "                    )\n",
    "                    swapped_ids = swapped_encoded[\"input_ids\"].to(device)\n",
    "                    swapped_mask = swapped_encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "                    # Get embeddings for swapped texts (no adversarial)\n",
    "                    swapped_out = model(swapped_ids, swapped_mask, adv_lambda=0)\n",
    "\n",
    "                    # Get original embeddings for corresponding indices\n",
    "                    z_original = out[\"z\"][original_indices]\n",
    "                    z_swapped = swapped_out[\"z\"]\n",
    "\n",
    "                    # Contrastive loss: push role-swapped embeddings apart\n",
    "                    # Hinge loss: max(0, margin - distance)\n",
    "                    distances = F.pairwise_distance(z_original, z_swapped)\n",
    "                    loss_role = F.relu(ROLE_CONTRASTIVE_MARGIN - distances).mean()\n",
    "\n",
    "                    # Clean up\n",
    "                    del swapped_ids, swapped_mask, swapped_out, z_original, z_swapped, distances\n",
    "\n",
    "            loss = loss + ROLE_CONTRASTIVE_WEIGHT * loss_role\n",
    "\n",
    "            if USE_AMP and scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "            # Delete intermediate tensors to prevent memory accumulation\n",
    "            del input_ids, attention_mask, bond_labels, language_labels, period_labels\n",
    "            del out, loss, loss_bond, loss_lang, loss_period\n",
    "            if USE_CONFIDENCE_WEIGHTING:\n",
    "                del sample_weights\n",
    "            if USE_CONTEXT_AUXILIARY:\n",
    "                del context_labels, loss_context\n",
    "            if USE_ROLE_AUGMENTATION:\n",
    "                del loss_role\n",
    "\n",
    "        avg_loss = total_loss / n_batches\n",
    "\n",
    "        # Aggressive memory cleanup after each epoch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            mem_alloc = torch.cuda.memory_allocated() / 1e9\n",
    "            mem_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "            print(\n",
    "                f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f}) [GPU: {mem_alloc:.1f}GB alloc, {mem_reserved:.1f}GB reserved]\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f})\")\n",
    "\n",
    "        # Save checkpoint every epoch (for crash recovery)\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"loss\": avg_loss,\n",
    "            \"best_loss\": best_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"models/checkpoints/latest_{split_name}.pt\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), f\"models/checkpoints/best_{split_name}.pt\")\n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/best_{split_name}.pt\")\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.load_state_dict(torch.load(f\"models/checkpoints/best_{split_name}.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = {\"bond\": [], \"lang\": []}\n",
    "    all_labels = {\"bond\": [], \"lang\": []}\n",
    "    all_languages = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            out = model(batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), 0)\n",
    "            all_preds[\"bond\"].extend(out[\"bond_pred\"].argmax(-1).cpu().tolist())\n",
    "            all_preds[\"lang\"].extend(out[\"language_pred\"].argmax(-1).cpu().tolist())\n",
    "            all_labels[\"bond\"].extend(batch[\"bond_labels\"].tolist())\n",
    "            all_labels[\"lang\"].extend(batch[\"language_labels\"].tolist())\n",
    "            all_languages.extend(batch[\"languages\"])\n",
    "\n",
    "    bond_f1 = f1_score(all_labels[\"bond\"], all_preds[\"bond\"], average=\"macro\", zero_division=0)\n",
    "    bond_acc = sum(p == l for p, l in zip(all_preds[\"bond\"], all_labels[\"bond\"])) / len(\n",
    "        all_preds[\"bond\"]\n",
    "    )\n",
    "    lang_acc = sum(p == l for p, l in zip(all_preds[\"lang\"], all_labels[\"lang\"])) / len(\n",
    "        all_preds[\"lang\"]\n",
    "    )\n",
    "\n",
    "    # Per-language F1\n",
    "    lang_f1 = {}\n",
    "    for lang in set(all_languages):\n",
    "        mask = [l == lang for l in all_languages]\n",
    "        if sum(mask) > 10:\n",
    "            preds = [p for p, m in zip(all_preds[\"bond\"], mask) if m]\n",
    "            labels = [l for l, m in zip(all_labels[\"bond\"], mask) if m]\n",
    "            lang_f1[lang] = {\n",
    "                \"f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "                \"n\": sum(mask),\n",
    "            }\n",
    "\n",
    "    all_results[split_name] = {\n",
    "        \"bond_f1_macro\": bond_f1,\n",
    "        \"bond_acc\": bond_acc,\n",
    "        \"language_acc\": lang_acc,\n",
    "        \"per_language_f1\": lang_f1,\n",
    "        \"training_time\": time.time() - split_start,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{split_name} RESULTS:\")\n",
    "    print(f\"  Bond F1 (macro): {bond_f1:.3f} ({bond_f1/0.1:.1f}x chance)\")\n",
    "    print(f\"  Bond accuracy:   {bond_acc:.1%}\")\n",
    "    print(f\"  Language acc:    {lang_acc:.1%} (want ~20% = invariant)\")\n",
    "    print(\"  Per-language:\")\n",
    "    for lang, m in sorted(lang_f1.items(), key=lambda x: -x[1][\"n\"]):\n",
    "        print(f\"    {lang:20s}: F1={m['f1']:.3f} (n={m['n']:,})\")\n",
    "\n",
    "    # Context analysis\n",
    "    high_conf = sum(1 for c in test_dataset.data if c[\"confidence\"] == \"high\")\n",
    "    prescriptive = sum(1 for c in test_dataset.data if c[\"context\"] == \"prescriptive\")\n",
    "    print(\n",
    "        f\"  Context: {prescriptive:,}/{len(test_dataset):,} prescriptive ({prescriptive/len(test_dataset)*100:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  High confidence: {high_conf:,}/{len(test_dataset):,} ({high_conf/len(test_dataset)*100:.1f}%)\"\n",
    "    )\n",
    "\n",
    "    # GPU memory usage before cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        mem = torch.cuda.memory_allocated() / 1e9\n",
    "        print(\n",
    "            f\"\\n  GPU memory (before cleanup): {mem:.1f} GB / {VRAM_GB:.1f} GB ({mem/VRAM_GB*100:.0f}%)\"\n",
    "        )\n",
    "\n",
    "    # Aggressive memory cleanup between splits\n",
    "    # Step 1: Move model to CPU to release GPU memory\n",
    "    model.cpu()\n",
    "\n",
    "    # Step 2: Delete all references\n",
    "    del model, train_dataset, test_dataset, train_loader, test_loader, optimizer\n",
    "    if USE_AMP and scaler:\n",
    "        del scaler\n",
    "\n",
    "    # Step 3: Force garbage collection (multiple passes)\n",
    "    for _ in range(3):\n",
    "        gc.collect()\n",
    "\n",
    "    # Step 4: Clear CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Step 5: Re-create scaler for next split\n",
    "    if USE_AMP:\n",
    "        scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "    # GPU memory after cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"  GPU memory (after cleanup): {mem_after:.1f} GB (freed {mem - mem_after:.1f} GB)\")\n",
    "        if mem_after > 1.0:\n",
    "            print(f\"  WARNING: {mem_after:.1f} GB still allocated - may cause OOM on next split\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 8. Geometric Analysis & Linear Probe { display-mode: \"form\" }\n",
    "# @markdown v10.9: New geometric analysis module + linear probe test\n",
    "# @markdown Tests latent space structure (axis discovery, role swap analysis)\n",
    "# @markdown Tests if z_bond encodes language/period (should be low = invariant)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "# ===== v10.9: GEOMETRIC ANALYZER CLASS =====\n",
    "class GeometricAnalyzer:\n",
    "    \"\"\"\n",
    "    Probe the latent space geometry to discover moral structure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        inputs = self.tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, max_length=128, padding=\"max_length\"\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        z = self.model.get_bond_embedding(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        return z.cpu().numpy().flatten()\n",
    "\n",
    "    def find_direction(self, positive_texts: List[str], negative_texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Find the direction in z-space that separates two concepts.\n",
    "        E.g., obligation vs permission, harm vs care.\n",
    "        \"\"\"\n",
    "        pos_embs = np.array([self.get_embedding(t) for t in positive_texts])\n",
    "        neg_embs = np.array([self.get_embedding(t) for t in negative_texts])\n",
    "\n",
    "        pos_mean = pos_embs.mean(axis=0)\n",
    "        neg_mean = neg_embs.mean(axis=0)\n",
    "\n",
    "        direction = pos_mean - neg_mean\n",
    "        direction = direction / (np.linalg.norm(direction) + 1e-9)\n",
    "        return direction\n",
    "\n",
    "    def test_direction_transfer(\n",
    "        self, direction: np.ndarray, test_pairs: List[Tuple[str, str]]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Test if a direction generalizes to new examples.\n",
    "        Returns accuracy of direction-based classification.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for pos_text, neg_text in test_pairs:\n",
    "            pos_proj = np.dot(self.get_embedding(pos_text), direction)\n",
    "            neg_proj = np.dot(self.get_embedding(neg_text), direction)\n",
    "            scores.append(1.0 if pos_proj > neg_proj else 0.0)\n",
    "        return np.mean(scores)\n",
    "\n",
    "    def pca_on_pairs(self, concept_pairs: Dict[str, List[Tuple[str, str]]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Run PCA on difference vectors to find dominant axes.\n",
    "\n",
    "        concept_pairs: {\"obligation_permission\": [(obl1, perm1), ...], ...}\n",
    "        \"\"\"\n",
    "        all_diffs = []\n",
    "        labels = []\n",
    "\n",
    "        for concept, pairs in concept_pairs.items():\n",
    "            for pos, neg in pairs:\n",
    "                diff = self.get_embedding(pos) - self.get_embedding(neg)\n",
    "                all_diffs.append(diff)\n",
    "                labels.append(concept)\n",
    "\n",
    "        X = np.array(all_diffs)\n",
    "\n",
    "        pca = PCA(n_components=min(10, len(X)))\n",
    "        pca.fit(X)\n",
    "\n",
    "        return {\n",
    "            \"components\": pca.components_,\n",
    "            \"explained_variance_ratio\": pca.explained_variance_ratio_,\n",
    "            \"labels\": labels,\n",
    "            \"transformed\": pca.transform(X),\n",
    "        }\n",
    "\n",
    "    def role_swap_analysis(self, agent_patient_pairs: List[Tuple[str, str]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Test if swapping agent/patient produces consistent transformation.\n",
    "\n",
    "        agent_patient_pairs: [(\"A harmed B\", \"B harmed A\"), ...]\n",
    "        \"\"\"\n",
    "        transformations = []\n",
    "\n",
    "        for original, swapped in agent_patient_pairs:\n",
    "            orig_emb = self.get_embedding(original)\n",
    "            swap_emb = self.get_embedding(swapped)\n",
    "            transformations.append(swap_emb - orig_emb)\n",
    "\n",
    "        T = np.array(transformations)\n",
    "\n",
    "        # Check consistency: are all transformations similar?\n",
    "        mean_transform = T.mean(axis=0)\n",
    "        cosines = [\n",
    "            np.dot(t, mean_transform) / (np.linalg.norm(t) * np.linalg.norm(mean_transform) + 1e-9)\n",
    "            for t in T\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"mean_transform\": mean_transform,\n",
    "            \"consistency\": np.mean(cosines),\n",
    "            \"consistency_std\": np.std(cosines),\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LINEAR PROBE TEST\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nIf probe accuracy is NEAR CHANCE, representation is INVARIANT\")\n",
    "print(\"(This is what we want for BIP)\")\n",
    "\n",
    "probe_results = {}\n",
    "\n",
    "for split_name in [\"hebrew_to_others\", \"semitic_to_non_semitic\"]:\n",
    "    model_path = f\"{SAVE_DIR}/best_{split_name}.pt\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"\\nSkipping {split_name} - no saved model\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PROBE: {split_name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model = BIPModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    test_ids = set(all_splits[split_name][\"test_ids\"][:5000])\n",
    "    test_dataset = NativeDataset(\n",
    "        test_ids, \"data/processed/passages.jsonl\", \"data/processed/bonds.jsonl\", tokenizer\n",
    "    )\n",
    "\n",
    "    if len(test_dataset) < 50:\n",
    "        print(f\"  Skip - only {len(test_dataset)} samples\")\n",
    "        continue\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "    all_z, all_lang, all_period = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Extract\"):\n",
    "            out = model(batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), 0)\n",
    "            all_z.append(out[\"z\"].cpu().numpy())\n",
    "            all_lang.extend(batch[\"language_labels\"].tolist())\n",
    "            all_period.extend(batch[\"period_labels\"].tolist())\n",
    "\n",
    "    X = np.vstack(all_z)\n",
    "    y_lang = np.array(all_lang)\n",
    "    y_period = np.array(all_period)\n",
    "\n",
    "    scaler_probe = StandardScaler()\n",
    "    X_scaled = scaler_probe.fit_transform(X)\n",
    "\n",
    "    # Train/test split for probes\n",
    "    n = len(X)\n",
    "    idx = np.random.permutation(n)\n",
    "    train_idx, test_idx = idx[: int(0.7 * n)], idx[int(0.7 * n) :]\n",
    "\n",
    "    # Language probe - check for multiple classes\n",
    "    unique_langs = np.unique(y_lang[test_idx])\n",
    "    if len(unique_langs) < 2:\n",
    "        print(f\"  SKIP language probe - only {len(unique_langs)} class\")\n",
    "        lang_acc = 1.0 / max(1, len(np.unique(y_lang)))\n",
    "        lang_chance = lang_acc\n",
    "    else:\n",
    "        lang_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        lang_probe.fit(X_scaled[train_idx], y_lang[train_idx])\n",
    "        lang_acc = (lang_probe.predict(X_scaled[test_idx]) == y_lang[test_idx]).mean()\n",
    "        lang_chance = 1.0 / len(unique_langs)\n",
    "\n",
    "    # Period probe - same check\n",
    "    unique_periods = np.unique(y_period[test_idx])\n",
    "    if len(unique_periods) < 2:\n",
    "        print(f\"  SKIP period probe - only {len(unique_periods)} class\")\n",
    "        period_acc = 1.0 / max(1, len(np.unique(y_period)))\n",
    "        period_chance = period_acc\n",
    "    else:\n",
    "        period_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        period_probe.fit(X_scaled[train_idx], y_period[train_idx])\n",
    "        period_acc = (period_probe.predict(X_scaled[test_idx]) == y_period[test_idx]).mean()\n",
    "        period_chance = 1.0 / len(unique_periods)\n",
    "\n",
    "    lang_status = \"INVARIANT\" if lang_acc < lang_chance + 0.15 else \"NOT invariant\"\n",
    "    period_status = \"INVARIANT\" if period_acc < period_chance + 0.15 else \"NOT invariant\"\n",
    "\n",
    "    probe_results[split_name] = {\n",
    "        \"language_acc\": lang_acc,\n",
    "        \"language_chance\": lang_chance,\n",
    "        \"language_status\": lang_status,\n",
    "        \"period_acc\": period_acc,\n",
    "        \"period_chance\": period_chance,\n",
    "        \"period_status\": period_status,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nRESULTS:\")\n",
    "    print(f\"  Language: {lang_acc:.1%} (chance: {lang_chance:.1%}) -> {lang_status}\")\n",
    "    print(f\"  Period:   {period_acc:.1%} (chance: {period_chance:.1%}) -> {period_status}\")\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Probe tests complete\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===== v10.9: GEOMETRIC ANALYSIS =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GEOMETRIC ANALYSIS (v10.9)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nDiscovering interpretable axes in latent space...\")\n",
    "\n",
    "# Test pairs for axis discovery (cross-lingual)\n",
    "OBLIGATION_PERMISSION_TRAIN = [\n",
    "    # English - training set\n",
    "    (\"You must help the elderly\", \"You may help the elderly\"),\n",
    "    (\"He is required to pay\", \"He is allowed to pay\"),\n",
    "    (\"Parents must protect children\", \"Parents may protect children\"),\n",
    "]\n",
    "\n",
    "OBLIGATION_PERMISSION_TEST = [\n",
    "    # Chinese\n",
    "    (\"君子必孝\", \"君子可孝\"),  # Gentleman must/may be filial\n",
    "    (\"民必從法\", \"民可從法\"),  # People must/may follow law\n",
    "    # Arabic\n",
    "    (\"يجب عليك أن تساعد\", \"يجوز لك أن تساعد\"),  # You must/may help\n",
    "    # Hebrew\n",
    "    (\"חייב לכבד\", \"מותר לכבד\"),  # Obligated/permitted to honor\n",
    "    # English - held out\n",
    "    (\"She must attend\", \"She may attend\"),\n",
    "]\n",
    "\n",
    "HARM_CARE_PAIRS = [\n",
    "    (\"He injured the child\", \"He protected the child\"),\n",
    "    (\"殺人者\", \"救人者\"),  # One who kills / one who saves\n",
    "    (\"ظلم الضعيف\", \"رحم الضعيف\"),  # Oppressed / showed mercy to the weak\n",
    "    (\"She hurt the patient\", \"She healed the patient\"),\n",
    "]\n",
    "\n",
    "ROLE_SWAP_PAIRS = [\n",
    "    (\"The master commands the servant\", \"The servant commands the master\"),\n",
    "    (\"君命臣\", \"臣命君\"),  # Lord commands minister / minister commands lord\n",
    "    (\"الأب يأمر الابن\", \"الابن يأمر الأب\"),  # Father commands son / son commands father\n",
    "    (\"The parent guides the child\", \"The child guides the parent\"),\n",
    "]\n",
    "\n",
    "geometry_results = {}\n",
    "\n",
    "# Use the best model from mixed_baseline split for geometric analysis\n",
    "model_path = f\"{SAVE_DIR}/best_mixed_baseline.pt\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"\\nLoading model for geometric analysis...\")\n",
    "    model = BIPModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    analyzer = GeometricAnalyzer(model, tokenizer, device)\n",
    "\n",
    "    # 1. Find obligation/permission axis\n",
    "    print(\"\\n--- Obligation/Permission Axis ---\")\n",
    "    obl_texts = [p[0] for p in OBLIGATION_PERMISSION_TRAIN]\n",
    "    perm_texts = [p[1] for p in OBLIGATION_PERMISSION_TRAIN]\n",
    "    obl_perm_axis = analyzer.find_direction(obl_texts, perm_texts)\n",
    "\n",
    "    # Test transfer to other languages\n",
    "    transfer_acc = analyzer.test_direction_transfer(obl_perm_axis, OBLIGATION_PERMISSION_TEST)\n",
    "    print(f\"  Direction found from English training pairs\")\n",
    "    print(f\"  Transfer accuracy to other languages: {transfer_acc:.1%}\")\n",
    "    axis_status = \"STRONG\" if transfer_acc > 0.8 else \"WEAK\" if transfer_acc > 0.5 else \"FAILED\"\n",
    "    print(f\"  Status: {axis_status} deontic axis\")\n",
    "\n",
    "    geometry_results[\"obligation_permission\"] = {\n",
    "        \"transfer_accuracy\": transfer_acc,\n",
    "        \"status\": axis_status,\n",
    "    }\n",
    "\n",
    "    # 2. Find harm/care axis\n",
    "    print(\"\\n--- Harm/Care Axis ---\")\n",
    "    harm_texts = [p[0] for p in HARM_CARE_PAIRS]\n",
    "    care_texts = [p[1] for p in HARM_CARE_PAIRS]\n",
    "    harm_care_axis = analyzer.find_direction(harm_texts, care_texts)\n",
    "\n",
    "    # Check axis orthogonality\n",
    "    axis_correlation = abs(np.dot(obl_perm_axis, harm_care_axis))\n",
    "    print(f\"  Axis found\")\n",
    "    print(f\"  Correlation with obl/perm axis: {axis_correlation:.3f}\")\n",
    "    orthogonal = \"ORTHOGONAL\" if axis_correlation < 0.3 else \"CORRELATED\"\n",
    "    print(f\"  Status: {orthogonal}\")\n",
    "\n",
    "    geometry_results[\"harm_care\"] = {\n",
    "        \"axis_correlation\": axis_correlation,\n",
    "        \"orthogonal\": axis_correlation < 0.3,\n",
    "    }\n",
    "\n",
    "    # 3. Role swap analysis\n",
    "    print(\"\\n--- Role Swap Analysis ---\")\n",
    "    role_analysis = analyzer.role_swap_analysis(ROLE_SWAP_PAIRS)\n",
    "    print(\n",
    "        f\"  Mean consistency: {role_analysis['consistency']:.3f} +/- {role_analysis['consistency_std']:.3f}\"\n",
    "    )\n",
    "    role_status = \"CONSISTENT\" if role_analysis[\"consistency\"] > 0.9 else \"VARIABLE\"\n",
    "    print(f\"  Status: {role_status} agent/patient transformation\")\n",
    "\n",
    "    geometry_results[\"role_swap\"] = {\n",
    "        \"consistency\": role_analysis[\"consistency\"],\n",
    "        \"consistency_std\": role_analysis[\"consistency_std\"],\n",
    "        \"status\": role_status,\n",
    "    }\n",
    "\n",
    "    # 4. PCA on all structural pairs\n",
    "    print(\"\\n--- PCA Analysis ---\")\n",
    "    all_concept_pairs = {\n",
    "        \"obligation_permission\": OBLIGATION_PERMISSION_TRAIN + OBLIGATION_PERMISSION_TEST,\n",
    "        \"harm_care\": HARM_CARE_PAIRS,\n",
    "    }\n",
    "    pca_results = analyzer.pca_on_pairs(all_concept_pairs)\n",
    "\n",
    "    cumsum = np.cumsum(pca_results[\"explained_variance_ratio\"])\n",
    "    n_components_90 = np.argmax(cumsum > 0.9) + 1 if any(cumsum > 0.9) else len(cumsum)\n",
    "\n",
    "    print(f\"  Explained variance ratio: {pca_results['explained_variance_ratio'][:5]}\")\n",
    "    print(f\"  Components for 90% variance: {n_components_90}\")\n",
    "    pca_status = \"LOW-DIM\" if n_components_90 <= 3 else \"HIGH-DIM\"\n",
    "    print(f\"  Status: {pca_status} moral structure\")\n",
    "\n",
    "    geometry_results[\"pca\"] = {\n",
    "        \"explained_variance\": pca_results[\"explained_variance_ratio\"].tolist(),\n",
    "        \"n_components_90pct\": n_components_90,\n",
    "        \"status\": pca_status,\n",
    "    }\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(f\"\\nSkipping geometric analysis - no model at {model_path}\")\n",
    "    geometry_results = {\"error\": \"No model available\"}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Geometric analysis complete\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 9. Fuzz Testing & Final Results { display-mode: \"form\" }\n",
    "# @markdown v10.9: Structural vs Surface fuzz testing + comprehensive summary\n",
    "# @markdown Tests if model responds to moral structure (good) vs surface features (bad)\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FUZZ TESTING (v10.9)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTesting: structural changes should move embeddings,\")\n",
    "print(\"         surface changes should NOT move embeddings.\")\n",
    "\n",
    "\n",
    "# ===== STRUCTURAL FUZZ TEST CLASS =====\n",
    "class StructuralFuzzTest:\n",
    "    \"\"\"\n",
    "    Extended fuzz testing with cross-lingual pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Structural perturbation templates (language-agnostic concepts)\n",
    "    STRUCTURAL_PAIRS = {\n",
    "        \"obligation_to_permission\": [\n",
    "            # English\n",
    "            (\"You must help the elderly\", \"You may help the elderly\"),\n",
    "            (\"He is required to pay\", \"He is allowed to pay\"),\n",
    "            (\"Parents must protect children\", \"Parents may protect children\"),\n",
    "            # Chinese\n",
    "            (\"君子必孝\", \"君子可孝\"),  # Gentleman must/may be filial\n",
    "            (\"民必從法\", \"民可從法\"),  # People must/may follow law\n",
    "            # Arabic\n",
    "            (\"يجب عليك أن تساعد\", \"يجوز لك أن تساعد\"),  # You must/may help\n",
    "            # Hebrew\n",
    "            (\"חייב לכבד\", \"מותר לכבד\"),  # Obligated/permitted to honor\n",
    "        ],\n",
    "        \"harm_to_care\": [\n",
    "            (\"He injured the child\", \"He protected the child\"),\n",
    "            (\"殺人者\", \"救人者\"),  # One who kills / one who saves\n",
    "            (\"ظلم الضعيف\", \"رحم الضعيف\"),  # Oppressed / showed mercy to the weak\n",
    "        ],\n",
    "        \"role_swap\": [\n",
    "            (\"The master commands the servant\", \"The servant commands the master\"),\n",
    "            (\"君命臣\", \"臣命君\"),  # Lord commands minister / minister commands lord\n",
    "            (\"الأب يأمر الابن\", \"الابن يأمر الأب\"),  # Father commands son / son commands father\n",
    "        ],\n",
    "        \"violation_to_fulfillment\": [\n",
    "            (\"He broke his promise\", \"He kept his promise\"),\n",
    "            (\"違約\", \"守約\"),  # Violate contract / keep contract\n",
    "            (\"نقض العهد\", \"وفى بالعهد\"),  # Broke covenant / fulfilled covenant\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Surface perturbation templates (should NOT move embeddings)\n",
    "    SURFACE_PERTURBATIONS = {\n",
    "        \"name_change\": lambda t: t.replace(\"John\", \"Michael\").replace(\"Mary\", \"Lisa\"),\n",
    "        \"irrelevant_detail\": lambda t: t + \" It was Tuesday.\",\n",
    "        \"add_location\": lambda t: t + \" in the city.\",\n",
    "    }\n",
    "\n",
    "    def run_comprehensive_test(self, analyzer) -> dict:\n",
    "        \"\"\"\n",
    "        Run full structural vs surface test battery.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # Test structural perturbations\n",
    "        for perturbation_type, pairs in self.STRUCTURAL_PAIRS.items():\n",
    "            distances = []\n",
    "            for text1, text2 in pairs:\n",
    "                emb1 = analyzer.get_embedding(text1)\n",
    "                emb2 = analyzer.get_embedding(text2)\n",
    "                # Cosine distance\n",
    "                dist = 1 - np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-9)\n",
    "                distances.append(dist)\n",
    "\n",
    "            results[f\"structural_{perturbation_type}\"] = {\n",
    "                \"mean_distance\": np.mean(distances),\n",
    "                \"std\": np.std(distances),\n",
    "                \"n\": len(distances),\n",
    "            }\n",
    "\n",
    "        # Surface perturbations on base sentences\n",
    "        base_sentences = [\n",
    "            \"John borrowed money from Mary and must repay it.\",\n",
    "            \"The doctor has a duty to help patients.\",\n",
    "            \"Parents should protect their children.\",\n",
    "        ]\n",
    "\n",
    "        surface_distances = []\n",
    "        for base in base_sentences:\n",
    "            base_emb = analyzer.get_embedding(base)\n",
    "            for name, perturb_fn in self.SURFACE_PERTURBATIONS.items():\n",
    "                perturbed = perturb_fn(base)\n",
    "                if perturbed != base:\n",
    "                    perturbed_emb = analyzer.get_embedding(perturbed)\n",
    "                    dist = 1 - np.dot(base_emb, perturbed_emb) / (\n",
    "                        np.linalg.norm(base_emb) * np.linalg.norm(perturbed_emb) + 1e-9\n",
    "                    )\n",
    "                    surface_distances.append(dist)\n",
    "\n",
    "        results[\"surface_all\"] = {\n",
    "            \"mean_distance\": np.mean(surface_distances) if surface_distances else 0,\n",
    "            \"std\": np.std(surface_distances) if surface_distances else 0,\n",
    "            \"n\": len(surface_distances),\n",
    "        }\n",
    "\n",
    "        # Statistical comparison\n",
    "        structural_all = []\n",
    "        for k, v in results.items():\n",
    "            if k.startswith(\"structural_\"):\n",
    "                structural_all.extend([v[\"mean_distance\"]] * v[\"n\"])\n",
    "\n",
    "        if structural_all and surface_distances:\n",
    "            t_stat, p_value = stats.ttest_ind(structural_all, surface_distances)\n",
    "        else:\n",
    "            t_stat, p_value = 0, 1.0\n",
    "\n",
    "        results[\"comparison\"] = {\n",
    "            \"structural_mean\": np.mean(structural_all) if structural_all else 0,\n",
    "            \"surface_mean\": np.mean(surface_distances) if surface_distances else 0,\n",
    "            \"ratio\": (\n",
    "                np.mean(structural_all) / (np.mean(surface_distances) + 1e-9)\n",
    "                if structural_all\n",
    "                else 0\n",
    "            ),\n",
    "            \"t_statistic\": t_stat,\n",
    "            \"p_value\": p_value,\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Run fuzz test if model is available\n",
    "fuzz_results = {}\n",
    "model_path = f\"{SAVE_DIR}/best_mixed_baseline.pt\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"\\nLoading model for fuzz testing...\")\n",
    "    model = BIPModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Reuse GeometricAnalyzer from cell 8\n",
    "    analyzer = GeometricAnalyzer(model, tokenizer, device)\n",
    "    fuzz_test = StructuralFuzzTest()\n",
    "\n",
    "    print(\"\\nRunning structural vs surface comparison...\")\n",
    "    fuzz_results = fuzz_test.run_comprehensive_test(analyzer)\n",
    "\n",
    "    print(\"\\n--- Structural Perturbations (should be HIGH) ---\")\n",
    "    for k, v in fuzz_results.items():\n",
    "        if k.startswith(\"structural_\"):\n",
    "            print(f\"  {k}: distance={v['mean_distance']:.4f} +/- {v['std']:.4f} (n={v['n']})\")\n",
    "\n",
    "    print(\"\\n--- Surface Perturbations (should be LOW) ---\")\n",
    "    v = fuzz_results[\"surface_all\"]\n",
    "    print(f\"  surface_all: distance={v['mean_distance']:.4f} +/- {v['std']:.4f} (n={v['n']})\")\n",
    "\n",
    "    print(\"\\n--- Statistical Comparison ---\")\n",
    "    c = fuzz_results[\"comparison\"]\n",
    "    print(f\"  Structural mean: {c['structural_mean']:.4f}\")\n",
    "    print(f\"  Surface mean:    {c['surface_mean']:.4f}\")\n",
    "    print(f\"  Ratio:           {c['ratio']:.2f}x\")\n",
    "    print(f\"  t-statistic:     {c['t_statistic']:.2f}\")\n",
    "    print(f\"  p-value:         {c['p_value']:.4f}\")\n",
    "\n",
    "    # Interpret results\n",
    "    if c[\"ratio\"] > 2.0 and c[\"p_value\"] < 0.05:\n",
    "        fuzz_status = \"EXCELLENT\"\n",
    "        fuzz_msg = \"Model strongly distinguishes structural from surface\"\n",
    "    elif c[\"ratio\"] > 1.5:\n",
    "        fuzz_status = \"GOOD\"\n",
    "        fuzz_msg = \"Model distinguishes structural from surface\"\n",
    "    elif c[\"ratio\"] > 1.0:\n",
    "        fuzz_status = \"MARGINAL\"\n",
    "        fuzz_msg = \"Some structural sensitivity\"\n",
    "    else:\n",
    "        fuzz_status = \"FAILED\"\n",
    "        fuzz_msg = \"Model may be using surface features\"\n",
    "\n",
    "    print(f\"\\n  FUZZ STATUS: {fuzz_status}\")\n",
    "    print(f\"  {fuzz_msg}\")\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(f\"\\nSkipping fuzz test - no model at {model_path}\")\n",
    "    fuzz_status = \"SKIPPED\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL BIP EVALUATION (v10.9)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nHardware: {GPU_TIER} ({VRAM_GB:.0f}GB VRAM, {RAM_GB:.0f}GB RAM)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"CROSS-DOMAIN TRANSFER RESULTS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "successful_splits = []\n",
    "for name, r in all_results.items():\n",
    "    ratio = r[\"bond_f1_macro\"] / 0.1\n",
    "    lang_acc = r[\"language_acc\"]\n",
    "\n",
    "    transfer_ok = ratio > 1.3\n",
    "    invariant_ok = lang_acc < 0.35  # Near chance (20%)\n",
    "\n",
    "    status = \"SUCCESS\" if (transfer_ok and invariant_ok) else \"PARTIAL\" if transfer_ok else \"FAIL\"\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\n",
    "        f\"  Bond F1:     {r['bond_f1_macro']:.3f} ({ratio:.1f}x chance) {'OK' if transfer_ok else 'WEAK'}\"\n",
    "    )\n",
    "    print(f\"  Language:    {lang_acc:.1%} {'INVARIANT' if invariant_ok else 'LEAKING'}\")\n",
    "    print(f\"  -> {status}\")\n",
    "\n",
    "    if transfer_ok and invariant_ok:\n",
    "        successful_splits.append(name)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"VERDICT\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "n_success = len(successful_splits)\n",
    "if n_success >= 3:\n",
    "    verdict = \"STRONGLY_SUPPORTED\"\n",
    "    msg = \"Multiple independent transfer paths demonstrate universal structure\"\n",
    "elif n_success >= 2:\n",
    "    verdict = \"SUPPORTED\"\n",
    "    msg = \"Multiple transfer paths work\"\n",
    "elif n_success >= 1:\n",
    "    verdict = \"PARTIALLY_SUPPORTED\"\n",
    "    msg = \"At least one transfer path works\"\n",
    "elif any(r[\"bond_f1_macro\"] > 0.13 for r in all_results.values()):\n",
    "    verdict = \"WEAK\"\n",
    "    msg = \"Some transfer signal, but not robust\"\n",
    "else:\n",
    "    verdict = \"INCONCLUSIVE\"\n",
    "    msg = \"No clear transfer demonstrated\"\n",
    "\n",
    "print(f\"\\n  Successful transfers: {n_success}/{len(all_results)}\")\n",
    "print(f\"  Splits: {successful_splits if successful_splits else 'None'}\")\n",
    "print(f\"\\n  VERDICT: {verdict}\")\n",
    "print(f\"  {msg}\")\n",
    "\n",
    "# v10.9 specific checks\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"v10.9 SPECIFIC CRITERIA\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check key v10.9 splits\n",
    "v109_checks = {\n",
    "    \"confucian_to_buddhist\": \"Chinese diversity test\",\n",
    "    \"all_to_sanskrit\": \"Sanskrit transfer test\",\n",
    "    \"quran_to_fiqh\": \"Arabic improvement test\",\n",
    "}\n",
    "\n",
    "for split_name, test_name in v109_checks.items():\n",
    "    if split_name in all_results:\n",
    "        r = all_results[split_name]\n",
    "        f1 = r[\"bond_f1_macro\"]\n",
    "        threshold = 0.4 if \"sanskrit\" in split_name else 0.5\n",
    "        status = \"PASS\" if f1 >= threshold else \"FAIL\"\n",
    "        print(f\"  {test_name}: F1={f1:.3f} (need {threshold}) -> {status}\")\n",
    "    else:\n",
    "        print(f\"  {test_name}: NOT RUN\")\n",
    "\n",
    "# Geometry results\n",
    "if \"geometry_results\" in dir() and geometry_results:\n",
    "    print(\"\\n  Geometric Analysis:\")\n",
    "    if \"obligation_permission\" in geometry_results:\n",
    "        acc = geometry_results[\"obligation_permission\"].get(\"transfer_accuracy\", 0)\n",
    "        print(f\"    Deontic axis transfer: {acc:.1%} (need 80%)\")\n",
    "    if \"pca\" in geometry_results:\n",
    "        n_comp = geometry_results[\"pca\"].get(\"n_components_90pct\", 0)\n",
    "        print(f\"    PCA components for 90%: {n_comp} (need ≤3)\")\n",
    "\n",
    "# Fuzz results\n",
    "if fuzz_results and \"comparison\" in fuzz_results:\n",
    "    print(f\"\\n  Fuzz Test: {fuzz_status}\")\n",
    "    print(f\"    Structural/Surface ratio: {fuzz_results['comparison']['ratio']:.2f}x\")\n",
    "\n",
    "# Save results\n",
    "final_results = {\n",
    "    \"version\": \"v10.9\",\n",
    "    \"all_results\": all_results,\n",
    "    \"probe_results\": probe_results if \"probe_results\" in dir() else {},\n",
    "    \"geometry_results\": geometry_results if \"geometry_results\" in dir() else {},\n",
    "    \"fuzz_results\": fuzz_results,\n",
    "    \"successful_splits\": successful_splits,\n",
    "    \"verdict\": verdict,\n",
    "    \"hardware\": {\"gpu\": GPU_TIER, \"vram_gb\": VRAM_GB, \"ram_gb\": RAM_GB},\n",
    "    \"settings\": {\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"max_per_lang\": MAX_PER_LANG,\n",
    "        \"num_workers\": NUM_WORKERS,\n",
    "    },\n",
    "    \"experiment_time\": time.time() - EXPERIMENT_START,\n",
    "}\n",
    "\n",
    "with open(\"results/final_results.json\", \"w\") as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "shutil.copy(\"results/final_results.json\", f\"{SAVE_DIR}/final_results.json\")\n",
    "\n",
    "print(f\"\\nTotal time: {(time.time() - EXPERIMENT_START)/60:.1f} minutes\")\n",
    "print(\"Results saved to Drive!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 10. Save & Download Results { display-mode: \"form\" }\n",
    "# @markdown Persist results to Google Drive and optionally download as zip\n",
    "\n",
    "import shutil\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Always persist results to Drive\n",
    "if SAVE_DIR and os.path.exists(SAVE_DIR):\n",
    "    print(f\"\\nPersisting to: {SAVE_DIR}\")\n",
    "\n",
    "    # Save final results JSON\n",
    "    if os.path.exists(\"results/final_results.json\"):\n",
    "        dest = f\"{SAVE_DIR}/final_results.json\"\n",
    "        shutil.copy(\"results/final_results.json\", dest)\n",
    "        print(f\"  Saved: final_results.json\")\n",
    "\n",
    "    # Save splits config\n",
    "    if os.path.exists(\"data/splits/all_splits.json\"):\n",
    "        dest = f\"{SAVE_DIR}/all_splits.json\"\n",
    "        shutil.copy(\"data/splits/all_splits.json\", dest)\n",
    "        print(f\"  Saved: all_splits.json\")\n",
    "\n",
    "    # Models are already saved to SAVE_DIR during training\n",
    "    model_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(\".pt\")]\n",
    "    if model_files:\n",
    "        print(f\"  Models already in Drive: {len(model_files)} files\")\n",
    "        for mf in model_files[:5]:\n",
    "            print(f\"    - {mf}\")\n",
    "        if len(model_files) > 5:\n",
    "            print(f\"    ... and {len(model_files)-5} more\")\n",
    "\n",
    "    print(f\"\\nResults persisted to Google Drive: {SAVE_DIR}\")\n",
    "else:\n",
    "    print(\"WARNING: SAVE_DIR not available, results only in local directories\")\n",
    "\n",
    "# Optional: Create download zip\n",
    "if CREATE_DOWNLOAD_ZIP:\n",
    "    import zipfile\n",
    "\n",
    "    zip_path = \"BIP_v10.10_results.zip\"\n",
    "    print(f\"\\n\" + \"-\" * 60)\n",
    "    print(\"Creating download package...\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "        # Results\n",
    "        if os.path.exists(\"results/final_results.json\"):\n",
    "            zf.write(\"results/final_results.json\")\n",
    "\n",
    "        # Models (from Drive)\n",
    "        if SAVE_DIR and os.path.exists(SAVE_DIR):\n",
    "            for f in os.listdir(SAVE_DIR):\n",
    "                if f.endswith(\".pt\"):\n",
    "                    zf.write(f\"{SAVE_DIR}/{f}\", f\"models/{f}\")\n",
    "\n",
    "        # Config\n",
    "        if os.path.exists(\"data/splits/all_splits.json\"):\n",
    "            zf.write(\"data/splits/all_splits.json\")\n",
    "\n",
    "    print(f\"Download package ready: {zip_path}\")\n",
    "\n",
    "    # Download in Colab, or show path otherwise\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(zip_path)\n",
    "    except ImportError:\n",
    "        print(f\"Not running in Colab. Zip saved to: {os.path.abspath(zip_path)}\")\n",
    "else:\n",
    "    print(f\"\\n(Zip download disabled - set CREATE_DOWNLOAD_ZIP=True in cell 1 to enable)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    ""
   ]
  }
 ]
}