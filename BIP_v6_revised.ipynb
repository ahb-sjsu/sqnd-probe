{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# BIP Cross-Cultural Universal Morality Experiment (v6)\n\n**Testing the Bond Invariance Principle across 2000+ years AND across languages (Hebrew + Chinese + Arabic → English)**\n\nThis experiment tests whether moral cognition has invariant structure by:\n1. Training on ORIGINAL HEBREW texts (Sefaria corpus, ~500 BCE - 1800 CE)\n2. Testing transfer to modern ENGLISH advice columns (Dear Abby, 1956-2020)\n\n**Hypothesis**: If BIP holds, bond-level features should transfer across 2000 years with no accuracy drop.\n\n---\n\n## v6 Changes\n- **Added bond transfer accuracy test**: Now predicts `primary_relation` from `z_bond` and reports F1 by corpus\n- **Fixed bond extractor**: Added patterns for EMERGENCY/CONTRACT, added NONE class instead of CARE default\n- **Fixed TPU double-stepping bug**\n- **Fixed download cell filename**\n- **Renamed MAX_SEFARIA_FILES for clarity**\n\n---\n\n## Setup Instructions\n1. **Runtime -> Change runtime type -> GPU (T4) or TPU (v5e)**\n2. Run cells in order - each shows progress in real-time\n3. Expected runtime: ~1-2 hours (TPU) or ~2-4 hours (GPU)\n\n**Supported Accelerators:**\n- NVIDIA GPU (T4, V100, A100)\n- Google TPU (v2, v3, v4, v5e)\n- CPU (slow, not recommended)\n\n---"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 1. Setup and Install Dependencies { display-mode: \"form\" }\n#@markdown Installs packages and detects GPU/TPU. Memory-optimized for Colab.\n\nprint(\"=\" * 60)\nprint(\"BIP TEMPORAL INVARIANCE EXPERIMENT (v6)\")\nprint(\"=\" * 60)\nprint()\n\n# Progress tracker\nTASKS = [\n    \"Install dependencies\",\n    \"Clone Sefaria corpus (~8GB)\",\n    \"Clone sqnd-probe repo (Dear Abby data)\",\n    \"Preprocess corpora\",\n    \"Extract bond structures\",\n    \"Generate train/test splits\",\n    \"Train BIP model\",\n    \"Evaluate results\"\n]\ntask_status = {task: \"pending\" for task in TASKS}\n\ndef print_progress():\n    print()\n    print(\"-\" * 50)\n    print(\"EXPERIMENT PROGRESS:\")\n    print(\"-\" * 50)\n    for task in TASKS:\n        status = task_status[task]\n        if status == \"done\":\n            mark = \"[X]\"\n        elif status == \"running\":\n            mark = \"[>]\"\n        else:\n            mark = \"[ ]\"\n        print(f\"  {mark} {task}\")\n    print(\"-\" * 50)\n    print(flush=True)\n\ndef mark_task(task, status):\n    task_status[task] = status\n    print_progress()\n\nprint_progress()\n\nmark_task(\"Install dependencies\", \"running\")\n\nimport os\nimport subprocess\nimport sys\n\n# Install dependencies - MINIMAL set to save memory\nprint(\"Installing minimal dependencies...\")\ndeps = [\n    \"transformers\",\n    \"torch\", \n    \"sentence-transformers\",\n    \"pandas\",\n    \"tqdm\",\n    \"psutil\",\n    \"scikit-learn\"  # For F1 score\n]\n\nfor dep in deps:\n    print(f\"  Installing {dep}...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", dep])\n\nprint()\n\n# Detect accelerator - WITHOUT importing tensorflow\nUSE_TPU = False\nTPU_TYPE = None\n\n# Check for TPU\nif 'COLAB_TPU_ADDR' in os.environ:\n    USE_TPU = True\n    TPU_TYPE = \"TPU (Colab)\"\n    print(\"TPU detected!\")\n\n# Check for GPU\nimport torch\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    ACCELERATOR = f\"GPU: {gpu_name} ({gpu_mem:.1f}GB)\"\n    device = torch.device(\"cuda\")\nelif USE_TPU:\n    ACCELERATOR = TPU_TYPE\n    import torch_xla.core.xla_model as xm\n    device = xm.xla_device()\nelse:\n    ACCELERATOR = \"CPU (slow!)\"\n    device = torch.device(\"cpu\")\n\nprint(f\"Accelerator: {ACCELERATOR}\")\nprint(f\"Device: {device}\")\n\n# Memory status\nimport psutil\nmem = psutil.virtual_memory()\nprint(f\"System RAM: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n\nif torch.cuda.is_available():\n    print(f\"GPU RAM: {torch.cuda.memory_allocated()/1e9:.1f}/{torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n\n# Enable mixed precision for 2-3x speedup\nif torch.cuda.is_available():\n    print()\n    print(\"Enabling mixed precision (FP16) for faster training...\")\n    from torch.cuda.amp import autocast, GradScaler\n    USE_AMP = True\n    scaler = GradScaler()\nelse:\n    USE_AMP = False\n    scaler = None\n\n# torch.compile for PyTorch 2.0+ (10-30% speedup)\nTORCH_COMPILE = False\nif hasattr(torch, 'compile') and torch.cuda.is_available():\n    print(\"PyTorch 2.0+ detected - torch.compile available\")\n    TORCH_COMPILE = False  # Disabled - overhead > benefit for short runs\n\n\n\n# ============================================================\n# GOOGLE DRIVE - SAVE RESULTS EVEN IF SESSION DIES\n# ============================================================\nprint()\nprint(\"=\" * 60)\nprint(\"MOUNTING GOOGLE DRIVE FOR PERSISTENT STORAGE\")\nprint(\"=\" * 60)\nfrom google.colab import drive\ndrive.mount('/content/drive')\nSAVE_DIR = '/content/drive/MyDrive/BIP_results'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# Create local directories\nos.makedirs(\"data/processed\", exist_ok=True)\nos.makedirs(\"data/splits\", exist_ok=True)\nos.makedirs(\"models/checkpoints\", exist_ok=True)\nos.makedirs(\"results\", exist_ok=True)\nprint(f\"Results will be saved to: {SAVE_DIR}\")\nprint(\"If session crashes, your data survives.\")\nprint()\n\nmark_task(\"Install dependencies\", \"done\")\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 2. Download Sefaria Corpus (~8GB) { display-mode: \"form\" }\n",
        "#@markdown Downloads the complete Sefaria corpus with real-time git progress.\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "mark_task(\"Clone Sefaria corpus (~8GB)\", \"running\")\n",
        "\n",
        "sefaria_path = 'data/raw/Sefaria-Export'\n",
        "\n",
        "if not os.path.exists(sefaria_path) or not os.path.exists(f\"{sefaria_path}/json\"):\n",
        "    print(\"=\"*60)\n",
        "    print(\"CLONING SEFARIA CORPUS\")\n",
        "    print(\"=\"*60)\n",
        "    print()\n",
        "    print(\"This downloads ~3.5GB and takes 5-15 minutes.\")\n",
        "    print(\"Git's native progress will display below:\")\n",
        "    print(\"-\"*60)\n",
        "    print(flush=True)\n",
        "    \n",
        "    # Use subprocess.Popen for real-time output streaming\n",
        "    process = subprocess.Popen(\n",
        "        ['git', 'clone', '--depth', '1', '--progress',\n",
        "         'https://github.com/Sefaria/Sefaria-Export.git', sefaria_path],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,  # Git writes progress to stderr\n",
        "        text=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    \n",
        "    # Stream output in real-time\n",
        "    for line in process.stdout:\n",
        "        print(line, end='', flush=True)\n",
        "    \n",
        "    process.wait()\n",
        "    \n",
        "    print(\"-\"*60)\n",
        "    if process.returncode == 0:\n",
        "        print(\"\\nSefaria clone COMPLETE!\")\n",
        "    else:\n",
        "        print(f\"\\nERROR: Git clone failed with code {process.returncode}\")\n",
        "        print(\"Try running this cell again, or check your internet connection.\")\n",
        "else:\n",
        "    print(\"Sefaria already exists, skipping download.\")\n",
        "\n",
        "# Verify and count files\n",
        "print()\n",
        "print(\"Verifying download...\")\n",
        "!du -sh {sefaria_path} 2>/dev/null || echo \"Directory not found\"\n",
        "json_count = !find {sefaria_path}/json -name \"*.json\" 2>/dev/null | wc -l\n",
        "print(f\"Sefaria JSON files found: {json_count[0]}\")\n",
        "\n",
        "mark_task(\"Clone Sefaria corpus (~8GB)\", \"done\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 3. Download Dear Abby Dataset { display-mode: \"form\" }\n",
        "#@markdown Downloads the Dear Abby advice column dataset (68,330 entries).\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "mark_task(\"Clone sqnd-probe repo (Dear Abby data)\", \"running\")\n",
        "\n",
        "sqnd_path = 'sqnd-probe-data'\n",
        "if not os.path.exists(sqnd_path):\n",
        "    print(\"Cloning sqnd-probe repo...\")\n",
        "    process = subprocess.Popen(\n",
        "        ['git', 'clone', '--depth', '1', '--progress',\n",
        "         'https://github.com/ahb-sjsu/sqnd-probe.git', sqnd_path],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    for line in process.stdout:\n",
        "        print(line, end='', flush=True)\n",
        "    process.wait()\n",
        "else:\n",
        "    print(\"Repo already cloned.\")\n",
        "\n",
        "# Copy Dear Abby data\n",
        "dear_abby_source = Path('sqnd-probe-data/dear_abby_data/raw_da_qs.csv')\n",
        "dear_abby_path = Path('data/raw/dear_abby.csv')\n",
        "\n",
        "if dear_abby_source.exists():\n",
        "    !cp \"{dear_abby_source}\" \"{dear_abby_path}\"\n",
        "    print(f\"\\nCopied Dear Abby data\")\n",
        "elif not dear_abby_path.exists():\n",
        "    raise FileNotFoundError(\"Dear Abby dataset not found!\")\n",
        "\n",
        "# Verify\n",
        "df_check = pd.read_csv(dear_abby_path)\n",
        "print(f\"\\n\" + \"=\" * 50)\n",
        "print(f\"Dear Abby dataset: {len(df_check):,} entries\")\n",
        "print(f\"Columns: {list(df_check.columns)}\")\n",
        "print(f\"Year range: {df_check['year'].min():.0f} - {df_check['year'].max():.0f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "mark_task(\"Clone sqnd-probe repo (Dear Abby data)\", \"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 4. Define Data Classes and Loaders { display-mode: \"form\" }\n",
        "#@markdown Defines enums, dataclasses, and corpus loaders.\n",
        "#@markdown **v6 FIX**: Added NONE bond type, patterns for EMERGENCY/CONTRACT.\n",
        "\n",
        "import json\n",
        "import hashlib\n",
        "import re\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import List, Dict\n",
        "from enum import Enum\n",
        "from collections import defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"Defining data structures...\")\n",
        "\n",
        "class TimePeriod(Enum):\n",
        "    BIBLICAL = 0        # ~1000-500 BCE\n",
        "    SECOND_TEMPLE = 1   # ~500 BCE - 70 CE\n",
        "    TANNAITIC = 2       # ~70-200 CE\n",
        "    AMORAIC = 3         # ~200-500 CE\n",
        "    GEONIC = 4          # ~600-1000 CE\n",
        "    RISHONIM = 5        # ~1000-1500 CE\n",
        "    ACHRONIM = 6        # ~1500-1800 CE\n",
        "    MODERN_HEBREW = 7   # ~1800-present\n",
        "    DEAR_ABBY = 8       # 1956-2020\n",
        "\n",
        "# v6 FIX: Added NONE as explicit class (index 10) instead of defaulting to CARE\n",
        "class BondType(Enum):\n",
        "    HARM_PREVENTION = 0\n",
        "    RECIPROCITY = 1\n",
        "    AUTONOMY = 2\n",
        "    PROPERTY = 3\n",
        "    FAMILY = 4\n",
        "    AUTHORITY = 5\n",
        "    EMERGENCY = 6\n",
        "    CONTRACT = 7\n",
        "    CARE = 8\n",
        "    FAIRNESS = 9\n",
        "    NONE = 10  # v6: Explicit NONE class instead of sink to CARE\n",
        "\n",
        "class HohfeldianState(Enum):\n",
        "    RIGHT = 0\n",
        "    OBLIGATION = 1\n",
        "    LIBERTY = 2\n",
        "    NO_RIGHT = 3\n",
        "\n",
        "@dataclass\n",
        "class Passage:\n",
        "    id: str\n",
        "    text_original: str\n",
        "    text_english: str\n",
        "    time_period: str\n",
        "    century: int\n",
        "    source: str\n",
        "    source_type: str\n",
        "    category: str\n",
        "    language: str = \"hebrew\"\n",
        "    word_count: int = 0\n",
        "    has_dispute: bool = False\n",
        "    consensus_tier: str = \"unknown\"\n",
        "    bond_types: List[str] = field(default_factory=list)\n",
        "    \n",
        "    def to_dict(self):\n",
        "        return asdict(self)\n",
        "\n",
        "CATEGORY_TO_PERIOD = {\n",
        "    'Tanakh': TimePeriod.BIBLICAL,\n",
        "    'Torah': TimePeriod.BIBLICAL,\n",
        "    'Mishnah': TimePeriod.TANNAITIC,\n",
        "    'Tosefta': TimePeriod.TANNAITIC,\n",
        "    'Talmud': TimePeriod.AMORAIC,\n",
        "    'Bavli': TimePeriod.AMORAIC,\n",
        "    'Midrash': TimePeriod.AMORAIC,\n",
        "    'Halakhah': TimePeriod.RISHONIM,\n",
        "    'Chasidut': TimePeriod.ACHRONIM,\n",
        "}\n",
        "\n",
        "PERIOD_TO_CENTURY = {\n",
        "    TimePeriod.BIBLICAL: -6,\n",
        "    TimePeriod.SECOND_TEMPLE: -2,\n",
        "    TimePeriod.TANNAITIC: 2,\n",
        "    TimePeriod.AMORAIC: 4,\n",
        "    TimePeriod.GEONIC: 8,\n",
        "    TimePeriod.RISHONIM: 12,\n",
        "    TimePeriod.ACHRONIM: 17,\n",
        "    TimePeriod.MODERN_HEBREW: 20,\n",
        "}\n",
        "\n",
        "def load_sefaria(base_path: str, max_files: int = None) -> List[Passage]:\n",
        "    \"\"\"Load Sefaria corpus with progress bar.\n",
        "    \n",
        "    Args:\n",
        "        base_path: Path to Sefaria-Export directory\n",
        "        max_files: Maximum number of JSON files to process (NOT passages)\n",
        "                   v6 FIX: Renamed from max_passages for clarity\n",
        "    \"\"\"\n",
        "    passages = []\n",
        "    json_path = Path(base_path) / \"json\"\n",
        "    \n",
        "    if not json_path.exists():\n",
        "        print(f\"Warning: {json_path} not found\")\n",
        "        return []\n",
        "    \n",
        "    json_files = list(json_path.rglob(\"*.json\"))\n",
        "    print(f\"Found {len(json_files):,} JSON files...\")\n",
        "    \n",
        "    # v6 FIX: Clarify that we're limiting FILES, not passages\n",
        "    files_to_process = json_files[:max_files] if max_files else json_files\n",
        "    print(f\"Processing {len(files_to_process):,} files (max_files={max_files})...\")\n",
        "    \n",
        "    for json_file in tqdm(files_to_process, desc=\"Loading Sefaria\", unit=\"file\"):\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "        except:\n",
        "            continue\n",
        "        \n",
        "        rel_path = json_file.relative_to(json_path)\n",
        "        category = str(rel_path.parts[0]) if rel_path.parts else \"unknown\"\n",
        "        time_period = CATEGORY_TO_PERIOD.get(category, TimePeriod.AMORAIC)\n",
        "        century = PERIOD_TO_CENTURY.get(time_period, 0)\n",
        "        \n",
        "        if isinstance(data, dict):\n",
        "            hebrew = data.get('he', data.get('text', []))\n",
        "            english = data.get('text', data.get('en', []))\n",
        "            \n",
        "            def flatten(h, e, ref=\"\"):\n",
        "                if isinstance(h, str) and isinstance(e, str):\n",
        "                    h_clean = re.sub(r'<[^>]+>', '', h).strip()\n",
        "                    e_clean = re.sub(r'<[^>]+>', '', e).strip()\n",
        "                    if 50 <= len(e_clean) <= 2000:\n",
        "                        pid = hashlib.md5(f\"{json_file.stem}:{ref}:{h_clean[:50]}\".encode()).hexdigest()[:12]\n",
        "                        return [Passage(\n",
        "                            id=f\"sefaria_{pid}\",\n",
        "                            text_original=h_clean,\n",
        "                            text_english=e_clean,\n",
        "                            time_period=time_period.name,\n",
        "                            century=century,\n",
        "                            source=f\"{json_file.stem} {ref}\".strip(),\n",
        "                            source_type=\"sefaria\",\n",
        "                            category=category,\n",
        "                            language=\"hebrew\",\n",
        "                            word_count=len(e_clean.split())\n",
        "                        )]\n",
        "                    return []\n",
        "                elif isinstance(h, list) and isinstance(e, list):\n",
        "                    result = []\n",
        "                    for i, (hh, ee) in enumerate(zip(h, e)):\n",
        "                        result.extend(flatten(hh, ee, f\"{ref}.{i+1}\" if ref else str(i+1)))\n",
        "                    return result\n",
        "                return []\n",
        "            \n",
        "            passages.extend(flatten(hebrew, english))\n",
        "    \n",
        "    return passages\n",
        "\n",
        "def load_dear_abby(path: str, max_passages: int = None) -> List[Passage]:\n",
        "    \"\"\"Load Dear Abby corpus with progress bar.\"\"\"\n",
        "    passages = []\n",
        "    df = pd.read_csv(path)\n",
        "    \n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading Dear Abby\", unit=\"row\"):\n",
        "        question = str(row.get('question_only', ''))\n",
        "        if not question or question == 'nan' or len(question) < 50 or len(question) > 2000:\n",
        "            continue\n",
        "        \n",
        "        year = int(row.get('year', 1990))\n",
        "        pid = hashlib.md5(f\"abby:{idx}:{question[:50]}\".encode()).hexdigest()[:12]\n",
        "        \n",
        "        passages.append(Passage(\n",
        "            id=f\"abby_{pid}\",\n",
        "            text_original=question,\n",
        "            text_english=question,\n",
        "            time_period=TimePeriod.DEAR_ABBY.name,\n",
        "            century=20 if year < 2000 else 21,\n",
        "            source=f\"Dear Abby {year}\",\n",
        "            source_type=\"dear_abby\",\n",
        "            category=\"general\",\n",
        "            language=\"english\",\n",
        "            word_count=len(question.split())\n",
        "        ))\n",
        "        \n",
        "        if max_passages and len(passages) >= max_passages:\n",
        "            break\n",
        "    \n",
        "    return passages\n",
        "\n",
        "print(\"Data structures defined!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 5. Load and Preprocess Corpora { display-mode: \"form\" }\n",
        "#@markdown Loads both corpora.\n",
        "#@markdown **v6 FIX**: Renamed parameter to MAX_SEFARIA_FILES for clarity.\n",
        "\n",
        "#@markdown **Memory Management:**\n",
        "MAX_SEFARIA_FILES = 5000  #@param {type:\"integer\"}\n",
        "#@markdown **NOTE**: This limits JSON FILES processed, not total passages.\n",
        "#@markdown Each file may contain multiple passages. Set to 0 for unlimited.\n",
        "\n",
        "import gc\n",
        "\n",
        "mark_task(\"Preprocess corpora\", \"running\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING CORPORA\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "if MAX_SEFARIA_FILES > 0:\n",
        "    print(f\"*** MEMORY MODE: Limited to {MAX_SEFARIA_FILES:,} Sefaria JSON FILES ***\")\n",
        "    print(\"(Each file may yield multiple passages)\")\n",
        "    print()\n",
        "\n",
        "# Load Sefaria with optional limit\n",
        "# v6 FIX: Renamed parameter for clarity\n",
        "limit = MAX_SEFARIA_FILES if MAX_SEFARIA_FILES > 0 else None\n",
        "sefaria_passages = load_sefaria(\"data/raw/Sefaria-Export\", max_files=limit)\n",
        "print(f\"\\nSefaria passages loaded: {len(sefaria_passages):,}\")\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# Load Dear Abby\n",
        "print()\n",
        "abby_passages = load_dear_abby(\"data/raw/dear_abby.csv\")\n",
        "print(f\"\\nDear Abby passages loaded: {len(abby_passages):,}\")\n",
        "\n",
        "# Combine\n",
        "all_passages = sefaria_passages + abby_passages\n",
        "\n",
        "# Clear individual lists to save memory\n",
        "del sefaria_passages\n",
        "del abby_passages\n",
        "gc.collect()\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(f\"TOTAL PASSAGES: {len(all_passages):,}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Statistics\n",
        "by_period = defaultdict(int)\n",
        "by_source = defaultdict(int)\n",
        "for p in all_passages:\n",
        "    by_period[p.time_period] += 1\n",
        "    by_source[p.source_type] += 1\n",
        "\n",
        "print(\"\\nBy source:\")\n",
        "for source, count in sorted(by_source.items()):\n",
        "    print(f\"  {source}: {count:,}\")\n",
        "\n",
        "print(\"\\nBy time period:\")\n",
        "for period, count in sorted(by_period.items()):\n",
        "    pct = count / len(all_passages) * 100\n",
        "    bar = '#' * int(pct / 2)\n",
        "    print(f\"  {period:20s}: {count:6,} ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "# Memory status\n",
        "import psutil\n",
        "mem = psutil.virtual_memory()\n",
        "print(f\"\\nMemory: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n",
        "\n",
        "mark_task(\"Preprocess corpora\", \"done\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 6. Extract Bond Structures { display-mode: \"form\" }\n",
        "#@markdown Extracts moral bond structures. Streams to disk to save memory.\n",
        "#@markdown **v6 FIX**: Added EMERGENCY/CONTRACT patterns, NONE instead of CARE default.\n",
        "\n",
        "import gc\n",
        "\n",
        "mark_task(\"Extract bond structures\", \"running\")\n",
        "\n",
        "# v6 FIX: Added patterns for EMERGENCY and CONTRACT\n",
        "RELATION_PATTERNS = {\n",
        "    BondType.HARM_PREVENTION: [r'\\b(kill|murder|harm|hurt|save|rescue|protect|danger|attack|injure|wound|destroy)\\b'],\n",
        "    BondType.RECIPROCITY: [r'\\b(return|repay|owe|debt|mutual|exchange|give back|pay back|reciprocate)\\b'],\n",
        "    BondType.AUTONOMY: [r'\\b(choose|decision|consent|agree|force|coerce|right|freedom|liberty|self-determination)\\b'],\n",
        "    BondType.PROPERTY: [r'\\b(property|own|steal|theft|buy|sell|land|possess|belong|asset)\\b'],\n",
        "    BondType.FAMILY: [r'\\b(honor|parent|marry|divorce|inherit|family|mother|father|child|son|daughter|spouse|husband|wife)\\b'],\n",
        "    BondType.AUTHORITY: [r'\\b(obey|command|law|judge|rule|teach|leader|king|master|servant|subject)\\b'],\n",
        "    BondType.CARE: [r'\\b(care|help|assist|feed|clothe|visit|nurture|tend|support|comfort)\\b'],\n",
        "    BondType.FAIRNESS: [r'\\b(fair|just|equal|deserve|bias|impartial|equity|discrimination)\\b'],\n",
        "    # v6 FIX: Added EMERGENCY patterns\n",
        "    BondType.EMERGENCY: [r'\\b(emergency|urgent|crisis|danger|life-threatening|immediate|desperate|dire|peril|rescue)\\b'],\n",
        "    # v6 FIX: Added CONTRACT patterns  \n",
        "    BondType.CONTRACT: [r'\\b(contract|agreement|promise|vow|oath|covenant|pledge|commit|bind|treaty|negotiate)\\b'],\n",
        "}\n",
        "\n",
        "HOHFELD_PATTERNS = {\n",
        "    HohfeldianState.OBLIGATION: [r'\\b(must|shall|duty|require|should|ought|obligated)\\b'],\n",
        "    HohfeldianState.RIGHT: [r'\\b(right to|entitled|deserve|claim|due)\\b'],\n",
        "    HohfeldianState.LIBERTY: [r'\\b(may|can|permitted|allowed|free to|at liberty)\\b'],\n",
        "}\n",
        "\n",
        "def extract_bond_structure(passage: Passage) -> Dict:\n",
        "    \"\"\"Extract bond structure from passage.\n",
        "    \n",
        "    v6 FIX: Now defaults to NONE instead of CARE when no patterns match.\n",
        "    \"\"\"\n",
        "    text = passage.text_english.lower()\n",
        "    \n",
        "    relations = []\n",
        "    for rel_type, patterns in RELATION_PATTERNS.items():\n",
        "        for pattern in patterns:\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                relations.append(rel_type.name)\n",
        "                break\n",
        "    \n",
        "    # v6 FIX: Default to NONE instead of CARE to avoid biased sink label\n",
        "    if not relations:\n",
        "        relations = ['NONE']\n",
        "    \n",
        "    hohfeld = None\n",
        "    for state, patterns in HOHFELD_PATTERNS.items():\n",
        "        for pattern in patterns:\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                hohfeld = state.name\n",
        "                break\n",
        "        if hohfeld:\n",
        "            break\n",
        "    \n",
        "    signature = \"|\".join(sorted(set(relations)))\n",
        "    \n",
        "    return {\n",
        "        'bonds': [{'relation': r} for r in relations],\n",
        "        'primary_relation': relations[0],\n",
        "        'hohfeld_state': hohfeld,\n",
        "        'signature': signature\n",
        "    }\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXTRACTING & SAVING (STREAMING)\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(\"Writing directly to disk to conserve memory...\")\n",
        "print()\n",
        "\n",
        "bond_counts = defaultdict(int)\n",
        "\n",
        "# Stream directly to files - don't accumulate in memory\n",
        "with open(\"data/processed/passages.jsonl\", 'w') as f_pass, \\\n",
        "     open(\"data/processed/bond_structures.jsonl\", 'w') as f_bond:\n",
        "    \n",
        "    for passage in tqdm(all_passages, desc=\"Processing\", unit=\"passage\"):\n",
        "        # Extract bonds\n",
        "        bond_struct = extract_bond_structure(passage)\n",
        "        passage.bond_types = [b['relation'] for b in bond_struct['bonds']]\n",
        "        \n",
        "        # Count for stats\n",
        "        for bond in bond_struct['bonds']:\n",
        "            bond_counts[bond['relation']] += 1\n",
        "        \n",
        "        # Write immediately (don't accumulate)\n",
        "        f_pass.write(json.dumps(passage.to_dict()) + '\\n')\n",
        "        f_bond.write(json.dumps({\n",
        "            'passage_id': passage.id,\n",
        "            'bond_structure': bond_struct\n",
        "        }) + '\\n')\n",
        "\n",
        "# Clear passages from memory - we've saved them to disk\n",
        "n_passages = len(all_passages)\n",
        "del all_passages\n",
        "gc.collect()\n",
        "\n",
        "print()\n",
        "print(f\"Saved {n_passages:,} passages to disk\")\n",
        "print(\"Cleared passages from memory\")\n",
        "\n",
        "# Memory status\n",
        "import psutil\n",
        "mem = psutil.virtual_memory()\n",
        "print(f\"Memory: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n",
        "\n",
        "print()\n",
        "print(\"Bond type distribution:\")\n",
        "for bond_type, count in sorted(bond_counts.items(), key=lambda x: -x[1]):\n",
        "    pct = count / sum(bond_counts.values()) * 100\n",
        "    bar = '#' * int(pct)\n",
        "    print(f\"  {bond_type:20s}: {count:6,} ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "# v6: Warn if NONE is dominant (may indicate patterns need expansion)\n",
        "if bond_counts.get('NONE', 0) / sum(bond_counts.values()) > 0.5:\n",
        "    print()\n",
        "    print(\"WARNING: NONE class is >50% - consider expanding patterns.\")\n",
        "\n",
        "mark_task(\"Extract bond structures\", \"done\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 7. Generate Train/Test Splits { display-mode: \"form\" }\n#@markdown Creates splits from saved files. Memory efficient - reads only IDs.\n\nimport random\nimport gc\nrandom.seed(42)\n\nmark_task(\"Generate train/test splits\", \"running\")\n\nprint(\"=\" * 60)\nprint(\"GENERATING SPLITS (MEMORY EFFICIENT)\")\nprint(\"=\" * 60)\nprint()\n\n# Read only IDs and time periods from disk - don't load full passages\nprint(\"Reading passage metadata from disk...\")\npassage_info = []  # List of (id, time_period) tuples - minimal memory\n\nwith open(\"data/processed/passages.jsonl\", 'r') as f:\n    for line in tqdm(f, desc=\"Reading IDs\", unit=\"line\"):\n        p = json.loads(line)\n        passage_info.append((p['id'], p['time_period']))\n\nprint(f\"Loaded {len(passage_info):,} passage IDs\")\n\n# Define time periods\ntrain_periods = {'BIBLICAL', 'SECOND_TEMPLE', 'TANNAITIC', 'AMORAIC', 'GEONIC', 'RISHONIM'}\nvalid_periods = {'ACHRONIM'}\ntest_periods = {'MODERN_HEBREW', 'DEAR_ABBY'}\n\nprint()\nprint(\"Filtering by time period...\")\nancient_ids = [(pid, tp) for pid, tp in passage_info if tp in train_periods]\nearly_modern_ids = [(pid, tp) for pid, tp in passage_info if tp in valid_periods]\nmodern_ids = [(pid, tp) for pid, tp in passage_info if tp in test_periods]\n\nprint(f\"  Ancient/Medieval: {len(ancient_ids):,}\")\nprint(f\"  Early Modern:     {len(early_modern_ids):,}\")\nprint(f\"  Modern:           {len(modern_ids):,}\")\n\n# Shuffle\nrandom.shuffle(ancient_ids)\nrandom.shuffle(early_modern_ids)\nrandom.shuffle(modern_ids)\n\n# ============================================================\n# SPLIT A: ANCIENT -> MODERN\n# ============================================================\nprint()\nprint(\"-\" * 60)\nprint(\"SPLIT A: Train ANCIENT, Test MODERN\")\nprint(\"-\" * 60)\n\ntemporal_A = {\n    'name': 'ancient_to_modern',\n    'direction': 'A->M',\n    'train_ids': [pid for pid, _ in ancient_ids],\n    'valid_ids': [pid for pid, _ in early_modern_ids],\n    'test_ids': [pid for pid, _ in modern_ids],\n    'train_size': len(ancient_ids),\n    'valid_size': len(early_modern_ids),\n    'test_size': len(modern_ids)\n}\nprint(f\"  Train: {temporal_A['train_size']:,}\")\nprint(f\"  Valid: {temporal_A['valid_size']:,}\")\nprint(f\"  Test:  {temporal_A['test_size']:,}\")\n\n# ============================================================\n# SPLIT B: MODERN -> ANCIENT\n# ============================================================\nprint()\nprint(\"-\" * 60)\nprint(\"SPLIT B: Train MODERN, Test ANCIENT\")\nprint(\"-\" * 60)\n\nn_modern = len(modern_ids)\nancient_test = ancient_ids[n_modern:n_modern*2] if len(ancient_ids) >= n_modern*2 else ancient_ids[n_modern:]\n\ntemporal_B = {\n    'name': 'modern_to_ancient',\n    'direction': 'M->A',\n    'train_ids': [pid for pid, _ in modern_ids],\n    'valid_ids': [pid for pid, _ in early_modern_ids[:len(early_modern_ids)//2]],\n    'test_ids': [pid for pid, _ in ancient_test],\n    'train_size': len(modern_ids),\n    'valid_size': len(early_modern_ids) // 2,\n    'test_size': len(ancient_test)\n}\nprint(f\"  Train: {temporal_B['train_size']:,}\")\nprint(f\"  Valid: {temporal_B['valid_size']:,}\")\nprint(f\"  Test:  {temporal_B['test_size']:,}\")\n\n# ============================================================\n# SPLIT C: MIXED CONTROL\n# ============================================================\nprint()\nprint(\"-\" * 60)\nprint(\"SPLIT C: MIXED (Control)\")\nprint(\"-\" * 60)\n\nall_ids = ancient_ids + modern_ids\nrandom.shuffle(all_ids)\nn = len(all_ids)\nn_train = int(0.7 * n)\nn_valid = int(0.15 * n)\n\ntemporal_C = {\n    'name': 'mixed_control',\n    'direction': 'MIXED',\n    'train_ids': [pid for pid, _ in all_ids[:n_train]],\n    'valid_ids': [pid for pid, _ in all_ids[n_train:n_train+n_valid]],\n    'test_ids': [pid for pid, _ in all_ids[n_train+n_valid:]],\n    'train_size': n_train,\n    'valid_size': n_valid,\n    'test_size': n - n_train - n_valid\n}\nprint(f\"  Train: {temporal_C['train_size']:,}\")\nprint(f\"  Valid: {temporal_C['valid_size']:,}\")\nprint(f\"  Test:  {temporal_C['test_size']:,}\")\n\n# Clear temporary data\ndel passage_info, ancient_ids, early_modern_ids, modern_ids, all_ids\ngc.collect()\n\n# Save\nprint()\nprint(\"Saving splits...\")\nsplits = {\n    'ancient_to_modern': temporal_A,\n    'modern_to_ancient': temporal_B,\n    'mixed_control': temporal_C\n}\n\nwith open(\"data/splits/all_splits.json\", 'w') as f:\n    json.dump(splits, f, indent=2)\n\n# Memory status\nimport psutil\nmem = psutil.virtual_memory()\nprint(f\"Memory: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n\nprint()\nprint(\"SPLITS SAVED:\")\nprint(\"  - ancient_to_modern (A->M)\")\nprint(\"  - modern_to_ancient (M->A)\")  \nprint(\"  - mixed_control\")\n\n\n\n\n\n# ============================================================\n# DISTRIBUTION CHECK - Catch problems early\n# ============================================================\nprint()\nprint(\"=\" * 60)\nprint(\"LABEL DISTRIBUTION CHECK\")\nprint(\"=\" * 60)\n\n# Count Hohfeld labels AND primary_relation labels\nhohfeld_counts = {}\ntime_counts = {}\nbond_type_counts = {}  # v6: Track bond types for transfer test\n\nwith open(\"data/processed/bond_structures.jsonl\", 'r') as fb, \\\n     open(\"data/processed/passages.jsonl\", 'r') as fp:\n    for b_line, p_line in zip(fb, fp):\n        b = json.loads(b_line)\n        p = json.loads(p_line)\n        h = b['bond_structure'].get('hohfeld_state', None)\n        t = p['time_period']\n        bond = b['bond_structure'].get('primary_relation', 'NONE')\n        \n        hohfeld_counts[h] = hohfeld_counts.get(h, 0) + 1\n        time_counts[t] = time_counts.get(t, 0) + 1\n        bond_type_counts[bond] = bond_type_counts.get(bond, 0) + 1\n\nprint()\nprint(\"Hohfeld distribution:\")\ntotal_h = sum(hohfeld_counts.values())\nfor h, c in sorted(hohfeld_counts.items(), key=lambda x: -x[1]):\n    pct = 100 * c / total_h\n    bar = \"#\" * int(pct / 2)\n    print(f\"  {str(h):15s}: {c:>8,} ({pct:5.1f}%) {bar}\")\n\nprint()\nprint(\"Time period distribution:\")\ntotal_t = sum(time_counts.values())\nfor t, c in sorted(time_counts.items(), key=lambda x: -x[1]):\n    pct = 100 * c / total_t\n    bar = \"#\" * int(pct / 2)\n    print(f\"  {t:15s}: {c:>8,} ({pct:5.1f}%) {bar}\")\n\n# v6: Show bond type distribution\nprint()\nprint(\"Bond type distribution (for transfer test):\")\ntotal_b = sum(bond_type_counts.values())\nfor b, c in sorted(bond_type_counts.items(), key=lambda x: -x[1]):\n    pct = 100 * c / total_b\n    bar = \"#\" * int(pct / 2)\n    print(f\"  {b:20s}: {c:>8,} ({pct:5.1f}%) {bar}\")\n\n# Compute actual chance baselines\nN_HOHFELD_CLASSES = len([h for h in hohfeld_counts if h is not None]) + 1  # +1 for None\nN_TIME_CLASSES = len(time_counts)\nN_BOND_CLASSES = len(bond_type_counts)  # v6: Bond classes for transfer test\n\nCHANCE_HOHFELD = 1.0 / N_HOHFELD_CLASSES\nCHANCE_TIME = 1.0 / N_TIME_CLASSES\nCHANCE_BOND = 1.0 / N_BOND_CLASSES  # v6\n\nprint()\nprint(f\"Chance baseline - Hohfeld: {CHANCE_HOHFELD:.1%} ({N_HOHFELD_CLASSES} classes)\")\nprint(f\"Chance baseline - Time:    {CHANCE_TIME:.1%} ({N_TIME_CLASSES} classes)\")\nprint(f\"Chance baseline - Bond:    {CHANCE_BOND:.1%} ({N_BOND_CLASSES} classes)\")\n\n# Save baselines for later\nbaselines = {\n    'hohfeld_counts': {str(k): v for k, v in hohfeld_counts.items()},\n    'time_counts': time_counts,\n    'bond_type_counts': bond_type_counts,  # v6\n    'chance_hohfeld': CHANCE_HOHFELD,\n    'chance_time': CHANCE_TIME,\n    'chance_bond': CHANCE_BOND,  # v6\n    'n_hohfeld_classes': N_HOHFELD_CLASSES,\n    'n_time_classes': N_TIME_CLASSES,\n    'n_bond_classes': N_BOND_CLASSES  # v6\n}\nwith open(\"data/splits/baselines.json\", 'w') as f:\n    json.dump(baselines, f, indent=2)\n\n# Warn if severe imbalance\nmost_common_hohfeld = max(hohfeld_counts.values()) / total_h\nif most_common_hohfeld > 0.7:\n    print()\n    print(f\"WARNING: Hohfeld labels severely imbalanced! Most common = {most_common_hohfeld:.1%}\")\n    print(\"         Model may just predict majority class.\")\n\n# ============================================================\n# SAVE PREPROCESSING TO DRIVE\n# ============================================================\nprint()\nprint(\"=\" * 60)\nprint(\"SAVING PREPROCESSED DATA TO GOOGLE DRIVE\")\nprint(\"=\" * 60)\nimport shutil\nshutil.copytree(\"data/processed\", f\"{SAVE_DIR}/processed\", dirs_exist_ok=True)\nshutil.copytree(\"data/splits\", f\"{SAVE_DIR}/splits\", dirs_exist_ok=True)\nprint(f\"Saved to {SAVE_DIR}\")\nprint(\"If session dies, run: !cp -r {SAVE_DIR}/* data/\")\nprint(\"Then skip to Cell 8.\")\nprint()\n\nmark_task(\"Generate train/test splits\", \"done\")\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 8. Define BIP Model Architecture { display-mode: \"form\" }\n#@markdown Defines the model with bond prediction head for transfer test.\n#@markdown **v6 NEW**: Added bond_classifier and bond prediction output.\n\nimport gc\nimport psutil\n\n# CRITICAL: Clear memory before loading model\nprint(\"Clearing memory before model load...\")\ngc.collect()\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nmem = psutil.virtual_memory()\nprint(f\"Memory before model: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\nprint()\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nimport gc\n\nprint(\"=\" * 60)\nprint(\"DEFINING MODEL ARCHITECTURE (v6)\")\nprint(\"=\" * 60)\nprint()\nprint(\"*** CROSS-CULTURAL MODE ***\")\nprint(\"Encoder: paraphrase-multilingual-MiniLM-L12-v2\")\nprint(\"  - Trained on 50+ languages including Hebrew and English\")\nprint(\"  - Maps both languages into shared embedding space\")\nprint(\"  - Sefaria passages: ORIGINAL HEBREW\")\nprint(\"  - Dear Abby passages: ENGLISH\")\nprint()\nprint(\"v6 NEW: Added bond_classifier for explicit bond transfer test\")\nprint()\n\nclass GradientReversal(torch.autograd.Function):\n    \"\"\"Gradient reversal layer for adversarial training.\"\"\"\n    @staticmethod\n    def forward(ctx, x, lambda_):\n        ctx.lambda_ = lambda_\n        return x.clone()\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        return -ctx.lambda_ * grad_output, None\n\ndef gradient_reversal(x, lambda_=1.0):\n    return GradientReversal.apply(x, lambda_)\n\nclass BIPEncoder(nn.Module):\n    \"\"\"Sentence encoder using pretrained transformer.\"\"\"\n    def __init__(self, model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", d_model=384):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        self.d_model = d_model\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        hidden = outputs.last_hidden_state\n        mask = attention_mask.unsqueeze(-1).float()\n        pooled = (hidden * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n        return pooled\n\nclass BIPModel(nn.Module):\n    \"\"\"Bond Invariance Principle Model with adversarial disentanglement.\n    \n    v6 NEW: Added bond_classifier for explicit bond prediction (primary_relation)\n    to measure bond-level transfer accuracy.\n    \"\"\"\n    def __init__(self, d_model=384, d_bond=64, d_label=32, n_periods=14, n_hohfeld=4, n_bonds=11):\n        super().__init__()\n        \n        self.encoder = BIPEncoder()\n        \n        self.bond_proj = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, d_bond)\n        )\n        \n        self.label_proj = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, d_label)\n        )\n        \n        self.time_classifier_bond = nn.Linear(d_bond, n_periods)\n        self.time_classifier_label = nn.Linear(d_label, n_periods)\n        self.hohfeld_classifier = nn.Linear(d_bond, n_hohfeld)\n        \n        # v6 NEW: Bond type classifier for transfer accuracy measurement\n        self.bond_classifier = nn.Linear(d_bond, n_bonds)\n    \n    def forward(self, input_ids, attention_mask, adversarial_lambda=1.0):\n        h = self.encoder(input_ids, attention_mask)\n        \n        z_bond = self.bond_proj(h)\n        z_label = self.label_proj(h)\n        \n        z_bond_adv = gradient_reversal(z_bond, adversarial_lambda)\n        time_pred_bond = self.time_classifier_bond(z_bond_adv)\n        time_pred_label = self.time_classifier_label(z_label)\n        hohfeld_pred = self.hohfeld_classifier(z_bond)\n        \n        # v6 NEW: Predict primary bond type from z_bond\n        bond_pred = self.bond_classifier(z_bond)\n        \n        return {\n            'z_bond': z_bond,\n            'z_label': z_label,\n            'time_pred_bond': time_pred_bond,\n            'time_pred_label': time_pred_label,\n            'hohfeld_pred': hohfeld_pred,\n            'bond_pred': bond_pred  # v6 NEW\n        }\n\n# Time period mapping\nTIME_PERIOD_TO_IDX = {\n    'BIBLICAL': 0, 'SECOND_TEMPLE': 1, 'TANNAITIC': 2, 'AMORAIC': 3,\n    'GEONIC': 4, 'RISHONIM': 5, 'ACHRONIM': 6, 'MODERN_HEBREW': 7,\n    # Chinese\n    'CONFUCIAN': 8, 'DAOIST': 9, 'MOHIST': 10,\n    # Arabic  \n    'QURANIC': 11, 'HADITH': 12,\n    # Modern\n    'DEAR_ABBY': 13\n}\n\nHOHFELD_TO_IDX = {\n    'OBLIGATION': 0, 'RIGHT': 1, 'LIBERTY': 2, None: 3\n}\n\n# v6 NEW: Bond type mapping (matches BondType enum order)\nBOND_TYPE_TO_IDX = {\n    'HARM_PREVENTION': 0,\n    'RECIPROCITY': 1,\n    'AUTONOMY': 2,\n    'PROPERTY': 3,\n    'FAMILY': 4,\n    'AUTHORITY': 5,\n    'EMERGENCY': 6,\n    'CONTRACT': 7,\n    'CARE': 8,\n    'FAIRNESS': 9,\n    'NONE': 10\n}\nIDX_TO_BOND_TYPE = {v: k for k, v in BOND_TYPE_TO_IDX.items()}\n\nclass MoralDataset(Dataset):\n    \"\"\"\n    MEMORY-EFFICIENT Dataset that reads from disk on demand.\n    v6 NEW: Also returns bond_label for transfer accuracy test.\n    \"\"\"\n    def __init__(self, passage_ids: set, passages_file: str, bonds_file: str, tokenizer, max_len=64):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.passage_ids = passage_ids\n        \n        print(f\"  Indexing {len(passage_ids):,} passages...\")\n        \n        self.data = []\n        \n        # Load only the passages we need\n        with open(passages_file, 'r') as f_pass, open(bonds_file, 'r') as f_bond:\n            for p_line, b_line in tqdm(zip(f_pass, f_bond), desc=\"  Loading subset\", unit=\"line\", total=None):\n                p = json.loads(p_line)\n                if p['id'] in passage_ids:\n                    b = json.loads(b_line)\n                    self.data.append({\n                        'text': (p.get('text_original', '') if p.get('language') in ['hebrew', 'chinese', 'arabic'] else p.get('text_english', ''))[:1000],\n                        'time_period': p['time_period'],\n                        'source_type': p['source_type'],  # v6: Track corpus for per-corpus F1\n                        'hohfeld': b['bond_structure']['hohfeld_state'],\n                        'primary_relation': b['bond_structure']['primary_relation']  # v6 NEW\n                    })\n        \n        print(f\"  Loaded {len(self.data):,} samples\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        encoding = self.tokenizer(\n            item['text'],\n            truncation=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'time_label': TIME_PERIOD_TO_IDX.get(item['time_period'], 8),\n            'hohfeld_label': HOHFELD_TO_IDX.get(item['hohfeld'], 3),\n            'bond_label': BOND_TYPE_TO_IDX.get(item['primary_relation'], 10),  # v6 NEW\n            'source_type': item['source_type']  # v6: For per-corpus metrics\n        }\n\ndef collate_fn(batch):\n    return {\n        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n        'time_labels': torch.tensor([x['time_label'] for x in batch]),\n        'hohfeld_labels': torch.tensor([x['hohfeld_label'] for x in batch]),\n        'bond_labels': torch.tensor([x['bond_label'] for x in batch]),  # v6 NEW\n        'source_types': [x['source_type'] for x in batch]  # v6: Keep as list for grouping\n    }\n\n# Memory cleanup\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(\"Model architecture defined!\")\nprint(f\"  - Bond types: {len(BOND_TYPE_TO_IDX)} classes\")\nprint(f\"  - Time periods: {len(TIME_PERIOD_TO_IDX)} classes\")\nprint(f\"  - Hohfeld states: {len(HOHFELD_TO_IDX)} classes\")\nprint()\n\n# Memory status\nimport psutil\nmem = psutil.virtual_memory()\nprint(f\"Memory: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 9. Train BIP Model - BIDIRECTIONAL { display-mode: \"form\" }\n#@markdown Trains on BOTH directions with bond transfer accuracy test.\n#@markdown **v6 FIX**: Fixed TPU double-stepping bug.\n#@markdown **v6 NEW**: Trains bond classifier and reports F1 by corpus.\n\nimport gc\nimport psutil\nfrom sklearn.metrics import f1_score, classification_report\n\nmark_task(\"Train BIP model\", \"running\")\n\n# Memory cleanup before training\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nmem = psutil.virtual_memory()\nprint(f\"Memory at start: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB\")\n\nprint(\"=\" * 60)\nprint(\"BIDIRECTIONAL BIP TRAINING (v6)\")\nprint(\"=\" * 60)\nprint()\nprint(f\"Accelerator: {ACCELERATOR}\")\nprint(f\"Device: {device}\")\nprint()\nprint(\"v6 NEW: Now training bond classifier for transfer accuracy test\")\nprint()\n\n# Load tokenizer once\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n\n# Store results for both directions\nall_results = {}\n\nfor split_name in ['ancient_to_modern', 'modern_to_ancient']:\n    print()\n    print(\"=\" * 60)\n    print(f\"DIRECTION {split_name}: {'Ancient → Modern' if split_name == 'ancient_to_modern' else 'Modern → Ancient'}\")\n    print(\"=\" * 60)\n    print()\n    \n    # Load appropriate split\n    with open(\"data/splits/all_splits.json\", 'r') as f:\n        splits = json.load(f)\n    split = splits[split_name]\n    \n    print(f\"Train: {split['train_size']:,}\")\n    print(f\"Valid: {split['valid_size']:,}\")\n    print(f\"Test:  {split['test_size']:,}\")\n    print()\n    \n    # Create fresh model for each direction\n    print(\"Creating fresh model...\")\n    model = BIPModel().to(device)\n    \n    # Compile model for speed (PyTorch 2.0+)\n    if TORCH_COMPILE:\n        print(\"Compiling model with torch.compile...\")\n        model = torch.compile(model, mode=\"reduce-overhead\")\n    \n    if split_name == 'ancient_to_modern':\n        print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Create datasets\n    print(\"Creating datasets...\")\n    train_dataset = MoralDataset(\n        set(split['train_ids']),\n        \"data/processed/passages.jsonl\",\n        \"data/processed/bond_structures.jsonl\",\n        tokenizer\n    )\n    valid_dataset = MoralDataset(\n        set(split['valid_ids']),\n        \"data/processed/passages.jsonl\",\n        \"data/processed/bond_structures.jsonl\",\n        tokenizer\n    )\n    test_dataset = MoralDataset(\n        set(split['test_ids']),\n        \"data/processed/passages.jsonl\",\n        \"data/processed/bond_structures.jsonl\",\n        tokenizer\n    )\n    \n    print(f\"Train samples: {len(train_dataset):,}\")\n    print(f\"Valid samples: {len(valid_dataset):,}\")\n    print(f\"Test samples:  {len(test_dataset):,}\")\n    print()\n    \n    if len(train_dataset) == 0:\n        print(\"ERROR: No training data!\")\n        continue\n    \n    # Adjust batch size based on dataset size\n    batch_size = 256 if split_name == 'ancient_to_modern' else min(32, len(train_dataset) // 10)\n    batch_size = max(32, batch_size)  # Minimum batch size\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                              collate_fn=collate_fn, drop_last=True, num_workers=4, pin_memory=True, prefetch_factor=4, persistent_workers=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size*2, shuffle=False,\n                              collate_fn=collate_fn, num_workers=4, pin_memory=True, prefetch_factor=4, persistent_workers=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False,\n                             collate_fn=collate_fn, num_workers=4, pin_memory=True, prefetch_factor=4, persistent_workers=True)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    \n    n_epochs = 3\n    best_valid_loss = float('inf')\n    patience = 3\n    patience_counter = 0\n    \n    print(f\"Training for {n_epochs} epochs (batch_size={batch_size})...\")\n    print()\n    \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        total_loss = 0\n        n_batches = 0\n        \n        pbar = tqdm(train_loader, desc=f\"[{split_name}] Epoch {epoch}/{n_epochs}\", unit=\"batch\")\n        for batch in pbar:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            time_labels = batch['time_labels'].to(device)\n            hohfeld_labels = batch['hohfeld_labels'].to(device)\n            bond_labels = batch['bond_labels'].to(device)  # v6 NEW\n            \n            # Mixed precision forward pass\n            with torch.cuda.amp.autocast(enabled=USE_AMP):\n                outputs = model(input_ids, attention_mask, adversarial_lambda=1.0)\n                \n                # Losses\n                loss_time_bond = F.cross_entropy(outputs['time_pred_bond'], time_labels)\n                loss_time_label = F.cross_entropy(outputs['time_pred_label'], time_labels)\n                loss_hohfeld = F.cross_entropy(outputs['hohfeld_pred'], hohfeld_labels)\n                loss_bond = F.cross_entropy(outputs['bond_pred'], bond_labels)  # v6 NEW\n            \n            # v6: Include bond loss in total\n            loss = loss_hohfeld + loss_time_label + loss_time_bond + loss_bond\n            \n            optimizer.zero_grad()\n            \n            # v6 FIX: Choose ONE stepping mechanism, not both\n            if USE_TPU:\n                # TPU: Use XLA optimizer step ONLY\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                xm.optimizer_step(optimizer)\n                xm.mark_step()\n            elif USE_AMP and scaler is not None:\n                # GPU with AMP\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                # CPU or GPU without AMP\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n            \n            total_loss += loss.item()\n            n_batches += 1\n            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n        \n        avg_train_loss = total_loss / n_batches\n        \n        # Validation\n        model.eval()\n        valid_loss = 0\n        valid_batches = 0\n        time_correct = 0\n        time_total = 0\n        hohfeld_correct = 0\n        hohfeld_total = 0\n        bond_correct = 0  # v6 NEW\n        bond_total = 0\n        \n        with torch.no_grad():\n            for batch in valid_loader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                time_labels = batch['time_labels'].to(device)\n                hohfeld_labels = batch['hohfeld_labels'].to(device)\n                bond_labels = batch['bond_labels'].to(device)  # v6 NEW\n                \n                outputs = model(input_ids, attention_mask, adversarial_lambda=0)\n                loss = F.cross_entropy(outputs['hohfeld_pred'], hohfeld_labels)\n                valid_loss += loss.item()\n                valid_batches += 1\n                \n                time_preds = outputs['time_pred_bond'].argmax(dim=-1)\n                time_correct += (time_preds == time_labels).sum().item()\n                time_total += len(time_labels)\n                \n                hohfeld_preds = outputs['hohfeld_pred'].argmax(dim=-1)\n                hohfeld_correct += (hohfeld_preds == hohfeld_labels).sum().item()\n                hohfeld_total += len(hohfeld_labels)\n                \n                # v6 NEW: Bond accuracy\n                bond_preds = outputs['bond_pred'].argmax(dim=-1)\n                bond_correct += (bond_preds == bond_labels).sum().item()\n                bond_total += len(bond_labels)\n                \n                if USE_TPU:\n                    xm.mark_step()\n        \n        avg_valid_loss = valid_loss / valid_batches if valid_batches > 0 else 0\n        time_acc = time_correct / time_total if time_total > 0 else 0\n        hohfeld_acc_val = hohfeld_correct / hohfeld_total if hohfeld_total > 0 else 0\n        bond_acc_val = bond_correct / bond_total if bond_total > 0 else 0  # v6 NEW\n        \n        print(f\"[{split_name}] Epoch {epoch}: Loss={avg_train_loss:.4f}/{avg_valid_loss:.4f}, Hohfeld={hohfeld_acc_val:.1%}, Bond={bond_acc_val:.1%}, TimeAcc={time_acc:.1%}\")\n        \n        if avg_valid_loss < best_valid_loss:\n            best_valid_loss = avg_valid_loss\n            model_path = f\"models/checkpoints/best_model_{split_name}.pt\"\n            if USE_TPU:\n                xm.save(model.state_dict(), model_path)\n            else:\n                torch.save(model.state_dict(), model_path)\n            print(f\"  -> Saved best model for {split_name}!\")\n            # Backup to Drive\n            import shutil\n            shutil.copy(model_path, f\"{SAVE_DIR}/best_model_{split_name}.pt\")\n            print(f\"  -> Backed up to Google Drive\")\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"  Early stopping at epoch {epoch} (no improvement for {patience} epochs)\")\n                break\n    \n    # ================================================================\n    # EVALUATE ON TEST SET - Including bond transfer accuracy (v6 NEW)\n    # ================================================================\n    print()\n    print(f\"Evaluating {split_name} on test set...\")\n    \n    model.load_state_dict(torch.load(f\"models/checkpoints/best_model_{split_name}.pt\", map_location='cpu'))\n    model = model.to(device)\n    model.eval()\n    \n    all_time_preds = []\n    all_time_labels = []\n    all_hohfeld_preds = []\n    all_hohfeld_labels = []\n    all_bond_preds = []  # v6 NEW\n    all_bond_labels = []  # v6 NEW\n    all_source_types = []  # v6: For per-corpus metrics\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=f\"[{split_name}] Testing\", unit=\"batch\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            outputs = model(input_ids, attention_mask, adversarial_lambda=0)\n            \n            all_time_preds.extend(outputs['time_pred_bond'].argmax(dim=-1).cpu().tolist())\n            all_time_labels.extend(batch['time_labels'].tolist())\n            all_hohfeld_preds.extend(outputs['hohfeld_pred'].argmax(dim=-1).cpu().tolist())\n            all_hohfeld_labels.extend(batch['hohfeld_labels'].tolist())\n            all_bond_preds.extend(outputs['bond_pred'].argmax(dim=-1).cpu().tolist())  # v6 NEW\n            all_bond_labels.extend(batch['bond_labels'].tolist())  # v6 NEW\n            all_source_types.extend(batch['source_types'])  # v6\n            \n            if USE_TPU:\n                xm.mark_step()\n    \n    # Calculate metrics\n    time_acc = sum(p == l for p, l in zip(all_time_preds, all_time_labels)) / len(all_time_preds)\n    hohfeld_acc = sum(p == l for p, l in zip(all_hohfeld_preds, all_hohfeld_labels)) / len(all_hohfeld_preds)\n    bond_acc = sum(p == l for p, l in zip(all_bond_preds, all_bond_labels)) / len(all_bond_preds)  # v6 NEW\n    \n    # v6 NEW: Calculate F1 scores\n    bond_f1_macro = f1_score(all_bond_labels, all_bond_preds, average='macro', zero_division=0)\n    bond_f1_weighted = f1_score(all_bond_labels, all_bond_preds, average='weighted', zero_division=0)\n    hohfeld_f1_macro = f1_score(all_hohfeld_labels, all_hohfeld_preds, average='macro', zero_division=0)\n    \n    # v6 NEW: Per-corpus bond F1\n    corpus_bond_f1 = {}\n    for corpus in set(all_source_types):\n        mask = [s == corpus for s in all_source_types]\n        corpus_preds = [p for p, m in zip(all_bond_preds, mask) if m]\n        corpus_labels = [l for l, m in zip(all_bond_labels, mask) if m]\n        if len(corpus_labels) > 0:\n            corpus_bond_f1[corpus] = {\n                'f1_macro': f1_score(corpus_labels, corpus_preds, average='macro', zero_division=0),\n                'f1_weighted': f1_score(corpus_labels, corpus_preds, average='weighted', zero_division=0),\n                'accuracy': sum(p == l for p, l in zip(corpus_preds, corpus_labels)) / len(corpus_labels),\n                'n_samples': len(corpus_labels)\n            }\n    \n    all_results[split_name] = {\n        'time_acc': time_acc,\n        'hohfeld_acc': hohfeld_acc,\n        'hohfeld_f1_macro': hohfeld_f1_macro,\n        'bond_acc': bond_acc,  # v6 NEW\n        'bond_f1_macro': bond_f1_macro,  # v6 NEW\n        'bond_f1_weighted': bond_f1_weighted,  # v6 NEW\n        'corpus_bond_f1': corpus_bond_f1,  # v6 NEW: Per-corpus metrics\n        'train_size': split['train_size'],\n        'test_size': split['test_size']\n    }\n    \n    print()\n    print(f\"{split_name.upper()} RESULTS:\")\n    print(f\"  Time prediction from z_bond: {time_acc:.1%} (chance ~11%)\")\n    print(f\"  Hohfeld classification:      {hohfeld_acc:.1%} (F1={hohfeld_f1_macro:.3f})\")\n    print(f\"  Bond classification:         {bond_acc:.1%} (F1={bond_f1_macro:.3f})\")\n    print()\n    print(\"  Bond F1 by corpus:\")\n    for corpus, metrics in corpus_bond_f1.items():\n        print(f\"    {corpus}: F1={metrics['f1_macro']:.3f}, Acc={metrics['accuracy']:.1%} (n={metrics['n_samples']:,})\")\n\nprint()\nprint(\"=\" * 60)\nprint(\"TRAINING COMPLETE - BOTH DIRECTIONS\")\nprint(\"=\" * 60)\n\nmark_task(\"Train BIP model\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 10. Evaluate Bidirectional Results { display-mode: \"form\" }\n#@markdown Compares results from BOTH directions to assess true invariance.\n#@markdown **v6 NEW**: Reports bond transfer accuracy and F1 by corpus.\n\nimport gc\nimport psutil\n\nmark_task(\"Evaluate results\", \"running\")\n\nfrom collections import Counter\ntry:\n    from sklearn.metrics import confusion_matrix, classification_report\n    HAS_SKLEARN = True\nexcept ImportError:\n    HAS_SKLEARN = False\n    print(\"sklearn not available - skipping confusion matrices\")\n\nmem = psutil.virtual_memory()\nprint(f\"Memory at eval start: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB\")\n\nprint(\"=\" * 60)\nprint(\"BIDIRECTIONAL BIP RESULTS (v6)\")\nprint(\"=\" * 60)\nprint()\n\n# Load baselines\ntry:\n    with open(\"data/splits/baselines.json\", 'r') as f:\n        baselines = json.load(f)\n    chance_time = baselines['chance_time']\n    chance_hohfeld = baselines['chance_hohfeld']\n    chance_bond = baselines.get('chance_bond', 1/11)\nexcept:\n    chance_time = 1/9\n    chance_hohfeld = 1/4\n    chance_bond = 1/11\n\nprint(\"DIRECTION A: Ancient → Modern\")\nprint(\"-\" * 40)\nres_A = all_results.get('ancient_to_modern', {})\nprint(f\"  Trained on:    {res_A.get('train_size', 0):,} ancient passages\")\nprint(f\"  Tested on:     {res_A.get('test_size', 0):,} modern passages\")\nprint(f\"  Time acc:      {res_A.get('time_acc', 0):.1%} (chance: {chance_time:.1%})\")\nprint(f\"  Hohfeld acc:   {res_A.get('hohfeld_acc', 0):.1%} (F1: {res_A.get('hohfeld_f1_macro', 0):.3f})\")\nprint(f\"  Bond acc:      {res_A.get('bond_acc', 0):.1%} (F1: {res_A.get('bond_f1_macro', 0):.3f})\")\nprint()\n\n# v6 NEW: Bond F1 by corpus\nif 'corpus_bond_f1' in res_A:\n    print(\"  Bond transfer by corpus:\")\n    for corpus, metrics in res_A['corpus_bond_f1'].items():\n        print(f\"    {corpus}: F1={metrics['f1_macro']:.3f}, Acc={metrics['accuracy']:.1%}\")\n    print()\n\nA_time_near_chance = abs(res_A.get('time_acc', 0) - chance_time) < 0.05\nA_hohfeld_good = res_A.get('hohfeld_acc', 0) > 0.35\nA_bond_good = res_A.get('bond_f1_macro', 0) > chance_bond * 2  # v6: Bond transfer threshold\n\nprint(f\"  Time invariant?    {'YES ✓' if A_time_near_chance else 'NO ✗'}\")\nprint(f\"  Moral structure?   {'YES ✓' if A_hohfeld_good else 'WEAK'}\")\nprint(f\"  Bond transfer?     {'YES ✓' if A_bond_good else 'WEAK'} (F1 > {chance_bond*2:.1%})\")\nprint()\n\nprint(\"DIRECTION B: Modern → Ancient\")\nprint(\"-\" * 40)\nres_B = all_results.get('modern_to_ancient', {})\nprint(f\"  Trained on:    {res_B.get('train_size', 0):,} modern passages\")\nprint(f\"  Tested on:     {res_B.get('test_size', 0):,} ancient passages\")\nprint(f\"  Time acc:      {res_B.get('time_acc', 0):.1%} (chance: {chance_time:.1%})\")\nprint(f\"  Hohfeld acc:   {res_B.get('hohfeld_acc', 0):.1%} (F1: {res_B.get('hohfeld_f1_macro', 0):.3f})\")\nprint(f\"  Bond acc:      {res_B.get('bond_acc', 0):.1%} (F1: {res_B.get('bond_f1_macro', 0):.3f})\")\nprint()\n\n# v6 NEW: Bond F1 by corpus\nif 'corpus_bond_f1' in res_B:\n    print(\"  Bond transfer by corpus:\")\n    for corpus, metrics in res_B['corpus_bond_f1'].items():\n        print(f\"    {corpus}: F1={metrics['f1_macro']:.3f}, Acc={metrics['accuracy']:.1%}\")\n    print()\n\nB_time_near_chance = abs(res_B.get('time_acc', 0) - chance_time) < 0.05\nB_hohfeld_good = res_B.get('hohfeld_acc', 0) > 0.35\nB_bond_good = res_B.get('bond_f1_macro', 0) > chance_bond * 2\n\nprint(f\"  Time invariant?    {'YES ✓' if B_time_near_chance else 'NO ✗'}\")\nprint(f\"  Moral structure?   {'YES ✓' if B_hohfeld_good else 'WEAK'}\")\nprint(f\"  Bond transfer?     {'YES ✓' if B_bond_good else 'WEAK'} (F1 > {chance_bond*2:.1%})\")\nprint()\n\nprint(\"=\" * 60)\nprint(\"BIDIRECTIONAL INVARIANCE TEST\")\nprint(\"=\" * 60)\nprint()\n\n# v6: Updated verdict logic to include bond transfer\nif A_time_near_chance and B_time_near_chance and A_hohfeld_good and B_hohfeld_good and A_bond_good and B_bond_good:\n    print(\"\"\"\n    ╔══════════════════════════════════════════════════════════╗\n    ║                                                          ║\n    ║     BIDIRECTIONAL BIP: STRONGLY SUPPORTED                ║\n    ║                                                          ║\n    ╠══════════════════════════════════════════════════════════╣\n    ║                                                          ║\n    ║  ✓ Ancient → Modern: Bond structure transfers            ║\n    ║  ✓ Modern → Ancient: Bond structure transfers            ║\n    ║  ✓ BOTH directions show time-invariant moral geometry    ║\n    ║  ✓ Bond-level transfer confirmed with F1 metrics         ║\n    ║                                                          ║\n    ║  This is STRONG evidence for universal moral structure.  ║\n    ║                                                          ║\n    ╚══════════════════════════════════════════════════════════╝\n    \"\"\")\n    bip_result = \"STRONGLY_SUPPORTED\"\nelif (A_time_near_chance and A_hohfeld_good and A_bond_good) or (B_time_near_chance and B_hohfeld_good and B_bond_good):\n    print(\"\"\"\n    ╔══════════════════════════════════════════════════════════╗\n    ║                                                          ║\n    ║     BIP: SUPPORTED (One direction)                       ║\n    ║                                                          ║\n    ╠══════════════════════════════════════════════════════════╣\n    ║                                                          ║\n    ║  At least one direction shows:                           ║\n    ║    - Time-invariant representation                       ║\n    ║    - Good Hohfeld classification                         ║\n    ║    - Bond transfer above chance                          ║\n    ║                                                          ║\n    ║  Asymmetry may reflect corpus size/diversity differences ║\n    ║                                                          ║\n    ╚══════════════════════════════════════════════════════════╝\n    \"\"\")\n    bip_result = \"SUPPORTED_UNIDIRECTIONAL\"\nelif A_hohfeld_good or B_hohfeld_good:\n    print(\"\"\"\n    ╔══════════════════════════════════════════════════════════╗\n    ║                                                          ║\n    ║     BIP: PARTIAL SUPPORT                                 ║\n    ║                                                          ║\n    ╠══════════════════════════════════════════════════════════╣\n    ║                                                          ║\n    ║  Hohfeld classification works, but:                      ║\n    ║    - Bond transfer may be weak                           ║\n    ║    - Time may still be decodable                         ║\n    ║                                                          ║\n    ║  The representation captures moral structure but         ║\n    ║  may not be fully time-invariant at bond level.          ║\n    ║                                                          ║\n    ╚══════════════════════════════════════════════════════════╝\n    \"\"\")\n    bip_result = \"PARTIAL_SUPPORT\"\nelse:\n    print(\"\"\"\n    ╔══════════════════════════════════════════════════════════╗\n    ║                                                          ║\n    ║     BIP: INCONCLUSIVE                                    ║\n    ║                                                          ║\n    ╠══════════════════════════════════════════════════════════╣\n    ║                                                          ║\n    ║  Neither direction shows clear invariance.               ║\n    ║                                                          ║\n    ║  Possible issues:                                        ║\n    ║  - Need more training epochs                             ║\n    ║  - Need better bond extraction patterns                  ║\n    ║  - BIP may not hold (null result)                        ║\n    ║                                                          ║\n    ╚══════════════════════════════════════════════════════════╝\n    \"\"\")\n    bip_result = \"INCONCLUSIVE\"\n\n# Save detailed results including predictions\ndetailed_results = {\n    'ancient_to_modern': all_results.get('ancient_to_modern', {}),\n    'modern_to_ancient': all_results.get('modern_to_ancient', {}),\n}\n\n# Save to Drive for post-mortem\nwith open(f\"{SAVE_DIR}/detailed_results.json\", 'w') as f:\n    json.dump(detailed_results, f, indent=2, default=str)\nprint(f\"Detailed results saved to {SAVE_DIR}/detailed_results.json\")\n\n# Save results\nresults_summary = {\n    'ancient_to_modern': all_results.get('ancient_to_modern', {}),\n    'modern_to_ancient': all_results.get('modern_to_ancient', {}),\n    'A_time_invariant': A_time_near_chance,\n    'A_moral_structure': A_hohfeld_good,\n    'A_bond_transfer': A_bond_good,\n    'B_time_invariant': B_time_near_chance,\n    'B_moral_structure': B_hohfeld_good,\n    'B_bond_transfer': B_bond_good,\n    'bip_result': bip_result,\n    'chance_time': chance_time,\n    'chance_hohfeld': chance_hohfeld,\n    'chance_bond': chance_bond\n}\n\nwith open('results/bidirectional_results.json', 'w') as f:\n    json.dump(results_summary, f, indent=2, default=str)\n\nprint()\nprint(\"Results saved to results/bidirectional_results.json\")\n\nmark_task(\"Evaluate results\", \"done\")\n\nprint()\nprint(\"=\" * 60)\nprint(\"EXPERIMENT COMPLETE\")\nprint(\"=\" * 60)\nprint_progress()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Save Results\n",
        "\n",
        "Run the cell below to download your trained model and results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 11. Download Results (Optional) { display-mode: \"form\" }\n",
        "#@markdown Creates a zip file with model checkpoint and metrics.\n",
        "#@markdown **v6 FIX**: Now copies correct model filenames.\n",
        "\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create results directory\n",
        "!mkdir -p results\n",
        "\n",
        "# v6 FIX: Copy the actual model checkpoints (with split_name suffix)\n",
        "for split_name in ['ancient_to_modern', 'modern_to_ancient']:\n",
        "    model_path = f\"models/checkpoints/best_model_{split_name}.pt\"\n",
        "    if os.path.exists(model_path):\n",
        "        !cp \"{model_path}\" results/\n",
        "        print(f\"Copied {model_path}\")\n",
        "    else:\n",
        "        print(f\"Warning: {model_path} not found\")\n",
        "\n",
        "!cp data/splits/all_splits.json results/ 2>/dev/null || echo \"No splits file\"\n",
        "!cp data/splits/baselines.json results/ 2>/dev/null || echo \"No baselines file\"\n",
        "\n",
        "# Save metrics\n",
        "if 'all_results' in dir() and all_results:\n",
        "    metrics = {\n",
        "        'accelerator': ACCELERATOR,\n",
        "        'results': all_results,\n",
        "        'bip_result': bip_result if 'bip_result' in dir() else 'unknown'\n",
        "    }\n",
        "    with open('results/metrics.json', 'w') as f:\n",
        "        json.dump(metrics, f, indent=2, default=str)\n",
        "    print(\"Saved metrics.json\")\n",
        "\n",
        "# Zip\n",
        "shutil.make_archive('bip_results_v6', 'zip', 'results')\n",
        "print()\n",
        "print(\"Results saved to bip_results_v6.zip\")\n",
        "print()\n",
        "print(\"Contents:\")\n",
        "!ls -la results/\n",
        "\n",
        "# Download\n",
        "files.download('bip_results_v6.zip')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
