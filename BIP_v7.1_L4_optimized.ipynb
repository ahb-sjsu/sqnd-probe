{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# BIP Cross-Temporal Morality Experiment (v7.1)\n\n**Testing the Bond Invariance Principle across 2000+ years: Hebrew → English**\n\nThis experiment tests whether moral cognition has invariant structure by:\n1. Training on ORIGINAL HEBREW texts (Sefaria corpus, ~500 BCE - 1800 CE)\n2. Testing transfer to modern ENGLISH advice columns (Dear Abby, 1956-2020)\n\n**Hypothesis**: If BIP holds, bond-level features should transfer across 2000 years with minimal degradation compared to in-domain baselines.\n\n---\n\n## v7.1 Changes (L4 Optimized)\n- **Tuned for L4 GPU** (22.5GB VRAM, 53GB RAM, 236GB disk)\n- **Larger batch sizes**: 512 for training (vs 256 on T4)\n- **Full corpus by default**: MAX_SEFARIA_FILES = 0 (unlimited)\n- **Better instrumentation**: Disk space, GPU memory throughout\n- **Fixed missing imports**: All cells are self-contained\n- **Runtime estimate**: ~45-90 minutes on L4\n\n---\n\n## Important Methodological Notes\n\n**Label Source**: Bond types and Hohfeld states are extracted from **English translations** (`text_english`), not the original Hebrew. This means we're testing whether Hebrew text encodes moral structures that align with labels derived from English translations.\n\n**Current Scope**: Hebrew (Sefaria) ↔ English (Dear Abby) transfer only.\n\n---\n\n## Setup Instructions\n1. **Runtime -> Change runtime type -> L4 GPU**\n2. Run cells in order\n3. Expected runtime: ~45-90 minutes on L4\n\n---"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 1. Setup and Install Dependencies { display-mode: \"form\" }\n#@markdown Installs packages and detects GPU. Tuned for L4 runtime.\n\nimport time\nEXPERIMENT_START = time.time()\n\nprint(\"=\" * 60)\nprint(\"BIP TEMPORAL INVARIANCE EXPERIMENT (v7.1 - L4 Optimized)\")\nprint(\"=\" * 60)\nprint()\n\n# Progress tracker\nTASKS = [\n    \"Install dependencies\",\n    \"Clone Sefaria corpus (~8GB)\",\n    \"Clone sqnd-probe repo (Dear Abby data)\",\n    \"Preprocess corpora\",\n    \"Extract bond structures\",\n    \"Generate train/test splits\",\n    \"Train BIP model\",\n    \"Linear probe test\",\n    \"Evaluate results\"\n]\ntask_status = {task: \"pending\" for task in TASKS}\ntask_times = {}\n\ndef print_progress():\n    print()\n    print(\"-\" * 50)\n    print(\"EXPERIMENT PROGRESS:\")\n    print(\"-\" * 50)\n    for task in TASKS:\n        status = task_status[task]\n        if status == \"done\":\n            mark = \"[X]\"\n            time_str = f\" ({task_times.get(task, 0):.1f}s)\" if task in task_times else \"\"\n        elif status == \"running\":\n            mark = \"[>]\"\n            time_str = \"\"\n        else:\n            mark = \"[ ]\"\n            time_str = \"\"\n        print(f\"  {mark} {task}{time_str}\")\n    elapsed = time.time() - EXPERIMENT_START\n    print(\"-\" * 50)\n    print(f\"  Total elapsed: {elapsed/60:.1f} minutes\")\n    print(flush=True)\n\ndef mark_task(task, status):\n    global task_start_time\n    if status == \"running\":\n        task_start_time = time.time()\n    elif status == \"done\" and 'task_start_time' in dir():\n        task_times[task] = time.time() - task_start_time\n    task_status[task] = status\n    print_progress()\n\nprint_progress()\n\nmark_task(\"Install dependencies\", \"running\")\n\nimport os\nimport subprocess\nimport sys\n\n# Install dependencies\nprint(\"Installing dependencies...\")\ndeps = [\n    \"transformers\",\n    \"torch\", \n    \"sentence-transformers\",\n    \"pandas\",\n    \"tqdm\",\n    \"psutil\",\n    \"scikit-learn\"\n]\n\nfor dep in deps:\n    print(f\"  Installing {dep}...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", dep])\n\nprint()\n\n# Detect accelerator\nUSE_TPU = False\nTPU_TYPE = None\n\nif 'COLAB_TPU_ADDR' in os.environ:\n    USE_TPU = True\n    TPU_TYPE = \"TPU (Colab)\"\n    print(\"TPU detected!\")\n\nimport torch\nimport json\nimport psutil\nimport shutil\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    ACCELERATOR = f\"GPU: {gpu_name} ({gpu_mem:.1f}GB)\"\n    device = torch.device(\"cuda\")\n    \n    # L4 detection for batch size tuning\n    IS_L4 = 'L4' in gpu_name\n    IS_A100 = 'A100' in gpu_name\n    IS_V100 = 'V100' in gpu_name\nelif USE_TPU:\n    ACCELERATOR = TPU_TYPE\n    import torch_xla.core.xla_model as xm\n    device = xm.xla_device()\n    IS_L4 = False\n    IS_A100 = False\n    IS_V100 = False\nelse:\n    ACCELERATOR = \"CPU (slow!)\"\n    device = torch.device(\"cpu\")\n    IS_L4 = False\n    IS_A100 = False\n    IS_V100 = False\n\nprint(f\"Accelerator: {ACCELERATOR}\")\nprint(f\"Device: {device}\")\n\n# System resources\nmem = psutil.virtual_memory()\ndisk = shutil.disk_usage('/')\nprint(f\"System RAM: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\nprint(f\"Disk: {disk.used/1e9:.1f}/{disk.total/1e9:.1f} GB ({100*disk.used/disk.total:.1f}%)\")\n\nif torch.cuda.is_available():\n    gpu_used = torch.cuda.memory_allocated()/1e9\n    gpu_total = torch.cuda.get_device_properties(0).total_memory/1e9\n    print(f\"GPU RAM: {gpu_used:.1f}/{gpu_total:.1f} GB ({100*gpu_used/gpu_total:.1f}%)\")\n\n# Set batch sizes based on GPU\nif IS_L4 or IS_A100:\n    BASE_BATCH_SIZE = 512  # L4 has 22.5GB, can handle larger batches\n    print(f\"\\n*** L4/A100 detected: Using large batch size ({BASE_BATCH_SIZE}) ***\")\nelif IS_V100:\n    BASE_BATCH_SIZE = 384\n    print(f\"\\n*** V100 detected: Using batch size {BASE_BATCH_SIZE} ***\")\nelse:\n    BASE_BATCH_SIZE = 256  # T4 or smaller\n    print(f\"\\n*** Using standard batch size ({BASE_BATCH_SIZE}) ***\")\n\n# Enable mixed precision\nif torch.cuda.is_available():\n    print(\"\\nEnabling mixed precision (FP16) for faster training...\")\n    from torch.cuda.amp import autocast, GradScaler\n    USE_AMP = True\n    scaler = GradScaler()\nelse:\n    USE_AMP = False\n    scaler = None\n\nTORCH_COMPILE = False\n\n# Google Drive mount\nprint()\nprint(\"=\" * 60)\nprint(\"MOUNTING GOOGLE DRIVE FOR PERSISTENT STORAGE\")\nprint(\"=\" * 60)\nfrom google.colab import drive\ndrive.mount('/content/drive')\nSAVE_DIR = '/content/drive/MyDrive/BIP_results'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\nos.makedirs(\"data/processed\", exist_ok=True)\nos.makedirs(\"data/splits\", exist_ok=True)\nos.makedirs(\"data/raw\", exist_ok=True)\nos.makedirs(\"models/checkpoints\", exist_ok=True)\nos.makedirs(\"results\", exist_ok=True)\nprint(f\"Results will be saved to: {SAVE_DIR}\")\nprint()\n\ndef print_resources(label=\"\"):\n    \"\"\"Print current resource usage.\"\"\"\n    mem = psutil.virtual_memory()\n    disk = shutil.disk_usage('/')\n    msg = f\"[{label}] \" if label else \"\"\n    msg += f\"RAM: {mem.used/1e9:.1f}/{mem.total/1e9:.1f}GB\"\n    if torch.cuda.is_available():\n        gpu_used = torch.cuda.memory_allocated()/1e9\n        gpu_total = torch.cuda.get_device_properties(0).total_memory/1e9\n        msg += f\" | GPU: {gpu_used:.1f}/{gpu_total:.1f}GB\"\n    msg += f\" | Disk: {disk.used/1e9:.0f}/{disk.total/1e9:.0f}GB\"\n    print(msg)\n\nprint_resources(\"After setup\")\n\nmark_task(\"Install dependencies\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 2. Download Sefaria Corpus (~8GB) { display-mode: \"form\" }\n#@markdown Downloads the complete Sefaria corpus with real-time git progress.\n\nimport subprocess\nimport sys\nimport os\n\nmark_task(\"Clone Sefaria corpus (~8GB)\", \"running\")\n\nsefaria_path = 'data/raw/Sefaria-Export'\n\nif not os.path.exists(sefaria_path) or not os.path.exists(f\"{sefaria_path}/json\"):\n    print(\"=\"*60)\n    print(\"CLONING SEFARIA CORPUS\")\n    print(\"=\"*60)\n    print()\n    print(\"This downloads ~3.5GB and takes 5-15 minutes.\")\n    print(\"Git's native progress will display below:\")\n    print(\"-\"*60)\n    print(flush=True)\n    \n    process = subprocess.Popen(\n        ['git', 'clone', '--depth', '1', '--progress',\n         'https://github.com/Sefaria/Sefaria-Export.git', sefaria_path],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True,\n        bufsize=1\n    )\n    \n    for line in process.stdout:\n        print(line, end='', flush=True)\n    \n    process.wait()\n    \n    print(\"-\"*60)\n    if process.returncode == 0:\n        print(\"\\nSefaria clone COMPLETE!\")\n    else:\n        print(f\"\\nERROR: Git clone failed with code {process.returncode}\")\nelse:\n    print(\"Sefaria already exists, skipping download.\")\n\nprint()\nprint(\"Verifying download...\")\n!du -sh {sefaria_path} 2>/dev/null || echo \"Directory not found\"\njson_count = !find {sefaria_path}/json -name \"*.json\" 2>/dev/null | wc -l\nprint(f\"Sefaria JSON files found: {json_count[0]}\")\n\nprint_resources(\"After Sefaria download\")\n\nmark_task(\"Clone Sefaria corpus (~8GB)\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 3. Download Dear Abby Dataset { display-mode: \"form\" }\n#@markdown Downloads the Dear Abby advice column dataset (68,330 entries).\n\nimport subprocess\nimport os\nimport pandas as pd\nfrom pathlib import Path\n\nmark_task(\"Clone sqnd-probe repo (Dear Abby data)\", \"running\")\n\nsqnd_path = 'sqnd-probe-data'\nif not os.path.exists(sqnd_path):\n    print(\"Cloning sqnd-probe repo...\")\n    process = subprocess.Popen(\n        ['git', 'clone', '--depth', '1', '--progress',\n         'https://github.com/ahb-sjsu/sqnd-probe.git', sqnd_path],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True,\n        bufsize=1\n    )\n    for line in process.stdout:\n        print(line, end='', flush=True)\n    process.wait()\nelse:\n    print(\"Repo already cloned.\")\n\ndear_abby_source = Path('sqnd-probe-data/dear_abby_data/raw_da_qs.csv')\ndear_abby_path = Path('data/raw/dear_abby.csv')\n\nif dear_abby_source.exists():\n    !cp \"{dear_abby_source}\" \"{dear_abby_path}\"\n    print(f\"\\nCopied Dear Abby data\")\nelif not dear_abby_path.exists():\n    raise FileNotFoundError(\"Dear Abby dataset not found!\")\n\ndf_check = pd.read_csv(dear_abby_path)\nprint(f\"\\n\" + \"=\" * 50)\nprint(f\"Dear Abby dataset: {len(df_check):,} entries\")\nprint(f\"Columns: {list(df_check.columns)}\")\nprint(f\"Year range: {df_check['year'].min():.0f} - {df_check['year'].max():.0f}\")\nprint(\"=\" * 50)\n\nprint_resources(\"After Dear Abby download\")\n\nmark_task(\"Clone sqnd-probe repo (Dear Abby data)\", \"done\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 4. Define Data Classes and Loaders { display-mode: \"form\" }\n#@markdown Defines enums, dataclasses, and corpus loaders.\n#@markdown **Note**: Labels extracted from English translations (text_english).\n\nimport json\nimport hashlib\nimport re\nimport os\nimport pandas as pd\nfrom pathlib import Path\nfrom dataclasses import dataclass, field, asdict\nfrom typing import List, Dict, Set\nfrom enum import Enum\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\n\nprint(\"Defining data structures...\")\n\nclass TimePeriod(Enum):\n    BIBLICAL = 0        # ~1000-500 BCE\n    SECOND_TEMPLE = 1   # ~500 BCE - 70 CE\n    TANNAITIC = 2       # ~70-200 CE\n    AMORAIC = 3         # ~200-500 CE\n    GEONIC = 4          # ~600-1000 CE\n    RISHONIM = 5        # ~1000-1500 CE\n    ACHRONIM = 6        # ~1500-1800 CE\n    MODERN_HEBREW = 7   # ~1800-present\n    DEAR_ABBY = 8       # 1956-2020\n\nclass BondType(Enum):\n    HARM_PREVENTION = 0\n    RECIPROCITY = 1\n    AUTONOMY = 2\n    PROPERTY = 3\n    FAMILY = 4\n    AUTHORITY = 5\n    EMERGENCY = 6\n    CONTRACT = 7\n    CARE = 8\n    FAIRNESS = 9\n    NONE = 10  # Explicit NONE class\n\nclass HohfeldianState(Enum):\n    RIGHT = 0\n    OBLIGATION = 1\n    LIBERTY = 2\n    NO_RIGHT = 3\n\n@dataclass\nclass Passage:\n    id: str\n    text_original: str\n    text_english: str\n    time_period: str\n    century: int\n    source: str\n    source_type: str\n    category: str\n    language: str = \"hebrew\"\n    word_count: int = 0\n    has_dispute: bool = False\n    consensus_tier: str = \"unknown\"\n    bond_types: List[str] = field(default_factory=list)\n    \n    def to_dict(self):\n        return asdict(self)\n\nCATEGORY_TO_PERIOD = {\n    'Tanakh': TimePeriod.BIBLICAL,\n    'Torah': TimePeriod.BIBLICAL,\n    'Mishnah': TimePeriod.TANNAITIC,\n    'Tosefta': TimePeriod.TANNAITIC,\n    'Talmud': TimePeriod.AMORAIC,\n    'Bavli': TimePeriod.AMORAIC,\n    'Midrash': TimePeriod.AMORAIC,\n    'Halakhah': TimePeriod.RISHONIM,\n    'Chasidut': TimePeriod.ACHRONIM,\n}\n\nPERIOD_TO_CENTURY = {\n    TimePeriod.BIBLICAL: -6,\n    TimePeriod.SECOND_TEMPLE: -2,\n    TimePeriod.TANNAITIC: 2,\n    TimePeriod.AMORAIC: 4,\n    TimePeriod.GEONIC: 8,\n    TimePeriod.RISHONIM: 12,\n    TimePeriod.ACHRONIM: 17,\n    TimePeriod.MODERN_HEBREW: 20,\n}\n\ndef load_sefaria(base_path: str, max_files: int = None) -> List[Passage]:\n    \"\"\"Load Sefaria corpus.\n    \n    Args:\n        base_path: Path to Sefaria-Export directory\n        max_files: Maximum number of JSON files to process (NOT passages).\n                   Set to None or 0 for unlimited.\n    \"\"\"\n    passages = []\n    json_path = Path(base_path) / \"json\"\n    \n    if not json_path.exists():\n        print(f\"Warning: {json_path} not found\")\n        return []\n    \n    json_files = list(json_path.rglob(\"*.json\"))\n    print(f\"Found {len(json_files):,} JSON files...\")\n    \n    if max_files and max_files > 0:\n        files_to_process = json_files[:max_files]\n        print(f\"Processing {len(files_to_process):,} files (max_files={max_files})...\")\n    else:\n        files_to_process = json_files\n        print(f\"Processing ALL {len(files_to_process):,} files...\")\n    \n    for json_file in tqdm(files_to_process, desc=\"Loading Sefaria\", unit=\"file\"):\n        try:\n            with open(json_file, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n        except:\n            continue\n        \n        rel_path = json_file.relative_to(json_path)\n        category = str(rel_path.parts[0]) if rel_path.parts else \"unknown\"\n        time_period = CATEGORY_TO_PERIOD.get(category, TimePeriod.AMORAIC)\n        century = PERIOD_TO_CENTURY.get(time_period, 0)\n        \n        if isinstance(data, dict):\n            hebrew = data.get('he', data.get('text', []))\n            english = data.get('text', data.get('en', []))\n            \n            def flatten(h, e, ref=\"\"):\n                if isinstance(h, str) and isinstance(e, str):\n                    h_clean = re.sub(r'<[^>]+>', '', h).strip()\n                    e_clean = re.sub(r'<[^>]+>', '', e).strip()\n                    if 50 <= len(e_clean) <= 2000:\n                        pid = hashlib.md5(f\"{json_file.stem}:{ref}:{h_clean[:50]}\".encode()).hexdigest()[:12]\n                        return [Passage(\n                            id=f\"sefaria_{pid}\",\n                            text_original=h_clean,\n                            text_english=e_clean,\n                            time_period=time_period.name,\n                            century=century,\n                            source=f\"{json_file.stem} {ref}\".strip(),\n                            source_type=\"sefaria\",\n                            category=category,\n                            language=\"hebrew\",\n                            word_count=len(e_clean.split())\n                        )]\n                    return []\n                elif isinstance(h, list) and isinstance(e, list):\n                    result = []\n                    for i, (hh, ee) in enumerate(zip(h, e)):\n                        result.extend(flatten(hh, ee, f\"{ref}.{i+1}\" if ref else str(i+1)))\n                    return result\n                return []\n            \n            passages.extend(flatten(hebrew, english))\n    \n    return passages\n\ndef load_dear_abby(path: str, max_passages: int = None) -> List[Passage]:\n    \"\"\"Load Dear Abby corpus.\"\"\"\n    passages = []\n    df = pd.read_csv(path)\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading Dear Abby\", unit=\"row\"):\n        question = str(row.get('question_only', ''))\n        if not question or question == 'nan' or len(question) < 50 or len(question) > 2000:\n            continue\n        \n        year = int(row.get('year', 1990))\n        pid = hashlib.md5(f\"abby:{idx}:{question[:50]}\".encode()).hexdigest()[:12]\n        \n        passages.append(Passage(\n            id=f\"abby_{pid}\",\n            text_original=question,\n            text_english=question,\n            time_period=TimePeriod.DEAR_ABBY.name,\n            century=20 if year < 2000 else 21,\n            source=f\"Dear Abby {year}\",\n            source_type=\"dear_abby\",\n            category=\"general\",\n            language=\"english\",\n            word_count=len(question.split())\n        ))\n        \n        if max_passages and len(passages) >= max_passages:\n            break\n    \n    return passages\n\nprint(\"Data structures defined!\")\nprint()\nprint(\"NOTE: Bond/Hohfeld labels will be extracted from text_english.\")\nprint(\"For Hebrew texts, this means labels come from English translations.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 5. Load and Preprocess Corpora { display-mode: \"form\" }\n#@markdown Loads Hebrew (Sefaria) and English (Dear Abby) corpora.\n#@markdown **L4 Optimized**: Set to 0 for full corpus (recommended for L4).\n\n#@markdown **Memory Management:**\nMAX_SEFARIA_FILES = 0  #@param {type:\"integer\"}\n#@markdown Set to 0 for FULL corpus (recommended for L4 with 53GB RAM).\n#@markdown Set to 5000 for faster testing.\n\nimport gc\nimport json\nfrom collections import defaultdict\n\nmark_task(\"Preprocess corpora\", \"running\")\n\nprint(\"=\" * 60)\nprint(\"LOADING CORPORA\")\nprint(\"=\" * 60)\nprint()\nprint(\"Current scope: Hebrew (Sefaria) ↔ English (Dear Abby)\")\nprint()\n\nif MAX_SEFARIA_FILES > 0:\n    print(f\"*** LIMITED MODE: Processing {MAX_SEFARIA_FILES:,} Sefaria JSON FILES ***\")\nelse:\n    print(\"*** FULL CORPUS MODE: Processing ALL Sefaria files ***\")\nprint()\n\nlimit = MAX_SEFARIA_FILES if MAX_SEFARIA_FILES > 0 else None\nsefaria_passages = load_sefaria(\"data/raw/Sefaria-Export\", max_files=limit)\nprint(f\"\\nSefaria passages loaded: {len(sefaria_passages):,}\")\n\nprint_resources(\"After Sefaria load\")\ngc.collect()\n\nprint()\nabby_passages = load_dear_abby(\"data/raw/dear_abby.csv\")\nprint(f\"\\nDear Abby passages loaded: {len(abby_passages):,}\")\n\nall_passages = sefaria_passages + abby_passages\n\ndel sefaria_passages\ndel abby_passages\ngc.collect()\n\nprint()\nprint(\"=\" * 60)\nprint(f\"TOTAL PASSAGES: {len(all_passages):,}\")\nprint(\"=\" * 60)\n\nby_period = defaultdict(int)\nby_source = defaultdict(int)\nfor p in all_passages:\n    by_period[p.time_period] += 1\n    by_source[p.source_type] += 1\n\nprint(\"\\nBy source:\")\nfor source, count in sorted(by_source.items()):\n    print(f\"  {source}: {count:,}\")\n\nprint(\"\\nBy time period:\")\nfor period, count in sorted(by_period.items()):\n    pct = count / len(all_passages) * 100\n    bar = '#' * int(pct / 2)\n    print(f\"  {period:20s}: {count:6,} ({pct:5.1f}%) {bar}\")\n\nprint_resources(\"After loading\")\n\nmark_task(\"Preprocess corpora\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 6. Extract Bond Structures { display-mode: \"form\" }\n#@markdown Extracts moral bond structures from **English text** (translations for Hebrew).\n\nimport gc\nimport json\nimport re\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\n\nmark_task(\"Extract bond structures\", \"running\")\n\nprint(\"=\" * 60)\nprint(\"EXTRACTING BOND STRUCTURES\")\nprint(\"=\" * 60)\nprint()\nprint(\"NOTE: Patterns applied to text_english (translations for Hebrew).\")\nprint(\"This means Hebrew text labels are derived from English translations.\")\nprint()\n\nRELATION_PATTERNS = {\n    BondType.HARM_PREVENTION: [r'\\b(kill|murder|harm|hurt|save|rescue|protect|danger|attack|injure|wound|destroy)\\b'],\n    BondType.RECIPROCITY: [r'\\b(return|repay|owe|debt|mutual|exchange|give back|pay back|reciprocate)\\b'],\n    BondType.AUTONOMY: [r'\\b(choose|decision|consent|agree|force|coerce|right|freedom|liberty|self-determination)\\b'],\n    BondType.PROPERTY: [r'\\b(property|own|steal|theft|buy|sell|land|possess|belong|asset)\\b'],\n    BondType.FAMILY: [r'\\b(honor|parent|marry|divorce|inherit|family|mother|father|child|son|daughter|spouse|husband|wife)\\b'],\n    BondType.AUTHORITY: [r'\\b(obey|command|law|judge|rule|teach|leader|king|master|servant|subject)\\b'],\n    BondType.CARE: [r'\\b(care|help|assist|feed|clothe|visit|nurture|tend|support|comfort)\\b'],\n    BondType.FAIRNESS: [r'\\b(fair|just|equal|deserve|bias|impartial|equity|discrimination)\\b'],\n    BondType.EMERGENCY: [r'\\b(emergency|urgent|crisis|danger|life-threatening|immediate|desperate|dire|peril|rescue)\\b'],\n    BondType.CONTRACT: [r'\\b(contract|agreement|promise|vow|oath|covenant|pledge|commit|bind|treaty|negotiate)\\b'],\n}\n\nHOHFELD_PATTERNS = {\n    HohfeldianState.OBLIGATION: [r'\\b(must|shall|duty|require|should|ought|obligated)\\b'],\n    HohfeldianState.RIGHT: [r'\\b(right to|entitled|deserve|claim|due)\\b'],\n    HohfeldianState.LIBERTY: [r'\\b(may|can|permitted|allowed|free to|at liberty)\\b'],\n}\n\ndef extract_bond_structure(passage: Passage) -> Dict:\n    \"\"\"Extract bond structure from passage's English text.\"\"\"\n    text = passage.text_english.lower()\n    \n    relations = []\n    for rel_type, patterns in RELATION_PATTERNS.items():\n        for pattern in patterns:\n            if re.search(pattern, text, re.IGNORECASE):\n                relations.append(rel_type.name)\n                break\n    \n    if not relations:\n        relations = ['NONE']\n    \n    hohfeld = None\n    for state, patterns in HOHFELD_PATTERNS.items():\n        for pattern in patterns:\n            if re.search(pattern, text, re.IGNORECASE):\n                hohfeld = state.name\n                break\n        if hohfeld:\n            break\n    \n    signature = \"|\".join(sorted(set(relations)))\n    \n    return {\n        'bonds': [{'relation': r} for r in relations],\n        'primary_relation': relations[0],\n        'hohfeld_state': hohfeld,\n        'signature': signature\n    }\n\nprint(\"Writing to disk...\")\nprint()\n\nbond_counts = defaultdict(int)\n\nwith open(\"data/processed/passages.jsonl\", 'w') as f_pass, \\\n     open(\"data/processed/bond_structures.jsonl\", 'w') as f_bond:\n    \n    for passage in tqdm(all_passages, desc=\"Processing\", unit=\"passage\"):\n        bond_struct = extract_bond_structure(passage)\n        passage.bond_types = [b['relation'] for b in bond_struct['bonds']]\n        \n        for bond in bond_struct['bonds']:\n            bond_counts[bond['relation']] += 1\n        \n        f_pass.write(json.dumps(passage.to_dict()) + '\\n')\n        f_bond.write(json.dumps({\n            'passage_id': passage.id,\n            'bond_structure': bond_struct\n        }) + '\\n')\n\nn_passages = len(all_passages)\ndel all_passages\ngc.collect()\n\nprint()\nprint(f\"Saved {n_passages:,} passages to disk\")\n\nprint_resources(\"After extraction\")\n\nprint()\nprint(\"Bond type distribution:\")\nfor bond_type, count in sorted(bond_counts.items(), key=lambda x: -x[1]):\n    pct = count / sum(bond_counts.values()) * 100\n    bar = '#' * int(pct)\n    print(f\"  {bond_type:20s}: {count:6,} ({pct:5.1f}%) {bar}\")\n\nif bond_counts.get('NONE', 0) / sum(bond_counts.values()) > 0.5:\n    print()\n    print(\"WARNING: NONE class is >50% - consider expanding patterns.\")\n\nmark_task(\"Extract bond structures\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 7. Generate Train/Test Splits { display-mode: \"form\" }\n#@markdown Creates temporal splits for cross-era evaluation.\n\nimport random\nimport gc\nimport json\nfrom tqdm.auto import tqdm\nfrom collections import defaultdict\n\nrandom.seed(42)\n\nmark_task(\"Generate train/test splits\", \"running\")\n\nprint(\"=\" * 60)\nprint(\"GENERATING SPLITS\")\nprint(\"=\" * 60)\nprint()\n\nprint(\"Reading passage metadata from disk...\")\npassage_info = []\n\nwith open(\"data/processed/passages.jsonl\", 'r') as f:\n    for line in tqdm(f, desc=\"Reading IDs\", unit=\"line\"):\n        p = json.loads(line)\n        passage_info.append((p['id'], p['time_period']))\n\nprint(f\"Loaded {len(passage_info):,} passage IDs\")\n\ntrain_periods = {'BIBLICAL', 'SECOND_TEMPLE', 'TANNAITIC', 'AMORAIC', 'GEONIC', 'RISHONIM'}\nvalid_periods = {'ACHRONIM'}\ntest_periods = {'MODERN_HEBREW', 'DEAR_ABBY'}\n\nprint()\nprint(\"Filtering by time period...\")\nancient_ids = [(pid, tp) for pid, tp in passage_info if tp in train_periods]\nearly_modern_ids = [(pid, tp) for pid, tp in passage_info if tp in valid_periods]\nmodern_ids = [(pid, tp) for pid, tp in passage_info if tp in test_periods]\n\nprint(f\"  Ancient/Medieval: {len(ancient_ids):,}\")\nprint(f\"  Early Modern:     {len(early_modern_ids):,}\")\nprint(f\"  Modern:           {len(modern_ids):,}\")\n\nrandom.shuffle(ancient_ids)\nrandom.shuffle(early_modern_ids)\nrandom.shuffle(modern_ids)\n\n# SPLIT A: ANCIENT -> MODERN\nprint()\nprint(\"-\" * 60)\nprint(\"SPLIT A: Train ANCIENT, Test MODERN\")\nprint(\"-\" * 60)\n\ntemporal_A = {\n    'name': 'ancient_to_modern',\n    'direction': 'A->M',\n    'train_ids': [pid for pid, _ in ancient_ids],\n    'valid_ids': [pid for pid, _ in early_modern_ids],\n    'test_ids': [pid for pid, _ in modern_ids],\n    'train_size': len(ancient_ids),\n    'valid_size': len(early_modern_ids),\n    'test_size': len(modern_ids)\n}\nprint(f\"  Train: {temporal_A['train_size']:,}\")\nprint(f\"  Valid: {temporal_A['valid_size']:,}\")\nprint(f\"  Test:  {temporal_A['test_size']:,}\")\n\n# SPLIT B: MODERN -> ANCIENT\nprint()\nprint(\"-\" * 60)\nprint(\"SPLIT B: Train MODERN, Test ANCIENT\")\nprint(\"-\" * 60)\n\nn_modern = len(modern_ids)\nancient_test = ancient_ids[n_modern:n_modern*2] if len(ancient_ids) >= n_modern*2 else ancient_ids[n_modern:]\n\ntemporal_B = {\n    'name': 'modern_to_ancient',\n    'direction': 'M->A',\n    'train_ids': [pid for pid, _ in modern_ids],\n    'valid_ids': [pid for pid, _ in early_modern_ids[:len(early_modern_ids)//2]],\n    'test_ids': [pid for pid, _ in ancient_test],\n    'train_size': len(modern_ids),\n    'valid_size': len(early_modern_ids) // 2,\n    'test_size': len(ancient_test)\n}\nprint(f\"  Train: {temporal_B['train_size']:,}\")\nprint(f\"  Valid: {temporal_B['valid_size']:,}\")\nprint(f\"  Test:  {temporal_B['test_size']:,}\")\n\n# SPLIT C: MIXED (in-domain baseline)\nprint()\nprint(\"-\" * 60)\nprint(\"SPLIT C: MIXED (In-Domain Baseline)\")\nprint(\"-\" * 60)\n\nall_ids = ancient_ids + modern_ids\nrandom.shuffle(all_ids)\nn = len(all_ids)\nn_train = int(0.7 * n)\nn_valid = int(0.15 * n)\n\ntemporal_C = {\n    'name': 'mixed_control',\n    'direction': 'MIXED',\n    'train_ids': [pid for pid, _ in all_ids[:n_train]],\n    'valid_ids': [pid for pid, _ in all_ids[n_train:n_train+n_valid]],\n    'test_ids': [pid for pid, _ in all_ids[n_train+n_valid:]],\n    'train_size': n_train,\n    'valid_size': n_valid,\n    'test_size': n - n_train - n_valid\n}\nprint(f\"  Train: {temporal_C['train_size']:,}\")\nprint(f\"  Valid: {temporal_C['valid_size']:,}\")\nprint(f\"  Test:  {temporal_C['test_size']:,}\")\n\ndel passage_info, ancient_ids, early_modern_ids, modern_ids, all_ids\ngc.collect()\n\nprint()\nprint(\"Saving splits...\")\nsplits = {\n    'ancient_to_modern': temporal_A,\n    'modern_to_ancient': temporal_B,\n    'mixed_control': temporal_C\n}\n\nwith open(\"data/splits/all_splits.json\", 'w') as f:\n    json.dump(splits, f, indent=2)\n\n# DISTRIBUTION CHECK with ID integrity\nprint()\nprint(\"=\" * 60)\nprint(\"LABEL DISTRIBUTION CHECK (with ID integrity verification)\")\nprint(\"=\" * 60)\n\nhohfeld_counts = {}\ntime_counts = {}\nbond_type_counts = {}\nid_mismatches = 0\n\nwith open(\"data/processed/bond_structures.jsonl\", 'r') as fb, \\\n     open(\"data/processed/passages.jsonl\", 'r') as fp:\n    for b_line, p_line in zip(fb, fp):\n        b = json.loads(b_line)\n        p = json.loads(p_line)\n        \n        # ID integrity check\n        if b['passage_id'] != p['id']:\n            id_mismatches += 1\n            continue\n        \n        h = b['bond_structure'].get('hohfeld_state', None)\n        t = p['time_period']\n        bond = b['bond_structure'].get('primary_relation', 'NONE')\n        \n        hohfeld_counts[h] = hohfeld_counts.get(h, 0) + 1\n        time_counts[t] = time_counts.get(t, 0) + 1\n        bond_type_counts[bond] = bond_type_counts.get(bond, 0) + 1\n\nif id_mismatches > 0:\n    print(f\"WARNING: {id_mismatches} ID mismatches found!\")\nelse:\n    print(\"ID integrity check: PASSED ✓\")\nprint()\n\nprint(\"Hohfeld distribution:\")\ntotal_h = sum(hohfeld_counts.values())\nfor h, c in sorted(hohfeld_counts.items(), key=lambda x: -x[1]):\n    pct = 100 * c / total_h\n    bar = \"#\" * int(pct / 2)\n    print(f\"  {str(h):15s}: {c:>8,} ({pct:5.1f}%) {bar}\")\n\nprint()\nprint(\"Time period distribution:\")\ntotal_t = sum(time_counts.values())\nfor t, c in sorted(time_counts.items(), key=lambda x: -x[1]):\n    pct = 100 * c / total_t\n    bar = \"#\" * int(pct / 2)\n    print(f\"  {t:15s}: {c:>8,} ({pct:5.1f}%) {bar}\")\n\nprint()\nprint(\"Bond type distribution:\")\ntotal_b = sum(bond_type_counts.values())\nfor b, c in sorted(bond_type_counts.items(), key=lambda x: -x[1]):\n    pct = 100 * c / total_b\n    bar = \"#\" * int(pct / 2)\n    print(f\"  {b:20s}: {c:>8,} ({pct:5.1f}%) {bar}\")\n\nN_HOHFELD_CLASSES = len([h for h in hohfeld_counts if h is not None]) + 1\nN_TIME_CLASSES = len(time_counts)\nN_BOND_CLASSES = len(bond_type_counts)\n\nCHANCE_HOHFELD = 1.0 / N_HOHFELD_CLASSES\nCHANCE_TIME = 1.0 / N_TIME_CLASSES\nCHANCE_BOND = 1.0 / N_BOND_CLASSES\n\nprint()\nprint(f\"Chance baseline - Hohfeld: {CHANCE_HOHFELD:.1%} ({N_HOHFELD_CLASSES} classes)\")\nprint(f\"Chance baseline - Time:    {CHANCE_TIME:.1%} ({N_TIME_CLASSES} classes)\")\nprint(f\"Chance baseline - Bond:    {CHANCE_BOND:.1%} ({N_BOND_CLASSES} classes)\")\n\nbaselines = {\n    'hohfeld_counts': {str(k): v for k, v in hohfeld_counts.items()},\n    'time_counts': time_counts,\n    'bond_type_counts': bond_type_counts,\n    'chance_hohfeld': CHANCE_HOHFELD,\n    'chance_time': CHANCE_TIME,\n    'chance_bond': CHANCE_BOND,\n    'n_hohfeld_classes': N_HOHFELD_CLASSES,\n    'n_time_classes': N_TIME_CLASSES,\n    'n_bond_classes': N_BOND_CLASSES\n}\nwith open(\"data/splits/baselines.json\", 'w') as f:\n    json.dump(baselines, f, indent=2)\n\nmost_common_hohfeld = max(hohfeld_counts.values()) / total_h\nif most_common_hohfeld > 0.7:\n    print()\n    print(f\"WARNING: Hohfeld labels severely imbalanced! Most common = {most_common_hohfeld:.1%}\")\n\n# Save to Drive\nprint()\nprint(\"Saving preprocessed data to Google Drive...\")\nimport shutil\nshutil.copytree(\"data/processed\", f\"{SAVE_DIR}/processed\", dirs_exist_ok=True)\nshutil.copytree(\"data/splits\", f\"{SAVE_DIR}/splits\", dirs_exist_ok=True)\nprint(f\"Saved to {SAVE_DIR}\")\n\nprint_resources(\"After splits\")\n\nmark_task(\"Generate train/test splits\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 8. Define BIP Model Architecture { display-mode: \"form\" }\n#@markdown Model with bond prediction head and support for linear probe extraction.\n\nimport gc\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoTokenizer\nfrom tqdm.auto import tqdm\n\nprint(\"Clearing memory before model load...\")\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint_resources(\"Before model definition\")\nprint()\n\nprint(\"=\" * 60)\nprint(\"DEFINING MODEL ARCHITECTURE (v7.1)\")\nprint(\"=\" * 60)\nprint()\nprint(\"Encoder: paraphrase-multilingual-MiniLM-L12-v2\")\nprint(\"  - Maps Hebrew and English into shared embedding space\")\nprint(\"  - Input: Hebrew (Sefaria) or English (Dear Abby)\")\nprint(\"  - Labels: Derived from English translations\")\nprint()\n\nclass GradientReversal(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, lambda_):\n        ctx.lambda_ = lambda_\n        return x.clone()\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        return -ctx.lambda_ * grad_output, None\n\ndef gradient_reversal(x, lambda_=1.0):\n    return GradientReversal.apply(x, lambda_)\n\nclass BIPEncoder(nn.Module):\n    def __init__(self, model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", d_model=384):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        self.d_model = d_model\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        hidden = outputs.last_hidden_state\n        mask = attention_mask.unsqueeze(-1).float()\n        pooled = (hidden * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n        return pooled\n\nclass BIPModel(nn.Module):\n    \"\"\"BIP Model with z_bond extraction for linear probe.\"\"\"\n    def __init__(self, d_model=384, d_bond=64, d_label=32, n_periods=9, n_hohfeld=4, n_bonds=11):\n        super().__init__()\n        \n        self.encoder = BIPEncoder()\n        \n        self.bond_proj = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, d_bond)\n        )\n        \n        self.label_proj = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, d_label)\n        )\n        \n        self.time_classifier_bond = nn.Linear(d_bond, n_periods)\n        self.time_classifier_label = nn.Linear(d_label, n_periods)\n        self.hohfeld_classifier = nn.Linear(d_bond, n_hohfeld)\n        self.bond_classifier = nn.Linear(d_bond, n_bonds)\n    \n    def forward(self, input_ids, attention_mask, adversarial_lambda=1.0):\n        h = self.encoder(input_ids, attention_mask)\n        \n        z_bond = self.bond_proj(h)\n        z_label = self.label_proj(h)\n        \n        z_bond_adv = gradient_reversal(z_bond, adversarial_lambda)\n        time_pred_bond = self.time_classifier_bond(z_bond_adv)\n        time_pred_label = self.time_classifier_label(z_label)\n        hohfeld_pred = self.hohfeld_classifier(z_bond)\n        bond_pred = self.bond_classifier(z_bond)\n        \n        return {\n            'z_bond': z_bond,\n            'z_label': z_label,\n            'time_pred_bond': time_pred_bond,\n            'time_pred_label': time_pred_label,\n            'hohfeld_pred': hohfeld_pred,\n            'bond_pred': bond_pred\n        }\n    \n    def extract_z_bond(self, input_ids, attention_mask):\n        \"\"\"Extract z_bond embeddings only (for linear probe).\"\"\"\n        with torch.no_grad():\n            h = self.encoder(input_ids, attention_mask)\n            z_bond = self.bond_proj(h)\n        return z_bond\n\nTIME_PERIOD_TO_IDX = {\n    'BIBLICAL': 0, 'SECOND_TEMPLE': 1, 'TANNAITIC': 2, 'AMORAIC': 3,\n    'GEONIC': 4, 'RISHONIM': 5, 'ACHRONIM': 6, 'MODERN_HEBREW': 7,\n    'DEAR_ABBY': 8\n}\nIDX_TO_TIME_PERIOD = {v: k for k, v in TIME_PERIOD_TO_IDX.items()}\n\nHOHFELD_TO_IDX = {\n    'OBLIGATION': 0, 'RIGHT': 1, 'LIBERTY': 2, None: 3\n}\n\nBOND_TYPE_TO_IDX = {\n    'HARM_PREVENTION': 0, 'RECIPROCITY': 1, 'AUTONOMY': 2, 'PROPERTY': 3,\n    'FAMILY': 4, 'AUTHORITY': 5, 'EMERGENCY': 6, 'CONTRACT': 7,\n    'CARE': 8, 'FAIRNESS': 9, 'NONE': 10\n}\nIDX_TO_BOND_TYPE = {v: k for k, v in BOND_TYPE_TO_IDX.items()}\n\nclass MoralDataset(Dataset):\n    \"\"\"Dataset with ID integrity checking.\"\"\"\n    def __init__(self, passage_ids: set, passages_file: str, bonds_file: str, tokenizer, max_len=64):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.passage_ids = passage_ids\n        \n        print(f\"  Indexing {len(passage_ids):,} passages...\")\n        \n        self.data = []\n        id_mismatches = 0\n        \n        with open(passages_file, 'r') as f_pass, open(bonds_file, 'r') as f_bond:\n            for p_line, b_line in tqdm(zip(f_pass, f_bond), desc=\"  Loading subset\", unit=\"line\", total=None):\n                p = json.loads(p_line)\n                b = json.loads(b_line)\n                \n                # ID integrity check\n                if b['passage_id'] != p['id']:\n                    id_mismatches += 1\n                    continue\n                \n                if p['id'] in passage_ids:\n                    self.data.append({\n                        'text': (p.get('text_original', '') if p.get('language') in ['hebrew', 'chinese', 'arabic'] else p.get('text_english', ''))[:1000],\n                        'time_period': p['time_period'],\n                        'source_type': p['source_type'],\n                        'hohfeld': b['bond_structure']['hohfeld_state'],\n                        'primary_relation': b['bond_structure']['primary_relation']\n                    })\n        \n        if id_mismatches > 0:\n            print(f\"  WARNING: {id_mismatches} ID mismatches found and skipped!\")\n        print(f\"  Loaded {len(self.data):,} samples\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        encoding = self.tokenizer(\n            item['text'],\n            truncation=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'time_label': TIME_PERIOD_TO_IDX.get(item['time_period'], 8),\n            'hohfeld_label': HOHFELD_TO_IDX.get(item['hohfeld'], 3),\n            'bond_label': BOND_TYPE_TO_IDX.get(item['primary_relation'], 10),\n            'source_type': item['source_type']\n        }\n\ndef collate_fn(batch):\n    return {\n        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n        'time_labels': torch.tensor([x['time_label'] for x in batch]),\n        'hohfeld_labels': torch.tensor([x['hohfeld_label'] for x in batch]),\n        'bond_labels': torch.tensor([x['bond_label'] for x in batch]),\n        'source_types': [x['source_type'] for x in batch]\n    }\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(\"Model architecture defined!\")\nprint(f\"  - Bond types: {len(BOND_TYPE_TO_IDX)} classes\")\nprint(f\"  - Time periods: {len(TIME_PERIOD_TO_IDX)} classes\")\nprint(f\"  - Hohfeld states: {len(HOHFELD_TO_IDX)} classes\")\nprint(f\"  - Base batch size: {BASE_BATCH_SIZE}\")\n\nprint_resources(\"After model definition\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 9. Train BIP Model { display-mode: \"form\" }\n#@markdown Trains bidirectionally with bond classification.\n#@markdown **L4 Optimized**: Larger batch sizes, more workers.\n\nimport gc\nimport json\nimport time\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\nfrom sklearn.metrics import f1_score\nfrom tqdm.auto import tqdm\n\nmark_task(\"Train BIP model\", \"running\")\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint_resources(\"Training start\")\n\nprint(\"=\" * 60)\nprint(\"BIDIRECTIONAL BIP TRAINING (v7.1 - L4 Optimized)\")\nprint(\"=\" * 60)\nprint()\nprint(f\"Accelerator: {ACCELERATOR}\")\nprint(f\"Device: {device}\")\nprint(f\"Base batch size: {BASE_BATCH_SIZE}\")\nprint()\n\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n\nall_results = {}\n\nfor split_idx, split_name in enumerate(['ancient_to_modern', 'modern_to_ancient', 'mixed_control']):\n    split_start = time.time()\n    print()\n    print(\"=\" * 60)\n    direction_label = {\n        'ancient_to_modern': 'Ancient → Modern',\n        'modern_to_ancient': 'Modern → Ancient',\n        'mixed_control': 'Mixed (In-Domain Baseline)'\n    }[split_name]\n    print(f\"TRAINING [{split_idx+1}/3]: {direction_label}\")\n    print(\"=\" * 60)\n    print()\n    \n    with open(\"data/splits/all_splits.json\", 'r') as f:\n        splits = json.load(f)\n    split = splits[split_name]\n    \n    print(f\"Train: {split['train_size']:,}\")\n    print(f\"Valid: {split['valid_size']:,}\")\n    print(f\"Test:  {split['test_size']:,}\")\n    print()\n    \n    print(\"Creating fresh model...\")\n    model = BIPModel().to(device)\n    \n    if split_name == 'ancient_to_modern':\n        n_params = sum(p.numel() for p in model.parameters())\n        print(f\"Model parameters: {n_params:,}\")\n    \n    print(\"Creating datasets...\")\n    train_dataset = MoralDataset(\n        set(split['train_ids']),\n        \"data/processed/passages.jsonl\",\n        \"data/processed/bond_structures.jsonl\",\n        tokenizer\n    )\n    valid_dataset = MoralDataset(\n        set(split['valid_ids']),\n        \"data/processed/passages.jsonl\",\n        \"data/processed/bond_structures.jsonl\",\n        tokenizer\n    )\n    test_dataset = MoralDataset(\n        set(split['test_ids']),\n        \"data/processed/passages.jsonl\",\n        \"data/processed/bond_structures.jsonl\",\n        tokenizer\n    )\n    \n    print(f\"Train samples: {len(train_dataset):,}\")\n    print(f\"Valid samples: {len(valid_dataset):,}\")\n    print(f\"Test samples:  {len(test_dataset):,}\")\n    print()\n    \n    if len(train_dataset) == 0:\n        print(\"ERROR: No training data!\")\n        continue\n    \n    # L4-optimized batch sizes\n    if split_name == 'ancient_to_modern':\n        batch_size = BASE_BATCH_SIZE\n    else:\n        batch_size = min(BASE_BATCH_SIZE, max(32, len(train_dataset) // 10))\n    \n    # More workers for L4's better CPU\n    num_workers = 4 if IS_L4 or IS_A100 else 2\n    \n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True,\n        collate_fn=collate_fn, drop_last=True, \n        num_workers=num_workers, pin_memory=True, \n        prefetch_factor=4, persistent_workers=True\n    )\n    valid_loader = DataLoader(\n        valid_dataset, batch_size=batch_size*2, shuffle=False,\n        collate_fn=collate_fn, \n        num_workers=num_workers, pin_memory=True,\n        prefetch_factor=4, persistent_workers=True\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size*2, shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=num_workers, pin_memory=True,\n        prefetch_factor=4, persistent_workers=True\n    )\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    \n    n_epochs = 5  # More epochs for L4\n    best_valid_loss = float('inf')\n    patience = 3\n    patience_counter = 0\n    \n    print(f\"Training for {n_epochs} epochs (batch_size={batch_size}, workers={num_workers})...\")\n    print()\n    \n    for epoch in range(1, n_epochs + 1):\n        epoch_start = time.time()\n        model.train()\n        total_loss = 0\n        n_batches = 0\n        \n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{n_epochs}\", unit=\"batch\")\n        for batch in pbar:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            time_labels = batch['time_labels'].to(device)\n            hohfeld_labels = batch['hohfeld_labels'].to(device)\n            bond_labels = batch['bond_labels'].to(device)\n            \n            with torch.cuda.amp.autocast(enabled=USE_AMP):\n                outputs = model(input_ids, attention_mask, adversarial_lambda=1.0)\n                \n                loss_time_bond = F.cross_entropy(outputs['time_pred_bond'], time_labels)\n                loss_time_label = F.cross_entropy(outputs['time_pred_label'], time_labels)\n                loss_hohfeld = F.cross_entropy(outputs['hohfeld_pred'], hohfeld_labels)\n                loss_bond = F.cross_entropy(outputs['bond_pred'], bond_labels)\n            \n            loss = loss_hohfeld + loss_time_label + loss_time_bond + loss_bond\n            \n            optimizer.zero_grad()\n            \n            if USE_TPU:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                xm.optimizer_step(optimizer)\n                xm.mark_step()\n            elif USE_AMP and scaler is not None:\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n            \n            total_loss += loss.item()\n            n_batches += 1\n            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n        \n        avg_train_loss = total_loss / n_batches\n        \n        # Validation\n        model.eval()\n        valid_loss = 0\n        valid_batches = 0\n        \n        with torch.no_grad():\n            for batch in valid_loader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                hohfeld_labels = batch['hohfeld_labels'].to(device)\n                \n                outputs = model(input_ids, attention_mask, adversarial_lambda=0)\n                loss = F.cross_entropy(outputs['hohfeld_pred'], hohfeld_labels)\n                valid_loss += loss.item()\n                valid_batches += 1\n                \n                if USE_TPU:\n                    xm.mark_step()\n        \n        avg_valid_loss = valid_loss / valid_batches if valid_batches > 0 else 0\n        epoch_time = time.time() - epoch_start\n        \n        # GPU memory check\n        if torch.cuda.is_available():\n            gpu_used = torch.cuda.memory_allocated()/1e9\n            gpu_total = torch.cuda.get_device_properties(0).total_memory/1e9\n            gpu_str = f\" | GPU: {gpu_used:.1f}/{gpu_total:.1f}GB\"\n        else:\n            gpu_str = \"\"\n        \n        print(f\"Epoch {epoch}: Loss={avg_train_loss:.4f}/{avg_valid_loss:.4f} | {epoch_time:.1f}s{gpu_str}\")\n        \n        if avg_valid_loss < best_valid_loss:\n            best_valid_loss = avg_valid_loss\n            model_path = f\"models/checkpoints/best_model_{split_name}.pt\"\n            if USE_TPU:\n                xm.save(model.state_dict(), model_path)\n            else:\n                torch.save(model.state_dict(), model_path)\n            print(f\"  -> Saved best model!\")\n            import shutil\n            shutil.copy(model_path, f\"{SAVE_DIR}/best_model_{split_name}.pt\")\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"  Early stopping at epoch {epoch}\")\n                break\n    \n    # EVALUATE\n    print()\n    print(f\"Evaluating {split_name}...\")\n    \n    model.load_state_dict(torch.load(f\"models/checkpoints/best_model_{split_name}.pt\", map_location='cpu'))\n    model = model.to(device)\n    model.eval()\n    \n    all_time_preds = []\n    all_time_labels = []\n    all_hohfeld_preds = []\n    all_hohfeld_labels = []\n    all_bond_preds = []\n    all_bond_labels = []\n    all_source_types = []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            outputs = model(input_ids, attention_mask, adversarial_lambda=0)\n            \n            all_time_preds.extend(outputs['time_pred_bond'].argmax(dim=-1).cpu().tolist())\n            all_time_labels.extend(batch['time_labels'].tolist())\n            all_hohfeld_preds.extend(outputs['hohfeld_pred'].argmax(dim=-1).cpu().tolist())\n            all_hohfeld_labels.extend(batch['hohfeld_labels'].tolist())\n            all_bond_preds.extend(outputs['bond_pred'].argmax(dim=-1).cpu().tolist())\n            all_bond_labels.extend(batch['bond_labels'].tolist())\n            all_source_types.extend(batch['source_types'])\n            \n            if USE_TPU:\n                xm.mark_step()\n    \n    time_acc = sum(p == l for p, l in zip(all_time_preds, all_time_labels)) / len(all_time_preds)\n    hohfeld_acc = sum(p == l for p, l in zip(all_hohfeld_preds, all_hohfeld_labels)) / len(all_hohfeld_preds)\n    bond_acc = sum(p == l for p, l in zip(all_bond_preds, all_bond_labels)) / len(all_bond_preds)\n    \n    bond_f1_macro = f1_score(all_bond_labels, all_bond_preds, average='macro', zero_division=0)\n    bond_f1_weighted = f1_score(all_bond_labels, all_bond_preds, average='weighted', zero_division=0)\n    hohfeld_f1_macro = f1_score(all_hohfeld_labels, all_hohfeld_preds, average='macro', zero_division=0)\n    \n    corpus_bond_f1 = {}\n    for corpus in set(all_source_types):\n        mask = [s == corpus for s in all_source_types]\n        corpus_preds = [p for p, m in zip(all_bond_preds, mask) if m]\n        corpus_labels = [l for l, m in zip(all_bond_labels, mask) if m]\n        if len(corpus_labels) > 0:\n            corpus_bond_f1[corpus] = {\n                'f1_macro': f1_score(corpus_labels, corpus_preds, average='macro', zero_division=0),\n                'accuracy': sum(p == l for p, l in zip(corpus_preds, corpus_labels)) / len(corpus_labels),\n                'n_samples': len(corpus_labels)\n            }\n    \n    split_time = time.time() - split_start\n    \n    all_results[split_name] = {\n        'time_acc': time_acc,\n        'hohfeld_acc': hohfeld_acc,\n        'hohfeld_f1_macro': hohfeld_f1_macro,\n        'bond_acc': bond_acc,\n        'bond_f1_macro': bond_f1_macro,\n        'bond_f1_weighted': bond_f1_weighted,\n        'corpus_bond_f1': corpus_bond_f1,\n        'train_size': split['train_size'],\n        'test_size': split['test_size'],\n        'training_time_seconds': split_time\n    }\n    \n    print()\n    print(f\"{split_name.upper()} RESULTS ({split_time/60:.1f} min):\")\n    print(f\"  Time from z_bond (adversary): {time_acc:.1%}\")\n    print(f\"  Hohfeld classification:       {hohfeld_acc:.1%} (F1={hohfeld_f1_macro:.3f})\")\n    print(f\"  Bond classification:          {bond_acc:.1%} (F1={bond_f1_macro:.3f})\")\n    if corpus_bond_f1:\n        print(\"  Bond F1 by corpus:\")\n        for corpus, metrics in corpus_bond_f1.items():\n            print(f\"    {corpus}: F1={metrics['f1_macro']:.3f}, Acc={metrics['accuracy']:.1%}\")\n    \n    # Cleanup between splits\n    del model, train_dataset, valid_dataset, test_dataset\n    del train_loader, valid_loader, test_loader\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nprint()\nprint(\"=\" * 60)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\" * 60)\n\nprint_resources(\"After training\")\n\nmark_task(\"Train BIP model\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 10. Linear Probe Test for Time Invariance { display-mode: \"form\" }\n#@markdown **Primary invariance test**: Can a fresh probe decode time from frozen z_bond?\n\nimport json\nimport gc\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.auto import tqdm\n\nmark_task(\"Linear probe test\", \"running\")\n\nprint(\"=\" * 60)\nprint(\"LINEAR PROBE TEST FOR TIME INVARIANCE\")\nprint(\"=\" * 60)\nprint()\nprint(\"This is the strongest test of time invariance:\")\nprint(\"  1. Freeze encoder + bond_proj\")\nprint(\"  2. Extract z_bond on test set\")\nprint(\"  3. Fit fresh logistic regression to predict time\")\nprint(\"  4. If probe accuracy ≈ chance, time is truly removed\")\nprint()\n\nlinear_probe_results = {}\n\nfor split_name in ['ancient_to_modern', 'modern_to_ancient']:\n    print(f\"\\n{'='*50}\")\n    print(f\"LINEAR PROBE: {split_name}\")\n    print(f\"{'='*50}\")\n    \n    # Load best model\n    model_path = f\"models/checkpoints/best_model_{split_name}.pt\"\n    model = BIPModel().to(device)\n    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n    model = model.to(device)\n    model.eval()\n    \n    # Freeze everything\n    for param in model.parameters():\n        param.requires_grad = False\n    \n    # Load test data\n    with open(\"data/splits/all_splits.json\", 'r') as f:\n        splits = json.load(f)\n    split = splits[split_name]\n    \n    test_dataset = MoralDataset(\n        set(split['test_ids']),\n        \"data/processed/passages.jsonl\",\n        \"data/processed/bond_structures.jsonl\",\n        tokenizer\n    )\n    \n    test_loader = DataLoader(\n        test_dataset, batch_size=BASE_BATCH_SIZE, shuffle=False,\n        collate_fn=collate_fn, num_workers=4, pin_memory=True\n    )\n    \n    # Extract z_bond embeddings\n    print(\"Extracting z_bond embeddings...\")\n    all_z_bond = []\n    all_time_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Extracting\", unit=\"batch\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            z_bond = model.extract_z_bond(input_ids, attention_mask)\n            \n            all_z_bond.append(z_bond.cpu().numpy())\n            all_time_labels.extend(batch['time_labels'].tolist())\n            \n            if USE_TPU:\n                xm.mark_step()\n    \n    X = np.vstack(all_z_bond)\n    y = np.array(all_time_labels)\n    \n    print(f\"Extracted {X.shape[0]} embeddings of dimension {X.shape[1]}\")\n    print(f\"Time classes in test set: {len(np.unique(y))}\")\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Split for probe training (50/50)\n    n = len(X_scaled)\n    np.random.seed(42)\n    indices = np.random.permutation(n)\n    train_idx = indices[:n//2]\n    test_idx = indices[n//2:]\n    \n    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    print(f\"\\nProbe training set: {len(X_train)}\")\n    print(f\"Probe test set: {len(X_test)}\")\n    \n    # Fit logistic regression\n    print(\"\\nFitting logistic regression probe...\")\n    probe = LogisticRegression(\n        max_iter=1000,\n        multi_class='multinomial',\n        solver='lbfgs',\n        random_state=42,\n        n_jobs=-1\n    )\n    probe.fit(X_train, y_train)\n    \n    # Evaluate probe\n    probe_preds = probe.predict(X_test)\n    probe_acc = (probe_preds == y_test).mean()\n    \n    # Calculate chance level\n    unique_classes = np.unique(y_test)\n    chance_level = 1.0 / len(unique_classes)\n    \n    # Check if probe accuracy is near chance\n    is_time_invariant = probe_acc < (chance_level + 0.10)\n    \n    print()\n    print(f\"PROBE RESULTS:\")\n    print(f\"  Probe accuracy: {probe_acc:.1%}\")\n    print(f\"  Chance level:   {chance_level:.1%} ({len(unique_classes)} classes)\")\n    print(f\"  Above chance:   {probe_acc - chance_level:+.1%}\")\n    print()\n    \n    if is_time_invariant:\n        print(f\"  ✓ TIME INVARIANT: Probe cannot decode time from z_bond\")\n    else:\n        print(f\"  ✗ TIME LEAKAGE: Probe can still decode time from z_bond\")\n    \n    linear_probe_results[split_name] = {\n        'probe_acc': float(probe_acc),\n        'chance_level': float(chance_level),\n        'above_chance': float(probe_acc - chance_level),\n        'is_time_invariant': bool(is_time_invariant),\n        'n_classes': int(len(unique_classes)),\n        'n_samples': int(len(X_test))\n    }\n    \n    # Cleanup\n    del model, test_dataset, test_loader, X, y, X_scaled\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n# Save probe results\nwith open('results/linear_probe_results.json', 'w') as f:\n    json.dump(linear_probe_results, f, indent=2)\n\nprint()\nprint(\"=\" * 60)\nprint(\"LINEAR PROBE SUMMARY\")\nprint(\"=\" * 60)\nprint()\nfor split_name, res in linear_probe_results.items():\n    status = \"✓ INVARIANT\" if res['is_time_invariant'] else \"✗ LEAKAGE\"\n    print(f\"{split_name}:\")\n    print(f\"  Probe: {res['probe_acc']:.1%} vs Chance: {res['chance_level']:.1%} → {status}\")\n\nprint_resources(\"After linear probe\")\n\nmark_task(\"Linear probe test\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 11. Evaluate Final Results { display-mode: \"form\" }\n#@markdown Comprehensive evaluation with linear probe as primary invariance test.\n\nimport gc\nimport json\nimport time\n\nmark_task(\"Evaluate results\", \"running\")\n\nprint_resources(\"Evaluation start\")\n\nprint(\"=\" * 60)\nprint(\"FINAL BIP EVALUATION (v7.1)\")\nprint(\"=\" * 60)\nprint()\n\n# Load baselines\ntry:\n    with open(\"data/splits/baselines.json\", 'r') as f:\n        baselines = json.load(f)\n    chance_time = baselines['chance_time']\n    chance_hohfeld = baselines['chance_hohfeld']\n    chance_bond = baselines.get('chance_bond', 1/11)\nexcept:\n    chance_time = 1/9\n    chance_hohfeld = 1/4\n    chance_bond = 1/11\n\n# Get in-domain baseline if available\nin_domain_baseline = all_results.get('mixed_control', {})\n\nprint(\"=\" * 60)\nprint(\"CROSS-TEMPORAL TRANSFER RESULTS\")\nprint(\"=\" * 60)\nprint()\n\nfor split_name in ['ancient_to_modern', 'modern_to_ancient']:\n    res = all_results.get(split_name, {})\n    probe_res = linear_probe_results.get(split_name, {})\n    \n    direction = 'Ancient → Modern' if split_name == 'ancient_to_modern' else 'Modern → Ancient'\n    print(f\"DIRECTION: {direction}\")\n    print(\"-\" * 40)\n    train_time = res.get('training_time_seconds', 0)\n    print(f\"  Train: {res.get('train_size', 0):,} | Test: {res.get('test_size', 0):,} | Time: {train_time/60:.1f}min\")\n    print()\n    \n    # Time invariance (LINEAR PROBE - primary test)\n    probe_acc = probe_res.get('probe_acc', 1.0)\n    probe_chance = probe_res.get('chance_level', chance_time)\n    is_invariant = probe_res.get('is_time_invariant', False)\n    \n    print(f\"  TIME INVARIANCE (Linear Probe - Primary Test):\")\n    print(f\"    Probe accuracy: {probe_acc:.1%} (chance: {probe_chance:.1%})\")\n    print(f\"    Status: {'✓ INVARIANT' if is_invariant else '✗ LEAKAGE'}\")\n    print()\n    \n    # Adversary head (secondary)\n    adv_acc = res.get('time_acc', 0)\n    print(f\"  Time (Adversary Head - Secondary):\")\n    print(f\"    Accuracy: {adv_acc:.1%} (chance: {chance_time:.1%})\")\n    print()\n    \n    # Bond transfer\n    bond_f1 = res.get('bond_f1_macro', 0)\n    bond_acc = res.get('bond_acc', 0)\n    print(f\"  BOND TRANSFER:\")\n    print(f\"    Accuracy: {bond_acc:.1%} | F1 (macro): {bond_f1:.3f}\")\n    print(f\"    Chance: {chance_bond:.1%}\")\n    \n    # Compare to in-domain baseline\n    if in_domain_baseline:\n        baseline_bond_f1 = in_domain_baseline.get('bond_f1_macro', 0)\n        if baseline_bond_f1 > 0:\n            degradation = baseline_bond_f1 - bond_f1\n            print(f\"    In-domain baseline F1: {baseline_bond_f1:.3f}\")\n            print(f\"    Degradation: {degradation:+.3f} ({degradation/baseline_bond_f1*100:+.1f}%)\")\n    print()\n    \n    # Per-corpus breakdown\n    if 'corpus_bond_f1' in res and res['corpus_bond_f1']:\n        print(f\"  BY CORPUS:\")\n        for corpus, metrics in res['corpus_bond_f1'].items():\n            print(f\"    {corpus}: F1={metrics['f1_macro']:.3f}, Acc={metrics['accuracy']:.1%} (n={metrics['n_samples']:,})\")\n    print()\n    \n    # Hohfeld\n    hohfeld_f1 = res.get('hohfeld_f1_macro', 0)\n    hohfeld_acc = res.get('hohfeld_acc', 0)\n    print(f\"  HOHFELD CLASSIFICATION:\")\n    print(f\"    Accuracy: {hohfeld_acc:.1%} | F1 (macro): {hohfeld_f1:.3f}\")\n    print(f\"    Chance: {chance_hohfeld:.1%}\")\n    print()\n\n# Summary verdict\nprint(\"=\" * 60)\nprint(\"SUMMARY VERDICT\")\nprint(\"=\" * 60)\nprint()\n\nA_probe = linear_probe_results.get('ancient_to_modern', {})\nB_probe = linear_probe_results.get('modern_to_ancient', {})\nA_res = all_results.get('ancient_to_modern', {})\nB_res = all_results.get('modern_to_ancient', {})\n\nA_invariant = A_probe.get('is_time_invariant', False)\nB_invariant = B_probe.get('is_time_invariant', False)\nA_bond_good = A_res.get('bond_f1_macro', 0) > chance_bond * 1.5\nB_bond_good = B_res.get('bond_f1_macro', 0) > chance_bond * 1.5\n\nprint(f\"Ancient → Modern:\")\nprint(f\"  Time invariant (probe): {'✓' if A_invariant else '✗'}\")\nprint(f\"  Bond transfer (F1 > {chance_bond*1.5:.1%}): {'✓' if A_bond_good else '✗'}\")\nprint()\nprint(f\"Modern → Ancient:\")\nprint(f\"  Time invariant (probe): {'✓' if B_invariant else '✗'}\")\nprint(f\"  Bond transfer (F1 > {chance_bond*1.5:.1%}): {'✓' if B_bond_good else '✗'}\")\nprint()\n\nif A_invariant and B_invariant and A_bond_good and B_bond_good:\n    verdict = \"STRONGLY_SUPPORTED\"\n    verdict_box = \"\"\"\n    ╔══════════════════════════════════════════════════════════╗\n    ║     BIP: STRONGLY SUPPORTED                              ║\n    ╠══════════════════════════════════════════════════════════╣\n    ║  Both directions show:                                   ║\n    ║    ✓ Time-invariant bond representation (probe test)     ║\n    ║    ✓ Bond transfer well above chance                     ║\n    ║  Cross-domain performance shows minimal degradation.     ║\n    ╚══════════════════════════════════════════════════════════╝\n    \"\"\"\nelif (A_invariant and A_bond_good) or (B_invariant and B_bond_good):\n    verdict = \"SUPPORTED_UNIDIRECTIONAL\"\n    verdict_box = \"\"\"\n    ╔══════════════════════════════════════════════════════════╗\n    ║     BIP: SUPPORTED (One Direction)                       ║\n    ╠══════════════════════════════════════════════════════════╣\n    ║  At least one direction shows time-invariant bond        ║\n    ║  representation with transfer above chance.              ║\n    ╚══════════════════════════════════════════════════════════╝\n    \"\"\"\nelif A_bond_good or B_bond_good:\n    verdict = \"PARTIAL_SUPPORT\"\n    verdict_box = \"\"\"\n    ╔══════════════════════════════════════════════════════════╗\n    ║     BIP: PARTIAL SUPPORT                                 ║\n    ╠══════════════════════════════════════════════════════════╣\n    ║  Bond transfer works, but time information may still     ║\n    ║  leak through z_bond (probe can decode it).              ║\n    ╚══════════════════════════════════════════════════════════╝\n    \"\"\"\nelse:\n    verdict = \"INCONCLUSIVE\"\n    verdict_box = \"\"\"\n    ╔══════════════════════════════════════════════════════════╗\n    ║     BIP: INCONCLUSIVE                                    ║\n    ╠══════════════════════════════════════════════════════════╣\n    ║  Neither direction shows clear invariance with transfer. ║\n    ╚══════════════════════════════════════════════════════════╝\n    \"\"\"\n\nprint(verdict_box)\n\n# Total runtime\ntotal_time = time.time() - EXPERIMENT_START\nprint(f\"\\nTotal experiment time: {total_time/60:.1f} minutes\")\n\n# Save all results\nfinal_results = {\n    'model_results': {k: {kk: vv for kk, vv in v.items() if kk != 'test_preds'} \n                      for k, v in all_results.items()},\n    'linear_probe_results': linear_probe_results,\n    'verdict': verdict,\n    'total_time_seconds': total_time,\n    'accelerator': ACCELERATOR,\n    'baselines': {\n        'chance_time': chance_time,\n        'chance_hohfeld': chance_hohfeld,\n        'chance_bond': chance_bond\n    },\n    'methodology_notes': {\n        'label_source': 'English translations (text_english)',\n        'languages': 'Hebrew (Sefaria) ↔ English (Dear Abby)',\n        'primary_invariance_test': 'Linear probe on frozen z_bond'\n    }\n}\n\nwith open('results/final_results.json', 'w') as f:\n    json.dump(final_results, f, indent=2, default=str)\n\nwith open(f\"{SAVE_DIR}/final_results.json\", 'w') as f:\n    json.dump(final_results, f, indent=2, default=str)\n\nprint(f\"\\nResults saved to results/final_results.json\")\nprint(f\"Backed up to {SAVE_DIR}/final_results.json\")\n\nprint_resources(\"Final\")\n\nmark_task(\"Evaluate results\", \"done\")\n\nprint()\nprint(\"=\" * 60)\nprint(\"EXPERIMENT COMPLETE\")\nprint(\"=\" * 60)\nprint_progress()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Save Results\n",
        "\n",
        "Run the cell below to download your trained model and results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 12. Download Results (Optional) { display-mode: \"form\" }\n#@markdown Creates a zip file with model checkpoints, metrics, and probe results.\n\nimport shutil\nimport os\nfrom google.colab import files\n\n!mkdir -p results\n\n# Copy model checkpoints\nfor split_name in ['ancient_to_modern', 'modern_to_ancient', 'mixed_control']:\n    model_path = f\"models/checkpoints/best_model_{split_name}.pt\"\n    if os.path.exists(model_path):\n        !cp \"{model_path}\" results/\n        print(f\"Copied {model_path}\")\n\n!cp data/splits/all_splits.json results/ 2>/dev/null || true\n!cp data/splits/baselines.json results/ 2>/dev/null || true\n!cp results/final_results.json results/ 2>/dev/null || true\n!cp results/linear_probe_results.json results/ 2>/dev/null || true\n\n# Zip\nshutil.make_archive('bip_results_v7', 'zip', 'results')\nprint()\nprint(\"Results saved to bip_results_v7.zip\")\nprint()\nprint(\"Contents:\")\n!ls -la results/\n\nfiles.download('bip_results_v7.zip')",
      "execution_count": null,
      "outputs": []
    }
  ]
}
