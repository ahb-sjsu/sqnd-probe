{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIP v10.11 - Bond Invariance Probe\n",
    "\n",
    "**v10.11 Changes:**\n",
    "- All corpora from verified APIs/datasets (no hardcoding)\n",
    "- Parallel downloads with rate limiting\n",
    "- 500+ passages per tradition (6-sigma confidence)\n",
    "- Runtime-adaptive fuzz test thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Configuration & Setup { display-mode: \"form\" }\n",
    "# @markdown ## Data Source Configuration\n",
    "\n",
    "DATA_MODE = \"Update missing\"  # @param [\"Refresh all\", \"Update missing\", \"Cache only\"]\n",
    "# @markdown - **Refresh all**: Re-download everything from source (slow, ~2hrs)\n",
    "# @markdown - **Update missing**: Use cache, download only what's missing (recommended)\n",
    "# @markdown - **Cache only**: Use only cached data, fail if missing\n",
    "\n",
    "DRIVE_FOLDER = \"BIP_v10.11\"  # @param {type:\"string\"}\n",
    "# @markdown **Folder name for persistent storage** (edit above to change)\n",
    "\n",
    "# Derive flags from DATA_MODE\n",
    "USE_DRIVE_DATA = True  # Always use Drive for caching\n",
    "REFRESH_DATA_FROM_SOURCE = DATA_MODE == \"Refresh all\"\n",
    "CACHE_ONLY = DATA_MODE == \"Cache only\"\n",
    "# @markdown ---\n",
    "# @markdown ## Model Backbone\n",
    "BACKBONE = \"MiniLM\"  # @param [\"MiniLM\", \"LaBSE\", \"XLM-R-base\", \"XLM-R-large\"]\n",
    "# @markdown - **MiniLM**: Fast, 118M params, good baseline\n",
    "# @markdown - **LaBSE**: Best cross-lingual alignment, 471M params (recommended)\n",
    "# @markdown - **XLM-R-base**: Strong multilingual, 270M params\n",
    "# @markdown - **XLM-R-large**: Strongest representations, 550M params\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ## Output Options\n",
    "CREATE_DOWNLOAD_ZIP = False  # @param {type:\"boolean\"}\n",
    "# @markdown - **CREATE_DOWNLOAD_ZIP**: Create and download a zip file of results (optional)\n",
    "# @markdown - Results are always persisted to Google Drive regardless of this setting\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ## Training Hyperparameters\n",
    "FREEZE_ENCODER = True  # @param {type:\"boolean\"}\n",
    "# @markdown - **FREEZE_ENCODER**: Only train probe head (recommended for stability)\n",
    "# @markdown - Unfrozen: Fine-tune entire encoder (471M params, risk of catastrophic forgetting)\n",
    "\n",
    "LEARNING_RATE = 1e-5  # @param {type:\"number\"}\n",
    "# @markdown - **Frozen encoder**: 1e-4 to 1e-3 works well\n",
    "# @markdown - **Unfrozen encoder**: Use 1e-5 to 5e-6 (lower = more stable)\n",
    "\n",
    "WARMUP_RATIO = 0.1  # @param {type:\"number\"}\n",
    "# @markdown - Fraction of training for learning rate warmup (0.0 to 0.2)\n",
    "\n",
    "GRADIENT_CLIP = 1.0  # @param {type:\"number\"}\n",
    "# @markdown - Max gradient norm (prevents exploding gradients, 0 = disabled)\n",
    "\n",
    "NUM_EPOCHS = 10  # @param {type:\"integer\"}\n",
    "# @markdown - Number of training epochs per split\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 3  # @param {type:\"integer\"}\n",
    "# @markdown - Stop if no improvement for N epochs (0 = disabled)\n",
    "\n",
    "# Backbone configurations\n",
    "BACKBONE_CONFIGS = {\n",
    "    \"MiniLM\": {\n",
    "        \"model_name\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        \"hidden_size\": 384,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 512,\n",
    "            \"T4\": 256,\n",
    "            \"2xT4\": 512,\n",
    "            \"SMALL\": 128,\n",
    "            \"MINIMAL/CPU\": 64,\n",
    "        },\n",
    "    },\n",
    "    \"LaBSE\": {\n",
    "        \"model_name\": \"sentence-transformers/LaBSE\",\n",
    "        \"hidden_size\": 768,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 256,\n",
    "            \"T4\": 128,\n",
    "            \"2xT4\": 256,\n",
    "            \"SMALL\": 64,\n",
    "            \"MINIMAL/CPU\": 32,\n",
    "        },\n",
    "    },\n",
    "    \"XLM-R-base\": {\n",
    "        \"model_name\": \"xlm-roberta-base\",\n",
    "        \"hidden_size\": 768,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 256,\n",
    "            \"T4\": 128,\n",
    "            \"2xT4\": 256,\n",
    "            \"SMALL\": 64,\n",
    "            \"MINIMAL/CPU\": 32,\n",
    "        },\n",
    "    },\n",
    "    \"XLM-R-large\": {\n",
    "        \"model_name\": \"xlm-roberta-large\",\n",
    "        \"hidden_size\": 1024,\n",
    "        \"recommended_batch\": {\n",
    "            \"L4/A100\": 128,\n",
    "            \"T4\": 64,\n",
    "            \"2xT4\": 128,\n",
    "            \"SMALL\": 32,\n",
    "            \"MINIMAL/CPU\": 16,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "BACKBONE_CONFIG = BACKBONE_CONFIGS[BACKBONE]\n",
    "MODEL_NAME = BACKBONE_CONFIG[\"model_name\"]\n",
    "BACKBONE_HIDDEN = BACKBONE_CONFIG[\"hidden_size\"]\n",
    "\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown ## Run Setup\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "EXPERIMENT_START = time.time()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BIP v10.9 - ENVIRONMENT DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===== ENVIRONMENT DETECTION =====\n",
    "# Detect which cloud platform we're running on\n",
    "\n",
    "ENV_NAME = \"UNKNOWN\"\n",
    "ENV_GPU_QUOTA = \"Unknown\"\n",
    "PERSISTENT_STORAGE = None\n",
    "DATA_DIR = \"/content\"  # Default\n",
    "\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"Detect cloud environment and return (name, gpu_quota, storage_path, data_dir)\"\"\"\n",
    "\n",
    "    # 1. Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "\n",
    "        return (\"COLAB\", \"Free: T4 ~12h/day, Pro: L4/A100\", \"/content/drive/MyDrive\", \"/content\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    # 2. Kaggle Kernels\n",
    "    if os.path.exists(\"/kaggle\"):\n",
    "        # Kaggle has /kaggle/input for datasets, /kaggle/working for output\n",
    "        return (\"KAGGLE\", \"Free: 2xT4 30h/week, TPU 30h/week\", \"/kaggle/working\", \"/kaggle/working\")\n",
    "\n",
    "    # 3. Lightning.ai Studios\n",
    "    if os.environ.get(\"LIGHTNING_CLOUDSPACE_HOST\") or os.path.exists(\"/teamspace\"):\n",
    "        # Lightning.ai has /teamspace/studios for persistent storage\n",
    "        return (\n",
    "            \"LIGHTNING_AI\",\n",
    "            \"Free: 22h/month GPU, Pro: A10G/H100\",\n",
    "            \"/teamspace/studios\",\n",
    "            \"/teamspace/studios\",\n",
    "        )\n",
    "\n",
    "    # 4. Paperspace Gradient\n",
    "    if os.environ.get(\"PAPERSPACE_NOTEBOOK_REPO_ID\") or os.path.exists(\"/notebooks\"):\n",
    "        return (\"PAPERSPACE\", \"Free: M4000 6h, Pro: A100/H100\", \"/storage\", \"/notebooks\")\n",
    "\n",
    "    # 5. Saturn Cloud\n",
    "    if os.environ.get(\"SATURN_RESOURCE_ID\") or \"saturn\" in os.environ.get(\"HOSTNAME\", \"\").lower():\n",
    "        return (\n",
    "            \"SATURN_CLOUD\",\n",
    "            \"Free: T4 10h/month, Pro: A10G/A100\",\n",
    "            \"/home/jovyan/workspace\",\n",
    "            \"/home/jovyan\",\n",
    "        )\n",
    "\n",
    "    # 6. HuggingFace Spaces\n",
    "    if os.environ.get(\"SPACE_ID\") or os.environ.get(\"HF_SPACE_ID\"):\n",
    "        return (\n",
    "            \"HUGGINGFACE_SPACES\",\n",
    "            \"Free: CPU only, ZeroGPU: A10G/A100 quota\",\n",
    "            \"/data\",\n",
    "            \"/home/user/app\",\n",
    "        )\n",
    "\n",
    "    # 7. AWS SageMaker Studio Lab\n",
    "    if os.path.exists(\"/home/studio-lab-user\"):\n",
    "        return (\n",
    "            \"SAGEMAKER_STUDIO_LAB\",\n",
    "            \"Free: T4 4h/session, 24h max/day\",\n",
    "            \"/home/studio-lab-user\",\n",
    "            \"/home/studio-lab-user\",\n",
    "        )\n",
    "\n",
    "    # 8. Deepnote\n",
    "    if os.environ.get(\"DEEPNOTE_PROJECT_ID\"):\n",
    "        return (\"DEEPNOTE\", \"Free: CPU, Pro: T4/A10G\", \"/work\", \"/work\")\n",
    "\n",
    "    # 9. Local/Unknown\n",
    "    return (\"LOCAL\", \"Depends on local hardware\", os.getcwd(), os.getcwd())\n",
    "\n",
    "\n",
    "ENV_NAME, ENV_GPU_QUOTA, PERSISTENT_STORAGE, DATA_DIR = detect_environment()\n",
    "\n",
    "print(f\"\\nEnvironment: {ENV_NAME}\")\n",
    "print(f\"GPU Quota:   {ENV_GPU_QUOTA}\")\n",
    "print(f\"Storage:     {PERSISTENT_STORAGE}\")\n",
    "print(f\"Data Dir:    {DATA_DIR}\")\n",
    "\n",
    "# Environment-specific setup\n",
    "ENV_TIPS = {\n",
    "    \"COLAB\": [\n",
    "        \"Tip: Use GPU runtime (Runtime -> Change runtime type -> T4 GPU)\",\n",
    "        \"Tip: Colab Pro gives L4 GPU access (~2x faster than T4)\",\n",
    "    ],\n",
    "    \"KAGGLE\": [\n",
    "        \"Tip: Enable GPU (Settings -> Accelerator -> GPU T4 x2)\",\n",
    "        \"Tip: 30h/week GPU quota resets every Saturday\",\n",
    "        \"Tip: Upload data as a Kaggle Dataset for persistence\",\n",
    "    ],\n",
    "    \"LIGHTNING_AI\": [\n",
    "        \"Tip: Select GPU studio (A10G recommended for this workload)\",\n",
    "        \"Tip: /teamspace/studios persists across sessions\",\n",
    "    ],\n",
    "    \"PAPERSPACE\": [\n",
    "        \"Tip: Use /storage for persistent data across runs\",\n",
    "        \"Tip: Free tier has 6h/month GPU limit\",\n",
    "    ],\n",
    "    \"SATURN_CLOUD\": [\n",
    "        \"Tip: Start a T4 instance from the Resources tab\",\n",
    "        \"Tip: 10h/month free GPU quota\",\n",
    "    ],\n",
    "    \"HUGGINGFACE_SPACES\": [\n",
    "        \"Tip: ZeroGPU provides A10G/A100 access with quota system\",\n",
    "        \"Tip: Use Gradio/Streamlit for interactive demos\",\n",
    "    ],\n",
    "    \"SAGEMAKER_STUDIO_LAB\": [\n",
    "        \"Tip: Request GPU runtime from the launcher\",\n",
    "        \"Tip: Sessions timeout after 4h, max 24h/day\",\n",
    "    ],\n",
    "    \"LOCAL\": [\"Tip: Running locally - ensure CUDA is installed for GPU support\"],\n",
    "}\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(\"ENVIRONMENT TIPS:\")\n",
    "for tip in ENV_TIPS.get(ENV_NAME, [\"No specific tips for this environment\"]):\n",
    "    print(f\"  {tip}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ===== INSTALL DEPENDENCIES =====\n",
    "import subprocess\n",
    "\n",
    "print(\"\\nInstalling dependencies...\")\n",
    "for pkg in [\n",
    "    \"transformers\",\n",
    "    \"sentence-transformers\",\n",
    "    \"pandas\",\n",
    "    \"tqdm\",\n",
    "    \"scikit-learn\",\n",
    "    \"pyyaml\",\n",
    "    \"psutil\",\n",
    "    \"datasets\",\n",
    "]:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GPU DETECTION & RESOURCE ALLOCATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Detect hardware\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    GPU_COUNT = torch.cuda.device_count()\n",
    "else:\n",
    "    GPU_NAME = \"CPU\"\n",
    "    VRAM_GB = 0\n",
    "    GPU_COUNT = 0\n",
    "\n",
    "RAM_GB = psutil.virtual_memory().total / 1e9\n",
    "\n",
    "print(f\"\\nDetected Hardware:\")\n",
    "print(f\"  GPU:  {GPU_NAME}\" + (f\" (x{GPU_COUNT})\" if GPU_COUNT > 1 else \"\"))\n",
    "print(\n",
    "    f\"  VRAM: {VRAM_GB:.1f} GB\" + (f\" (total: {VRAM_GB*GPU_COUNT:.1f} GB)\" if GPU_COUNT > 1 else \"\")\n",
    ")\n",
    "print(f\"  RAM:  {RAM_GB:.1f} GB\")\n",
    "\n",
    "# Set optimal parameters based on hardware\n",
    "if VRAM_GB >= 22:  # L4 (24GB) or A100\n",
    "    GPU_TIER = \"L4/A100\"\n",
    "elif VRAM_GB >= 14:  # T4 (16GB)\n",
    "    GPU_TIER = \"T4\"\n",
    "elif VRAM_GB >= 10:\n",
    "    GPU_TIER = \"SMALL\"\n",
    "else:\n",
    "    GPU_TIER = \"MINIMAL/CPU\"\n",
    "\n",
    "# Kaggle with 2xT4 can use larger batch\n",
    "if ENV_NAME == \"KAGGLE\" and GPU_COUNT >= 2:\n",
    "    GPU_TIER = \"2xT4\"\n",
    "    print(f\"  ** Kaggle 2xT4 detected **\")\n",
    "\n",
    "# Get backbone-specific batch size\n",
    "BATCH_SIZE = BACKBONE_CONFIG[\"recommended_batch\"].get(GPU_TIER, 64)\n",
    "print(f\"  Backbone: {BACKBONE} -> batch size {BATCH_SIZE}\")\n",
    "\n",
    "MAX_PER_LANG = 50000  # Language sample limit\n",
    "CPU_CORES = os.cpu_count() or 2\n",
    "NUM_WORKERS = min(4, CPU_CORES - 1) if RAM_GB >= 24 and VRAM_GB >= 14 else 0\n",
    "MAX_TEST_SAMPLES = 20000\n",
    "# Use LEARNING_RATE from UI, or scale with batch size\n",
    "if LEARNING_RATE and LEARNING_RATE != 1e-5:  # 1e-5 is the default\n",
    "    LR = LEARNING_RATE\n",
    "else:\n",
    "    LR = 2e-5 * (BATCH_SIZE / 256)  # Auto-scale with batch size\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(f\"OPTIMAL SETTINGS:\")\n",
    "print(f\"-\" * 60)\n",
    "print(f\"  Environment:     {ENV_NAME}\")\n",
    "print(f\"  GPU Tier:        {GPU_TIER}\")\n",
    "print(f\"  Backbone:        {BACKBONE}\")\n",
    "print(f\"  Batch size:      {BATCH_SIZE}\")\n",
    "print(f\"  Max per lang:    {MAX_PER_LANG:,}\")\n",
    "print(f\"  DataLoader workers: {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate:   {LR:.2e}\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "scaler = torch.amp.GradScaler(\"cuda\") if USE_AMP else None\n",
    "\n",
    "# ===== PERSISTENT STORAGE SETUP =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERSISTENT STORAGE SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "SAVE_DIR = None\n",
    "DRIVE_HAS_DATA = False\n",
    "DRIVE_FILES = set()  # Use set for O(1) lookup\n",
    "\n",
    "if ENV_NAME == \"COLAB\":\n",
    "    # Google Colab - mount Drive\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "\n",
    "        DRIVE_MOUNT_PATH = \"/content/drive\"\n",
    "\n",
    "        if os.path.exists(f\"{DRIVE_MOUNT_PATH}/MyDrive\"):\n",
    "            print(\"Google Drive already mounted\")\n",
    "        else:\n",
    "            try:\n",
    "                drive.mount(DRIVE_MOUNT_PATH, force_remount=False)\n",
    "                print(\"Google Drive mounted successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Drive mount issue: {e}\")\n",
    "                try:\n",
    "                    drive.mount(DRIVE_MOUNT_PATH, force_remount=True)\n",
    "                    print(\"Google Drive mounted (force remount)\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"WARNING: Could not mount Drive: {e2}\")\n",
    "                    print(\"Falling back to local storage\")\n",
    "                    PERSISTENT_STORAGE = DATA_DIR\n",
    "\n",
    "        SAVE_DIR = f\"{DRIVE_MOUNT_PATH}/MyDrive/{DRIVE_FOLDER}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Colab Drive setup failed: {e}\")\n",
    "        SAVE_DIR = f\"{DATA_DIR}/{DRIVE_FOLDER}\"\n",
    "\n",
    "elif ENV_NAME == \"KAGGLE\":\n",
    "    # Kaggle - use working directory\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using Kaggle working directory: {SAVE_DIR}\")\n",
    "    print(\"Note: Data persists until kernel is reset\")\n",
    "    # Check for uploaded datasets\n",
    "    if os.path.exists(\"/kaggle/input\"):\n",
    "        datasets = os.listdir(\"/kaggle/input\")\n",
    "        if datasets:\n",
    "            print(f\"Available datasets: {datasets[:5]}\")\n",
    "\n",
    "elif ENV_NAME == \"LIGHTNING_AI\":\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using Lightning.ai studio storage: {SAVE_DIR}\")\n",
    "\n",
    "elif ENV_NAME == \"PAPERSPACE\":\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using Paperspace /storage: {SAVE_DIR}\")\n",
    "\n",
    "elif ENV_NAME == \"HUGGINGFACE_SPACES\":\n",
    "    # HF Spaces has limited persistent storage\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using HuggingFace Spaces storage: {SAVE_DIR}\")\n",
    "    print(\"Warning: HF Spaces storage is limited\")\n",
    "\n",
    "else:\n",
    "    SAVE_DIR = f\"{PERSISTENT_STORAGE}/{DRIVE_FOLDER}\"\n",
    "    print(f\"Using local storage: {SAVE_DIR}\")\n",
    "\n",
    "# Check if folder exists BEFORE creating it\n",
    "folder_existed = os.path.exists(SAVE_DIR)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Check what's available in storage - use BOTH listdir AND direct exists checks\n",
    "# (Google Drive can have sync issues where listdir misses files)\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    DRIVE_FILES = set(os.listdir(SAVE_DIR))  # O(1) membership test\n",
    "\n",
    "    # Direct existence checks for key files (bypasses listdir caching issues)\n",
    "    key_files = [\"passages.jsonl\", \"bonds.jsonl\", \"dear_abby.csv\", \"all_splits.json\"]\n",
    "    for kf in key_files:\n",
    "        kf_path = os.path.join(SAVE_DIR, kf)\n",
    "        if os.path.exists(kf_path) and kf not in DRIVE_FILES:\n",
    "            print(f\"  [Drive sync fix] Found {kf} via os.path.exists() but not listdir()\")\n",
    "            DRIVE_FILES.add(kf)\n",
    "\n",
    "    DRIVE_HAS_DATA = \"passages.jsonl\" in DRIVE_FILES and \"bonds.jsonl\" in DRIVE_FILES\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 60)\n",
    "print(f\"STORAGE STATUS:\")\n",
    "print(f\"-\" * 60)\n",
    "print(f\"  Folder: {SAVE_DIR}\")\n",
    "print(f\"  Folder existed: {folder_existed}\")\n",
    "print(f\"  Files found: {len(DRIVE_FILES)}\")\n",
    "\n",
    "# If folder was empty/new, show what folders exist in parent to help debug\n",
    "if not DRIVE_FILES and ENV_NAME == \"COLAB\":\n",
    "    parent = os.path.dirname(SAVE_DIR)  # e.g., /content/drive/MyDrive\n",
    "    if os.path.exists(parent):\n",
    "        siblings = [d for d in os.listdir(parent) if \"bip\" in d.lower() or \"BIP\" in d]\n",
    "        if siblings:\n",
    "            print(f\"  ** Similar folders in {parent}: {siblings}\")\n",
    "        else:\n",
    "            print(f\"  ** No BIP folders found in {parent}\")\n",
    "if DRIVE_FILES:\n",
    "    for f in sorted(DRIVE_FILES)[:10]:  # sorted() converts to list for slicing\n",
    "        print(f\"    - {f}\")\n",
    "    if len(DRIVE_FILES) > 10:\n",
    "        print(f\"    ... and {len(DRIVE_FILES)-10} more\")\n",
    "print(f\"  Pre-processed data available: {DRIVE_HAS_DATA}\")\n",
    "\n",
    "# Decide data loading strategy\n",
    "LOAD_FROM_DRIVE = USE_DRIVE_DATA and DRIVE_HAS_DATA and not REFRESH_DATA_FROM_SOURCE\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"DATA LOADING STRATEGY: {DATA_MODE}\")\n",
    "print(\"-\" * 60)\n",
    "if DATA_MODE == \"Refresh all\":\n",
    "    print(f\"  -> Will re-download ALL data from online sources\")\n",
    "    print(f\"     (This takes ~2 hours, use 'Update missing' to save time)\")\n",
    "elif DATA_MODE == \"Cache only\":\n",
    "    if LOAD_FROM_DRIVE:\n",
    "        print(f\"  -> Using cached data only (no downloads)\")\n",
    "    else:\n",
    "        print(f\"  -> ERROR: Cache-only mode but no cached data found!\")\n",
    "        print(f\"     Change DATA_MODE to 'Update missing'\")\n",
    "else:  # Update missing (default)\n",
    "    if LOAD_FROM_DRIVE:\n",
    "        print(f\"  -> Using cached processed data from Drive\")\n",
    "        print(f\"     (v10.9 corpora will be added if missing)\")\n",
    "    else:\n",
    "        print(f\"  -> Will download missing data, use cached where available\")\n",
    "        print(\n",
    "            f\"     Sefaria: {'cached' if os.path.exists(f'{SAVE_DIR}/Sefaria-Export-json.tar.gz') else 'will download'}\"\n",
    "        )\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create local directories\n",
    "for d in [\"data/processed\", \"data/splits\", \"data/raw\", \"models/checkpoints\", \"results\"]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"SETUP COMPLETE\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"  Environment: {ENV_NAME}\")\n",
    "print(f\"  GPU:         {GPU_NAME} ({GPU_TIER})\")\n",
    "print(f\"  Storage:     {SAVE_DIR}\")\n",
    "print(f\"  Ready to run: Cell 2 (Imports)\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2. Load Corpora (v10.11 - Self-Contained) { display-mode: \"form\" }\n",
    "# @markdown Downloads from verified external sources - fully self-contained, no external imports\n",
    "# @markdown\n",
    "# @markdown **Sources (15 verified):**\n",
    "# @markdown - Sanskrit: Itihasa (93K shlokas)\n",
    "# @markdown - Pali: SuttaCentral API (Full Canon)\n",
    "# @markdown - Arabic: Tanzil.net (Quran)\n",
    "# @markdown - Hebrew/Aramaic: Sefaria GitHub\n",
    "# @markdown - Chinese: ctext.org API\n",
    "# @markdown - Greek/Latin: Perseus Digital Library\n",
    "# @markdown - Romance: Don Quijote, Montaigne, Voltaire, Rousseau, Machiavelli, Dante\n",
    "# @markdown - Folklore: Ashliman Folktexts (incl. Native American)\n",
    "# @markdown - English: Gutenberg philosophy, Dear Abby (68K), hendrycks/ethics (134K)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "import subprocess\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING CORPORA (v10.11 - Self-Contained)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "DATA_DIR = Path(\"data/raw/v10.11\")\n",
    "CACHE_DIR = DATA_DIR / \"cache\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get settings from Cell 1\n",
    "try:\n",
    "    _cache_only = CACHE_ONLY\n",
    "except NameError:\n",
    "    _cache_only = False\n",
    "\n",
    "try:\n",
    "    _save_dir = SAVE_DIR\n",
    "except NameError:\n",
    "    _save_dir = \"data/processed\"\n",
    "\n",
    "# Memory limits per language (L4 GPU safe)\n",
    "MAX_PASSAGES_PER_LANG = {\n",
    "    \"sanskrit\": 15000,\n",
    "    \"pali\": 10000,\n",
    "    \"arabic\": 10000,\n",
    "    \"classical_chinese\": 10000,\n",
    "    \"hebrew\": 15000,\n",
    "    \"aramaic\": 10000,\n",
    "    \"english\": 50000,  # Increased for folklore + ethics\n",
    "    \"greek\": 10000,\n",
    "    \"latin\": 10000,\n",
    "    \"spanish\": 5000,\n",
    "    \"french\": 5000,\n",
    "    \"italian\": 5000,\n",
    "    \"default\": 5000,\n",
    "}\n",
    "\n",
    "MIN_PASSAGES = 500  # For 6-sigma confidence\n",
    "\n",
    "# ============================================================================\n",
    "# RATE LIMITING\n",
    "# ============================================================================\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, calls_per_minute: int = 20):\n",
    "        self.min_interval = 60.0 / calls_per_minute\n",
    "        self.last_call = 0.0\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def wait(self):\n",
    "        with self.lock:\n",
    "            elapsed = time.time() - self.last_call\n",
    "            if elapsed < self.min_interval:\n",
    "                time.sleep(self.min_interval - elapsed)\n",
    "            self.last_call = time.time()\n",
    "\n",
    "GITHUB_LIMITER = RateLimiter(calls_per_minute=60)\n",
    "SUTTACENTRAL_LIMITER = RateLimiter(calls_per_minute=120)\n",
    "CTEXT_LIMITER = RateLimiter(calls_per_minute=30)\n",
    "\n",
    "# ============================================================================\n",
    "# SANSKRIT - Itihasa from GitHub (VERIFIED)\n",
    "# https://github.com/rahular/itihasa - 93K shlokas\n",
    "# ============================================================================\n",
    "\n",
    "def load_itihasa_github() -> List[Dict]:\n",
    "    \"\"\"Load Itihasa Sanskrit shlokas from GitHub.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"itihasa.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Itihasa: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Downloading Itihasa from GitHub...\")\n",
    "    data_path = DATA_DIR / \"itihasa\"\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = [\n",
    "        (\"train.sn\", \"https://raw.githubusercontent.com/rahular/itihasa/main/data/train.sn\"),\n",
    "        (\"dev.sn\", \"https://raw.githubusercontent.com/rahular/itihasa/main/data/dev.sn\"),\n",
    "        (\"test.sn\", \"https://raw.githubusercontent.com/rahular/itihasa/main/data/test.sn\"),\n",
    "    ]\n",
    "\n",
    "    for name, url in files:\n",
    "        local_file = data_path / name\n",
    "        if not local_file.exists():\n",
    "            try:\n",
    "                GITHUB_LIMITER.wait()\n",
    "                resp = requests.get(url, timeout=120)\n",
    "                if resp.status_code == 200:\n",
    "                    with open(local_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(resp.text)\n",
    "                    print(f\"    Downloaded {name}: {len(resp.text)//1024}KB\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Failed {name}: {e}\")\n",
    "\n",
    "    # Parse .sn files\n",
    "    for sn_file in data_path.glob(\"*.sn\"):\n",
    "        with open(sn_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f, 1):\n",
    "                text = line.strip()\n",
    "                if text and len(text) > 10:\n",
    "                    passages.append({\n",
    "                        \"id\": f\"itihasa_{sn_file.stem}_{i}\",\n",
    "                        \"text\": text,\n",
    "                        \"language\": \"sanskrit\",\n",
    "                        \"source\": f\"Itihasa/{sn_file.stem}\",\n",
    "                        \"time_period\": \"DHARMA\",\n",
    "                    })\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  Itihasa: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "# ============================================================================\n",
    "# PALI - SuttaCentral API (VERIFIED)\n",
    "# https://suttacentral.net/api/bilarasuttas/{id}/pli\n",
    "# ============================================================================\n",
    "\n",
    "def load_pali_suttacentral() -> List[Dict]:\n",
    "    \"\"\"Load Pali texts from SuttaCentral API.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"suttacentral.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  SuttaCentral: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Fetching from SuttaCentral API...\")\n",
    "\n",
    "    # Expanded sutta list\n",
    "    sutta_ids = []\n",
    "    # Majjhima Nikaya (152 suttas)\n",
    "    sutta_ids.extend([f\"mn{i}\" for i in range(1, 153)])\n",
    "    # Digha Nikaya (34 suttas)\n",
    "    sutta_ids.extend([f\"dn{i}\" for i in range(1, 35)])\n",
    "    # Samyutta Nikaya (key vaggas)\n",
    "    for v in [1, 3, 6, 12, 22, 35, 45, 56]:\n",
    "        sutta_ids.extend([f\"sn{v}.{i}\" for i in range(1, 20)])\n",
    "    # Anguttara Nikaya\n",
    "    for n in [1, 2, 3, 4, 5, 6, 7, 8, 10]:\n",
    "        sutta_ids.extend([f\"an{n}.{i}\" for i in range(1, 50)])\n",
    "    # Dhammapada\n",
    "    sutta_ids.extend([f\"dhp{i}\" for i in range(1, 27)])\n",
    "\n",
    "    def fetch_sutta(sid):\n",
    "        results = []\n",
    "        try:\n",
    "            SUTTACENTRAL_LIMITER.wait()\n",
    "            url = f\"https://suttacentral.net/api/bilarasuttas/{sid}/pli\"\n",
    "            resp = requests.get(url, timeout=30)\n",
    "            if resp.status_code == 200:\n",
    "                data = resp.json()\n",
    "                if isinstance(data, dict):\n",
    "                    segments = data.get(\"root_text\", {})\n",
    "                    if isinstance(segments, dict):\n",
    "                        for seg_id, text in segments.items():\n",
    "                            if text and len(text) > 20:\n",
    "                                results.append({\n",
    "                                    \"id\": f\"pali_{sid}_{seg_id}\",\n",
    "                                    \"text\": text.strip(),\n",
    "                                    \"language\": \"pali\",\n",
    "                                    \"source\": sid,\n",
    "                                    \"time_period\": \"PALI\",\n",
    "                                })\n",
    "        except:\n",
    "            pass\n",
    "        return results\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(fetch_sutta, sid) for sid in sutta_ids[:300]]\n",
    "        done = 0\n",
    "        for future in as_completed(futures):\n",
    "            passages.extend(future.result())\n",
    "            done += 1\n",
    "            if done % 50 == 0:\n",
    "                print(f\"    Fetched {done}/{min(300, len(sutta_ids))} suttas...\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  SuttaCentral: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "# ============================================================================\n",
    "# ARABIC - Tanzil.net (VERIFIED)\n",
    "# https://tanzil.net/download/\n",
    "# ============================================================================\n",
    "\n",
    "def load_quran_tanzil() -> List[Dict]:\n",
    "    \"\"\"Load Quran from Tanzil.net.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"tanzil.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Tanzil Quran: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Downloading Quran from Tanzil.net...\")\n",
    "    try:\n",
    "        url = \"https://tanzil.net/pub/download/index.php?quranType=uthmani&outType=txt-2&agree=true\"\n",
    "        resp = requests.get(url, timeout=60)\n",
    "        if resp.status_code == 200:\n",
    "            for line in resp.text.strip().split(\"\\n\"):\n",
    "                if \"|\" in line:\n",
    "                    parts = line.split(\"|\")\n",
    "                    if len(parts) >= 3:\n",
    "                        surah, ayah, text = parts[0], parts[1], parts[2].strip()\n",
    "                        if len(text) > 10:\n",
    "                            passages.append({\n",
    "                                \"id\": f\"quran_{surah}_{ayah}\",\n",
    "                                \"text\": text,\n",
    "                                \"language\": \"arabic\",\n",
    "                                \"source\": f\"Quran {surah}:{ayah}\",\n",
    "                                \"time_period\": \"QURANIC\",\n",
    "                            })\n",
    "            print(f\"    Downloaded {len(passages)} verses\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  Tanzil Quran: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "# ============================================================================\n",
    "# HEBREW/ARAMAIC - Sefaria GitHub (VERIFIED)\n",
    "# https://github.com/Sefaria/Sefaria-Export\n",
    "# ============================================================================\n",
    "\n",
    "def load_sefaria_github() -> List[Dict]:\n",
    "    \"\"\"Load Hebrew/Aramaic from Sefaria GitHub.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"sefaria.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Sefaria: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    base_path = DATA_DIR / \"Sefaria-Export\"\n",
    "    json_path = base_path / \"json\"\n",
    "\n",
    "    if not json_path.exists():\n",
    "        print(\"  Cloning Sefaria-Export from GitHub (~2GB)...\")\n",
    "        try:\n",
    "            proc = subprocess.run(\n",
    "                [\"git\", \"clone\", \"--depth\", \"1\",\n",
    "                 \"https://github.com/Sefaria/Sefaria-Export.git\",\n",
    "                 str(base_path)],\n",
    "                timeout=300, capture_output=True, text=True\n",
    "            )\n",
    "            if proc.returncode != 0:\n",
    "                print(f\"    Clone failed: {proc.stderr[:200]}\")\n",
    "                return passages\n",
    "            print(\"    Downloaded!\")\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"    TIMEOUT\")\n",
    "            return passages\n",
    "        except Exception as e:\n",
    "            print(f\"    Failed: {e}\")\n",
    "            return passages\n",
    "\n",
    "    key_texts = [\n",
    "        (\"Tanakh/Torah/Genesis\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Torah/Exodus\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Torah/Leviticus\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Torah/Numbers\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Torah/Deuteronomy\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Isaiah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Jeremiah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Ezekiel\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Amos\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Prophets/Micah\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Psalms\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Proverbs\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Job\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Tanakh/Writings/Ecclesiastes\", \"hebrew\", \"BIBLICAL\"),\n",
    "        (\"Mishnah/Seder Nezikin/Pirkei Avot\", \"hebrew\", \"TANNAITIC\"),\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Sanhedrin\", \"hebrew\", \"TANNAITIC\"),\n",
    "        (\"Mishnah/Seder Nezikin/Mishnah Bava Kamma\", \"hebrew\", \"TANNAITIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Sanhedrin\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Bava Kamma\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Nezikin/Bava Metzia\", \"aramaic\", \"AMORAIC\"),\n",
    "        (\"Talmud/Bavli/Seder Moed/Shabbat\", \"aramaic\", \"AMORAIC\"),\n",
    "    ]\n",
    "\n",
    "    def extract_text(obj, depth=0):\n",
    "        if depth > 5:\n",
    "            return []\n",
    "        texts = []\n",
    "        if isinstance(obj, str) and len(obj) > 20:\n",
    "            texts.append(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                texts.extend(extract_text(item, depth + 1))\n",
    "        elif isinstance(obj, dict):\n",
    "            for key in [\"he\", \"text\", \"content\"]:\n",
    "                if key in obj:\n",
    "                    texts.extend(extract_text(obj[key], depth + 1))\n",
    "        return texts\n",
    "\n",
    "    for text_path, lang, period in key_texts:\n",
    "        full_path = json_path / text_path\n",
    "        if not full_path.exists():\n",
    "            json_file = json_path / f\"{text_path}.json\"\n",
    "            if json_file.exists():\n",
    "                full_path = json_file\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            files_to_parse = []\n",
    "            if full_path.is_file():\n",
    "                files_to_parse = [full_path]\n",
    "            elif full_path.is_dir():\n",
    "                files_to_parse = list(full_path.rglob(\"*.json\"))[:100]\n",
    "\n",
    "            for jf in files_to_parse:\n",
    "                with open(jf, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                texts = extract_text(data)\n",
    "                for text in texts[:200]:\n",
    "                    if len(passages) >= MAX_PASSAGES_PER_LANG.get(lang, 5000):\n",
    "                        break\n",
    "                    passages.append({\n",
    "                        \"id\": f\"sefaria_{len(passages)}\",\n",
    "                        \"text\": text.strip(),\n",
    "                        \"language\": lang,\n",
    "                        \"source\": text_path.split(\"/\")[-1],\n",
    "                        \"time_period\": period,\n",
    "                    })\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  Sefaria: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "# ============================================================================\n",
    "# CHINESE - ctext.org API (VERIFIED)\n",
    "# https://api.ctext.org/gettext?urn=ctp:analects/xue-er\n",
    "# ============================================================================\n",
    "\n",
    "def load_chinese_ctext() -> List[Dict]:\n",
    "    \"\"\"Load Chinese classics from ctext.org API.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"ctext.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  ctext.org: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Fetching from ctext.org API...\")\n",
    "\n",
    "    # ctext.org requires chapter-level URNs\n",
    "    texts = [\n",
    "        (\"ctp:analects\", \"Analects\", \"CONFUCIAN\", [\"xue-er\", \"wei-zheng\", \"ba-yi\", \"li-ren\", \"gong-ye-chang\",\n",
    "            \"yong-ye\", \"shu-er\", \"tai-bo\", \"zi-han\", \"xiang-dang\", \"xian-jin\", \"yan-yuan\",\n",
    "            \"zi-lu\", \"xian-wen\", \"wei-ling-gong\", \"ji-shi\", \"yang-huo\", \"wei-zi\", \"zi-zhang\", \"yao-yue\"]),\n",
    "        (\"ctp:mengzi\", \"Mencius\", \"CONFUCIAN\", [\"liang-hui-wang-i\", \"liang-hui-wang-ii\", \"gong-sun-chou-i\",\n",
    "            \"gong-sun-chou-ii\", \"teng-wen-gong-i\", \"teng-wen-gong-ii\", \"li-lou-i\", \"li-lou-ii\",\n",
    "            \"wan-zhang-i\", \"wan-zhang-ii\", \"gao-zi-i\", \"gao-zi-ii\", \"jin-xin-i\", \"jin-xin-ii\"]),\n",
    "        (\"ctp:dao-de-jing\", \"Daodejing\", \"DAOIST\", [str(i) for i in range(1, 82)]),\n",
    "        (\"ctp:zhuangzi\", \"Zhuangzi\", \"DAOIST\", [\"xiao-yao-you\", \"qi-wu-lun\", \"yang-sheng-zhu\", \"ren-jian-shi\",\n",
    "            \"de-chong-fu\", \"da-zong-shi\", \"ying-di-wang\"]),\n",
    "        (\"ctp:xunzi\", \"Xunzi\", \"CONFUCIAN\", [\"quan-xue\", \"xiu-shen\", \"bu-gou\", \"rong-ru\", \"fei-xiang\", \"wang-zhi\"]),\n",
    "        (\"ctp:mozi\", \"Mozi\", \"MOHIST\", [\"qin-shi\", \"xiu-shen\", \"suo-ran\", \"fa-yi\", \"qi-huan\", \"ci-guo\"]),\n",
    "    ]\n",
    "\n",
    "    for text_id, name, period, chapters in texts:\n",
    "        count = 0\n",
    "        for chapter in chapters:\n",
    "            try:\n",
    "                CTEXT_LIMITER.wait()\n",
    "                urn = f\"{text_id}/{chapter}\"\n",
    "                url = f\"https://api.ctext.org/gettext?urn={urn}\"\n",
    "                resp = requests.get(url, timeout=30)\n",
    "                if resp.status_code == 200:\n",
    "                    data = resp.json()\n",
    "                    if isinstance(data, list):\n",
    "                        for item in data:\n",
    "                            text = item.get(\"text\", \"\")\n",
    "                            if text and len(text) > 10:\n",
    "                                passages.append({\n",
    "                                    \"id\": f\"ctext_{len(passages)}\",\n",
    "                                    \"text\": text,\n",
    "                                    \"language\": \"classical_chinese\",\n",
    "                                    \"source\": f\"{name}/{chapter}\",\n",
    "                                    \"time_period\": period,\n",
    "                                })\n",
    "                                count += 1\n",
    "            except:\n",
    "                continue\n",
    "        if count > 0:\n",
    "            print(f\"    {name}: {count} passages\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  ctext.org: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "# ============================================================================\n",
    "# GREEK/LATIN - Perseus Digital Library (VERIFIED)\n",
    "# https://github.com/PerseusDL/canonical-greekLit\n",
    "# https://github.com/PerseusDL/canonical-latinLit\n",
    "# ============================================================================\n",
    "\n",
    "def load_perseus_github() -> List[Dict]:\n",
    "    \"\"\"Load Greek and Latin philosophy from Perseus Digital Library.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"perseus.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Perseus: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Fetching from Perseus GitHub...\")\n",
    "\n",
    "    # Key philosophical texts - raw GitHub URLs\n",
    "    greek_texts = [\n",
    "        (\"tlg0059.tlg030\", \"Plato Republic\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0059.tlg031\", \"Plato Laws\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0059.tlg017\", \"Plato Apology\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0059.tlg004\", \"Plato Symposium\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0059.tlg003\", \"Plato Phaedo\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0086.tlg010\", \"Aristotle Nicomachean Ethics\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0086.tlg028\", \"Aristotle Politics\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0086.tlg035\", \"Aristotle Rhetoric\", \"CLASSICAL_GREEK\"),\n",
    "        (\"tlg0555.tlg001\", \"Epictetus Discourses\", \"HELLENISTIC\"),\n",
    "        (\"tlg0555.tlg002\", \"Epictetus Enchiridion\", \"HELLENISTIC\"),\n",
    "        (\"tlg0562.tlg001\", \"Marcus Aurelius Meditations\", \"HELLENISTIC\"),\n",
    "    ]\n",
    "\n",
    "    latin_texts = [\n",
    "        (\"phi0474.phi038\", \"Cicero De Officiis\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0474.phi044\", \"Cicero Tusculan Disputations\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0474.phi019\", \"Cicero De Finibus\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0690.phi003\", \"Seneca Epistles\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0690.phi001\", \"Seneca De Beneficiis\", \"CLASSICAL_LATIN\"),\n",
    "        (\"phi0959.phi001\", \"Lucretius De Rerum Natura\", \"CLASSICAL_LATIN\"),\n",
    "    ]\n",
    "\n",
    "    def fetch_perseus_text(text_id, name, period, lang_prefix):\n",
    "        results = []\n",
    "        base_url = f\"https://raw.githubusercontent.com/PerseusDL/canonical-{lang_prefix}Lit/master/data/{text_id}\"\n",
    "\n",
    "        try:\n",
    "            GITHUB_LIMITER.wait()\n",
    "            # Try different file patterns\n",
    "            for pattern in [f\"{text_id}.{lang_prefix}1.xml\", f\"{text_id}.xml\"]:\n",
    "                url = f\"{base_url}/{pattern}\"\n",
    "                resp = requests.get(url, timeout=30)\n",
    "                if resp.status_code == 200:\n",
    "                    # Simple XML text extraction\n",
    "                    import re\n",
    "                    # Extract text between tags, remove markup\n",
    "                    text_content = re.sub(r\"<[^>]+>\", \" \", resp.text)\n",
    "                    text_content = re.sub(r\"\\s+\", \" \", text_content).strip()\n",
    "\n",
    "                    # Split into chunks of ~500 chars\n",
    "                    chunks = []\n",
    "                    words = text_content.split()\n",
    "                    current = []\n",
    "                    current_len = 0\n",
    "                    for word in words:\n",
    "                        current.append(word)\n",
    "                        current_len += len(word) + 1\n",
    "                        if current_len > 400:\n",
    "                            chunks.append(\" \".join(current))\n",
    "                            current = []\n",
    "                            current_len = 0\n",
    "                    if current:\n",
    "                        chunks.append(\" \".join(current))\n",
    "\n",
    "                    lang = \"greek\" if lang_prefix == \"greek\" else \"latin\"\n",
    "                    for i, chunk in enumerate(chunks[:500]):  # Limit per text\n",
    "                        if len(chunk) > 50:\n",
    "                            results.append({\n",
    "                                \"id\": f\"perseus_{text_id}_{i}\",\n",
    "                                \"text\": chunk,\n",
    "                                \"language\": lang,\n",
    "                                \"source\": name,\n",
    "                                \"time_period\": period,\n",
    "                            })\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "        return results\n",
    "\n",
    "    # Fetch Greek texts\n",
    "    for text_id, name, period in greek_texts:\n",
    "        result = fetch_perseus_text(text_id, name, period, \"greek\")\n",
    "        passages.extend(result)\n",
    "        if result:\n",
    "            print(f\"    {name}: {len(result)} passages\")\n",
    "\n",
    "    # Fetch Latin texts\n",
    "    for text_id, name, period in latin_texts:\n",
    "        result = fetch_perseus_text(text_id, name, period, \"latin\")\n",
    "        passages.extend(result)\n",
    "        if result:\n",
    "            print(f\"    {name}: {len(result)} passages\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  Perseus: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "# ============================================================================\n",
    "# WESTERN PHILOSOPHY - Project Gutenberg (VERIFIED)\n",
    "# ============================================================================\n",
    "\n",
    "def load_gutenberg_philosophy() -> List[Dict]:\n",
    "    \"\"\"Load Western philosophy classics from Project Gutenberg.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"gutenberg.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Gutenberg: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Fetching from Project Gutenberg...\")\n",
    "\n",
    "    # Ethics-focused texts (Gutenberg IDs)\n",
    "    texts = [\n",
    "        (5684, \"Kant Critique of Practical Reason\", \"MODERN_ETHICS\"),\n",
    "        (5683, \"Kant Groundwork of the Metaphysics of Morals\", \"MODERN_ETHICS\"),\n",
    "        (5682, \"Kant Metaphysics of Morals\", \"MODERN_ETHICS\"),\n",
    "        (9662, \"Mill Utilitarianism\", \"MODERN_ETHICS\"),\n",
    "        (34901, \"Mill On Liberty\", \"MODERN_ETHICS\"),\n",
    "        (3207, \"Spinoza Ethics\", \"MODERN_ETHICS\"),\n",
    "        (10662, \"Aristotle Nicomachean Ethics (English)\", \"CLASSICAL_GREEK\"),\n",
    "        (1497, \"Plato Republic (English)\", \"CLASSICAL_GREEK\"),\n",
    "        (1656, \"Plato Apology (English)\", \"CLASSICAL_GREEK\"),\n",
    "        (55201, \"Epictetus Discourses (English)\", \"HELLENISTIC\"),\n",
    "        (2680, \"Meditations Marcus Aurelius (English)\", \"HELLENISTIC\"),\n",
    "    ]\n",
    "\n",
    "    for guten_id, name, period in texts:\n",
    "        try:\n",
    "            # Gutenberg URL pattern\n",
    "            url = f\"https://www.gutenberg.org/cache/epub/{guten_id}/pg{guten_id}.txt\"\n",
    "            resp = requests.get(url, timeout=60)\n",
    "            if resp.status_code == 200:\n",
    "                content = resp.text\n",
    "                # Skip Gutenberg header/footer\n",
    "                start_marker = \"*** START OF\"\n",
    "                end_marker = \"*** END OF\"\n",
    "                start_idx = content.find(start_marker)\n",
    "                end_idx = content.find(end_marker)\n",
    "                if start_idx > 0:\n",
    "                    content = content[start_idx:]\n",
    "                    newline_idx = content.find(\"\\n\")\n",
    "                    if newline_idx > 0:\n",
    "                        content = content[newline_idx + 1:]\n",
    "                if end_idx > 0:\n",
    "                    content = content[:end_idx - start_idx - 100] if start_idx > 0 else content[:end_idx]\n",
    "\n",
    "                # Split into paragraphs\n",
    "                paras = content.split(\"\\n\\n\")\n",
    "                count = 0\n",
    "                for para in paras:\n",
    "                    para = para.strip().replace(\"\\n\", \" \")\n",
    "                    para = \" \".join(para.split())  # Normalize whitespace\n",
    "                    if len(para) > 100 and len(para) < 2000:\n",
    "                        passages.append({\n",
    "                            \"id\": f\"gutenberg_{guten_id}_{count}\",\n",
    "                            \"text\": para,\n",
    "                            \"language\": \"english\",\n",
    "                            \"source\": name,\n",
    "                            \"time_period\": period,\n",
    "                        })\n",
    "                        count += 1\n",
    "                        if count >= 500:  # Limit per text\n",
    "                            break\n",
    "                if count > 0:\n",
    "                    print(f\"    {name}: {count} passages\")\n",
    "            time.sleep(1)  # Rate limit\n",
    "        except Exception as e:\n",
    "            print(f\"    {name}: failed ({e})\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  Gutenberg: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "# ============================================================================\n",
    "# NATIVE AMERICAN & WORLD FOLKLORE - HuggingFace (VERIFIED)\n",
    "# Source: merve/folk-mythology-tales (246K stories from Ashliman Folktexts)\n",
    "# ============================================================================\n",
    "\n",
    "def load_folk_mythology() -> List[Dict]:\n",
    "    \"\"\"Load folk tales and mythology including Native American from HuggingFace.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"folk_mythology.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Folk/Mythology: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Loading folk-mythology-tales from HuggingFace...\")\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        ds = load_dataset(\"merve/folk-mythology-tales\", split=\"train\")\n",
    "\n",
    "        for i, item in enumerate(ds):\n",
    "            text = item.get(\"text\", \"\")\n",
    "            if text and len(text) > 50:\n",
    "                passages.append({\n",
    "                    \"id\": f\"folk_{i}\",\n",
    "                    \"text\": text.strip()[:2000],  # Limit length\n",
    "                    \"language\": \"english\",\n",
    "                    \"source\": \"Ashliman Folktexts\",\n",
    "                    \"time_period\": \"FOLKLORE\",\n",
    "                })\n",
    "                if len(passages) >= 50000:  # Limit total\n",
    "                    break\n",
    "        print(f\"    Loaded {len(passages):,} folk tales\")\n",
    "    except ImportError:\n",
    "        print(\"    datasets library not available, skipping\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  Folk/Mythology: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "# ============================================================================\n",
    "# ROMANCE LANGUAGE PHILOSOPHY - Gutenberg (VERIFIED)\n",
    "# Spanish: Don Quijote, El Criticn (Gracin)\n",
    "# French: Montaigne Essais, Voltaire Candide, Rousseau Social Contract\n",
    "# Italian: Machiavelli, Dante\n",
    "# ============================================================================\n",
    "\n",
    "def load_romance_philosophy() -> List[Dict]:\n",
    "    \"\"\"Load Romance language philosophy from Project Gutenberg.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"romance_philosophy.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Romance Philosophy: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Fetching Romance language philosophy from Gutenberg...\")\n",
    "\n",
    "    # Romance language ethics/philosophy texts\n",
    "    texts = [\n",
    "        # Spanish\n",
    "        (2000, \"Don Quijote\", \"spanish\", \"SPANISH_GOLDEN_AGE\"),\n",
    "        (62691, \"El Criticn (Gracin)\", \"spanish\", \"SPANISH_GOLDEN_AGE\"),\n",
    "        # French\n",
    "        (48529, \"Essais de Montaigne\", \"french\", \"FRENCH_RENAISSANCE\"),\n",
    "        (19942, \"Candide (Voltaire)\", \"french\", \"FRENCH_ENLIGHTENMENT\"),\n",
    "        (46333, \"Social Contract (Rousseau)\", \"french\", \"FRENCH_ENLIGHTENMENT\"),\n",
    "        # Italian\n",
    "        (1232, \"The Prince (Machiavelli)\", \"italian\", \"ITALIAN_RENAISSANCE\"),\n",
    "        (8800, \"Divina Commedia\", \"italian\", \"MEDIEVAL_ITALIAN\"),\n",
    "        # Mixed philosophical essays\n",
    "        (5637, \"Literary Philosophical Essays\", \"french\", \"EUROPEAN_PHILOSOPHY\"),\n",
    "    ]\n",
    "\n",
    "    def fetch_gutenberg(args):\n",
    "        guten_id, name, lang, period = args\n",
    "        results = []\n",
    "        try:\n",
    "            url = f\"https://www.gutenberg.org/cache/epub/{guten_id}/pg{guten_id}.txt\"\n",
    "            resp = requests.get(url, timeout=60)\n",
    "            if resp.status_code == 200:\n",
    "                content = resp.text\n",
    "                # Skip Gutenberg header/footer\n",
    "                start_marker = \"*** START OF\"\n",
    "                end_marker = \"*** END OF\"\n",
    "                start_idx = content.find(start_marker)\n",
    "                end_idx = content.find(end_marker)\n",
    "                if start_idx > 0:\n",
    "                    content = content[start_idx:]\n",
    "                    nl = content.find(\"\\n\")\n",
    "                    if nl > 0:\n",
    "                        content = content[nl + 1:]\n",
    "                if end_idx > 0 and start_idx > 0:\n",
    "                    content = content[:end_idx - start_idx - 100]\n",
    "                elif end_idx > 0:\n",
    "                    content = content[:end_idx]\n",
    "\n",
    "                # Split into paragraphs\n",
    "                paras = content.split(\"\\n\\n\")\n",
    "                count = 0\n",
    "                for para in paras:\n",
    "                    para = para.strip().replace(\"\\n\", \" \")\n",
    "                    para = \" \".join(para.split())\n",
    "                    if len(para) > 100 and len(para) < 2000:\n",
    "                        results.append({\n",
    "                            \"id\": f\"romance_{guten_id}_{count}\",\n",
    "                            \"text\": para,\n",
    "                            \"language\": lang,\n",
    "                            \"source\": name,\n",
    "                            \"time_period\": period,\n",
    "                        })\n",
    "                        count += 1\n",
    "                        if count >= 500:\n",
    "                            break\n",
    "                return name, results\n",
    "        except Exception as e:\n",
    "            return name, []\n",
    "        return name, results\n",
    "\n",
    "    # Parallel download\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(fetch_gutenberg, t) for t in texts]\n",
    "        for future in as_completed(futures):\n",
    "            name, results = future.result()\n",
    "            if results:\n",
    "                passages.extend(results)\n",
    "                print(f\"    {name}: {len(results)} passages\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  Romance Philosophy: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "# ============================================================================\n",
    "# ENGLISH ETHICS - Dear Abby (68K letters)\n",
    "# Source: https://github.com/Mac-STAT/data (VERIFIED)\n",
    "# ============================================================================\n",
    "\n",
    "DEAR_ABBY_URL = \"https://raw.githubusercontent.com/Mac-STAT/data/main/dear_abby.csv\"\n",
    "\n",
    "def load_dear_abby() -> List[Dict]:\n",
    "    \"\"\"Load Dear Abby advice columns (68K letters) from Mac-STAT GitHub.\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"dear_abby.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  Dear Abby: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    # Check local file first\n",
    "    local_paths = [\n",
    "        Path(\"data/raw/dear_abby.csv\"),\n",
    "        DATA_DIR / \"dear_abby.csv\",\n",
    "    ]\n",
    "\n",
    "    csv_file = None\n",
    "    for p in local_paths:\n",
    "        if p.exists():\n",
    "            csv_file = p\n",
    "            print(f\"  Found local: {csv_file}\")\n",
    "            break\n",
    "\n",
    "    # Download if not local\n",
    "    if not csv_file:\n",
    "        print(f\"  Downloading Dear Abby from GitHub (17.9MB)...\")\n",
    "        csv_file = DATA_DIR / \"dear_abby.csv\"\n",
    "        try:\n",
    "            resp = requests.get(DEAR_ABBY_URL, timeout=120)\n",
    "            if resp.status_code == 200:\n",
    "                csv_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                with open(csv_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(resp.text)\n",
    "                print(f\"    Downloaded to {csv_file}\")\n",
    "            else:\n",
    "                print(f\"    Failed: HTTP {resp.status_code}\")\n",
    "                return passages\n",
    "        except Exception as e:\n",
    "            print(f\"    Download failed: {e}\")\n",
    "            return passages\n",
    "\n",
    "    # Parse CSV\n",
    "    print(f\"  Parsing Dear Abby CSV...\")\n",
    "    try:\n",
    "        with open(csv_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                # Column is \"question_only\" based on file structure\n",
    "                text = row.get(\"question_only\", \"\")\n",
    "                if text and len(text) > 50:\n",
    "                    year = row.get(\"year\", \"\")\n",
    "                    passages.append({\n",
    "                        \"id\": f\"abby_{row.get('index', len(passages))}\",\n",
    "                        \"text\": text.strip(),\n",
    "                        \"language\": \"english\",\n",
    "                        \"source\": f\"Dear Abby {year}\",\n",
    "                        \"time_period\": \"DEAR_ABBY\",\n",
    "                    })\n",
    "        print(f\"    Loaded {len(passages):,} letters\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed to parse CSV: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  Dear Abby: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ENGLISH ETHICS - hendrycks/ethics (134K examples) - SUPPLEMENTAL\n",
    "# ============================================================================\n",
    "\n",
    "def load_hendrycks_ethics() -> List[Dict]:\n",
    "    \"\"\"Load ethics scenarios from hendrycks/ethics dataset (supplemental).\"\"\"\n",
    "    passages = []\n",
    "    cache_file = CACHE_DIR / \"hendrycks_ethics.json\"\n",
    "\n",
    "    if cache_file.exists():\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            passages = json.load(f)\n",
    "            print(f\"  hendrycks/ethics: {len(passages):,} passages (cached)\")\n",
    "            return passages\n",
    "\n",
    "    print(\"  Loading hendrycks/ethics from HuggingFace...\")\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "\n",
    "        # Load all subsets\n",
    "        subsets = [\"commonsense\", \"deontology\", \"justice\", \"utilitarianism\", \"virtue\"]\n",
    "        for subset in subsets:\n",
    "            try:\n",
    "                ds = load_dataset(\"hendrycks/ethics\", subset, split=\"train\", trust_remote_code=True)\n",
    "                count = 0\n",
    "                for item in ds:\n",
    "                    text = None\n",
    "                    if \"scenario\" in item:\n",
    "                        text = item[\"scenario\"]\n",
    "                    elif \"input\" in item:\n",
    "                        text = item[\"input\"]\n",
    "                    elif \"sentence\" in item:\n",
    "                        text = item[\"sentence\"]\n",
    "                    elif \"text\" in item:\n",
    "                        text = item[\"text\"]\n",
    "\n",
    "                    if text and len(text) > 30:\n",
    "                        passages.append({\n",
    "                            \"id\": f\"ethics_{subset}_{count}\",\n",
    "                            \"text\": text.strip(),\n",
    "                            \"language\": \"english\",\n",
    "                            \"source\": f\"hendrycks/ethics/{subset}\",\n",
    "                            \"time_period\": \"MODERN_ETHICS\",\n",
    "                        })\n",
    "                        count += 1\n",
    "                print(f\"    {subset}: {count} passages\")\n",
    "            except Exception as e:\n",
    "                print(f\"    {subset}: failed ({e})\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"    datasets library not available, skipping\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Failed: {e}\")\n",
    "\n",
    "    if passages:\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(passages, f, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  hendrycks/ethics: {len(passages):,} passages\")\n",
    "    return passages\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN LOADER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Fetching from verified external sources...\")\n",
    "print(f\"Cache only mode: {_cache_only}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "by_language = defaultdict(list)\n",
    "\n",
    "# Load all sources\n",
    "print(\"\\n[SANSKRIT]\")\n",
    "by_language[\"sanskrit\"].extend(load_itihasa_github())\n",
    "\n",
    "print(\"\\n[PALI]\")\n",
    "by_language[\"pali\"].extend(load_pali_suttacentral())\n",
    "\n",
    "print(\"\\n[ARABIC]\")\n",
    "by_language[\"arabic\"].extend(load_quran_tanzil())\n",
    "\n",
    "print(\"\\n[HEBREW/ARAMAIC]\")\n",
    "sefaria = load_sefaria_github()\n",
    "for p in sefaria:\n",
    "    by_language[p[\"language\"]].append(p)\n",
    "\n",
    "print(\"\\n[CHINESE]\")\n",
    "by_language[\"classical_chinese\"].extend(load_chinese_ctext())\n",
    "\n",
    "print(\"\\n[GREEK/LATIN]\")\n",
    "perseus = load_perseus_github()\n",
    "for p in perseus:\n",
    "    by_language[p[\"language\"]].append(p)\n",
    "\n",
    "print(\"\\n[WESTERN PHILOSOPHY]\")\n",
    "by_language[\"english\"].extend(load_gutenberg_philosophy())\n",
    "\n",
    "print(\"\\n[ROMANCE LANGUAGES]\")\n",
    "romance = load_romance_philosophy()\n",
    "for p in romance:\n",
    "    by_language[p[\"language\"]].append(p)\n",
    "\n",
    "print(\"\\n[FOLKLORE/NATIVE AMERICAN]\")\n",
    "by_language[\"english\"].extend(load_folk_mythology())\n",
    "\n",
    "print(\"\\n[ENGLISH ETHICS]\")\n",
    "by_language[\"english\"].extend(load_dear_abby())\n",
    "by_language[\"english\"].extend(load_hendrycks_ethics())  # Supplemental\n",
    "\n",
    "# ============================================================================\n",
    "# APPLY MEMORY LIMITS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Applying memory limits...\")\n",
    "for lang in list(by_language.keys()):\n",
    "    max_count = MAX_PASSAGES_PER_LANG.get(lang, MAX_PASSAGES_PER_LANG[\"default\"])\n",
    "    if len(by_language[lang]) > max_count:\n",
    "        original = len(by_language[lang])\n",
    "        by_language[lang] = by_language[lang][:max_count]\n",
    "        print(f\"  {lang}: {original:,} -> {max_count:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CORPUS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total = 0\n",
    "for lang, passages in sorted(by_language.items(), key=lambda x: -len(x[1])):\n",
    "    count = len(passages)\n",
    "    total += count\n",
    "    status = \"OK\" if count >= MIN_PASSAGES else f\"NEED {MIN_PASSAGES - count} MORE\"\n",
    "    print(f\"  {lang:20s}: {count:6,} passages  [{status}]\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"  {'TOTAL':20s}: {total:6,} passages\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE TO JSONL FOR LATER CELLS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nConverting to training format...\")\n",
    "\n",
    "all_passages = []\n",
    "for lang, passages in by_language.items():\n",
    "    for p in passages:\n",
    "        all_passages.append({\n",
    "            \"id\": p[\"id\"],\n",
    "            \"text\": p[\"text\"],\n",
    "            \"language\": p[\"language\"],\n",
    "            \"source\": p[\"source\"],\n",
    "            \"time_period\": p[\"time_period\"],\n",
    "        })\n",
    "\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "with open(\"data/processed/passages.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in all_passages:\n",
    "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved to data/processed/passages.jsonl\")\n",
    "\n",
    "# Cache to Drive if available\n",
    "if _save_dir and os.path.exists(os.path.dirname(_save_dir)):\n",
    "    os.makedirs(_save_dir, exist_ok=True)\n",
    "    import shutil\n",
    "    shutil.copy(\"data/processed/passages.jsonl\", f\"{_save_dir}/passages.jsonl\")\n",
    "    print(f\"Cached to {_save_dir}/passages.jsonl\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CORPUS LOADING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3. Patterns + Normalization { display-mode: \"form\" }\n",
    "# @markdown BIP v10.9: Complete native patterns for moral concepts in 7 languages\n",
    "# @markdown - Added: Sanskrit, Pali patterns\n",
    "# @markdown - Added: NLP improvements (negation detection, modal classification)\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from enum import Enum, auto\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT NORMALIZATION & PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ===== TEXT NORMALIZATION =====\n",
    "def normalize_hebrew(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"[\\u0591-\\u05C7]\", \"\", text)  # Remove nikud\n",
    "    for final, regular in [\n",
    "        (\"\\u05da\", \"\\u05db\"),\n",
    "        (\"\\u05dd\", \"\\u05de\"),\n",
    "        (\"\\u05df\", \"\\u05e0\"),\n",
    "        (\"\\u05e3\", \"\\u05e4\"),\n",
    "        (\"\\u05e5\", \"\\u05e6\"),\n",
    "    ]:\n",
    "        text = text.replace(final, regular)\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"[\\u064B-\\u065F]\", \"\", text)  # Remove tashkeel\n",
    "    text = text.replace(\"\\u0640\", \"\")  # Remove tatweel\n",
    "    for v in [\"\\u0623\", \"\\u0625\", \"\\u0622\", \"\\u0671\"]:\n",
    "        text = text.replace(v, \"\\u0627\")\n",
    "    text = text.replace(\"\\u0629\", \"\\u0647\").replace(\"\\u0649\", \"\\u064a\")\n",
    "    return text\n",
    "\n",
    "\n",
    "# NEW in v10.9: Sanskrit normalization\n",
    "def normalize_sanskrit(text):\n",
    "    \"\"\"Normalize Sanskrit/Devanagari text.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    # Remove vedic accents and other diacriticals\n",
    "    text = re.sub(r\"[\\u0951-\\u0954]\", \"\", text)  # Vedic tone marks\n",
    "    text = re.sub(r\"[\\u0900-\\u0902]\", \"\", text)  # Chandrabindu variants\n",
    "    return text\n",
    "\n",
    "\n",
    "# NEW in v10.9: Pali normalization\n",
    "def normalize_pali(text):\n",
    "    \"\"\"Normalize Pali text (romanized or script).\"\"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    # Normalize romanized Pali diacritics\n",
    "    text = text.lower()\n",
    "    # Handle common Pali romanization variations\n",
    "    text = text.replace(\"\", \"m\").replace(\"\", \"n\").replace(\"\", \"n\")\n",
    "    text = text.replace(\"\", \"t\").replace(\"\", \"d\").replace(\"\", \"n\")\n",
    "    text = text.replace(\"\", \"l\").replace(\"\", \"a\").replace(\"\", \"i\").replace(\"\", \"u\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_text(text, language):\n",
    "    if language in [\"hebrew\", \"aramaic\"]:\n",
    "        return normalize_hebrew(text)\n",
    "    elif language == \"arabic\":\n",
    "        return normalize_arabic(text)\n",
    "    elif language == \"classical_chinese\":\n",
    "        return unicodedata.normalize(\"NFKC\", text)\n",
    "    elif language == \"sanskrit\":\n",
    "        return normalize_sanskrit(text)\n",
    "    elif language == \"pali\":\n",
    "        return normalize_pali(text)\n",
    "    else:\n",
    "        return unicodedata.normalize(\"NFKC\", text.lower())\n",
    "\n",
    "\n",
    "# ===== BOND AND HOHFELD TYPES =====\n",
    "class BondType(Enum):\n",
    "    HARM_PREVENTION = auto()\n",
    "    RECIPROCITY = auto()\n",
    "    AUTONOMY = auto()\n",
    "    PROPERTY = auto()\n",
    "    FAMILY = auto()\n",
    "    AUTHORITY = auto()\n",
    "    CARE = auto()\n",
    "    FAIRNESS = auto()\n",
    "    CONTRACT = auto()\n",
    "    NONE = auto()\n",
    "\n",
    "\n",
    "class HohfeldState(Enum):\n",
    "    OBLIGATION = auto()\n",
    "    RIGHT = auto()\n",
    "    LIBERTY = auto()\n",
    "    NO_RIGHT = auto()\n",
    "\n",
    "\n",
    "# ===== COMPLETE BOND PATTERNS =====\n",
    "ALL_BOND_PATTERNS = {\n",
    "    \"hebrew\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u05d4\\u05e8\\u05d2\",\n",
    "            r\"\\u05e8\\u05e6\\u05d7\",\n",
    "            r\"\\u05e0\\u05d6\\u05e7\",\n",
    "            r\"\\u05d4\\u05db\\u05d4\",\n",
    "            r\"\\u05d4\\u05e6\\u05d9\\u05dc\",\n",
    "            r\"\\u05e9\\u05de\\u05e8\",\n",
    "            r\"\\u05e4\\u05e7\\u05d5\\u05d7.\\u05e0\\u05e4\\u05e9\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\u05d2\\u05de\\u05d5\\u05dc\",\n",
    "            r\"\\u05d4\\u05e9\\u05d9\\u05d1\",\n",
    "            r\"\\u05e4\\u05e8\\u05e2\",\n",
    "            r\"\\u05e0\\u05ea\\u05df.*\\u05e7\\u05d1\\u05dc\",\n",
    "            r\"\\u05de\\u05d3\\u05d4.\\u05db\\u05e0\\u05d2\\u05d3\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\u05d1\\u05d7\\u05e8\",\n",
    "            r\"\\u05e8\\u05e6\\u05d5\\u05df\",\n",
    "            r\"\\u05d7\\u05e4\\u05e9\",\n",
    "            r\"\\u05e2\\u05e6\\u05de\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u05e7\\u05e0\\u05d4\",\n",
    "            r\"\\u05de\\u05db\\u05e8\",\n",
    "            r\"\\u05d2\\u05d6\\u05dc\",\n",
    "            r\"\\u05d2\\u05e0\\u05d1\",\n",
    "            r\"\\u05de\\u05de\\u05d5\\u05df\",\n",
    "            r\"\\u05e0\\u05db\\u05e1\",\n",
    "            r\"\\u05d9\\u05e8\\u05e9\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u05d0\\u05d1\",\n",
    "            r\"\\u05d0\\u05de\",\n",
    "            r\"\\u05d1\\u05e0\",\n",
    "            r\"\\u05db\\u05d1\\u05d3.*\\u05d0\\u05d1\",\n",
    "            r\"\\u05db\\u05d1\\u05d3.*\\u05d0\\u05de\",\n",
    "            r\"\\u05de\\u05e9\\u05e4\\u05d7\\u05d4\",\n",
    "            r\"\\u05d0\\u05d7\",\n",
    "            r\"\\u05d0\\u05d7\\u05d5\\u05ea\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u05de\\u05dc\\u05db\",\n",
    "            r\"\\u05e9\\u05d5\\u05e4\\u05d8\",\n",
    "            r\"\\u05e6\\u05d5\\u05d4\",\n",
    "            r\"\\u05ea\\u05d5\\u05e8\\u05d4\",\n",
    "            r\"\\u05de\\u05e6\\u05d5\\u05d4\",\n",
    "            r\"\\u05d3\\u05d9\\u05df\",\n",
    "            r\"\\u05d7\\u05e7\",\n",
    "        ],\n",
    "        BondType.CARE: [\n",
    "            r\"\\u05d7\\u05e1\\u05d3\",\n",
    "            r\"\\u05e8\\u05d7\\u05de\",\n",
    "            r\"\\u05e2\\u05d6\\u05e8\",\n",
    "            r\"\\u05ea\\u05de\\u05db\",\n",
    "            r\"\\u05e6\\u05d3\\u05e7\\u05d4\",\n",
    "        ],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u05e6\\u05d3\\u05e7\",\n",
    "            r\"\\u05de\\u05e9\\u05e4\\u05d8\",\n",
    "            r\"\\u05d9\\u05e9\\u05e8\",\n",
    "            r\"\\u05e9\\u05d5\\u05d4\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u05d1\\u05e8\\u05d9\\u05ea\",\n",
    "            r\"\\u05e0\\u05d3\\u05e8\",\n",
    "            r\"\\u05e9\\u05d1\\u05d5\\u05e2\",\n",
    "            r\"\\u05d4\\u05ea\\u05d7\\u05d9\\u05d1\",\n",
    "            r\"\\u05e2\\u05e8\\u05d1\",\n",
    "        ],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u05e7\\u05d8\\u05dc\",\n",
    "            r\"\\u05e0\\u05d6\\u05e7\",\n",
    "            r\"\\u05d7\\u05d1\\u05dc\",\n",
    "            r\"\\u05e9\\u05d6\\u05d9\\u05d1\",\n",
    "            r\"\\u05e4\\u05e6\\u05d9\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [r\"\\u05e4\\u05e8\\u05e2\", r\"\\u05e9\\u05dc\\u05de\", r\"\\u05d0\\u05d2\\u05e8\"],\n",
    "        BondType.AUTONOMY: [r\"\\u05e6\\u05d1\\u05d9\", r\"\\u05e8\\u05e2\\u05d5\"],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u05d6\\u05d1\\u05e0\",\n",
    "            r\"\\u05e7\\u05e0\\u05d4\",\n",
    "            r\"\\u05d2\\u05d6\\u05dc\",\n",
    "            r\"\\u05de\\u05de\\u05d5\\u05e0\\u05d0\",\n",
    "            r\"\\u05e0\\u05db\\u05e1\\u05d9\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u05d0\\u05d1\\u05d0\",\n",
    "            r\"\\u05d0\\u05de\\u05d0\",\n",
    "            r\"\\u05d1\\u05e8\\u05d0\",\n",
    "            r\"\\u05d1\\u05e8\\u05ea\\u05d0\",\n",
    "            r\"\\u05d9\\u05e7\\u05e8\",\n",
    "            r\"\\u05d0\\u05d7\\u05d0\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u05de\\u05dc\\u05db\\u05d0\",\n",
    "            r\"\\u05d3\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05d3\\u05d9\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05e4\\u05e7\\u05d5\\u05d3\\u05d0\",\n",
    "            r\"\\u05d0\\u05d5\\u05e8\\u05d9\\u05ea\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\u05d7\\u05e1\\u05d3\", r\"\\u05e8\\u05d7\\u05de\", r\"\\u05e1\\u05e2\\u05d3\"],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u05d3\\u05d9\\u05e0\\u05d0\",\n",
    "            r\"\\u05e7\\u05e9\\u05d5\\u05d8\",\n",
    "            r\"\\u05ea\\u05e8\\u05d9\\u05e6\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u05e7\\u05d9\\u05de\\u05d0\",\n",
    "            r\"\\u05e9\\u05d1\\u05d5\\u05e2\\u05d4\",\n",
    "            r\"\\u05e0\\u05d3\\u05e8\\u05d0\",\n",
    "            r\"\\u05e2\\u05e8\\u05d1\\u05d0\",\n",
    "        ],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u6bba\",\n",
    "            r\"\\u5bb3\",\n",
    "            r\"\\u50b7\",\n",
    "            r\"\\u6551\",\n",
    "            r\"\\u8b77\",\n",
    "            r\"\\u885b\",\n",
    "            r\"\\u66b4\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [r\"\\u5831\", r\"\\u9084\", r\"\\u511f\", r\"\\u8ced\", r\"\\u7b54\"],\n",
    "        BondType.AUTONOMY: [r\"\\u81ea\", r\"\\u7531\", r\"\\u4efb\", r\"\\u610f\", r\"\\u5fd7\"],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u8ca1\",\n",
    "            r\"\\u7269\",\n",
    "            r\"\\u7522\",\n",
    "            r\"\\u76dc\",\n",
    "            r\"\\u7aca\",\n",
    "            r\"\\u8ce3\",\n",
    "            r\"\\u8cb7\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u5b5d\",\n",
    "            r\"\\u7236\",\n",
    "            r\"\\u6bcd\",\n",
    "            r\"\\u89aa\",\n",
    "            r\"\\u5b50\",\n",
    "            r\"\\u5f1f\",\n",
    "            r\"\\u5144\",\n",
    "            r\"\\u5bb6\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u541b\",\n",
    "            r\"\\u81e3\",\n",
    "            r\"\\u738b\",\n",
    "            r\"\\u547d\",\n",
    "            r\"\\u4ee4\",\n",
    "            r\"\\u6cd5\",\n",
    "            r\"\\u6cbb\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\u4ec1\", r\"\\u611b\", r\"\\u6148\", r\"\\u60e0\", r\"\\u6069\", r\"\\u6190\"],\n",
    "        BondType.FAIRNESS: [r\"\\u7fa9\", r\"\\u6b63\", r\"\\u516c\", r\"\\u5e73\", r\"\\u5747\"],\n",
    "        BondType.CONTRACT: [r\"\\u7d04\", r\"\\u76df\", r\"\\u8a93\", r\"\\u8afe\", r\"\\u4fe1\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\u0642\\u062a\\u0644\",\n",
    "            r\"\\u0636\\u0631\\u0631\",\n",
    "            r\"\\u0627\\u0630[\\u064a\\u0649]\",\n",
    "            r\"\\u0638\\u0644\\u0645\",\n",
    "            r\"\\u0627\\u0646\\u0642\\u0630\",\n",
    "            r\"\\u062d\\u0641\\u0638\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0646\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\u062c\\u0632\\u0627\",\n",
    "            r\"\\u0631\\u062f\",\n",
    "            r\"\\u0642\\u0635\\u0627\\u0635\",\n",
    "            r\"\\u0645\\u062b\\u0644\",\n",
    "            r\"\\u0639\\u0648\\u0636\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\u062d\\u0631\",\n",
    "            r\"\\u0627\\u0631\\u0627\\u062f\\u0629\",\n",
    "            r\"\\u0627\\u062e\\u062a\\u064a\\u0627\\u0631\",\n",
    "            r\"\\u0645\\u0634\\u064a\\u0626\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\u0645\\u0627\\u0644\",\n",
    "            r\"\\u0645\\u0644\\u0643\",\n",
    "            r\"\\u0633\\u0631\\u0642\",\n",
    "            r\"\\u0628\\u064a\\u0639\",\n",
    "            r\"\\u0634\\u0631\\u0627\",\n",
    "            r\"\\u0645\\u064a\\u0631\\u0627\\u062b\",\n",
    "            r\"\\u063a\\u0635\\u0628\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\u0648\\u0627\\u0644\\u062f\",\n",
    "            r\"\\u0627\\u0628\\u0648\",\n",
    "            r\"\\u0627\\u0645\",\n",
    "            r\"\\u0627\\u0628\\u0646\",\n",
    "            r\"\\u0628\\u0646\\u062a\",\n",
    "            r\"\\u0627\\u0647\\u0644\",\n",
    "            r\"\\u0642\\u0631\\u0628[\\u064a\\u0649]\",\n",
    "            r\"\\u0631\\u062d\\u0645\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\u0637\\u0627\\u0639\",\n",
    "            r\"\\u0627\\u0645\\u0631\",\n",
    "            r\"\\u062d\\u0643\\u0645\",\n",
    "            r\"\\u0633\\u0644\\u0637\\u0627\\u0646\",\n",
    "            r\"\\u062e\\u0644\\u064a\\u0641\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0645\",\n",
    "            r\"\\u0634\\u0631\\u064a\\u0639\",\n",
    "        ],\n",
    "        BondType.CARE: [\n",
    "            r\"\\u0631\\u062d\\u0645\",\n",
    "            r\"\\u0627\\u062d\\u0633\\u0627\\u0646\",\n",
    "            r\"\\u0639\\u0637\\u0641\",\n",
    "            r\"\\u0635\\u062f\\u0642\",\n",
    "            r\"\\u0632\\u0643\\u0627\",\n",
    "        ],\n",
    "        BondType.FAIRNESS: [\n",
    "            r\"\\u0639\\u062f\\u0644\",\n",
    "            r\"\\u0642\\u0633\\u0637\",\n",
    "            r\"\\u062d\\u0642\",\n",
    "            r\"\\u0627\\u0646\\u0635\\u0627\\u0641\",\n",
    "            r\"\\u0633\\u0648[\\u064a\\u0649]\",\n",
    "        ],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\u0639\\u0647\\u062f\",\n",
    "            r\"\\u0639\\u0642\\u062f\",\n",
    "            r\"\\u0646\\u0630\\u0631\",\n",
    "            r\"\\u064a\\u0645\\u064a\\u0646\",\n",
    "            r\"\\u0648\\u0641\\u0627\",\n",
    "            r\"\\u0627\\u0645\\u0627\\u0646\",\n",
    "        ],\n",
    "    },\n",
    "    \"english\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\\bkill\",\n",
    "            r\"\\bmurder\",\n",
    "            r\"\\bharm\",\n",
    "            r\"\\bhurt\",\n",
    "            r\"\\bsave\",\n",
    "            r\"\\bprotect\",\n",
    "            r\"\\bviolence\",\n",
    "        ],\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\\breturn\",\n",
    "            r\"\\brepay\",\n",
    "            r\"\\bexchange\",\n",
    "            r\"\\bgive.*back\",\n",
    "            r\"\\breciproc\",\n",
    "        ],\n",
    "        BondType.AUTONOMY: [\n",
    "            r\"\\bfree\",\n",
    "            r\"\\bchoice\",\n",
    "            r\"\\bchoose\",\n",
    "            r\"\\bconsent\",\n",
    "            r\"\\bautonomy\",\n",
    "            r\"\\bright to\",\n",
    "        ],\n",
    "        BondType.PROPERTY: [\n",
    "            r\"\\bsteal\",\n",
    "            r\"\\btheft\",\n",
    "            r\"\\bown\",\n",
    "            r\"\\bproperty\",\n",
    "            r\"\\bbelong\",\n",
    "            r\"\\binherit\",\n",
    "        ],\n",
    "        BondType.FAMILY: [\n",
    "            r\"\\bfather\",\n",
    "            r\"\\bmother\",\n",
    "            r\"\\bparent\",\n",
    "            r\"\\bchild\",\n",
    "            r\"\\bfamily\",\n",
    "            r\"\\bhonor.*parent\",\n",
    "        ],\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\\bobey\",\n",
    "            r\"\\bcommand\",\n",
    "            r\"\\bauthority\",\n",
    "            r\"\\blaw\",\n",
    "            r\"\\brule\",\n",
    "            r\"\\bgovern\",\n",
    "        ],\n",
    "        BondType.CARE: [r\"\\bcare\", r\"\\bhelp\", r\"\\bkind\", r\"\\bcompassion\", r\"\\bcharity\", r\"\\bmercy\"],\n",
    "        BondType.FAIRNESS: [r\"\\bfair\", r\"\\bjust\", r\"\\bequal\", r\"\\bequity\", r\"\\bright\\b\"],\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\\bpromise\",\n",
    "            r\"\\bcontract\",\n",
    "            r\"\\bagreem\",\n",
    "            r\"\\bvow\",\n",
    "            r\"\\boath\",\n",
    "            r\"\\bcommit\",\n",
    "        ],\n",
    "    },\n",
    "    # NEW in v10.9: Sanskrit patterns (Devanagari)\n",
    "    \"sanskrit\": {\n",
    "        BondType.HARM_PREVENTION: [\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "        ],  # himsa, ahimsa, vadha, raksha, trana\n",
    "        BondType.RECIPROCITY: [\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "        ],  # pratidana, pratyupakara, dana, rna\n",
    "        BondType.AUTONOMY: [r\"\", r\"\", r\"\"],  # swatantra, moksha, sveccha\n",
    "        BondType.PROPERTY: [r\"\", r\"\", r\"\", r\"\"],  # dhana, sva, chora, daya\n",
    "        BondType.FAMILY: [r\"\", r\"\", r\"\", r\"\", r\"\"],  # pitri, matri, putra, kula, grha\n",
    "        BondType.AUTHORITY: [\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "        ],  # raja, dharma, vidhi, niyama, shastra\n",
    "        BondType.CARE: [\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "        ],  # karuna, daya, prema, maitri, seva\n",
    "        BondType.FAIRNESS: [r\"\", r\"\", r\"\", r\"\"],  # nyaya, samata, dharma, rta\n",
    "        BondType.CONTRACT: [\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "            r\"\",\n",
    "        ],  # pratijna, samvid, vachana, shapatha\n",
    "    },\n",
    "    # NEW in v10.9: Pali patterns (romanized)\n",
    "    \"pali\": {\n",
    "        BondType.HARM_PREVENTION: [r\"himsa\", r\"ahimsa\", r\"panatipata\", r\"rakkhati\"],\n",
    "        BondType.RECIPROCITY: [r\"dana\", r\"patidana\", r\"ina\"],\n",
    "        BondType.AUTONOMY: [r\"vimutti\", r\"nibbana\", r\"attadhipa\"],\n",
    "        BondType.PROPERTY: [r\"dhana\", r\"theyya\", r\"adinnadana\"],\n",
    "        BondType.FAMILY: [r\"mata\", r\"pita\", r\"putta\", r\"kula\"],\n",
    "        BondType.AUTHORITY: [r\"raja\", r\"dhamma\", r\"vinaya\", r\"sikkhapada\"],\n",
    "        BondType.CARE: [r\"karuna\", r\"metta\", r\"mudita\", r\"upekkha\"],\n",
    "        BondType.FAIRNESS: [r\"samma\", r\"dhamma\", r\"sacca\"],\n",
    "        BondType.CONTRACT: [r\"patijna\", r\"vacana\", r\"sacca\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# ===== COMPLETE HOHFELD PATTERNS =====\n",
    "ALL_HOHFELD_PATTERNS = {\n",
    "    \"hebrew\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u05d7\\u05d9\\u05d9\\u05d1\",\n",
    "            r\"\\u05e6\\u05e8\\u05d9\\u05db\",\n",
    "            r\"\\u05de\\u05d5\\u05db\\u05e8\\u05d7\",\n",
    "            r\"\\u05de\\u05e6\\u05d5\\u05d5\\u05d4\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "            r\"\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d6\\u05db\\u05d0\\u05d9\",\n",
    "            r\"\\u05de\\u05d2\\u05d9\\u05e2\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u05de\\u05d5\\u05ea\\u05e8\",\n",
    "            r\"\\u05e8\\u05e9\\u05d5\\u05ea\",\n",
    "            r\"\\u05e4\\u05d8\\u05d5\\u05e8\",\n",
    "            r\"\\u05d9\\u05db\\u05d5\\u05dc\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u05d0\\u05e1\\u05d5\\u05e8\",\n",
    "            r\"\\u05d0\\u05d9\\u05e0\\u05d5 \\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d0\\u05d9\\u05df.*\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "        ],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u05d7\\u05d9\\u05d9\\u05d1\",\n",
    "            r\"\\u05de\\u05d7\\u05d5\\u05d9\\u05d1\",\n",
    "            r\"\\u05d1\\u05e2\\u05d9\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u05d6\\u05db\\u05d5\\u05ea\",\n",
    "            r\"\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "            r\"\\u05d6\\u05db\\u05d9\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u05e9\\u05e8\\u05d9\",\n",
    "            r\"\\u05de\\u05d5\\u05ea\\u05e8\",\n",
    "            r\"\\u05e4\\u05d8\\u05d5\\u05e8\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u05d0\\u05e1\\u05d5\\u05e8\",\n",
    "            r\"\\u05dc\\u05d0.*\\u05e8\\u05e9\\u05d0\\u05d9\",\n",
    "        ],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        HohfeldState.OBLIGATION: [r\"\\u5fc5\", r\"\\u9808\", r\"\\u7576\", r\"\\u61c9\", r\"\\u5b9c\"],\n",
    "        HohfeldState.RIGHT: [r\"\\u53ef\", r\"\\u5f97\", r\"\\u6b0a\", r\"\\u5b9c\"],\n",
    "        HohfeldState.LIBERTY: [r\"\\u8a31\", r\"\\u4efb\", r\"\\u807d\", r\"\\u514d\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"\\u4e0d\\u53ef\", r\"\\u52ff\", r\"\\u7981\", r\"\\u83ab\", r\"\\u975e\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        HohfeldState.OBLIGATION: [\n",
    "            r\"\\u064a\\u062c\\u0628\",\n",
    "            r\"\\u0648\\u0627\\u062c\\u0628\",\n",
    "            r\"\\u0641\\u0631\\u0636\",\n",
    "            r\"\\u0644\\u0627\\u0632\\u0645\",\n",
    "            r\"\\u0648\\u062c\\u0648\\u0628\",\n",
    "        ],\n",
    "        HohfeldState.RIGHT: [\n",
    "            r\"\\u062d\\u0642\",\n",
    "            r\"\\u064a\\u062d\\u0642\",\n",
    "            r\"\\u062c\\u0627\\u0626\\u0632\",\n",
    "            r\"\\u064a\\u062c\\u0648\\u0632\",\n",
    "        ],\n",
    "        HohfeldState.LIBERTY: [\n",
    "            r\"\\u0645\\u0628\\u0627\\u062d\",\n",
    "            r\"\\u062d\\u0644\\u0627\\u0644\",\n",
    "            r\"\\u062c\\u0627\\u0626\\u0632\",\n",
    "            r\"\\u0627\\u0628\\u0627\\u062d\",\n",
    "        ],\n",
    "        HohfeldState.NO_RIGHT: [\n",
    "            r\"\\u062d\\u0631\\u0627\\u0645\",\n",
    "            r\"\\u0645\\u062d\\u0631\\u0645\",\n",
    "            r\"\\u0645\\u0645\\u0646\\u0648\\u0639\",\n",
    "            r\"\\u0644\\u0627 \\u064a\\u062c\\u0648\\u0632\",\n",
    "            r\"\\u0646\\u0647[\\u064a\\u0649]\",\n",
    "        ],\n",
    "    },\n",
    "    \"english\": {\n",
    "        HohfeldState.OBLIGATION: [r\"\\bmust\\b\", r\"\\bshall\\b\", r\"\\bobligat\", r\"\\bduty\", r\"\\brequir\"],\n",
    "        HohfeldState.RIGHT: [r\"\\bright\\b\", r\"\\bentitle\", r\"\\bdeserve\", r\"\\bclaim\"],\n",
    "        HohfeldState.LIBERTY: [r\"\\bmay\\b\", r\"\\bpermit\", r\"\\ballow\", r\"\\bfree to\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"\\bforbid\", r\"\\bprohibit\", r\"\\bmust not\", r\"\\bshall not\"],\n",
    "    },\n",
    "    # NEW in v10.9: Sanskrit Hohfeld patterns (Devanagari)\n",
    "    \"sanskrit\": {\n",
    "        HohfeldState.OBLIGATION: [r\"\", r\"\", r\"\", r\"\"],  # kartavya, avashya\n",
    "        HohfeldState.RIGHT: [r\"\", r\"\"],  # adhikara, svatva\n",
    "        HohfeldState.LIBERTY: [r\"\", r\"\", r\"\"],  # shakya, anuja\n",
    "        HohfeldState.NO_RIGHT: [r\"\", r\"\", r\"\"],  # nishiddha, varjita\n",
    "    },\n",
    "    # NEW in v10.9: Pali Hohfeld patterns (romanized)\n",
    "    \"pali\": {\n",
    "        HohfeldState.OBLIGATION: [r\"kicca\", r\"karaniiya\", r\"dhammo\"],\n",
    "        HohfeldState.RIGHT: [r\"adhikaara\", r\"bhaaga\"],\n",
    "        HohfeldState.LIBERTY: [r\"anujaanati\", r\"kappati\"],\n",
    "        HohfeldState.NO_RIGHT: [r\"nisiddha\", r\"akaraniya\", r\"na kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# ===== CONTEXT MARKERS FOR GRAMMAR-AWARE EXTRACTION =====\n",
    "# These help distinguish \"thou shalt not kill\" from \"he killed\"\n",
    "CONTEXT_MARKERS = {\n",
    "    \"hebrew\": {\n",
    "        \"negation\": [r\"\", r\"\", r\"\", r\"\", r\"\"],\n",
    "        \"obligation\": [r\"\", r\"\", r\"\", r\"\"],\n",
    "        \"prohibition\": [r\"\", r\".*\"],\n",
    "        \"permission\": [r\"\", r\"\", r\"\"],\n",
    "    },\n",
    "    \"aramaic\": {\n",
    "        \"negation\": [r\"\", r\"\", r\"\"],\n",
    "        \"obligation\": [r\"\", r\"\"],\n",
    "        \"prohibition\": [r\"\"],\n",
    "        \"permission\": [r\"\", r\"\"],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        \"negation\": [r\"\", r\"\", r\"\", r\"\", r\"\"],\n",
    "        \"obligation\": [r\"\", r\"\", r\"\", r\"\", r\"\"],\n",
    "        \"prohibition\": [r\"\", r\"\", r\"\", r\"\"],\n",
    "        \"permission\": [r\"\", r\"\", r\"\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        \"negation\": [r\"\", r\"\", r\"\", r\"\", r\"\"],\n",
    "        \"obligation\": [r\"\", r\"\", r\"\", r\"\"],\n",
    "        \"prohibition\": [r\"\", r\"\", r\" \", r\"\"],\n",
    "        \"permission\": [r\"\", r\"\", r\"\"],\n",
    "    },\n",
    "    \"english\": {\n",
    "        \"negation\": [r\"not\", r\"no\", r\"never\", r\"neither\", r\"n't\"],\n",
    "        \"obligation\": [r\"must\", r\"shall\", r\"should\", r\"ought\", r\"required\"],\n",
    "        \"prohibition\": [r\"forbid\", r\"prohibit\", r\"must not\", r\"shall not\", r\"don't\"],\n",
    "        \"permission\": [r\"may\", r\"can\", r\"allowed\", r\"permit\"],\n",
    "    },\n",
    "    # NEW in v10.9: Sanskrit context markers\n",
    "    \"sanskrit\": {\n",
    "        \"negation\": [r\"\", r\"\", r\"\"],  # na, m, a- prefix\n",
    "        \"obligation\": [r\"\", r\"\", r\"\"],\n",
    "        \"prohibition\": [r\"\", r\"\", r\"\"],\n",
    "        \"permission\": [r\"\", r\"\"],\n",
    "    },\n",
    "    # NEW in v10.9: Pali context markers\n",
    "    \"pali\": {\n",
    "        \"negation\": [r\"na\", r\"ma\", r\"a-\"],\n",
    "        \"obligation\": [r\"kicca\", r\"karaniya\"],\n",
    "        \"prohibition\": [r\"nisiddha\", r\"akaraniya\"],\n",
    "        \"permission\": [r\"anujaanati\", r\"kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def detect_context(text, language, match_pos, window=30):\n",
    "    \"\"\"\n",
    "    Detect grammatical context around a pattern match.\n",
    "    Returns: ('prescriptive'/'descriptive'/'unknown', marker_type or None)\n",
    "    \"\"\"\n",
    "    markers = CONTEXT_MARKERS.get(language, {})\n",
    "    if not markers:\n",
    "        return \"unknown\", None\n",
    "\n",
    "    start = max(0, match_pos - window)\n",
    "    end = min(len(text), match_pos + window)\n",
    "    window_text = text[start:end]\n",
    "\n",
    "    # Check for deontic markers (prescriptive = moral statement)\n",
    "    for marker_type in [\"prohibition\", \"obligation\", \"permission\"]:\n",
    "        for pattern in markers.get(marker_type, []):\n",
    "            if re.search(pattern, window_text):\n",
    "                return \"prescriptive\", marker_type\n",
    "\n",
    "    # Check for simple negation (may be descriptive)\n",
    "    for pattern in markers.get(\"negation\", []):\n",
    "        if re.search(pattern, window_text):\n",
    "            return \"descriptive\", \"negated\"\n",
    "\n",
    "    return \"descriptive\", None\n",
    "\n",
    "\n",
    "# ===== NLP IMPROVEMENTS (v10.9 Phase 1) =====\n",
    "# These provide negation detection and modal classification without external dependencies\n",
    "\n",
    "NEGATION_CUES = {\n",
    "    \"english\": [\"not\", \"no\", \"never\", \"neither\", \"nor\", \"n't\", \"without\", \"lack\", \"none\"],\n",
    "    \"classical_chinese\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "    \"arabic\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "    \"hebrew\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "    \"aramaic\": [\"\", \"\", \"\"],\n",
    "    \"sanskrit\": [\"\", \"\", \"\"],  # na, m, a- (privative prefix)\n",
    "    \"pali\": [\"na\", \"ma\", \"a\", \"an\"],\n",
    "}\n",
    "\n",
    "MODAL_CLASSIFICATION = {\n",
    "    \"english\": {\n",
    "        \"obligation\": [\"must\", \"shall\", \"have to\", \"ought to\", \"need to\", \"required\", \"obligated\"],\n",
    "        \"permission\": [\"may\", \"can\", \"allowed\", \"permitted\", \"free to\", \"entitled\"],\n",
    "        \"prohibition\": [\"must not\", \"shall not\", \"cannot\", \"forbidden\", \"prohibited\", \"banned\"],\n",
    "        \"supererogation\": [\"should\", \"ought\", \"would be good\", \"ideally\", \"preferably\"],\n",
    "    },\n",
    "    \"classical_chinese\": {\n",
    "        \"obligation\": [\"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "        \"permission\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "        \"prohibition\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\"],\n",
    "        \"supererogation\": [\"\", \"\", \"\", \"\"],\n",
    "    },\n",
    "    \"arabic\": {\n",
    "        \"obligation\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "        \"permission\": [\"\", \"\", \"\", \"\"],\n",
    "        \"prohibition\": [\"\", \"\", \"\", \" \", \"\"],\n",
    "        \"supererogation\": [\"\", \"\", \"\", \"\"],\n",
    "    },\n",
    "    \"hebrew\": {\n",
    "        \"obligation\": [\"\", \"\", \"\", \"\", \"\"],\n",
    "        \"permission\": [\"\", \"\", \"\", \"\"],\n",
    "        \"prohibition\": [\"\", \" \", \"\", \"\"],\n",
    "        \"supererogation\": [\"\", \"\", \" \", \"  \"],\n",
    "    },\n",
    "    \"sanskrit\": {\n",
    "        \"obligation\": [\"\", \"\", \"\"],  # kartavya, avashya, niyama\n",
    "        \"permission\": [\"\", \"\"],  # shakya, anuja\n",
    "        \"prohibition\": [\"\", \"\", \"\"],  # nishiddha, varjita, m\n",
    "    },\n",
    "    \"pali\": {\n",
    "        \"obligation\": [\"kicca\", \"karaniya\", \"dhamma\"],\n",
    "        \"permission\": [\"kappati\", \"anujanati\"],\n",
    "        \"prohibition\": [\"akappiya\", \"akaraniya\", \"na kappati\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def enhanced_extract_bond(text: str, language: str) -> dict:\n",
    "    \"\"\"\n",
    "    Enhanced bond extraction with negation + modal detection.\n",
    "    Phase 1 implementation - no external NLP dependencies required.\n",
    "\n",
    "    Returns dict with:\n",
    "        - bond_type: BondType or None\n",
    "        - hohfeld_state: str (OBLIGATION/RIGHT/LIBERTY/NO_RIGHT)\n",
    "        - negated: bool\n",
    "        - modal: str or None (the matched modal marker)\n",
    "        - confidence: float\n",
    "        - context: str (prescriptive/descriptive/unknown)\n",
    "    \"\"\"\n",
    "    # 1. Normalize text\n",
    "    normalized = normalize_text(text, language)\n",
    "\n",
    "    # 2. Check negation\n",
    "    negation_cues = NEGATION_CUES.get(language, [])\n",
    "    is_negated = any(cue in normalized for cue in negation_cues)\n",
    "\n",
    "    # 3. Check modal and classify deontic status\n",
    "    modal_status = \"unknown\"\n",
    "    modal_text = None\n",
    "    for status, markers in MODAL_CLASSIFICATION.get(language, {}).items():\n",
    "        for marker in markers:\n",
    "            if marker in normalized:\n",
    "                modal_status = status\n",
    "                modal_text = marker\n",
    "                break\n",
    "        if modal_status != \"unknown\":\n",
    "            break\n",
    "\n",
    "    # 4. Map modal to Hohfeld state\n",
    "    hohfeld_map = {\n",
    "        \"obligation\": \"OBLIGATION\",\n",
    "        \"permission\": \"LIBERTY\",\n",
    "        \"prohibition\": \"NO_RIGHT\",\n",
    "        \"supererogation\": \"LIBERTY\",\n",
    "        \"unknown\": \"OBLIGATION\",  # Default assumption\n",
    "    }\n",
    "    hohfeld = hohfeld_map[modal_status]\n",
    "\n",
    "    # 5. Pattern matching for bond type\n",
    "    bond_type = None\n",
    "    confidence = 0.5\n",
    "    for bt, patterns in ALL_BOND_PATTERNS.get(language, {}).items():\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, normalized):\n",
    "                bond_type = bt\n",
    "                confidence = 0.9\n",
    "                break\n",
    "        if bond_type:\n",
    "            break\n",
    "\n",
    "    # 6. Adjust confidence for negation\n",
    "    if is_negated:\n",
    "        confidence *= 0.8  # Lower confidence for negated statements\n",
    "\n",
    "    # 7. Determine context\n",
    "    if modal_status in [\"obligation\", \"prohibition\"]:\n",
    "        context = \"prescriptive\"\n",
    "    elif modal_status == \"permission\":\n",
    "        context = \"descriptive\"  # Permissions are often statements of fact\n",
    "    else:\n",
    "        context = \"unknown\"\n",
    "\n",
    "    return {\n",
    "        \"bond_type\": bond_type,\n",
    "        \"hohfeld_state\": hohfeld,\n",
    "        \"negated\": is_negated,\n",
    "        \"modal\": modal_text,\n",
    "        \"confidence\": confidence,\n",
    "        \"context\": context,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\nContext markers defined for grammar-aware extraction\")\n",
    "print(\"  Detects: negation, obligation, prohibition, permission\")\n",
    "\n",
    "print(f\"\\nPatterns defined for {len(ALL_BOND_PATTERNS)} languages:\")\n",
    "for lang in ALL_BOND_PATTERNS:\n",
    "    n = sum(len(p) for p in ALL_BOND_PATTERNS[lang].values())\n",
    "    print(f\"  {lang}: {n} bond patterns\")\n",
    "\n",
    "print(\"\\nNLP improvements (Phase 1):\")\n",
    "print(f\"  NEGATION_CUES: {len(NEGATION_CUES)} languages\")\n",
    "print(f\"  MODAL_CLASSIFICATION: {len(MODAL_CLASSIFICATION)} languages\")\n",
    "print(\"  enhanced_extract_bond() ready\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 4. Generate Splits { display-mode: \"form\" }\n",
    "# @markdown Creates train/test splits for cross-lingual experiments\n",
    "# @markdown v10.9: Added confucian_to_buddhist, confucian_to_legalist,\n",
    "# @markdown        all_to_sanskrit, semitic_to_indic, quran_to_fiqh\n",
    "\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATING SPLITS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if splits already exist from Drive\n",
    "# Check if splits are valid (IDs match current passages)\n",
    "splits_valid = False\n",
    "if os.path.exists(\"data/splits/all_splits.json\"):\n",
    "    try:\n",
    "        with open(\"data/splits/all_splits.json\") as f:\n",
    "            cached_splits = json.load(f)\n",
    "        # Get sample of IDs from splits\n",
    "        sample_ids = set()\n",
    "        for split in cached_splits.values():\n",
    "            sample_ids.update(split[\"train_ids\"][:100])\n",
    "            sample_ids.update(split[\"test_ids\"][:100])\n",
    "        # Check if they exist in current passages\n",
    "        passage_ids = set()\n",
    "        with open(\"data/processed/passages.jsonl\") as f:\n",
    "            for line in f:\n",
    "                p = json.loads(line)\n",
    "                passage_ids.add(p[\"id\"])\n",
    "                if len(passage_ids) > 10000:\n",
    "                    break\n",
    "        matches = len(sample_ids & passage_ids)\n",
    "        splits_valid = matches > len(sample_ids) * 0.9  # 90% match\n",
    "        if not splits_valid:\n",
    "            print(f\"Splits invalid: only {matches}/{len(sample_ids)} IDs match current passages\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating splits: {e}\")\n",
    "\n",
    "if splits_valid and not REFRESH_DATA_FROM_SOURCE:\n",
    "    print(\"\\nSplits already loaded from Drive\")\n",
    "    with open(\"data/splits/all_splits.json\") as f:\n",
    "        all_splits = json.load(f)\n",
    "    for name, split in all_splits.items():\n",
    "        print(f\"  {name}: train={split['train_size']:,}, test={split['test_size']:,}\")\n",
    "else:\n",
    "    random.seed(42)\n",
    "\n",
    "    # Read passage metadata\n",
    "    passage_meta = []\n",
    "    with open(\"data/processed/passages.jsonl\", \"r\") as f:\n",
    "        for line in f:\n",
    "            p = json.loads(line)\n",
    "            passage_meta.append(p)\n",
    "\n",
    "    print(f\"Total passages: {len(passage_meta):,}\")\n",
    "\n",
    "    by_lang = defaultdict(list)\n",
    "    by_period = defaultdict(list)\n",
    "    for p in passage_meta:\n",
    "        by_lang[p[\"language\"]].append(p[\"id\"])\n",
    "        by_period[p[\"time_period\"]].append(p[\"id\"])\n",
    "\n",
    "    print(\"\\nBy language:\")\n",
    "    for lang, ids in sorted(by_lang.items(), key=lambda x: -len(x[1])):\n",
    "        print(f\"  {lang}: {len(ids):,}\")\n",
    "\n",
    "    print(\"\\nBy period:\")\n",
    "    for period, ids in sorted(by_period.items(), key=lambda x: -len(x[1])):\n",
    "        print(f\"  {period}: {len(ids):,}\")\n",
    "\n",
    "    all_splits = {}\n",
    "    # ===== SPLIT 1: Hebrew -> Others =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 1: HEBREW -> OTHERS\")\n",
    "    hebrew_ids = by_lang.get(\"hebrew\", [])\n",
    "    other_ids = [p[\"id\"] for p in passage_meta if p[\"language\"] != \"hebrew\"]\n",
    "    random.shuffle(hebrew_ids)\n",
    "    random.shuffle(other_ids)\n",
    "\n",
    "    all_splits[\"hebrew_to_others\"] = {\n",
    "        \"train_ids\": hebrew_ids,\n",
    "        \"test_ids\": other_ids,\n",
    "        \"train_size\": len(hebrew_ids),\n",
    "        \"test_size\": len(other_ids),\n",
    "    }\n",
    "    print(f\"  Train (Hebrew): {len(hebrew_ids):,}\")\n",
    "    print(f\"  Test (Others): {len(other_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 2: Semitic -> Non-Semitic =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 2: SEMITIC -> NON-SEMITIC\")\n",
    "    semitic_ids = by_lang.get(\"hebrew\", []) + by_lang.get(\"aramaic\", []) + by_lang.get(\"arabic\", [])\n",
    "    non_semitic_ids = by_lang.get(\"classical_chinese\", []) + by_lang.get(\"english\", [])\n",
    "    random.shuffle(semitic_ids)\n",
    "    random.shuffle(non_semitic_ids)\n",
    "\n",
    "    all_splits[\"semitic_to_non_semitic\"] = {\n",
    "        \"train_ids\": semitic_ids,\n",
    "        \"test_ids\": non_semitic_ids,\n",
    "        \"train_size\": len(semitic_ids),\n",
    "        \"test_size\": len(non_semitic_ids),\n",
    "    }\n",
    "    print(f\"  Train (Semitic): {len(semitic_ids):,}\")\n",
    "    print(f\"  Test (Non-Semitic): {len(non_semitic_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 3: Ancient -> Modern =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 3: ANCIENT -> MODERN\")\n",
    "    # Define modern periods explicitly, derive ancient dynamically\n",
    "    modern_periods = {\"MODERN\", \"DEAR_ABBY\"}\n",
    "    all_periods = set(by_period.keys())\n",
    "    ancient_periods = all_periods - modern_periods\n",
    "\n",
    "    print(f\"  Ancient periods: {sorted(ancient_periods)}\")\n",
    "    print(f\"  Modern periods: {sorted(modern_periods)}\")\n",
    "\n",
    "    ancient_ids = [p[\"id\"] for p in passage_meta if p[\"time_period\"] in ancient_periods]\n",
    "    modern_ids = [p[\"id\"] for p in passage_meta if p[\"time_period\"] in modern_periods]\n",
    "    random.shuffle(ancient_ids)\n",
    "    random.shuffle(modern_ids)\n",
    "\n",
    "    all_splits[\"ancient_to_modern\"] = {\n",
    "        \"train_ids\": ancient_ids,\n",
    "        \"test_ids\": modern_ids,\n",
    "        \"train_size\": len(ancient_ids),\n",
    "        \"test_size\": len(modern_ids),\n",
    "    }\n",
    "    print(f\"  Train (Ancient): {len(ancient_ids):,}\")\n",
    "    print(f\"  Test (Modern): {len(modern_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 4: Mixed Baseline =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 4: MIXED BASELINE\")\n",
    "    all_ids = [p[\"id\"] for p in passage_meta]\n",
    "    random.shuffle(all_ids)\n",
    "    split_idx = int(0.7 * len(all_ids))\n",
    "\n",
    "    all_splits[\"mixed_baseline\"] = {\n",
    "        \"train_ids\": all_ids[:split_idx],\n",
    "        \"test_ids\": all_ids[split_idx:],\n",
    "        \"train_size\": split_idx,\n",
    "        \"test_size\": len(all_ids) - split_idx,\n",
    "    }\n",
    "    print(f\"  Train: {split_idx:,}\")\n",
    "    print(f\"  Test: {len(all_ids) - split_idx:,}\")\n",
    "\n",
    "    # ===== SPLIT 5: Dear Abby -> Classical Chinese =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 5: DEAR ABBY -> CHINESE\")\n",
    "    abby_ids = [p[\"id\"] for p in passage_meta if p[\"time_period\"] == \"DEAR_ABBY\"]\n",
    "    chinese_ids = [p[\"id\"] for p in passage_meta if p[\"language\"] == \"classical_chinese\"]\n",
    "    random.shuffle(abby_ids)\n",
    "    random.shuffle(chinese_ids)\n",
    "\n",
    "    all_splits[\"abby_to_chinese\"] = {\n",
    "        \"train_ids\": abby_ids,\n",
    "        \"test_ids\": chinese_ids,\n",
    "        \"train_size\": len(abby_ids),\n",
    "        \"test_size\": len(chinese_ids),\n",
    "    }\n",
    "    print(f\"  Train (Dear Abby): {len(abby_ids):,}\")\n",
    "    print(f\"  Test (Chinese): {len(chinese_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 6: Western Classical -> Eastern =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 6: WESTERN CLASSICAL -> EASTERN\")\n",
    "    western_ids = [p[\"id\"] for p in passage_meta if p[\"time_period\"] == \"WESTERN_CLASSICAL\"]\n",
    "    eastern_ids = [\n",
    "        p[\"id\"] for p in passage_meta if p[\"language\"] in (\"classical_chinese\", \"hebrew\")\n",
    "    ]\n",
    "    random.shuffle(western_ids)\n",
    "    random.shuffle(eastern_ids)\n",
    "\n",
    "    all_splits[\"western_to_eastern\"] = {\n",
    "        \"train_ids\": western_ids,\n",
    "        \"test_ids\": eastern_ids,\n",
    "        \"train_size\": len(western_ids),\n",
    "        \"test_size\": len(eastern_ids),\n",
    "    }\n",
    "    print(f\"  Train (Western - Plato, Aristotle, Stoics): {len(western_ids):,}\")\n",
    "    print(f\"  Test (Eastern - Chinese, Hebrew): {len(eastern_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 7: Confucian -> Buddhist (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 7: CONFUCIAN -> BUDDHIST (Chinese intra-tradition)\")\n",
    "    confucian_daoist_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] in (\"CONFUCIAN\", \"DAOIST\") and p[\"language\"] == \"classical_chinese\"\n",
    "    ]\n",
    "    buddhist_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] == \"BUDDHIST\" and p[\"language\"] == \"classical_chinese\"\n",
    "    ]\n",
    "    random.shuffle(confucian_daoist_ids)\n",
    "    random.shuffle(buddhist_ids)\n",
    "\n",
    "    all_splits[\"confucian_to_buddhist\"] = {\n",
    "        \"train_ids\": confucian_daoist_ids,\n",
    "        \"test_ids\": buddhist_ids,\n",
    "        \"train_size\": len(confucian_daoist_ids),\n",
    "        \"test_size\": len(buddhist_ids),\n",
    "        \"description\": \"Test if Chinese performance is tradition-specific\",\n",
    "    }\n",
    "    print(f\"  Train (Confucian+Daoist): {len(confucian_daoist_ids):,}\")\n",
    "    print(f\"  Test (Buddhist): {len(buddhist_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 8: Confucian -> Legalist/Mohist (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 8: CONFUCIAN -> LEGALIST/MOHIST (virtue vs consequentialist)\")\n",
    "    confucian_only_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] == \"CONFUCIAN\" and p[\"language\"] == \"classical_chinese\"\n",
    "    ]\n",
    "    legalist_mohist_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] in (\"LEGALIST\", \"MOHIST\") and p[\"language\"] == \"classical_chinese\"\n",
    "    ]\n",
    "    random.shuffle(confucian_only_ids)\n",
    "    random.shuffle(legalist_mohist_ids)\n",
    "\n",
    "    all_splits[\"confucian_to_legalist\"] = {\n",
    "        \"train_ids\": confucian_only_ids,\n",
    "        \"test_ids\": legalist_mohist_ids,\n",
    "        \"train_size\": len(confucian_only_ids),\n",
    "        \"test_size\": len(legalist_mohist_ids),\n",
    "        \"description\": \"Virtue ethics  consequentialist/legalist\",\n",
    "    }\n",
    "    print(f\"  Train (Confucian): {len(confucian_only_ids):,}\")\n",
    "    print(f\"  Test (Legalist+Mohist): {len(legalist_mohist_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 9: All -> Sanskrit/Pali (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 9: ALL -> SANSKRIT/PALI (ultimate transfer test)\")\n",
    "    non_indic_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"language\"] in (\"hebrew\", \"aramaic\", \"classical_chinese\", \"arabic\", \"english\")\n",
    "    ]\n",
    "    indic_ids = [p[\"id\"] for p in passage_meta if p[\"language\"] in (\"sanskrit\", \"pali\")]\n",
    "    random.shuffle(non_indic_ids)\n",
    "    random.shuffle(indic_ids)\n",
    "\n",
    "    all_splits[\"all_to_sanskrit\"] = {\n",
    "        \"train_ids\": non_indic_ids,\n",
    "        \"test_ids\": indic_ids,\n",
    "        \"train_size\": len(non_indic_ids),\n",
    "        \"test_size\": len(indic_ids),\n",
    "        \"description\": \"Ultimate transfer test: completely held-out language family\",\n",
    "    }\n",
    "    print(f\"  Train (non-Indic): {len(non_indic_ids):,}\")\n",
    "    print(f\"  Test (Sanskrit+Pali): {len(indic_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 10: Semitic -> Indic (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 10: SEMITIC -> INDIC\")\n",
    "    semitic_only_ids = [\n",
    "        p[\"id\"] for p in passage_meta if p[\"language\"] in (\"hebrew\", \"aramaic\", \"arabic\")\n",
    "    ]\n",
    "    indic_only_ids = [p[\"id\"] for p in passage_meta if p[\"language\"] in (\"sanskrit\", \"pali\")]\n",
    "    random.shuffle(semitic_only_ids)\n",
    "    random.shuffle(indic_only_ids)\n",
    "\n",
    "    all_splits[\"semitic_to_indic\"] = {\n",
    "        \"train_ids\": semitic_only_ids,\n",
    "        \"test_ids\": indic_only_ids,\n",
    "        \"train_size\": len(semitic_only_ids),\n",
    "        \"test_size\": len(indic_only_ids),\n",
    "        \"description\": \"Semitic  Indo-Aryan transfer\",\n",
    "    }\n",
    "    print(f\"  Train (Semitic): {len(semitic_only_ids):,}\")\n",
    "    print(f\"  Test (Indic): {len(indic_only_ids):,}\")\n",
    "\n",
    "    # ===== SPLIT 11: Quran -> Fiqh (v10.9) =====\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SPLIT 11: QURAN -> FIQH (religious to legal/philosophical)\")\n",
    "    quranic_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] in (\"QURANIC\", \"HADITH\") and p[\"language\"] == \"arabic\"\n",
    "    ]\n",
    "    fiqh_ids = [\n",
    "        p[\"id\"]\n",
    "        for p in passage_meta\n",
    "        if p[\"time_period\"] in (\"FIQH\", \"SUFI\", \"FALSAFA\") and p[\"language\"] == \"arabic\"\n",
    "    ]\n",
    "    random.shuffle(quranic_ids)\n",
    "    random.shuffle(fiqh_ids)\n",
    "\n",
    "    all_splits[\"quran_to_fiqh\"] = {\n",
    "        \"train_ids\": quranic_ids,\n",
    "        \"test_ids\": fiqh_ids,\n",
    "        \"train_size\": len(quranic_ids),\n",
    "        \"test_size\": len(fiqh_ids),\n",
    "        \"description\": \"Religious  legal/philosophical Arabic\",\n",
    "    }\n",
    "    print(f\"  Train (Quranic+Hadith): {len(quranic_ids):,}\")\n",
    "    print(f\"  Test (Fiqh+Sufi+Falsafa): {len(fiqh_ids):,}\")\n",
    "\n",
    "    # Save splits\n",
    "    with open(\"data/splits/all_splits.json\", \"w\") as f:\n",
    "        json.dump(all_splits, f, indent=2)\n",
    "\n",
    "    # Save to Drive\n",
    "    shutil.copy(\"data/splits/all_splits.json\", f\"{SAVE_DIR}/all_splits.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Splits saved to local and Drive\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 5. Model Architecture { display-mode: \"form\" }\n",
    "# @markdown BIP v10.9 model with configurable backbone and adversarial heads\n",
    "# @markdown - Updated: 8 languages, 26 periods\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Backbone: {BACKBONE} ({MODEL_NAME})\")\n",
    "print(f\"Hidden size: {BACKBONE_HIDDEN}\")\n",
    "\n",
    "# Index mappings\n",
    "BOND_TO_IDX = {bt.name: i for i, bt in enumerate(BondType)}\n",
    "IDX_TO_BOND = {i: bt.name for i, bt in enumerate(BondType)}\n",
    "# v10.9: 8 languages (added Sanskrit, Pali, Greek placeholder)\n",
    "LANG_TO_IDX = {\n",
    "    \"hebrew\": 0,\n",
    "    \"aramaic\": 1,\n",
    "    \"classical_chinese\": 2,\n",
    "    \"arabic\": 3,\n",
    "    \"english\": 4,\n",
    "    \"sanskrit\": 5,  # NEW in v10.9\n",
    "    \"pali\": 6,  # NEW in v10.9\n",
    "    \"greek\": 7,  # FUTURE (placeholder)\n",
    "}\n",
    "IDX_TO_LANG = {i: l for l, i in LANG_TO_IDX.items()}\n",
    "\n",
    "# v10.9: 26 periods (expanded Chinese, Arabic, added Sanskrit/Pali traditions)\n",
    "PERIOD_TO_IDX = {\n",
    "    # Semitic traditions\n",
    "    \"BIBLICAL\": 0,\n",
    "    \"TANNAITIC\": 1,\n",
    "    \"AMORAIC\": 2,\n",
    "    \"RISHONIM\": 3,\n",
    "    \"ACHRONIM\": 4,\n",
    "    # Chinese traditions (expanded)\n",
    "    \"CONFUCIAN\": 5,\n",
    "    \"DAOIST\": 6,\n",
    "    \"MOHIST\": 7,  # NEW in v10.9\n",
    "    \"LEGALIST\": 8,  # NEW in v10.9\n",
    "    \"BUDDHIST\": 9,  # NEW in v10.9 (Chinese Buddhism)\n",
    "    \"NEO_CONFUCIAN\": 10,  # NEW in v10.9\n",
    "    # Arabic/Islamic traditions (expanded)\n",
    "    \"QURANIC\": 11,\n",
    "    \"HADITH\": 12,\n",
    "    \"FIQH\": 13,  # NEW in v10.9 (Islamic jurisprudence)\n",
    "    \"SUFI\": 14,  # NEW in v10.9\n",
    "    \"FALSAFA\": 15,  # NEW in v10.9 (Arabic philosophy)\n",
    "    # Sanskrit/Pali traditions (NEW in v10.9)\n",
    "    \"DHARMA\": 16,  # Dharmashastra\n",
    "    \"UPANISHAD\": 17,\n",
    "    \"GITA\": 18,\n",
    "    \"ARTHA\": 19,  # Arthashastra\n",
    "    \"PALI\": 20,  # Pali Canon\n",
    "    # Western traditions\n",
    "    \"WESTERN_CLASSICAL\": 21,\n",
    "    \"MEDIEVAL\": 22,\n",
    "    # Modern\n",
    "    \"DEAR_ABBY\": 23,\n",
    "    \"MODERN\": 24,\n",
    "    \"CLASSICAL\": 25,  # Generic classical (fallback)\n",
    "}  # 26 periods total (0-25)\n",
    "IDX_TO_PERIOD = {i: p for p, i in PERIOD_TO_IDX.items()}\n",
    "HOHFELD_TO_IDX = {hs.name: i for i, hs in enumerate(HohfeldState)}\n",
    "IDX_TO_HOHFELD = {i: hs.name for i, hs in enumerate(HohfeldState)}\n",
    "CONTEXT_TO_IDX = {\"prescriptive\": 0, \"descriptive\": 1, \"unknown\": 2}\n",
    "IDX_TO_CONTEXT = {i: c for c, i in CONTEXT_TO_IDX.items()}\n",
    "\n",
    "\n",
    "def get_confidence_weight(conf):\n",
    "    \"\"\"Map confidence to sample weight. Handles both string ('high'/'medium'/'low') and numeric (0.0-1.0) values.\"\"\"\n",
    "    if isinstance(conf, str):\n",
    "        return {\"high\": 2.0, \"medium\": 1.0, \"low\": 0.5}.get(conf, 1.0)\n",
    "    elif isinstance(conf, (int, float)):\n",
    "        return 2.0 if conf >= 0.8 else 1.0\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "class GradientReversalLayer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.alpha, None\n",
    "\n",
    "\n",
    "class BIPModel(nn.Module):\n",
    "    def __init__(self, model_name=None, hidden_size=None, z_dim=64):\n",
    "        super().__init__()\n",
    "        # Use global config if not specified\n",
    "        model_name = model_name or MODEL_NAME\n",
    "        hidden_size = hidden_size or BACKBONE_HIDDEN\n",
    "\n",
    "        print(f\"  Loading encoder: {model_name}\")\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Freeze encoder if configured (probe-only training)\n",
    "        try:\n",
    "            if FREEZE_ENCODER:\n",
    "                for param in self.encoder.parameters():\n",
    "                    param.requires_grad = False\n",
    "                print(f\"  Encoder FROZEN (probe-only mode)\")\n",
    "            else:\n",
    "                print(f\"  Encoder UNFROZEN (full fine-tuning)\")\n",
    "        except NameError:\n",
    "            print(f\"  Encoder unfrozen (FREEZE_ENCODER not set)\")\n",
    "\n",
    "        # Get actual hidden size from model config\n",
    "        actual_hidden = self.encoder.config.hidden_size\n",
    "        if actual_hidden != hidden_size:\n",
    "            print(f\"  Note: Using actual hidden size {actual_hidden}\")\n",
    "            hidden_size = actual_hidden\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Projection to z_bond space (scales with backbone size)\n",
    "        proj_hidden = min(512, hidden_size)\n",
    "        self.z_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_size, proj_hidden),\n",
    "            nn.LayerNorm(proj_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(proj_hidden, z_dim),\n",
    "        )\n",
    "\n",
    "        # Task heads\n",
    "        self.bond_head = nn.Linear(z_dim, len(BondType))\n",
    "        self.hohfeld_head = nn.Linear(z_dim, len(HohfeldState))\n",
    "\n",
    "        # Adversarial heads\n",
    "        self.language_head = nn.Linear(z_dim, len(LANG_TO_IDX))\n",
    "        self.period_head = nn.Linear(z_dim, len(PERIOD_TO_IDX))\n",
    "\n",
    "        # Context prediction head (auxiliary task)\n",
    "        self.context_head = nn.Linear(z_dim, len(CONTEXT_TO_IDX))\n",
    "\n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"  Total params: {total_params:,}\")\n",
    "        print(f\"  Trainable: {trainable_params:,}\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, adv_lambda=1.0):\n",
    "        enc = self.encoder(input_ids, attention_mask)\n",
    "\n",
    "        # Handle different pooling strategies\n",
    "        if hasattr(enc, \"pooler_output\") and enc.pooler_output is not None:\n",
    "            pooled = enc.pooler_output\n",
    "        else:\n",
    "            pooled = enc.last_hidden_state[:, 0]\n",
    "\n",
    "        z = self.z_proj(pooled)\n",
    "\n",
    "        # Bond prediction (main task)\n",
    "        bond_pred = self.bond_head(z)\n",
    "        hohfeld_pred = self.hohfeld_head(z)\n",
    "\n",
    "        # Adversarial predictions (gradient reversal)\n",
    "        z_rev = GradientReversalLayer.apply(z, adv_lambda)\n",
    "        language_pred = self.language_head(z_rev)\n",
    "        period_pred = self.period_head(z_rev)\n",
    "\n",
    "        return {\n",
    "            \"bond_pred\": bond_pred,\n",
    "            \"hohfeld_pred\": hohfeld_pred,\n",
    "            \"language_pred\": language_pred,\n",
    "            \"period_pred\": period_pred,\n",
    "            \"context_pred\": self.context_head(z),\n",
    "            \"z\": z,\n",
    "        }\n",
    "\n",
    "    def get_bond_embedding(self, input_ids, attention_mask):\n",
    "        \"\"\"Get z_bond embedding for geometric analysis.\"\"\"\n",
    "        enc = self.encoder(input_ids, attention_mask)\n",
    "        if hasattr(enc, \"pooler_output\") and enc.pooler_output is not None:\n",
    "            pooled = enc.pooler_output\n",
    "        else:\n",
    "            pooled = enc.last_hidden_state[:, 0]\n",
    "        return self.z_proj(pooled)\n",
    "\n",
    "\n",
    "# Initialize tokenizer for selected backbone\n",
    "print(f\"\\nLoading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "\n",
    "# Dataset with Hohfeld support\n",
    "class NativeDataset(Dataset):\n",
    "    def __init__(self, ids_set, passages_file, bonds_file, tokenizer, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.data = []\n",
    "\n",
    "        bonds_by_id = {}\n",
    "        with open(bonds_file) as fb:\n",
    "            for line in fb:\n",
    "                b = json.loads(line)\n",
    "                bonds_by_id[b[\"passage_id\"]] = b\n",
    "\n",
    "        with open(passages_file) as fp:\n",
    "            for line in tqdm(fp, desc=\"Loading\", unit=\"line\"):\n",
    "                p = json.loads(line)\n",
    "                if p[\"id\"] in ids_set and p[\"id\"] in bonds_by_id:\n",
    "                    b = bonds_by_id[p[\"id\"]]\n",
    "                    self.data.append(\n",
    "                        {\n",
    "                            \"text\": p[\"text\"][:1000],\n",
    "                            \"language\": p[\"language\"],\n",
    "                            \"period\": p[\"time_period\"],\n",
    "                            \"bond\": b.get(\"bond_type\") or b.get(\"bonds\", {}).get(\"primary_bond\"),\n",
    "                            \"hohfeld\": None,\n",
    "                            \"context\": b.get(\"context\")\n",
    "                            or b.get(\"bonds\", {}).get(\"context\", \"unknown\"),\n",
    "                            \"confidence\": b.get(\"confidence\")\n",
    "                            or b.get(\"bonds\", {}).get(\"confidence\", \"medium\"),\n",
    "                        }\n",
    "                    )\n",
    "        print(f\"  Loaded {len(self.data):,} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        enc = self.tokenizer(\n",
    "            item[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"bond_label\": BOND_TO_IDX.get(item[\"bond\"], 9),\n",
    "            \"language_label\": LANG_TO_IDX.get(item[\"language\"], 4),\n",
    "            \"period_label\": PERIOD_TO_IDX.get(item[\"period\"], 9),\n",
    "            \"hohfeld_label\": HOHFELD_TO_IDX.get(item[\"hohfeld\"], 0) if item[\"hohfeld\"] else 0,\n",
    "            \"context_label\": CONTEXT_TO_IDX.get(item[\"context\"], 2),\n",
    "            \"sample_weight\": get_confidence_weight(item[\"confidence\"]),\n",
    "            \"language\": item[\"language\"],\n",
    "            \"context\": item[\"context\"],\n",
    "            \"confidence\": item[\"confidence\"],\n",
    "            \"text\": item[\"text\"],  # Raw text for role augmentation\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
    "        \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n",
    "        \"bond_labels\": torch.tensor([x[\"bond_label\"] for x in batch]),\n",
    "        \"language_labels\": torch.tensor([x[\"language_label\"] for x in batch]),\n",
    "        \"period_labels\": torch.tensor([x[\"period_label\"] for x in batch]),\n",
    "        \"hohfeld_labels\": torch.tensor([x[\"hohfeld_label\"] for x in batch]),\n",
    "        \"context_labels\": torch.tensor([x[\"context_label\"] for x in batch]),\n",
    "        \"sample_weights\": torch.tensor([x[\"sample_weight\"] for x in batch], dtype=torch.float),\n",
    "        \"languages\": [x[\"language\"] for x in batch],\n",
    "        \"contexts\": [x[\"context\"] for x in batch],\n",
    "        \"confidences\": [x[\"confidence\"] for x in batch],\n",
    "        \"texts\": [x[\"text\"] for x in batch],  # v10.10: raw texts for role augmentation\n",
    "    }\n",
    "\n",
    "\n",
    "print(f\"\\nArchitecture ready for {BACKBONE}\")\n",
    "print(f\"  Bond classes: {len(BondType)}\")\n",
    "print(f\"  Languages: {len(LANG_TO_IDX)}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 6. Train BIP Model { display-mode: \"form\" }\n",
    "# @markdown Training with tuned adversarial weights and hardware-optimized parameters\n",
    "# @markdown v10.9: Added new splits (confucian_to_buddhist, all_to_sanskrit, etc.)\n",
    "\n",
    "# ===== SUPPRESS DATALOADER MULTIPROCESSING WARNINGS =====\n",
    "# These occur during garbage collection and bypass normal exception handling\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Method 1: Filter warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*can only test a child process.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.data\")\n",
    "\n",
    "# Method 2: Suppress logging\n",
    "logging.getLogger(\"torch.utils.data.dataloader\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "# Method 3: Redirect stderr during DataLoader cleanup (most effective)\n",
    "class StderrFilter(io.TextIOWrapper):\n",
    "    \"\"\"Filters out DataLoader multiprocessing cleanup messages from stderr\"\"\"\n",
    "\n",
    "    def __init__(self, original):\n",
    "        self.original = original\n",
    "        self.buffer_lines = []\n",
    "\n",
    "    def write(self, text):\n",
    "        # Filter out the specific error patterns\n",
    "        skip_patterns = [\n",
    "            \"can only test a child process\",\n",
    "            \"_MultiProcessingDataLoaderIter.__del__\",\n",
    "            \"_shutdown_workers\",\n",
    "            \"Exception ignored in:\",\n",
    "            \"w.is_alive()\",\n",
    "        ]\n",
    "        # Buffer multi-line error messages\n",
    "        if any(p in text for p in skip_patterns):\n",
    "            return len(text)  # Pretend we wrote it\n",
    "        # Also skip if it looks like part of a traceback for these errors\n",
    "        if text.strip().startswith(\"^\") and len(text.strip()) < 80:\n",
    "            return len(text)\n",
    "        if text.strip().startswith('File \"/usr') and \"dataloader.py\" in text:\n",
    "            return len(text)\n",
    "        if text.strip() == \"Traceback (most recent call last):\":\n",
    "            self.buffer_lines = [text]\n",
    "            return len(text)\n",
    "        if self.buffer_lines:\n",
    "            self.buffer_lines.append(text)\n",
    "            # Check if this is the DataLoader error traceback\n",
    "            full_msg = \"\".join(self.buffer_lines)\n",
    "            if any(p in full_msg for p in skip_patterns):\n",
    "                self.buffer_lines = []\n",
    "                return len(text)\n",
    "            # After 10 lines, flush if not the target error\n",
    "            if len(self.buffer_lines) > 10:\n",
    "                for line in self.buffer_lines:\n",
    "                    self.original.write(line)\n",
    "                self.buffer_lines = []\n",
    "        return self.original.write(text)\n",
    "\n",
    "    def flush(self):\n",
    "        if self.buffer_lines:\n",
    "            # Flush any remaining buffered content\n",
    "            for line in self.buffer_lines:\n",
    "                self.original.write(line)\n",
    "            self.buffer_lines = []\n",
    "        self.original.flush()\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.original, name)\n",
    "\n",
    "\n",
    "# Install the stderr filter\n",
    "_original_stderr = sys.stderr\n",
    "sys.stderr = StderrFilter(_original_stderr)\n",
    "\n",
    "# Method 4: Patch the DataLoader cleanup function directly\n",
    "try:\n",
    "    import torch.utils.data.dataloader as dl_module\n",
    "\n",
    "    _original_del = dl_module._MultiProcessingDataLoaderIter.__del__\n",
    "\n",
    "    def _patched_del(self):\n",
    "        try:\n",
    "            _original_del(self)\n",
    "        except (AssertionError, AttributeError, RuntimeError):\n",
    "            pass  # Silently ignore cleanup errors\n",
    "\n",
    "    dl_module._MultiProcessingDataLoaderIter.__del__ = _patched_del\n",
    "except Exception:\n",
    "    pass  # If patching fails, the stderr filter will still work\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import gc\n",
    "\n",
    "# ===== INITIAL MEMORY CLEANUP =====\n",
    "# Clean up any leftover GPU memory from previous runs before starting\n",
    "print(\"Cleaning up GPU memory from previous runs...\")\n",
    "if torch.cuda.is_available():\n",
    "    # Clear any existing models/tensors from globals\n",
    "    for var_name in list(globals().keys()):\n",
    "        obj = globals().get(var_name)\n",
    "        if isinstance(obj, torch.nn.Module):\n",
    "            try:\n",
    "                obj.cpu()\n",
    "                del globals()[var_name]\n",
    "            except:\n",
    "                pass\n",
    "        elif isinstance(obj, torch.Tensor) and obj.is_cuda:\n",
    "            try:\n",
    "                del globals()[var_name]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # Force garbage collection\n",
    "    for _ in range(5):\n",
    "        gc.collect()\n",
    "\n",
    "    # Clear CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Check memory status\n",
    "    mem_alloc = torch.cuda.memory_allocated() / 1e9\n",
    "    mem_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"  GPU memory: {mem_alloc:.2f} GB allocated, {mem_reserved:.2f} GB reserved\")\n",
    "\n",
    "    if mem_alloc > 1.0:\n",
    "        print(f\"  WARNING: {mem_alloc:.1f} GB still allocated - consider restarting runtime\")\n",
    "        # Try more aggressive cleanup\n",
    "        torch.cuda.ipc_collect()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"  No GPU detected\")\n",
    "\n",
    "print()\n",
    "\n",
    "# @markdown **Splits to train:**\n",
    "TRAIN_HEBREW_TO_OTHERS = True  # @param {type:\"boolean\"}\n",
    "TRAIN_SEMITIC_TO_NON_SEMITIC = True  # @param {type:\"boolean\"}\n",
    "TRAIN_ANCIENT_TO_MODERN = True  # @param {type:\"boolean\"}\n",
    "TRAIN_MIXED_BASELINE = True  # @param {type:\"boolean\"}\n",
    "TRAIN_ABBY_TO_CHINESE = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown **v10.9 New Splits:**\n",
    "TRAIN_CONFUCIAN_TO_BUDDHIST = True  # @param {type:\"boolean\"}\n",
    "TRAIN_CONFUCIAN_TO_LEGALIST = True  # @param {type:\"boolean\"}\n",
    "TRAIN_ALL_TO_SANSKRIT = True  # @param {type:\"boolean\"}\n",
    "TRAIN_SEMITIC_TO_INDIC = True  # @param {type:\"boolean\"}\n",
    "TRAIN_QURAN_TO_FIQH = True  # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown **Reproducibility:**\n",
    "USE_FIXED_SEED = True  # @param {type:\"boolean\"}\n",
    "RANDOM_SEED = 42  # @param {type:\"integer\"}\n",
    "# @markdown Set USE_FIXED_SEED=True for reproducible results, False for random initialization\n",
    "\n",
    "if USE_FIXED_SEED:\n",
    "    import numpy as np\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Using fixed seed: {RANDOM_SEED}\")\n",
    "else:\n",
    "    torch.backends.cudnn.benchmark = True  # Faster but non-deterministic\n",
    "    print(\"Using random initialization\")\n",
    "\n",
    "# @markdown **Hyperparameters:**\n",
    "LANG_WEIGHT = 0.1  # @param {type:\"number\"}\n",
    "PERIOD_WEIGHT = 0.066  # @param {type:\"number\"}\n",
    "# Use NUM_EPOCHS from Cell 1, or default\n",
    "try:\n",
    "    N_EPOCHS = NUM_EPOCHS\n",
    "except NameError:\n",
    "    N_EPOCHS = 10  # Default fallback\n",
    "\n",
    "# @markdown **Context-Aware Training:**\n",
    "USE_CONFIDENCE_WEIGHTING = True  # @param {type:\"boolean\"}\n",
    "# @markdown Weight prescriptive (high confidence) examples 2x in loss\n",
    "\n",
    "USE_CONTEXT_AUXILIARY = True  # @param {type:\"boolean\"}\n",
    "# @markdown Add context prediction as auxiliary training target\n",
    "\n",
    "CONTEXT_LOSS_WEIGHT = 0.33  # @param {type:\"number\"}\n",
    "# @markdown Weight for context prediction loss\n",
    "\n",
    "STRICT_PRESCRIPTIVE_TEST = False  # @param {type:\"boolean\"}\n",
    "# @markdown Only evaluate on prescriptive examples (reduces test set ~97%!)\n",
    "\n",
    "# @markdown **v10.10: Role-Aware Data Augmentation:**\n",
    "USE_ROLE_AUGMENTATION = True  # @param {type:\"boolean\"}\n",
    "# @markdown Adds contrastive loss for agent/patient role sensitivity\n",
    "ROLE_AUGMENT_PROB = 0.3  # @param {type:\"number\"}\n",
    "# @markdown Probability of augmenting each batch\n",
    "ROLE_CONTRASTIVE_WEIGHT = 0.2  # @param {type:\"number\"}\n",
    "# @markdown Weight for role contrastive loss\n",
    "ROLE_CONTRASTIVE_MARGIN = 0.5  # @param {type:\"number\"}\n",
    "# @markdown Minimum embedding distance for role-swapped pairs\n",
    "\n",
    "\n",
    "def swap_roles_simple(text, language):\n",
    "    \"\"\"Simple role swap using word order reversal for common patterns.\n",
    "    v10.10: Addresses weak role_swap sensitivity (0.003) from fuzz testing.\"\"\"\n",
    "    patterns = {\n",
    "        \"english\": [\n",
    "            (r\"(\\w+) must (\\w+) (\\w+)\", r\"\\3 must \\2 \\1\"),\n",
    "            (r\"(\\w+) should (\\w+) (\\w+)\", r\"\\3 should \\2 \\1\"),\n",
    "            (r\"(\\w+) shall (\\w+) (\\w+)\", r\"\\3 shall \\2 \\1\"),\n",
    "            (r\"the (\\w+) must (\\w+) the (\\w+)\", r\"the \\3 must \\2 the \\1\"),\n",
    "            (r\"(\\w+) is obligated to (\\w+) (\\w+)\", r\"\\3 is obligated to \\2 \\1\"),\n",
    "            (r\"(\\w+) has a duty to (\\w+) (\\w+)\", r\"\\3 has a duty to \\2 \\1\"),\n",
    "        ],\n",
    "        \"hebrew\": [\n",
    "            (r\" (\\S+) (\\S+)  (\\S+)\", r\" \\3 \\2  \\1\"),\n",
    "        ],\n",
    "        \"classical_chinese\": [\n",
    "            (r\"(\\S)(\\S)(\\S)\", r\"\\3\\2\\1\"),\n",
    "            (r\"(\\S)(\\S)(\\S)\", r\"\\3\\2\\1\"),\n",
    "            (r\"(\\S)(\\S)(\\S)\", r\"\\3\\2\\1\"),\n",
    "        ],\n",
    "        \"arabic\": [\n",
    "            (r\"  (\\S+)  (\\S+) (\\S+)\", r\"  \\3  \\2 \\1\"),\n",
    "            (r\"(\\S+)   (\\S+) (\\S+)\", r\"\\3   \\2 \\1\"),\n",
    "        ],\n",
    "        \"sanskrit\": [\n",
    "            (r\"(\\S+) (\\S+) (\\S+)\", r\"\\3 \\2 \\1\"),\n",
    "        ],\n",
    "        \"pali\": [\n",
    "            (r\"(\\S+)o (\\S+)a (\\S+)ti\", r\"\\3o \\2a \\1ti\"),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    lang_patterns = patterns.get(language, patterns[\"english\"])\n",
    "    for pattern, replacement in lang_patterns:\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            swapped = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "            if swapped != text:\n",
    "                return swapped\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING BIP MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSettings:\")\n",
    "print(f\"  Backbone:     {BACKBONE}\")\n",
    "print(f\"  GPU Tier:     {GPU_TIER}\")\n",
    "print(f\"  Batch size:   {BATCH_SIZE}\")\n",
    "print(f\"  Workers:      {NUM_WORKERS}\")\n",
    "print(f\"  Learning rate: {LR:.2e}\")\n",
    "print(f\"  Adv weights:  lang={LANG_WEIGHT}, period={PERIOD_WEIGHT}\")\n",
    "print(\"(0.01 prevents loss explosion while maintaining invariance)\")\n",
    "print(f\"  Confidence weighting: {USE_CONFIDENCE_WEIGHTING}\")\n",
    "print(f\"  Context auxiliary: {USE_CONTEXT_AUXILIARY} (weight={CONTEXT_LOSS_WEIGHT})\")\n",
    "print(f\"  Strict prescriptive test: {STRICT_PRESCRIPTIVE_TEST}\")\n",
    "print(f\"  Role augmentation: {USE_ROLE_AUGMENTATION} (prob={ROLE_AUGMENT_PROB}, weight={ROLE_CONTRASTIVE_WEIGHT})\")\n",
    "\n",
    "# tokenizer loaded in Cell 6 based on BACKBONE selection\n",
    "\n",
    "with open(\"data/splits/all_splits.json\") as f:\n",
    "    all_splits = json.load(f)\n",
    "\n",
    "splits_to_train = []\n",
    "if TRAIN_HEBREW_TO_OTHERS:\n",
    "    splits_to_train.append(\"hebrew_to_others\")\n",
    "if TRAIN_SEMITIC_TO_NON_SEMITIC:\n",
    "    splits_to_train.append(\"semitic_to_non_semitic\")\n",
    "if TRAIN_ANCIENT_TO_MODERN:\n",
    "    splits_to_train.append(\"ancient_to_modern\")\n",
    "if TRAIN_MIXED_BASELINE:\n",
    "    splits_to_train.append(\"mixed_baseline\")\n",
    "if TRAIN_ABBY_TO_CHINESE:\n",
    "    splits_to_train.append(\"abby_to_chinese\")\n",
    "# v10.9 new splits\n",
    "if TRAIN_CONFUCIAN_TO_BUDDHIST:\n",
    "    splits_to_train.append(\"confucian_to_buddhist\")\n",
    "if TRAIN_CONFUCIAN_TO_LEGALIST:\n",
    "    splits_to_train.append(\"confucian_to_legalist\")\n",
    "if TRAIN_ALL_TO_SANSKRIT:\n",
    "    splits_to_train.append(\"all_to_sanskrit\")\n",
    "if TRAIN_SEMITIC_TO_INDIC:\n",
    "    splits_to_train.append(\"semitic_to_indic\")\n",
    "if TRAIN_QURAN_TO_FIQH:\n",
    "    splits_to_train.append(\"quran_to_fiqh\")\n",
    "\n",
    "print(f\"\\nTraining {len(splits_to_train)} splits: {splits_to_train}\")\n",
    "\n",
    "all_results = {}\n",
    "MIN_TEST_SIZE = 100  # Lowered to allow smaller test sets like Chinese\n",
    "\n",
    "for split_idx, split_name in enumerate(splits_to_train):\n",
    "    split_start = time.time()\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"[{split_idx+1}/{len(splits_to_train)}] {split_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    split = all_splits[split_name]\n",
    "    print(f\"Train: {split['train_size']:,} | Test: {split['test_size']:,}\")\n",
    "\n",
    "    if split[\"test_size\"] < MIN_TEST_SIZE:\n",
    "        print(f\"WARNING: Test set only {split['test_size']} samples (need {MIN_TEST_SIZE})\")\n",
    "        print(\"Skipping this split - results would be unreliable\")\n",
    "        print(\"To fix: Add more data to the test languages/periods\")\n",
    "        continue\n",
    "\n",
    "    # Create model with OOM recovery\n",
    "    def create_model_with_retry():\n",
    "        \"\"\"Create model, cleaning up GPU memory if OOM occurs.\"\"\"\n",
    "        try:\n",
    "            return BIPModel().to(device)\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(\"  OOM on model creation - cleaning up and retrying...\")\n",
    "            # Clean up any existing model in globals\n",
    "            _g = globals()\n",
    "            for _var in [\"model\", \"analyzer\", \"encoder\"]:\n",
    "                if _var in _g and _g[_var] is not None:\n",
    "                    try:\n",
    "                        if hasattr(_g[_var], \"cpu\"):\n",
    "                            _g[_var].cpu()\n",
    "                        _g[_var] = None\n",
    "                    except:\n",
    "                        pass\n",
    "            # Force cleanup\n",
    "            gc.collect()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            # Retry\n",
    "            return BIPModel().to(device)\n",
    "\n",
    "    model = create_model_with_retry()\n",
    "\n",
    "    train_dataset = NativeDataset(\n",
    "        set(split[\"train_ids\"]),\n",
    "        \"data/processed/passages.jsonl\",\n",
    "        \"data/processed/bonds.jsonl\",\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    test_ids_to_use = split[\"test_ids\"][:MAX_TEST_SAMPLES]\n",
    "\n",
    "    # Optional: strict prescriptive-only test\n",
    "    if STRICT_PRESCRIPTIVE_TEST:\n",
    "        print(\"Filtering to prescriptive examples only...\")\n",
    "        # Load bonds to filter\n",
    "        prescriptive_ids = set()\n",
    "        with open(\"data/processed/bonds.jsonl\") as f:\n",
    "            for line in f:\n",
    "                b = json.loads(line)\n",
    "                if b.get(\"context\") == \"prescriptive\":\n",
    "                    prescriptive_ids.add(b[\"passage_id\"])\n",
    "        test_ids_to_use = [tid for tid in test_ids_to_use if tid in prescriptive_ids]\n",
    "        print(f\"  Filtered to {len(test_ids_to_use):,} prescriptive samples\")\n",
    "\n",
    "    test_dataset = NativeDataset(\n",
    "        set(test_ids_to_use),\n",
    "        \"data/processed/passages.jsonl\",\n",
    "        \"data/processed/bonds.jsonl\",\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"ERROR: No training data!\")\n",
    "        continue\n",
    "\n",
    "    # Use hardware-optimized batch size\n",
    "    actual_batch = min(BATCH_SIZE, max(32, len(train_dataset) // 20))\n",
    "    print(f\"Actual batch size: {actual_batch}\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=actual_batch,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=actual_batch * 2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "    \n",
    "    # Gradient clipping setup\n",
    "    try:\n",
    "        grad_clip = GRADIENT_CLIP if GRADIENT_CLIP > 0 else None\n",
    "    except NameError:\n",
    "        grad_clip = 1.0  # Default\n",
    "    \n",
    "    # Early stopping setup\n",
    "    try:\n",
    "        early_stop_patience = EARLY_STOPPING_PATIENCE if EARLY_STOPPING_PATIENCE > 0 else None\n",
    "    except NameError:\n",
    "        early_stop_patience = 3  # Default\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    def get_adv_lambda(epoch, warmup=3):\n",
    "        \"\"\"Ramp adversarial strength: 0.1 -> 1.0 over warmup, then hold at 1.0\"\"\"\n",
    "        if epoch <= warmup:\n",
    "            return 0.1 + 0.9 * (epoch / warmup)\n",
    "        return 1.0\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    start_epoch = 1\n",
    "\n",
    "    # Check for existing checkpoint to resume from\n",
    "    checkpoint_path = f\"models/checkpoints/latest_{split_name}.pt\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"  Found checkpoint, resuming...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        best_loss = checkpoint[\"best_loss\"]\n",
    "        print(f\"  Resuming from epoch {start_epoch}, best_loss={best_loss:.4f}\")\n",
    "\n",
    "    for epoch in range(start_epoch, N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            bond_labels = batch[\"bond_labels\"].to(device)\n",
    "            language_labels = batch[\"language_labels\"].to(device)\n",
    "            period_labels = batch[\"period_labels\"].to(device)\n",
    "\n",
    "            adv_lambda = get_adv_lambda(epoch)\n",
    "\n",
    "            # Use new autocast API\n",
    "            with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n",
    "                out = model(input_ids, attention_mask, adv_lambda=adv_lambda)\n",
    "\n",
    "                # Weighted bond loss\n",
    "                if USE_CONFIDENCE_WEIGHTING:\n",
    "                    sample_weights = batch[\"sample_weights\"].to(device)\n",
    "                    loss_bond = F.cross_entropy(out[\"bond_pred\"], bond_labels, reduction=\"none\")\n",
    "                    loss_bond = (loss_bond * sample_weights).mean()\n",
    "                else:\n",
    "                    loss_bond = F.cross_entropy(out[\"bond_pred\"], bond_labels)\n",
    "\n",
    "                # Context auxiliary loss\n",
    "                if USE_CONTEXT_AUXILIARY:\n",
    "                    context_labels = batch[\"context_labels\"].to(device)\n",
    "                    loss_context = F.cross_entropy(out[\"context_pred\"], context_labels)\n",
    "                else:\n",
    "                    loss_context = 0\n",
    "\n",
    "                loss_lang = F.cross_entropy(out[\"language_pred\"], language_labels)\n",
    "                loss_period = F.cross_entropy(out[\"period_pred\"], period_labels)\n",
    "\n",
    "            loss = (\n",
    "                loss_bond\n",
    "                + LANG_WEIGHT * loss_lang\n",
    "                + PERIOD_WEIGHT * loss_period\n",
    "                + CONTEXT_LOSS_WEIGHT * loss_context\n",
    "            )\n",
    "\n",
    "            # v10.10: Role contrastive loss for agent/patient sensitivity\n",
    "            loss_role = torch.tensor(0.0, device=device)\n",
    "            if USE_ROLE_AUGMENTATION and random.random() < ROLE_AUGMENT_PROB:\n",
    "                batch_texts = batch.get(\"texts\", [])\n",
    "                batch_languages = batch.get(\"languages\", [])\n",
    "\n",
    "                swapped_texts = []\n",
    "                original_indices = []\n",
    "\n",
    "                for i, (text, lang) in enumerate(zip(batch_texts, batch_languages)):\n",
    "                    swapped = swap_roles_simple(text, lang)\n",
    "                    if swapped:\n",
    "                        swapped_texts.append(swapped)\n",
    "                        original_indices.append(i)\n",
    "\n",
    "                if swapped_texts and len(swapped_texts) >= 2:\n",
    "                    # Tokenize swapped texts\n",
    "                    swapped_encoded = tokenizer(\n",
    "                        swapped_texts,\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=128,\n",
    "                        return_tensors=\"pt\",\n",
    "                    )\n",
    "                    swapped_ids = swapped_encoded[\"input_ids\"].to(device)\n",
    "                    swapped_mask = swapped_encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "                    # Get embeddings for swapped texts (no gradients needed - saves memory!)\n",
    "                    # We only need gradients through z_original, not z_swapped\n",
    "                    with torch.no_grad():\n",
    "                        swapped_out = model(swapped_ids, swapped_mask, adv_lambda=0)\n",
    "                        z_swapped = swapped_out[\"z\"].detach()\n",
    "\n",
    "                    # Get original embeddings for corresponding indices (keeps gradients)\n",
    "                    z_original = out[\"z\"][original_indices]\n",
    "\n",
    "                    # Contrastive loss: push role-swapped embeddings apart\n",
    "                    # Hinge loss: max(0, margin - distance)\n",
    "                    # Gradients flow through z_original only\n",
    "                    distances = F.pairwise_distance(z_original, z_swapped)\n",
    "                    loss_role = F.relu(ROLE_CONTRASTIVE_MARGIN - distances).mean()\n",
    "\n",
    "                    # Clean up to prevent memory accumulation\n",
    "                    del swapped_ids, swapped_mask, swapped_out, swapped_encoded\n",
    "                    del z_original, z_swapped, distances\n",
    "\n",
    "            loss = loss + ROLE_CONTRASTIVE_WEIGHT * loss_role\n",
    "\n",
    "            if USE_AMP and scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if grad_clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "            # Delete intermediate tensors to prevent memory accumulation\n",
    "            del input_ids, attention_mask, bond_labels, language_labels, period_labels\n",
    "            del out, loss, loss_bond, loss_lang, loss_period\n",
    "            if USE_CONFIDENCE_WEIGHTING:\n",
    "                del sample_weights\n",
    "            if USE_CONTEXT_AUXILIARY:\n",
    "                del context_labels, loss_context\n",
    "            if USE_ROLE_AUGMENTATION:\n",
    "                del loss_role\n",
    "\n",
    "            # Periodic memory cleanup every 50 batches\n",
    "            if n_batches % 50 == 0:\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        avg_loss = total_loss / n_batches\n",
    "\n",
    "        # Aggressive memory cleanup after each epoch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            mem_alloc = torch.cuda.memory_allocated() / 1e9\n",
    "            mem_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "            print(\n",
    "                f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f}) [GPU: {mem_alloc:.1f}GB alloc, {mem_reserved:.1f}GB reserved]\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_lambda={adv_lambda:.2f})\")\n",
    "\n",
    "        # Save checkpoint every epoch (for crash recovery)\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"loss\": avg_loss,\n",
    "            \"best_loss\": best_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"models/checkpoints/latest_{split_name}.pt\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), f\"models/checkpoints/best_{split_name}.pt\")\n",
    "            torch.save(model.state_dict(), f\"{SAVE_DIR}/best_{split_name}.pt\")\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.load_state_dict(torch.load(f\"models/checkpoints/best_{split_name}.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = {\"bond\": [], \"lang\": []}\n",
    "    all_labels = {\"bond\": [], \"lang\": []}\n",
    "    all_languages = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            out = model(batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), 0)\n",
    "            all_preds[\"bond\"].extend(out[\"bond_pred\"].argmax(-1).cpu().tolist())\n",
    "            all_preds[\"lang\"].extend(out[\"language_pred\"].argmax(-1).cpu().tolist())\n",
    "            all_labels[\"bond\"].extend(batch[\"bond_labels\"].tolist())\n",
    "            all_labels[\"lang\"].extend(batch[\"language_labels\"].tolist())\n",
    "            all_languages.extend(batch[\"languages\"])\n",
    "\n",
    "    bond_f1 = f1_score(all_labels[\"bond\"], all_preds[\"bond\"], average=\"macro\", zero_division=0)\n",
    "    bond_acc = sum(p == l for p, l in zip(all_preds[\"bond\"], all_labels[\"bond\"])) / len(\n",
    "        all_preds[\"bond\"]\n",
    "    )\n",
    "    lang_acc = sum(p == l for p, l in zip(all_preds[\"lang\"], all_labels[\"lang\"])) / len(\n",
    "        all_preds[\"lang\"]\n",
    "    )\n",
    "\n",
    "    # Per-language F1\n",
    "    lang_f1 = {}\n",
    "    for lang in set(all_languages):\n",
    "        mask = [l == lang for l in all_languages]\n",
    "        if sum(mask) > 10:\n",
    "            preds = [p for p, m in zip(all_preds[\"bond\"], mask) if m]\n",
    "            labels = [l for l, m in zip(all_labels[\"bond\"], mask) if m]\n",
    "            lang_f1[lang] = {\n",
    "                \"f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "                \"n\": sum(mask),\n",
    "            }\n",
    "\n",
    "    all_results[split_name] = {\n",
    "        \"bond_f1_macro\": bond_f1,\n",
    "        \"bond_acc\": bond_acc,\n",
    "        \"language_acc\": lang_acc,\n",
    "        \"per_language_f1\": lang_f1,\n",
    "        \"training_time\": time.time() - split_start,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{split_name} RESULTS:\")\n",
    "    print(f\"  Bond F1 (macro): {bond_f1:.3f} ({bond_f1/0.1:.1f}x chance)\")\n",
    "    print(f\"  Bond accuracy:   {bond_acc:.1%}\")\n",
    "    print(f\"  Language acc:    {lang_acc:.1%} (want ~20% = invariant)\")\n",
    "    print(\"  Per-language:\")\n",
    "    for lang, m in sorted(lang_f1.items(), key=lambda x: -x[1][\"n\"]):\n",
    "        print(f\"    {lang:20s}: F1={m['f1']:.3f} (n={m['n']:,})\")\n",
    "\n",
    "    # Context analysis\n",
    "    high_conf = sum(1 for c in test_dataset.data if c[\"confidence\"] == \"high\")\n",
    "    prescriptive = sum(1 for c in test_dataset.data if c[\"context\"] == \"prescriptive\")\n",
    "    print(\n",
    "        f\"  Context: {prescriptive:,}/{len(test_dataset):,} prescriptive ({prescriptive/len(test_dataset)*100:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  High confidence: {high_conf:,}/{len(test_dataset):,} ({high_conf/len(test_dataset)*100:.1f}%)\"\n",
    "    )\n",
    "\n",
    "    # GPU memory usage before cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        mem = torch.cuda.memory_allocated() / 1e9\n",
    "        print(\n",
    "            f\"\\n  GPU memory (before cleanup): {mem:.1f} GB / {VRAM_GB:.1f} GB ({mem/VRAM_GB*100:.0f}%)\"\n",
    "        )\n",
    "\n",
    "    # Aggressive memory cleanup between splits\n",
    "    # Step 1: Zero out gradients to release gradient memory\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "\n",
    "    # Step 2: Clear optimizer state (can hold significant memory)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    optimizer_state = optimizer.state\n",
    "    for state in optimizer_state.values():\n",
    "        for k, v in list(state.items()):\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = None\n",
    "\n",
    "    # Step 3: Move model to CPU to release GPU memory\n",
    "    model.cpu()\n",
    "\n",
    "    # Step 4: Delete all references\n",
    "    del model, train_dataset, test_dataset, train_loader, test_loader, optimizer\n",
    "    if USE_AMP and scaler:\n",
    "        del scaler\n",
    "\n",
    "    # Step 5: Force garbage collection (multiple passes)\n",
    "    for _ in range(5):\n",
    "        gc.collect()\n",
    "\n",
    "    # Step 6: Clear CUDA cache and reset memory stats\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        # If memory is still high, try more aggressive cleanup\n",
    "        mem_check = torch.cuda.memory_allocated() / 1e9\n",
    "        if mem_check > 2.0:\n",
    "            print(f\"  Memory still high ({mem_check:.1f}GB), attempting deeper cleanup...\")\n",
    "            # Clear all cached allocations\n",
    "            torch.cuda.memory._dump_snapshot = lambda: None  # Disable snapshot if enabled\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "\n",
    "    # Step 7: Re-create scaler for next split\n",
    "    if USE_AMP:\n",
    "        scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "    # GPU memory after cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"  GPU memory (after cleanup): {mem_after:.1f} GB (freed {mem - mem_after:.1f} GB)\")\n",
    "        if mem_after > 1.0:\n",
    "            print(f\"  WARNING: {mem_after:.1f} GB still allocated - may cause OOM on next split\")\n",
    "            print(f\"  Consider running with BACKBONE='MiniLM' for lower memory usage\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 7. Geometric Analysis & Linear Probe { display-mode: \"form\" }\n",
    "# @markdown v10.9: New geometric analysis module + linear probe test\n",
    "# @markdown Tests latent space structure (axis discovery, role swap analysis)\n",
    "# @markdown Tests if z_bond encodes language/period (should be low = invariant)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "# ===== v10.9: GEOMETRIC ANALYZER CLASS =====\n",
    "class GeometricAnalyzer:\n",
    "    \"\"\"\n",
    "    Probe the latent space geometry to discover moral structure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        inputs = self.tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, max_length=128, padding=\"max_length\"\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        z = self.model.get_bond_embedding(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        return z.cpu().numpy().flatten()\n",
    "\n",
    "    def find_direction(self, positive_texts: List[str], negative_texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Find the direction in z-space that separates two concepts.\n",
    "        E.g., obligation vs permission, harm vs care.\n",
    "        \"\"\"\n",
    "        pos_embs = np.array([self.get_embedding(t) for t in positive_texts])\n",
    "        neg_embs = np.array([self.get_embedding(t) for t in negative_texts])\n",
    "\n",
    "        pos_mean = pos_embs.mean(axis=0)\n",
    "        neg_mean = neg_embs.mean(axis=0)\n",
    "\n",
    "        direction = pos_mean - neg_mean\n",
    "        direction = direction / (np.linalg.norm(direction) + 1e-9)\n",
    "        return direction\n",
    "\n",
    "    def test_direction_transfer(\n",
    "        self, direction: np.ndarray, test_pairs: List[Tuple[str, str]]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Test if a direction generalizes to new examples.\n",
    "        Returns accuracy of direction-based classification.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for pos_text, neg_text in test_pairs:\n",
    "            pos_proj = np.dot(self.get_embedding(pos_text), direction)\n",
    "            neg_proj = np.dot(self.get_embedding(neg_text), direction)\n",
    "            scores.append(1.0 if pos_proj > neg_proj else 0.0)\n",
    "        return np.mean(scores)\n",
    "\n",
    "    def pca_on_pairs(self, concept_pairs: Dict[str, List[Tuple[str, str]]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Run PCA on difference vectors to find dominant axes.\n",
    "\n",
    "        concept_pairs: {\"obligation_permission\": [(obl1, perm1), ...], ...}\n",
    "        \"\"\"\n",
    "        all_diffs = []\n",
    "        labels = []\n",
    "\n",
    "        for concept, pairs in concept_pairs.items():\n",
    "            for pos, neg in pairs:\n",
    "                diff = self.get_embedding(pos) - self.get_embedding(neg)\n",
    "                all_diffs.append(diff)\n",
    "                labels.append(concept)\n",
    "\n",
    "        X = np.array(all_diffs)\n",
    "\n",
    "        pca = PCA(n_components=min(10, len(X)))\n",
    "        pca.fit(X)\n",
    "\n",
    "        return {\n",
    "            \"components\": pca.components_,\n",
    "            \"explained_variance_ratio\": pca.explained_variance_ratio_,\n",
    "            \"labels\": labels,\n",
    "            \"transformed\": pca.transform(X),\n",
    "        }\n",
    "\n",
    "    def role_swap_analysis(self, agent_patient_pairs: List[Tuple[str, str]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Test if swapping agent/patient produces consistent transformation.\n",
    "\n",
    "        agent_patient_pairs: [(\"A harmed B\", \"B harmed A\"), ...]\n",
    "        \"\"\"\n",
    "        transformations = []\n",
    "\n",
    "        for original, swapped in agent_patient_pairs:\n",
    "            orig_emb = self.get_embedding(original)\n",
    "            swap_emb = self.get_embedding(swapped)\n",
    "            transformations.append(swap_emb - orig_emb)\n",
    "\n",
    "        T = np.array(transformations)\n",
    "\n",
    "        # Check consistency: are all transformations similar?\n",
    "        mean_transform = T.mean(axis=0)\n",
    "        cosines = [\n",
    "            np.dot(t, mean_transform) / (np.linalg.norm(t) * np.linalg.norm(mean_transform) + 1e-9)\n",
    "            for t in T\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"mean_transform\": mean_transform,\n",
    "            \"consistency\": np.mean(cosines),\n",
    "            \"consistency_std\": np.std(cosines),\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LINEAR PROBE TEST\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nIf probe accuracy is NEAR CHANCE, representation is INVARIANT\")\n",
    "print(\"(This is what we want for BIP)\")\n",
    "\n",
    "probe_results = {}\n",
    "\n",
    "for split_name in [\"hebrew_to_others\", \"semitic_to_non_semitic\"]:\n",
    "    model_path = f\"{SAVE_DIR}/best_{split_name}.pt\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"\\nSkipping {split_name} - no saved model\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PROBE: {split_name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model = BIPModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    test_ids = set(all_splits[split_name][\"test_ids\"][:5000])\n",
    "    test_dataset = NativeDataset(\n",
    "        test_ids, \"data/processed/passages.jsonl\", \"data/processed/bonds.jsonl\", tokenizer\n",
    "    )\n",
    "\n",
    "    if len(test_dataset) < 50:\n",
    "        print(f\"  Skip - only {len(test_dataset)} samples\")\n",
    "        continue\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "    all_z, all_lang, all_period = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Extract\"):\n",
    "            out = model(batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), 0)\n",
    "            all_z.append(out[\"z\"].cpu().numpy())\n",
    "            all_lang.extend(batch[\"language_labels\"].tolist())\n",
    "            all_period.extend(batch[\"period_labels\"].tolist())\n",
    "\n",
    "    X = np.vstack(all_z)\n",
    "    y_lang = np.array(all_lang)\n",
    "    y_period = np.array(all_period)\n",
    "\n",
    "    scaler_probe = StandardScaler()\n",
    "    X_scaled = scaler_probe.fit_transform(X)\n",
    "\n",
    "    # Train/test split for probes\n",
    "    n = len(X)\n",
    "    idx = np.random.permutation(n)\n",
    "    train_idx, test_idx = idx[: int(0.7 * n)], idx[int(0.7 * n) :]\n",
    "\n",
    "    # Language probe - check for multiple classes\n",
    "    unique_langs = np.unique(y_lang[test_idx])\n",
    "    if len(unique_langs) < 2:\n",
    "        print(f\"  SKIP language probe - only {len(unique_langs)} class\")\n",
    "        lang_acc = 1.0 / max(1, len(np.unique(y_lang)))\n",
    "        lang_chance = lang_acc\n",
    "    else:\n",
    "        lang_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        lang_probe.fit(X_scaled[train_idx], y_lang[train_idx])\n",
    "        lang_acc = (lang_probe.predict(X_scaled[test_idx]) == y_lang[test_idx]).mean()\n",
    "        lang_chance = 1.0 / len(unique_langs)\n",
    "\n",
    "    # Period probe - same check\n",
    "    unique_periods = np.unique(y_period[test_idx])\n",
    "    if len(unique_periods) < 2:\n",
    "        print(f\"  SKIP period probe - only {len(unique_periods)} class\")\n",
    "        period_acc = 1.0 / max(1, len(np.unique(y_period)))\n",
    "        period_chance = period_acc\n",
    "    else:\n",
    "        period_probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "        period_probe.fit(X_scaled[train_idx], y_period[train_idx])\n",
    "        period_acc = (period_probe.predict(X_scaled[test_idx]) == y_period[test_idx]).mean()\n",
    "        period_chance = 1.0 / len(unique_periods)\n",
    "\n",
    "    lang_status = \"INVARIANT\" if lang_acc < lang_chance + 0.15 else \"NOT invariant\"\n",
    "    period_status = \"INVARIANT\" if period_acc < period_chance + 0.15 else \"NOT invariant\"\n",
    "\n",
    "    probe_results[split_name] = {\n",
    "        \"language_acc\": lang_acc,\n",
    "        \"language_chance\": lang_chance,\n",
    "        \"language_status\": lang_status,\n",
    "        \"period_acc\": period_acc,\n",
    "        \"period_chance\": period_chance,\n",
    "        \"period_status\": period_status,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nRESULTS:\")\n",
    "    print(f\"  Language: {lang_acc:.1%} (chance: {lang_chance:.1%}) -> {lang_status}\")\n",
    "    print(f\"  Period:   {period_acc:.1%} (chance: {period_chance:.1%}) -> {period_status}\")\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Probe tests complete\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===== v10.9: GEOMETRIC ANALYSIS =====\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GEOMETRIC ANALYSIS (v10.9)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nDiscovering interpretable axes in latent space...\")\n",
    "\n",
    "# Test pairs for axis discovery (cross-lingual)\n",
    "OBLIGATION_PERMISSION_TRAIN = [\n",
    "    # English - training set\n",
    "    (\"You must help the elderly\", \"You may help the elderly\"),\n",
    "    (\"He is required to pay\", \"He is allowed to pay\"),\n",
    "    (\"Parents must protect children\", \"Parents may protect children\"),\n",
    "]\n",
    "\n",
    "OBLIGATION_PERMISSION_TEST = [\n",
    "    # Chinese\n",
    "    (\"\", \"\"),  # Gentleman must/may be filial\n",
    "    (\"\", \"\"),  # People must/may follow law\n",
    "    # Arabic\n",
    "    (\"   \", \"   \"),  # You must/may help\n",
    "    # Hebrew\n",
    "    (\" \", \" \"),  # Obligated/permitted to honor\n",
    "    # English - held out\n",
    "    (\"She must attend\", \"She may attend\"),\n",
    "]\n",
    "\n",
    "HARM_CARE_PAIRS = [\n",
    "    (\"He injured the child\", \"He protected the child\"),\n",
    "    (\"\", \"\"),  # One who kills / one who saves\n",
    "    (\" \", \" \"),  # Oppressed / showed mercy to the weak\n",
    "    (\"She hurt the patient\", \"She healed the patient\"),\n",
    "]\n",
    "\n",
    "ROLE_SWAP_PAIRS = [\n",
    "    (\"The master commands the servant\", \"The servant commands the master\"),\n",
    "    (\"\", \"\"),  # Lord commands minister / minister commands lord\n",
    "    (\"  \", \"  \"),  # Father commands son / son commands father\n",
    "    (\"The parent guides the child\", \"The child guides the parent\"),\n",
    "]\n",
    "\n",
    "geometry_results = {}\n",
    "\n",
    "# Use the best model from mixed_baseline split for geometric analysis\n",
    "model_path = f\"{SAVE_DIR}/best_mixed_baseline.pt\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"\\nLoading model for geometric analysis...\")\n",
    "    model = BIPModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    analyzer = GeometricAnalyzer(model, tokenizer, device)\n",
    "\n",
    "    # 1. Find obligation/permission axis\n",
    "    print(\"\\n--- Obligation/Permission Axis ---\")\n",
    "    obl_texts = [p[0] for p in OBLIGATION_PERMISSION_TRAIN]\n",
    "    perm_texts = [p[1] for p in OBLIGATION_PERMISSION_TRAIN]\n",
    "    obl_perm_axis = analyzer.find_direction(obl_texts, perm_texts)\n",
    "\n",
    "    # Test transfer to other languages\n",
    "    transfer_acc = analyzer.test_direction_transfer(obl_perm_axis, OBLIGATION_PERMISSION_TEST)\n",
    "    print(f\"  Direction found from English training pairs\")\n",
    "    print(f\"  Transfer accuracy to other languages: {transfer_acc:.1%}\")\n",
    "    axis_status = \"STRONG\" if transfer_acc > 0.8 else \"WEAK\" if transfer_acc > 0.5 else \"FAILED\"\n",
    "    print(f\"  Status: {axis_status} deontic axis\")\n",
    "\n",
    "    geometry_results[\"obligation_permission\"] = {\n",
    "        \"transfer_accuracy\": transfer_acc,\n",
    "        \"status\": axis_status,\n",
    "    }\n",
    "\n",
    "    # 2. Find harm/care axis\n",
    "    print(\"\\n--- Harm/Care Axis ---\")\n",
    "    harm_texts = [p[0] for p in HARM_CARE_PAIRS]\n",
    "    care_texts = [p[1] for p in HARM_CARE_PAIRS]\n",
    "    harm_care_axis = analyzer.find_direction(harm_texts, care_texts)\n",
    "\n",
    "    # Check axis orthogonality\n",
    "    axis_correlation = abs(np.dot(obl_perm_axis, harm_care_axis))\n",
    "    print(f\"  Axis found\")\n",
    "    print(f\"  Correlation with obl/perm axis: {axis_correlation:.3f}\")\n",
    "    orthogonal = \"ORTHOGONAL\" if axis_correlation < 0.3 else \"CORRELATED\"\n",
    "    print(f\"  Status: {orthogonal}\")\n",
    "\n",
    "    geometry_results[\"harm_care\"] = {\n",
    "        \"axis_correlation\": axis_correlation,\n",
    "        \"orthogonal\": axis_correlation < 0.3,\n",
    "    }\n",
    "\n",
    "    # 3. Role swap analysis\n",
    "    print(\"\\n--- Role Swap Analysis ---\")\n",
    "    role_analysis = analyzer.role_swap_analysis(ROLE_SWAP_PAIRS)\n",
    "    print(\n",
    "        f\"  Mean consistency: {role_analysis['consistency']:.3f} +/- {role_analysis['consistency_std']:.3f}\"\n",
    "    )\n",
    "    role_status = \"CONSISTENT\" if role_analysis[\"consistency\"] > 0.9 else \"VARIABLE\"\n",
    "    print(f\"  Status: {role_status} agent/patient transformation\")\n",
    "\n",
    "    geometry_results[\"role_swap\"] = {\n",
    "        \"consistency\": role_analysis[\"consistency\"],\n",
    "        \"consistency_std\": role_analysis[\"consistency_std\"],\n",
    "        \"status\": role_status,\n",
    "    }\n",
    "\n",
    "    # 4. PCA on all structural pairs\n",
    "    print(\"\\n--- PCA Analysis ---\")\n",
    "    all_concept_pairs = {\n",
    "        \"obligation_permission\": OBLIGATION_PERMISSION_TRAIN + OBLIGATION_PERMISSION_TEST,\n",
    "        \"harm_care\": HARM_CARE_PAIRS,\n",
    "    }\n",
    "    pca_results = analyzer.pca_on_pairs(all_concept_pairs)\n",
    "\n",
    "    cumsum = np.cumsum(pca_results[\"explained_variance_ratio\"])\n",
    "    n_components_90 = np.argmax(cumsum > 0.9) + 1 if any(cumsum > 0.9) else len(cumsum)\n",
    "\n",
    "    print(f\"  Explained variance ratio: {pca_results['explained_variance_ratio'][:5]}\")\n",
    "    print(f\"  Components for 90% variance: {n_components_90}\")\n",
    "    pca_status = \"LOW-DIM\" if n_components_90 <= 3 else \"HIGH-DIM\"\n",
    "    print(f\"  Status: {pca_status} moral structure\")\n",
    "\n",
    "    geometry_results[\"pca\"] = {\n",
    "        \"explained_variance\": pca_results[\"explained_variance_ratio\"].tolist(),\n",
    "        \"n_components_90pct\": n_components_90,\n",
    "        \"status\": pca_status,\n",
    "    }\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(f\"\\nSkipping geometric analysis - no model at {model_path}\")\n",
    "    geometry_results = {\"error\": \"No model available\"}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Geometric analysis complete\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Fuzz Testing v10.11: Structural vs Surface Perturbations { display-mode: \"form\" }\n",
    "#@markdown Tests whether structural perturbations move embeddings more than surface perturbations.\n",
    "#@markdown **Run immediately after Cell 6/7 training completes - uses model in memory.**\n",
    "#@markdown\n",
    "#@markdown v10.11 enhancements:\n",
    "#@markdown - **30+ samples per category** for 6-sigma statistical confidence\n",
    "#@markdown - **Runtime-adaptive thresholds** based on GPU type (L4/A100/T4)\n",
    "#@markdown - **Extended bond type coverage** including cross-cultural scenarios\n",
    "#@markdown - **Bootstrap confidence intervals** for robust statistics\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ## Enable Fuzz Testing\n",
    "RUN_FUZZ_TEST = True  #@param {type:\"boolean\"}\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "\n",
    "# ============================================================================\n",
    "# RUNTIME DETECTION AND ADAPTIVE THRESHOLDS\n",
    "# ============================================================================\n",
    "\n",
    "def detect_runtime() -> Dict:\n",
    "    \"\"\"Detect GPU type and set appropriate thresholds.\"\"\"\n",
    "    runtime_config = {\n",
    "        \"gpu_type\": \"unknown\",\n",
    "        \"vram_gb\": 0,\n",
    "        \"batch_size\": 16,\n",
    "        \"max_scenarios\": 50,\n",
    "        \"bootstrap_samples\": 1000,\n",
    "    }\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0).lower()\n",
    "        vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        runtime_config[\"vram_gb\"] = vram\n",
    "\n",
    "        if \"a100\" in gpu_name:\n",
    "            runtime_config[\"gpu_type\"] = \"A100\"\n",
    "            runtime_config[\"batch_size\"] = 64\n",
    "            runtime_config[\"max_scenarios\"] = 100\n",
    "            runtime_config[\"bootstrap_samples\"] = 5000\n",
    "        elif \"l4\" in gpu_name:\n",
    "            runtime_config[\"gpu_type\"] = \"L4\"\n",
    "            runtime_config[\"batch_size\"] = 32\n",
    "            runtime_config[\"max_scenarios\"] = 75\n",
    "            runtime_config[\"bootstrap_samples\"] = 2000\n",
    "        elif \"t4\" in gpu_name:\n",
    "            runtime_config[\"gpu_type\"] = \"T4\"\n",
    "            runtime_config[\"batch_size\"] = 16\n",
    "            runtime_config[\"max_scenarios\"] = 50\n",
    "            runtime_config[\"bootstrap_samples\"] = 1000\n",
    "        elif \"v100\" in gpu_name:\n",
    "            runtime_config[\"gpu_type\"] = \"V100\"\n",
    "            runtime_config[\"batch_size\"] = 32\n",
    "            runtime_config[\"max_scenarios\"] = 60\n",
    "            runtime_config[\"bootstrap_samples\"] = 2000\n",
    "        else:\n",
    "            # Default based on VRAM\n",
    "            if vram >= 40:\n",
    "                runtime_config[\"gpu_type\"] = \"high_vram\"\n",
    "                runtime_config[\"batch_size\"] = 64\n",
    "                runtime_config[\"max_scenarios\"] = 100\n",
    "            elif vram >= 20:\n",
    "                runtime_config[\"gpu_type\"] = \"medium_vram\"\n",
    "                runtime_config[\"batch_size\"] = 32\n",
    "                runtime_config[\"max_scenarios\"] = 75\n",
    "            else:\n",
    "                runtime_config[\"gpu_type\"] = \"low_vram\"\n",
    "                runtime_config[\"batch_size\"] = 16\n",
    "                runtime_config[\"max_scenarios\"] = 40\n",
    "\n",
    "    return runtime_config\n",
    "\n",
    "if not RUN_FUZZ_TEST:\n",
    "    print(\"Fuzz testing disabled. Check RUN_FUZZ_TEST to enable.\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FUZZ TESTING v10.11: STRUCTURAL VS SURFACE PERTURBATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "\n",
    "    # Detect runtime and set thresholds\n",
    "    RUNTIME = detect_runtime()\n",
    "    print(f\"Runtime detected: {RUNTIME['gpu_type']} ({RUNTIME['vram_gb']:.1f} GB VRAM)\")\n",
    "    print(f\"Batch size: {RUNTIME['batch_size']}, Max scenarios: {RUNTIME['max_scenarios']}\")\n",
    "    print(f\"Bootstrap samples: {RUNTIME['bootstrap_samples']}\")\n",
    "    print()\n",
    "\n",
    "    # ========================================================================\n",
    "    # USE EXISTING MODEL FROM TRAINING SESSION\n",
    "    # ========================================================================\n",
    "\n",
    "    try:\n",
    "        if hasattr(model, 'module'):\n",
    "            _fuzz_model = model.module\n",
    "            print(\"Using unwrapped model from Accelerator\")\n",
    "        else:\n",
    "            _fuzz_model = model\n",
    "            print(\"Using model from training session\")\n",
    "\n",
    "        _fuzz_model.eval()\n",
    "        device = next(_fuzz_model.parameters()).device\n",
    "        print(f\"Device: {device}\")\n",
    "\n",
    "    except NameError:\n",
    "        print(\"ERROR: No model found in memory!\")\n",
    "        print(\"Please run training (Cell 6/7) first, or load from checkpoint manually.\")\n",
    "        RUN_FUZZ_TEST = False\n",
    "\n",
    "    if RUN_FUZZ_TEST:\n",
    "        print()\n",
    "\n",
    "        # ====================================================================\n",
    "        # EMBEDDING FUNCTIONS\n",
    "        # ====================================================================\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def get_embedding(text: str) -> np.ndarray:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                              max_length=128, padding=\"max_length\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            z = _fuzz_model.get_bond_embedding(inputs['input_ids'], inputs['attention_mask'])\n",
    "            return z.cpu().numpy().flatten()\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def get_embeddings_batch(texts: List[str]) -> np.ndarray:\n",
    "            \"\"\"Batch embedding for efficiency.\"\"\"\n",
    "            all_embeddings = []\n",
    "            batch_size = RUNTIME['batch_size']\n",
    "\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i+batch_size]\n",
    "                inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True,\n",
    "                                  max_length=128, padding=\"max_length\")\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                z = _fuzz_model.get_bond_embedding(inputs['input_ids'], inputs['attention_mask'])\n",
    "                all_embeddings.append(z.cpu().numpy())\n",
    "\n",
    "            return np.vstack(all_embeddings)\n",
    "\n",
    "        def cosine_distance(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "            sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-9)\n",
    "            return 1 - sim\n",
    "\n",
    "        # ====================================================================\n",
    "        # EXPANDED BASE SCENARIOS (30+ for statistical power)\n",
    "        # ====================================================================\n",
    "\n",
    "        BASE_SCENARIOS = [\n",
    "            # OBLIGATION / DUTY (8 scenarios)\n",
    "            {\"text\": \"John borrowed money from Mary and promised to repay it by Friday.\", \"bond_type\": \"OBLIGATION\", \"category\": \"promise\"},\n",
    "            {\"text\": \"The doctor has a duty to keep patient information confidential.\", \"bond_type\": \"DUTY\", \"category\": \"professional\"},\n",
    "            {\"text\": \"Parents must protect their children from harm.\", \"bond_type\": \"DUTY\", \"category\": \"familial\"},\n",
    "            {\"text\": \"The teacher promised to grade all exams by Monday.\", \"bond_type\": \"OBLIGATION\", \"category\": \"promise\"},\n",
    "            {\"text\": \"Soldiers are required to follow orders from superior officers.\", \"bond_type\": \"DUTY\", \"category\": \"institutional\"},\n",
    "            {\"text\": \"The witness swore to tell the truth in court.\", \"bond_type\": \"OBLIGATION\", \"category\": \"oath\"},\n",
    "            {\"text\": \"Citizens must pay their taxes to the government.\", \"bond_type\": \"DUTY\", \"category\": \"civic\"},\n",
    "            {\"text\": \"The contractor agreed to complete the building within six months.\", \"bond_type\": \"OBLIGATION\", \"category\": \"contract\"},\n",
    "\n",
    "            # CARE / HELP (8 scenarios)\n",
    "            {\"text\": \"Sarah helped her neighbor carry groceries, expecting nothing in return.\", \"bond_type\": \"CARE\", \"category\": \"altruism\"},\n",
    "            {\"text\": \"The nurse stayed late to comfort the dying patient.\", \"bond_type\": \"CARE\", \"category\": \"compassion\"},\n",
    "            {\"text\": \"She donated her savings to help earthquake victims.\", \"bond_type\": \"CARE\", \"category\": \"charity\"},\n",
    "            {\"text\": \"The mentor guided the young artist without asking for payment.\", \"bond_type\": \"CARE\", \"category\": \"guidance\"},\n",
    "            {\"text\": \"He gave his coat to the homeless man shivering in the cold.\", \"bond_type\": \"CARE\", \"category\": \"generosity\"},\n",
    "            {\"text\": \"The stranger stopped to help change the flat tire.\", \"bond_type\": \"CARE\", \"category\": \"assistance\"},\n",
    "            {\"text\": \"She listened patiently as he shared his troubles.\", \"bond_type\": \"CARE\", \"category\": \"empathy\"},\n",
    "            {\"text\": \"The community gathered to rebuild the family's burned house.\", \"bond_type\": \"CARE\", \"category\": \"solidarity\"},\n",
    "\n",
    "            # HARM / VIOLATION (8 scenarios)\n",
    "            {\"text\": \"He stole the wallet from the elderly woman.\", \"bond_type\": \"HARM\", \"category\": \"theft\"},\n",
    "            {\"text\": \"The company violated the contract by delivering late.\", \"bond_type\": \"VIOLATION\", \"category\": \"breach\"},\n",
    "            {\"text\": \"She spread false rumors to destroy his reputation.\", \"bond_type\": \"HARM\", \"category\": \"slander\"},\n",
    "            {\"text\": \"The politician broke his campaign promises after election.\", \"bond_type\": \"VIOLATION\", \"category\": \"betrayal\"},\n",
    "            {\"text\": \"He poisoned the well that the village depended on.\", \"bond_type\": \"HARM\", \"category\": \"sabotage\"},\n",
    "            {\"text\": \"The trustee embezzled funds from the charity.\", \"bond_type\": \"VIOLATION\", \"category\": \"fraud\"},\n",
    "            {\"text\": \"She abandoned her children to pursue her own interests.\", \"bond_type\": \"VIOLATION\", \"category\": \"abandonment\"},\n",
    "            {\"text\": \"The invaders destroyed the sacred temple.\", \"bond_type\": \"HARM\", \"category\": \"desecration\"},\n",
    "\n",
    "            # FAIRNESS / JUSTICE (8 scenarios)\n",
    "            {\"text\": \"The judge ruled fairly, giving each side equal consideration.\", \"bond_type\": \"FAIRNESS\", \"category\": \"impartiality\"},\n",
    "            {\"text\": \"She forgave him for breaking his promise.\", \"bond_type\": \"FORGIVENESS\", \"category\": \"mercy\"},\n",
    "            {\"text\": \"The council distributed resources equally among all villages.\", \"bond_type\": \"FAIRNESS\", \"category\": \"equity\"},\n",
    "            {\"text\": \"He returned the extra change the shopkeeper gave by mistake.\", \"bond_type\": \"FAIRNESS\", \"category\": \"honesty\"},\n",
    "            {\"text\": \"The elder mediated the dispute without favoring either party.\", \"bond_type\": \"FAIRNESS\", \"category\": \"mediation\"},\n",
    "            {\"text\": \"She gave credit to her assistant for the discovery.\", \"bond_type\": \"FAIRNESS\", \"category\": \"attribution\"},\n",
    "            {\"text\": \"The king pardoned the rebels who surrendered peacefully.\", \"bond_type\": \"FORGIVENESS\", \"category\": \"clemency\"},\n",
    "            {\"text\": \"They compensated the wrongly accused man for his suffering.\", \"bond_type\": \"FAIRNESS\", \"category\": \"restitution\"},\n",
    "\n",
    "            # CROSS-CULTURAL BOND TYPES (8 scenarios)\n",
    "            {\"text\": \"The student honored his teacher by caring for him in old age.\", \"bond_type\": \"PIETY\", \"category\": \"filial\"},\n",
    "            {\"text\": \"She upheld the family honor by keeping her grandfather's promise.\", \"bond_type\": \"LOYALTY\", \"category\": \"ancestral\"},\n",
    "            {\"text\": \"The warrior spared his defeated enemy as custom demanded.\", \"bond_type\": \"HONOR\", \"category\": \"chivalry\"},\n",
    "            {\"text\": \"He returned the sacred artifact to the temple it was taken from.\", \"bond_type\": \"REVERENCE\", \"category\": \"restoration\"},\n",
    "            {\"text\": \"The host provided shelter to the stranger as hospitality required.\", \"bond_type\": \"HOSPITALITY\", \"category\": \"xenia\"},\n",
    "            {\"text\": \"She maintained ritual purity to preserve cosmic order.\", \"bond_type\": \"PURITY\", \"category\": \"ritual\"},\n",
    "            {\"text\": \"The merchant kept his word even when it meant financial loss.\", \"bond_type\": \"INTEGRITY\", \"category\": \"commercial\"},\n",
    "            {\"text\": \"The community shunned him for violating the ancestral taboo.\", \"bond_type\": \"TABOO\", \"category\": \"prohibition\"},\n",
    "        ]\n",
    "\n",
    "        # ====================================================================\n",
    "        # PERTURBATION GENERATORS\n",
    "        # ====================================================================\n",
    "\n",
    "        # Name substitution pools for variety\n",
    "        NAME_POOLS = {\n",
    "            \"male\": [\"John\", \"Michael\", \"David\", \"James\", \"Robert\", \"William\", \"Thomas\", \"Daniel\"],\n",
    "            \"female\": [\"Mary\", \"Sarah\", \"Emma\", \"Lisa\", \"Anna\", \"Rachel\", \"Rebecca\", \"Hannah\"],\n",
    "        }\n",
    "\n",
    "        IRRELEVANT_DETAILS = [\n",
    "            \" It was Tuesday.\", \" The room was blue.\", \" Last summer.\",\n",
    "            \" The weather was pleasant.\", \" It happened at noon.\",\n",
    "            \" The year was uncertain.\", \" Birds sang nearby.\",\n",
    "            \" The moon was full.\", \" Rain had fallen earlier.\",\n",
    "            \" The road was dusty.\", \" Flowers bloomed outside.\",\n",
    "        ]\n",
    "\n",
    "        SYNONYMS = {\n",
    "            \"money\": [\"cash\", \"funds\", \"currency\"],\n",
    "            \"groceries\": [\"bags\", \"supplies\", \"provisions\"],\n",
    "            \"house\": [\"home\", \"dwelling\", \"residence\"],\n",
    "            \"promise\": [\"vow\", \"pledge\", \"commitment\"],\n",
    "            \"help\": [\"assist\", \"aid\", \"support\"],\n",
    "            \"truth\": [\"facts\", \"reality\", \"honesty\"],\n",
    "        }\n",
    "\n",
    "        def surface_perturbations(scenario: Dict) -> List[Dict]:\n",
    "            \"\"\"Generate surface perturbations that shouldn't change moral meaning.\"\"\"\n",
    "            text = scenario[\"text\"]\n",
    "            perturbs = []\n",
    "\n",
    "            # Name changes (multiple variations)\n",
    "            for old_name in NAME_POOLS[\"male\"] + NAME_POOLS[\"female\"]:\n",
    "                if old_name in text:\n",
    "                    for new_name in (NAME_POOLS[\"male\"] if old_name in NAME_POOLS[\"male\"] else NAME_POOLS[\"female\"]):\n",
    "                        if new_name != old_name:\n",
    "                            new_text = text.replace(old_name, new_name)\n",
    "                            if new_text != text:\n",
    "                                perturbs.append({\"text\": new_text, \"type\": \"name_change\", \"original\": old_name, \"new\": new_name})\n",
    "                                if len(perturbs) >= 3:  # Limit per scenario\n",
    "                                    break\n",
    "\n",
    "            # Irrelevant detail additions\n",
    "            for detail in IRRELEVANT_DETAILS[:4]:\n",
    "                perturbs.append({\"text\": text + detail, \"type\": \"irrelevant_detail\", \"detail\": detail})\n",
    "\n",
    "            # Synonym substitutions\n",
    "            new_text = text\n",
    "            for word, synonyms in SYNONYMS.items():\n",
    "                if word in new_text.lower():\n",
    "                    for syn in synonyms[:2]:\n",
    "                        test_text = new_text.replace(word, syn)\n",
    "                        if test_text != new_text:\n",
    "                            perturbs.append({\"text\": test_text, \"type\": \"synonym\", \"original\": word, \"new\": syn})\n",
    "                            break\n",
    "\n",
    "            return perturbs\n",
    "\n",
    "        def structural_perturbations(scenario: Dict) -> List[Dict]:\n",
    "            \"\"\"Generate structural perturbations that SHOULD change moral meaning.\"\"\"\n",
    "            text = scenario[\"text\"]\n",
    "            perturbs = []\n",
    "\n",
    "            # Role swaps (agent/patient reversal)\n",
    "            role_swaps = [\n",
    "                (\"John borrowed money from Mary\", \"Mary borrowed money from John\"),\n",
    "                (\"He stole the wallet from the elderly woman\", \"The elderly woman stole the wallet from him\"),\n",
    "                (\"She spread false rumors to destroy his reputation\", \"He spread false rumors to destroy her reputation\"),\n",
    "                (\"Sarah helped her neighbor\", \"Her neighbor helped Sarah\"),\n",
    "                (\"The teacher promised to grade\", \"The students promised to grade\"),\n",
    "                (\"He gave his coat to the homeless man\", \"The homeless man gave his coat to him\"),\n",
    "                (\"She donated her savings to help\", \"They donated their savings to help her\"),\n",
    "                (\"The host provided shelter to the stranger\", \"The stranger provided shelter to the host\"),\n",
    "            ]\n",
    "            for orig, swap in role_swaps:\n",
    "                if orig in text:\n",
    "                    perturbs.append({\"text\": text.replace(orig, swap), \"type\": \"role_swap\", \"swap\": (orig, swap)})\n",
    "\n",
    "            # Obligation to permission\n",
    "            obl_to_perm = [\n",
    "                (\"must protect\", \"may protect\"),\n",
    "                (\"has a duty to\", \"is allowed to\"),\n",
    "                (\"are required to\", \"are permitted to\"),\n",
    "                (\"swore to\", \"considered whether to\"),\n",
    "                (\"must pay\", \"may pay\"),\n",
    "                (\"agreed to\", \"considered whether to\"),\n",
    "            ]\n",
    "            for obl, perm in obl_to_perm:\n",
    "                if obl in text:\n",
    "                    perturbs.append({\"text\": text.replace(obl, perm), \"type\": \"obligation_to_permission\", \"change\": (obl, perm)})\n",
    "\n",
    "            # Positive to negative (harm introduction)\n",
    "            pos_to_neg = [\n",
    "                (\"helped\", \"refused to help\"),\n",
    "                (\"ruled fairly\", \"ruled unfairly\"),\n",
    "                (\"forgave\", \"refused to forgive\"),\n",
    "                (\"stayed late to comfort\", \"left early despite\"),\n",
    "                (\"donated\", \"hoarded\"),\n",
    "                (\"guided\", \"misled\"),\n",
    "                (\"gave\", \"took\"),\n",
    "                (\"stopped to help\", \"drove past without helping\"),\n",
    "                (\"listened patiently\", \"ignored\"),\n",
    "                (\"gathered to rebuild\", \"refused to rebuild\"),\n",
    "            ]\n",
    "            for pos, neg in pos_to_neg:\n",
    "                if pos in text:\n",
    "                    perturbs.append({\"text\": text.replace(pos, neg), \"type\": \"add_harm\", \"change\": (pos, neg)})\n",
    "\n",
    "            # Violation to fulfillment\n",
    "            viol_to_fulf = [\n",
    "                (\"violated\", \"honored\"),\n",
    "                (\"stole\", \"returned\"),\n",
    "                (\"breaking\", \"keeping\"),\n",
    "                (\"spread false rumors\", \"defended his reputation\"),\n",
    "                (\"broke his campaign promises\", \"kept his campaign promises\"),\n",
    "                (\"poisoned\", \"purified\"),\n",
    "                (\"embezzled\", \"safeguarded\"),\n",
    "                (\"abandoned\", \"cared for\"),\n",
    "                (\"destroyed\", \"preserved\"),\n",
    "            ]\n",
    "            for viol, fulf in viol_to_fulf:\n",
    "                if viol in text:\n",
    "                    perturbs.append({\"text\": text.replace(viol, fulf), \"type\": \"violation_to_fulfillment\", \"change\": (viol, fulf)})\n",
    "\n",
    "            return perturbs\n",
    "\n",
    "        # ====================================================================\n",
    "        # STATISTICAL ANALYSIS FUNCTIONS\n",
    "        # ====================================================================\n",
    "\n",
    "        def bootstrap_ci(data: np.ndarray, n_bootstrap: int = 1000,\n",
    "                        confidence: float = 0.95) -> Tuple[float, float, float]:\n",
    "            \"\"\"Calculate bootstrap confidence interval.\"\"\"\n",
    "            n = len(data)\n",
    "            if n < 2:\n",
    "                return data.mean(), data.mean(), data.mean()\n",
    "\n",
    "            boot_means = []\n",
    "            for _ in range(n_bootstrap):\n",
    "                sample = np.random.choice(data, size=n, replace=True)\n",
    "                boot_means.append(sample.mean())\n",
    "\n",
    "            boot_means = np.array(boot_means)\n",
    "            alpha = (1 - confidence) / 2\n",
    "            lower = np.percentile(boot_means, alpha * 100)\n",
    "            upper = np.percentile(boot_means, (1 - alpha) * 100)\n",
    "\n",
    "            return lower, data.mean(), upper\n",
    "\n",
    "        def effect_size_cohens_d(group1: np.ndarray, group2: np.ndarray) -> float:\n",
    "            \"\"\"Calculate Cohen's d effect size.\"\"\"\n",
    "            n1, n2 = len(group1), len(group2)\n",
    "            var1, var2 = group1.var(), group2.var()\n",
    "            pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n",
    "            return (group1.mean() - group2.mean()) / (pooled_std + 1e-9)\n",
    "\n",
    "        # ====================================================================\n",
    "        # RUN TESTS\n",
    "        # ====================================================================\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "        print(\"RUNNING FUZZ TESTS\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "\n",
    "        # Organize results by perturbation type\n",
    "        results_by_type = {\n",
    "            \"structural_obligation_to_permission\": [],\n",
    "            \"structural_add_harm\": [],\n",
    "            \"structural_role_swap\": [],\n",
    "            \"structural_violation_to_fulfillment\": [],\n",
    "            \"surface_name_change\": [],\n",
    "            \"surface_irrelevant_detail\": [],\n",
    "            \"surface_synonym\": [],\n",
    "        }\n",
    "\n",
    "        all_surface_distances = []\n",
    "        all_structural_distances = []\n",
    "\n",
    "        scenarios_to_run = BASE_SCENARIOS[:RUNTIME['max_scenarios']]\n",
    "        print(f\"Processing {len(scenarios_to_run)} scenarios...\")\n",
    "        print()\n",
    "\n",
    "        for i, scenario in enumerate(scenarios_to_run):\n",
    "            base_emb = get_embedding(scenario[\"text\"])\n",
    "\n",
    "            # Process surface perturbations\n",
    "            surface_perturbs = surface_perturbations(scenario)\n",
    "            for p in surface_perturbs:\n",
    "                dist = cosine_distance(base_emb, get_embedding(p[\"text\"]))\n",
    "                all_surface_distances.append(dist)\n",
    "                key = f\"surface_{p['type']}\"\n",
    "                if key in results_by_type:\n",
    "                    results_by_type[key].append(dist)\n",
    "\n",
    "            # Process structural perturbations\n",
    "            structural_perturbs = structural_perturbations(scenario)\n",
    "            for p in structural_perturbs:\n",
    "                dist = cosine_distance(base_emb, get_embedding(p[\"text\"]))\n",
    "                all_structural_distances.append(dist)\n",
    "                key = f\"structural_{p['type']}\"\n",
    "                if key in results_by_type:\n",
    "                    results_by_type[key].append(dist)\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Processed {i+1}/{len(scenarios_to_run)} scenarios...\")\n",
    "\n",
    "        print()\n",
    "        print(f\"Total surface perturbations: {len(all_surface_distances)}\")\n",
    "        print(f\"Total structural perturbations: {len(all_structural_distances)}\")\n",
    "        print()\n",
    "\n",
    "        # ====================================================================\n",
    "        # DETAILED RESULTS\n",
    "        # ====================================================================\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "        print(\"RESULTS BY PERTURBATION TYPE\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "\n",
    "        fuzz_results = {}\n",
    "\n",
    "        for ptype, distances in results_by_type.items():\n",
    "            if len(distances) > 0:\n",
    "                distances = np.array(distances)\n",
    "                lower, mean, upper = bootstrap_ci(distances, RUNTIME['bootstrap_samples'])\n",
    "                fuzz_results[ptype] = {\n",
    "                    \"mean_distance\": str(mean),\n",
    "                    \"std\": str(distances.std()),\n",
    "                    \"ci_lower\": str(lower),\n",
    "                    \"ci_upper\": str(upper),\n",
    "                    \"n\": len(distances),\n",
    "                }\n",
    "                category = \"STRUCTURAL\" if \"structural\" in ptype else \"SURFACE\"\n",
    "                print(f\"{ptype}:\")\n",
    "                print(f\"  n={len(distances)}, mean={mean:.4f}, std={distances.std():.4f}\")\n",
    "                print(f\"  95% CI: [{lower:.4f}, {upper:.4f}]\")\n",
    "                print()\n",
    "\n",
    "        # ====================================================================\n",
    "        # AGGREGATE COMPARISON\n",
    "        # ====================================================================\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "        print(\"AGGREGATE COMPARISON\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "\n",
    "        surface_arr = np.array(all_surface_distances)\n",
    "        structural_arr = np.array(all_structural_distances)\n",
    "\n",
    "        surf_lower, surf_mean, surf_upper = bootstrap_ci(surface_arr, RUNTIME['bootstrap_samples'])\n",
    "        struct_lower, struct_mean, struct_upper = bootstrap_ci(structural_arr, RUNTIME['bootstrap_samples'])\n",
    "\n",
    "        print(f\"Surface (should be SMALL):\")\n",
    "        print(f\"  mean={surf_mean:.4f}, std={surface_arr.std():.4f}\")\n",
    "        print(f\"  95% CI: [{surf_lower:.4f}, {surf_upper:.4f}]\")\n",
    "        print()\n",
    "        print(f\"Structural (should be LARGE):\")\n",
    "        print(f\"  mean={struct_mean:.4f}, std={structural_arr.std():.4f}\")\n",
    "        print(f\"  95% CI: [{struct_lower:.4f}, {struct_upper:.4f}]\")\n",
    "        print()\n",
    "\n",
    "        # Statistical tests\n",
    "        from scipy import stats\n",
    "        t_stat, p_value = stats.ttest_ind(structural_arr, surface_arr)\n",
    "\n",
    "        # Mann-Whitney U for non-parametric comparison\n",
    "        u_stat, u_pvalue = stats.mannwhitneyu(structural_arr, surface_arr, alternative='greater')\n",
    "\n",
    "        ratio = struct_mean / (surf_mean + 1e-9)\n",
    "        cohens_d = effect_size_cohens_d(structural_arr, surface_arr)\n",
    "\n",
    "        print(f\"Ratio (structural/surface): {ratio:.2f}x\")\n",
    "        print(f\"Cohen's d effect size: {cohens_d:.3f}\")\n",
    "        print(f\"t-statistic: {t_stat:.4f}, p-value: {p_value:.6f}\")\n",
    "        print(f\"Mann-Whitney U: {u_stat:.0f}, p-value: {u_pvalue:.6f}\")\n",
    "        print()\n",
    "\n",
    "        # Store comparison results\n",
    "        fuzz_results[\"comparison\"] = {\n",
    "            \"structural_mean\": str(struct_mean),\n",
    "            \"structural_ci\": [str(struct_lower), str(struct_upper)],\n",
    "            \"surface_mean\": str(surf_mean),\n",
    "            \"surface_ci\": [str(surf_lower), str(surf_upper)],\n",
    "            \"ratio\": str(ratio),\n",
    "            \"cohens_d\": str(cohens_d),\n",
    "            \"t_statistic\": t_stat,\n",
    "            \"p_value\": p_value,\n",
    "            \"mann_whitney_u\": float(u_stat),\n",
    "            \"mann_whitney_p\": float(u_pvalue),\n",
    "            \"n_structural\": len(structural_arr),\n",
    "            \"n_surface\": len(surface_arr),\n",
    "        }\n",
    "\n",
    "        # ====================================================================\n",
    "        # VERDICT (RUNTIME-ADAPTIVE THRESHOLDS)\n",
    "        # ====================================================================\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "        print(\"VERDICT\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "\n",
    "        # Adaptive thresholds based on sample size and runtime\n",
    "        if len(structural_arr) >= 30 and len(surface_arr) >= 30:\n",
    "            # Strong statistical power - use stricter thresholds\n",
    "            strong_ratio = 3.0\n",
    "            moderate_ratio = 2.0\n",
    "            weak_ratio = 1.5\n",
    "            p_threshold = 0.01\n",
    "        elif len(structural_arr) >= 15:\n",
    "            # Medium statistical power\n",
    "            strong_ratio = 2.5\n",
    "            moderate_ratio = 1.8\n",
    "            weak_ratio = 1.3\n",
    "            p_threshold = 0.05\n",
    "        else:\n",
    "            # Low statistical power - use looser thresholds but note uncertainty\n",
    "            strong_ratio = 2.0\n",
    "            moderate_ratio = 1.5\n",
    "            weak_ratio = 1.2\n",
    "            p_threshold = 0.10\n",
    "\n",
    "        verdict = \"NOT_SUPPORTED\"\n",
    "        verdict_detail = \"\"\n",
    "\n",
    "        if ratio >= strong_ratio and p_value < p_threshold and cohens_d > 0.8:\n",
    "            verdict = \"STRONG_SUPPORT\"\n",
    "            verdict_detail = f\"Model learned moral structure (ratio={ratio:.1f}x, d={cohens_d:.2f}, p={p_value:.4f})\"\n",
    "        elif ratio >= moderate_ratio and p_value < 0.05 and cohens_d > 0.5:\n",
    "            verdict = \"MODERATE_SUPPORT\"\n",
    "            verdict_detail = f\"Evidence for moral structure (ratio={ratio:.1f}x, d={cohens_d:.2f})\"\n",
    "        elif ratio >= weak_ratio and p_value < 0.10:\n",
    "            verdict = \"WEAK_SUPPORT\"\n",
    "            verdict_detail = f\"Weak evidence (ratio={ratio:.1f}x, needs more data)\"\n",
    "        else:\n",
    "            verdict = \"NOT_SUPPORTED\"\n",
    "            verdict_detail = \"May be encoding surface features rather than moral structure\"\n",
    "\n",
    "        print(f\"Verdict: {verdict}\")\n",
    "        print(f\"Detail: {verdict_detail}\")\n",
    "        print()\n",
    "        print(f\"Runtime: {RUNTIME['gpu_type']}\")\n",
    "        print(f\"Thresholds used: strong>{strong_ratio}x, moderate>{moderate_ratio}x, p<{p_threshold}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        fuzz_results[\"verdict\"] = verdict\n",
    "        fuzz_results[\"verdict_detail\"] = verdict_detail\n",
    "        fuzz_results[\"runtime\"] = RUNTIME\n",
    "\n",
    "        # Make results available for integration\n",
    "        FUZZ_RESULTS_V1011 = fuzz_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 9. Save & Download Results { display-mode: \"form\" }\n",
    "# @markdown Persist results to Google Drive and optionally download as zip\n",
    "\n",
    "import shutil\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Always persist results to Drive\n",
    "if SAVE_DIR and os.path.exists(SAVE_DIR):\n",
    "    print(f\"\\nPersisting to: {SAVE_DIR}\")\n",
    "\n",
    "    # Save final results JSON\n",
    "    if os.path.exists(\"results/final_results.json\"):\n",
    "        dest = f\"{SAVE_DIR}/final_results.json\"\n",
    "        shutil.copy(\"results/final_results.json\", dest)\n",
    "        print(f\"  Saved: final_results.json\")\n",
    "\n",
    "    # Save splits config\n",
    "    if os.path.exists(\"data/splits/all_splits.json\"):\n",
    "        dest = f\"{SAVE_DIR}/all_splits.json\"\n",
    "        shutil.copy(\"data/splits/all_splits.json\", dest)\n",
    "        print(f\"  Saved: all_splits.json\")\n",
    "\n",
    "    # Models are already saved to SAVE_DIR during training\n",
    "    model_files = [f for f in os.listdir(SAVE_DIR) if f.endswith(\".pt\")]\n",
    "    if model_files:\n",
    "        print(f\"  Models already in Drive: {len(model_files)} files\")\n",
    "        for mf in model_files[:5]:\n",
    "            print(f\"    - {mf}\")\n",
    "        if len(model_files) > 5:\n",
    "            print(f\"    ... and {len(model_files)-5} more\")\n",
    "\n",
    "    print(f\"\\nResults persisted to Google Drive: {SAVE_DIR}\")\n",
    "else:\n",
    "    print(\"WARNING: SAVE_DIR not available, results only in local directories\")\n",
    "\n",
    "# Optional: Create download zip\n",
    "if CREATE_DOWNLOAD_ZIP:\n",
    "    import zipfile\n",
    "\n",
    "    zip_path = \"BIP_v10.10_results.zip\"\n",
    "    print(f\"\\n\" + \"-\" * 60)\n",
    "    print(\"Creating download package...\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "        # Results\n",
    "        if os.path.exists(\"results/final_results.json\"):\n",
    "            zf.write(\"results/final_results.json\")\n",
    "\n",
    "        # Models (from Drive)\n",
    "        if SAVE_DIR and os.path.exists(SAVE_DIR):\n",
    "            for f in os.listdir(SAVE_DIR):\n",
    "                if f.endswith(\".pt\"):\n",
    "                    zf.write(f\"{SAVE_DIR}/{f}\", f\"models/{f}\")\n",
    "\n",
    "        # Config\n",
    "        if os.path.exists(\"data/splits/all_splits.json\"):\n",
    "            zf.write(\"data/splits/all_splits.json\")\n",
    "\n",
    "    print(f\"Download package ready: {zip_path}\")\n",
    "\n",
    "    # Download in Colab, or show path otherwise\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(zip_path)\n",
    "    except ImportError:\n",
    "        print(f\"Not running in Colab. Zip saved to: {os.path.abspath(zip_path)}\")\n",
    "else:\n",
    "    print(f\"\\n(Zip download disabled - set CREATE_DOWNLOAD_ZIP=True in cell 1 to enable)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    ""
   ]
  }
 ]
}