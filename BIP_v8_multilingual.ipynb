{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# BIP Cross-Temporal & Cross-Lingual Morality Experiment (v8)\n\n**Testing the Bond Invariance Principle across 2500+ years and 6 languages**\n\nThis experiment tests whether moral cognition has invariant structure by training and testing across:\n\n| Corpus | Language(s) | Time Period | Passages |\n|--------|-------------|-------------|----------|\n| **Sefaria** | Hebrew, Aramaic, Judeo-Arabic | ~1000 BCE - 1900 CE | ~200K+ |\n| **Chinese Classics** | Classical Chinese | ~500 BCE - 200 CE | ~15K |\n| **Islamic Texts** | Arabic | ~600 - 1300 CE | ~20K |\n| **Dear Abby** | English | 1956 - 2020 | ~50K |\n\n**Hypothesis**: If BIP holds, bond-level features should transfer across millennia AND languages with minimal degradation.\n\n---\n\n## v8 Changes\n- **6 languages**: Hebrew, Aramaic, Judeo-Arabic, Classical Chinese, Arabic, English\n- **New corpora**: Chinese Text Project classics, Quran + Hadith\n- **Language-based splits**: Train on one language family, test on another\n- **Full Sefaria**: All categories with proper language detection\n- **L4 optimized**: Large batch sizes, full corpus processing\n\n---\n\n## Methodological Notes\n\n**Label Source**: Bond/Hohfeld labels extracted from English translations for all corpora. This tests whether diverse source texts encode moral structures alignable via translation.\n\n**Encoder**: `paraphrase-multilingual-MiniLM-L12-v2` - trained on 50+ languages including all target languages.\n\n---"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 1. Setup and Install Dependencies { display-mode: \"form\" }\n#@markdown Installs packages and detects GPU. Tuned for L4 runtime.\n\nimport time\nEXPERIMENT_START = time.time()\n\nprint(\"=\" * 60)\nprint(\"BIP MULTILINGUAL EXPERIMENT (v8)\")\nprint(\"Cross-Temporal & Cross-Lingual Moral Transfer\")\nprint(\"=\" * 60)\nprint()\n\n# Progress tracker\nTASKS = [\n    \"Install dependencies\",\n    \"Download Sefaria corpus (Hebrew/Aramaic)\",\n    \"Download Chinese classics\",\n    \"Download Islamic texts (Quran + Hadith)\",\n    \"Download Dear Abby (English)\",\n    \"Preprocess all corpora\",\n    \"Extract bond structures\",\n    \"Generate splits (temporal + lingual)\",\n    \"Train BIP model\",\n    \"Linear probe test\",\n    \"Evaluate results\"\n]\ntask_status = {task: \"pending\" for task in TASKS}\ntask_times = {}\ntask_start_time = None\n\ndef print_progress():\n    print()\n    print(\"-\" * 50)\n    print(\"EXPERIMENT PROGRESS:\")\n    print(\"-\" * 50)\n    for task in TASKS:\n        status = task_status[task]\n        if status == \"done\":\n            mark = \"[X]\"\n            time_str = f\" ({task_times.get(task, 0):.1f}s)\" if task in task_times else \"\"\n        elif status == \"running\":\n            mark = \"[>]\"\n            time_str = \"\"\n        else:\n            mark = \"[ ]\"\n            time_str = \"\"\n        print(f\"  {mark} {task}{time_str}\")\n    elapsed = time.time() - EXPERIMENT_START\n    print(\"-\" * 50)\n    print(f\"  Total elapsed: {elapsed/60:.1f} minutes\")\n    print(flush=True)\n\ndef mark_task(task, status):\n    global task_start_time\n    if status == \"running\":\n        task_start_time = time.time()\n    elif status == \"done\" and task_start_time is not None:\n        task_times[task] = time.time() - task_start_time\n    task_status[task] = status\n    print_progress()\n\nprint_progress()\n\nmark_task(\"Install dependencies\", \"running\")\n\nimport os\nimport subprocess\nimport sys\n\n# Install dependencies\nprint(\"Installing dependencies...\")\ndeps = [\n    \"transformers\",\n    \"torch\", \n    \"sentence-transformers\",\n    \"pandas\",\n    \"tqdm\",\n    \"psutil\",\n    \"scikit-learn\",\n    \"requests\",\n    \"beautifulsoup4\",\n    \"lxml\"\n]\n\nfor dep in deps:\n    print(f\"  Installing {dep}...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", dep])\n\nprint()\n\n# Detect accelerator\nUSE_TPU = False\nTPU_TYPE = None\n\nif 'COLAB_TPU_ADDR' in os.environ:\n    USE_TPU = True\n    TPU_TYPE = \"TPU (Colab)\"\n    print(\"TPU detected!\")\n\nimport torch\nimport json\nimport psutil\nimport shutil\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    ACCELERATOR = f\"GPU: {gpu_name} ({gpu_mem:.1f}GB)\"\n    device = torch.device(\"cuda\")\n    \n    IS_L4 = 'L4' in gpu_name\n    IS_A100 = 'A100' in gpu_name\n    IS_V100 = 'V100' in gpu_name\nelif USE_TPU:\n    ACCELERATOR = TPU_TYPE\n    import torch_xla.core.xla_model as xm\n    device = xm.xla_device()\n    IS_L4, IS_A100, IS_V100 = False, False, False\nelse:\n    ACCELERATOR = \"CPU (slow!)\"\n    device = torch.device(\"cpu\")\n    IS_L4, IS_A100, IS_V100 = False, False, False\n\nprint(f\"Accelerator: {ACCELERATOR}\")\nprint(f\"Device: {device}\")\n\n# System resources\nmem = psutil.virtual_memory()\ndisk = shutil.disk_usage('/')\nprint(f\"System RAM: {mem.used/1e9:.1f}/{mem.total/1e9:.1f} GB ({mem.percent}%)\")\nprint(f\"Disk: {disk.used/1e9:.1f}/{disk.total/1e9:.1f} GB ({100*disk.used/disk.total:.1f}%)\")\n\nif torch.cuda.is_available():\n    gpu_used = torch.cuda.memory_allocated()/1e9\n    gpu_total = torch.cuda.get_device_properties(0).total_memory/1e9\n    print(f\"GPU RAM: {gpu_used:.1f}/{gpu_total:.1f} GB\")\n\n# Batch sizes based on GPU\nif IS_L4 or IS_A100:\n    BASE_BATCH_SIZE = 512\n    print(f\"\\n*** L4/A100: batch_size={BASE_BATCH_SIZE} ***\")\nelif IS_V100:\n    BASE_BATCH_SIZE = 384\nelse:\n    BASE_BATCH_SIZE = 256\n\n# Mixed precision\nif torch.cuda.is_available():\n    print(\"Enabling mixed precision (FP16)...\")\n    from torch.cuda.amp import autocast, GradScaler\n    USE_AMP = True\n    scaler = GradScaler()\nelse:\n    USE_AMP = False\n    scaler = None\n\n# Google Drive mount\nprint()\nprint(\"=\" * 60)\nprint(\"MOUNTING GOOGLE DRIVE\")\nprint(\"=\" * 60)\nfrom google.colab import drive\ndrive.mount('/content/drive')\nSAVE_DIR = '/content/drive/MyDrive/BIP_multilingual_results'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\nfor d in [\"data/processed\", \"data/splits\", \"data/raw\", \"models/checkpoints\", \"results\"]:\n    os.makedirs(d, exist_ok=True)\n\ndef print_resources(label=\"\"):\n    mem = psutil.virtual_memory()\n    disk = shutil.disk_usage('/')\n    msg = f\"[{label}] \" if label else \"\"\n    msg += f\"RAM: {mem.used/1e9:.1f}/{mem.total/1e9:.1f}GB\"\n    if torch.cuda.is_available():\n        gpu_used = torch.cuda.memory_allocated()/1e9\n        gpu_total = torch.cuda.get_device_properties(0).total_memory/1e9\n        msg += f\" | GPU: {gpu_used:.1f}/{gpu_total:.1f}GB\"\n    msg += f\" | Disk: {disk.used/1e9:.0f}/{disk.total/1e9:.0f}GB\"\n    print(msg)\n\nprint_resources(\"Setup complete\")\n\nmark_task(\"Install dependencies\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 2. Download Sefaria Corpus (Hebrew/Aramaic/Judeo-Arabic) { display-mode: \"form\" }\n#@markdown Full Sefaria corpus with language detection.\n\nimport subprocess\nimport os\n\nmark_task(\"Download Sefaria corpus (Hebrew/Aramaic)\", \"running\")\n\nsefaria_path = 'data/raw/Sefaria-Export'\n\nif not os.path.exists(sefaria_path) or not os.path.exists(f\"{sefaria_path}/json\"):\n    print(\"=\"*60)\n    print(\"CLONING SEFARIA CORPUS\")\n    print(\"=\"*60)\n    print(\"This downloads ~3.5GB and takes 5-15 minutes.\")\n    print(\"-\"*60)\n    \n    process = subprocess.Popen(\n        ['git', 'clone', '--depth', '1', '--progress',\n         'https://github.com/Sefaria/Sefaria-Export.git', sefaria_path],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True,\n        bufsize=1\n    )\n    \n    for line in process.stdout:\n        print(line, end='', flush=True)\n    process.wait()\n    print(\"-\"*60)\nelse:\n    print(\"Sefaria already exists, skipping download.\")\n\nprint(\"\\nVerifying...\")\n!du -sh {sefaria_path} 2>/dev/null || echo \"Not found\"\njson_count = !find {sefaria_path}/json -name \"*.json\" 2>/dev/null | wc -l\nprint(f\"JSON files: {json_count[0]}\")\n\nprint_resources(\"After Sefaria\")\n\nmark_task(\"Download Sefaria corpus (Hebrew/Aramaic)\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 3. Download Chinese Classics (Confucian/Daoist) { display-mode: \"form\" }\n#@markdown Downloads classical Chinese texts with English translations.\n#@markdown Sources: Analerta, Dao De Jing, Mencius, etc.\n\nimport os\nimport json\nimport requests\nfrom pathlib import Path\n\nmark_task(\"Download Chinese classics\", \"running\")\n\nchinese_dir = Path('data/raw/chinese_classics')\nchinese_dir.mkdir(parents=True, exist_ok=True)\n\nprint(\"=\"*60)\nprint(\"DOWNLOADING CHINESE CLASSICS\")\nprint(\"=\"*60)\nprint()\n\n# We'll create a curated dataset of classical Chinese texts with translations\n# Using publicly available bilingual editions\n\nCHINESE_TEXTS = {\n    'analects': {\n        'title': 'Analects of Confucius (論語)',\n        'period': 'CONFUCIAN',\n        'century': -5,\n        'url': 'https://raw.githubusercontent.com/ctext-org/ctext-data/master/analects.json'\n    },\n    'daodejing': {\n        'title': 'Dao De Jing (道德經)',\n        'period': 'DAOIST', \n        'century': -6,\n        'url': 'https://raw.githubusercontent.com/ctext-org/ctext-data/master/daodejing.json'\n    },\n    'mencius': {\n        'title': 'Mencius (孟子)',\n        'period': 'CONFUCIAN',\n        'century': -4,\n        'url': 'https://raw.githubusercontent.com/ctext-org/ctext-data/master/mencius.json'\n    },\n    'xunzi': {\n        'title': 'Xunzi (荀子)',\n        'period': 'CONFUCIAN',\n        'century': -3,\n        'url': 'https://raw.githubusercontent.com/ctext-org/ctext-data/master/xunzi.json'\n    },\n    'zhuangzi': {\n        'title': 'Zhuangzi (莊子)',\n        'period': 'DAOIST',\n        'century': -4,\n        'url': 'https://raw.githubusercontent.com/ctext-org/ctext-data/master/zhuangzi.json'\n    }\n}\n\n# Fallback: Create synthetic bilingual corpus from available sources\n# Since ctext-org data might not be directly available, we'll use a backup approach\n\ndef download_chinese_texts():\n    \"\"\"Download or generate Chinese classics corpus.\"\"\"\n    passages = []\n    \n    # Try to download from ctext-org first\n    for text_id, info in CHINESE_TEXTS.items():\n        print(f\"Processing {info['title']}...\")\n        \n        # Try GitHub raw content\n        try:\n            resp = requests.get(info['url'], timeout=30)\n            if resp.status_code == 200:\n                data = resp.json()\n                # Process ctext format\n                for i, entry in enumerate(data.get('passages', data if isinstance(data, list) else [])):\n                    if isinstance(entry, dict):\n                        chinese = entry.get('chinese', entry.get('text', ''))\n                        english = entry.get('english', entry.get('translation', ''))\n                    else:\n                        continue\n                    \n                    if chinese and english and len(english) >= 30:\n                        passages.append({\n                            'id': f'chinese_{text_id}_{i}',\n                            'text_original': chinese,\n                            'text_english': english,\n                            'source': info['title'],\n                            'period': info['period'],\n                            'century': info['century']\n                        })\n                print(f\"  Downloaded {len([p for p in passages if text_id in p['id']])} passages\")\n                continue\n        except Exception as e:\n            print(f\"  Direct download failed: {e}\")\n        \n        # Fallback: Use embedded sample data for core texts\n        print(f\"  Using embedded samples...\")\n    \n    # If downloads failed, use embedded classical Chinese samples\n    if len(passages) < 100:\n        print(\"\\nUsing embedded Chinese classics dataset...\")\n        passages = generate_embedded_chinese_corpus()\n    \n    return passages\n\ndef generate_embedded_chinese_corpus():\n    \"\"\"Generate corpus from well-known passages with translations.\"\"\"\n    # Core Analects passages (Confucius)\n    analects = [\n        (\"學而時習之，不亦說乎？有朋自遠方來，不亦樂乎？人不知而不慍，不亦君子乎？\",\n         \"Is it not pleasant to learn with a constant perseverance and application? Is it not delightful to have friends coming from distant quarters? Is he not a man of complete virtue, who feels no discomposure though men may take no note of him?\"),\n        (\"其為人也孝弟，而好犯上者，鮮矣；不好犯上，而好作亂者，未之有也。君子務本，本立而道生。孝弟也者，其為仁之本與！\",\n         \"They are few who, being filial and fraternal, are fond of offending against their superiors. There have been none, who, not liking to offend against their superiors, have been fond of stirring up confusion. The superior man bends his attention to what is radical. That being established, all practical courses naturally grow up. Filial piety and fraternal submission are the root of all benevolent actions.\"),\n        (\"巧言令色，鮮矣仁！\",\n         \"Fine words and an insinuating appearance are seldom associated with true virtue.\"),\n        (\"吾日三省吾身：為人謀而不忠乎？與朋友交而不信乎？傳不習乎？\",\n         \"I daily examine myself on three points: whether in transacting business for others I have been faithful; whether in intercourse with friends I have been sincere; whether I have mastered and practiced the instructions of my teacher.\"),\n        (\"弟子入則孝，出則弟，謹而信，汎愛眾，而親仁。行有餘力，則以學文。\",\n         \"A youth, when at home, should be filial, and, abroad, respectful to his elders. He should be earnest and truthful. He should overflow in love to all, and cultivate the friendship of the good. When he has time and opportunity, after the performance of these things, he should employ them in polite studies.\"),\n        (\"君子不重則不威，學則不固。主忠信，無友不如己者，過則勿憚改。\",\n         \"If the scholar be not grave, he will not call forth any veneration, and his learning will not be solid. Hold faithfulness and sincerity as first principles. Have no friends not equal to yourself. When you have faults, do not fear to abandon them.\"),\n        (\"父在，觀其志；父沒，觀其行；三年無改於父之道，可謂孝矣。\",\n         \"While a man's father is alive, look at the bent of his will; when his father is dead, look at his conduct. If for three years he does not alter from the way of his father, he may be called filial.\"),\n        (\"禮之用，和為貴。先王之道，斯為美；小大由之。有所不行，知和而和，不以禮節之，亦不可行也。\",\n         \"In practicing the rules of propriety, a natural ease is to be prized. In the ways prescribed by the ancient kings, this is the excellent quality, and in things small and great we follow them. Yet it is not to be observed in all cases. If one, knowing how such ease should be prized, manifests it without regulating it by the rules of propriety, this likewise is not to be done.\"),\n        (\"信近於義，言可復也。恭近於禮，遠恥辱也。因不失其親，亦可宗也。\",\n         \"When agreements are made according to what is right, what is spoken can be made good. When respect is shown according to what is proper, one keeps far from shame and disgrace. When the parties upon whom a man leans are proper persons to be intimate with, he can make them his guides and masters.\"),\n        (\"君子食無求飽，居無求安，敏於事而慎於言，就有道而正焉，可謂好學也已。\",\n         \"The superior man, in the world, does not set his mind either for anything, or against anything; what is right he will follow. The superior man thinks of virtue; the small man thinks of comfort.\"),\n    ]\n    \n    # Dao De Jing passages (Laozi)\n    daodejing = [\n        (\"道可道，非常道。名可名，非常名。無名天地之始；有名萬物之母。\",\n         \"The Tao that can be told is not the eternal Tao. The name that can be named is not the eternal name. The nameless is the beginning of heaven and earth. The named is the mother of ten thousand things.\"),\n        (\"天下皆知美之為美，斯惡已。皆知善之為善，斯不善已。\",\n         \"When people see some things as beautiful, other things become ugly. When people see some things as good, other things become bad.\"),\n        (\"不尚賢，使民不爭；不貴難得之貨，使民不為盜；不見可欲，使民心不亂。\",\n         \"Not exalting the gifted prevents quarreling. Not collecting treasures prevents stealing. Not seeing desirable things prevents confusion of the heart.\"),\n        (\"道沖而用之或不盈。淵兮似萬物之宗。\",\n         \"The Tao is an empty vessel; it is used, but never filled. Oh, unfathomable source of ten thousand things!\"),\n        (\"天地不仁，以萬物為芻狗；聖人不仁，以百姓為芻狗。\",\n         \"Heaven and Earth are impartial; they see the ten thousand things as straw dogs. The wise are impartial; they see the people as straw dogs.\"),\n        (\"上善若水。水善利萬物而不爭，處眾人之所惡，故幾於道。\",\n         \"The highest good is like water. Water gives life to the ten thousand things and does not strive. It flows in places men reject and so is like the Tao.\"),\n        (\"持而盈之，不如其已；揣而銳之，不可長保。金玉滿堂，莫之能守；富貴而驕，自遺其咎。功成身退，天之道。\",\n         \"Fill your bowl to the brim and it will spill. Keep sharpening your knife and it will blunt. Chase after money and security and your heart will never unclench. Care about people's approval and you will be their prisoner. Do your work, then step back. The only path to serenity.\"),\n        (\"三十輻共一轂，當其無，有車之用。埏埴以為器，當其無，有器之用。鑿戶牖以為室，當其無，有室之用。故有之以為利，無之以為用。\",\n         \"Thirty spokes share the wheel's hub; it is the center hole that makes it useful. Shape clay into a vessel; it is the space within that makes it useful. Cut doors and windows for a room; it is the holes which make it useful. Therefore benefit comes from what is there; usefulness from what is not there.\"),\n        (\"五色令人目盲；五音令人耳聾；五味令人口爽；馳騁畋獵，令人心發狂；難得之貨，令人行妨。\",\n         \"Colors blind the eye. Sounds deafen the ear. Flavors numb the taste. Thoughts weaken the mind. Desires wither the heart.\"),\n        (\"大道廢，有仁義；智慧出，有大偽；六親不和，有孝慈；國家昏亂，有忠臣。\",\n         \"When the great Tao is forgotten, goodness and piety appear. When the body's intelligence declines, cleverness and knowledge step forth. When there is no peace in the family, filial piety begins. When the country falls into chaos, patriotism is born.\"),\n    ]\n    \n    # Mencius passages\n    mencius = [\n        (\"孟子見梁惠王。王曰：叟不遠千里而來，亦將有以利吾國乎？孟子對曰：王何必曰利？亦有仁義而已矣。\",\n         \"Mencius went to see king Hui of Liang. The king said, 'Venerable sir, since you have not counted it far to come here, a distance of a thousand li, may I presume that you are provided with counsels to profit my kingdom?' Mencius replied, 'Why must your Majesty use that word profit? What I am provided with, are counsels to benevolence and righteousness, and these are my only topics.'\"),\n        (\"人皆有不忍人之心。先王有不忍人之心，斯有不忍人之政矣。\",\n         \"All men have a mind which cannot bear to see the sufferings of others. The ancient kings had this commiserating mind, and they had likewise a commiserating government.\"),\n        (\"惻隱之心，仁之端也；羞惡之心，義之端也；辭讓之心，禮之端也；是非之心，智之端也。\",\n         \"The feeling of commiseration is the principle of benevolence. The feeling of shame and dislike is the principle of righteousness. The feeling of modesty and complaisance is the principle of propriety. The feeling of approving and disapproving is the principle of knowledge.\"),\n        (\"人之所以異於禽獸者幾希，庶民去之，君子存之。\",\n         \"That whereby man differs from the lower animals is but small. The mass of people cast it away, while superior men preserve it.\"),\n        (\"得道者多助，失道者寡助。寡助之至，親戚畔之；多助之至，天下順之。\",\n         \"When one by his dao commands much support, the whole kingdom will submit to him. When one by his dao commands little support, even his relatives will turn away from him.\"),\n    ]\n    \n    # Zhuangzi passages\n    zhuangzi = [\n        (\"北冥有魚，其名為鯤。鯤之大，不知其幾千里也。化而為鳥，其名為鵬。鵬之背，不知其幾千里也。\",\n         \"In the northern darkness there is a fish and his name is Kun. The Kun is so huge I don't know how many thousand li he measures. He changes and becomes a bird whose name is Peng. The back of the Peng measures I don't know how many thousand li across.\"),\n        (\"昔者莊周夢為胡蝶，栩栩然胡蝶也，自喻適志與！不知周也。俄然覺，則蘧蘧然周也。不知周之夢為胡蝶與，胡蝶之夢為周與？\",\n         \"Once Zhuang Zhou dreamed he was a butterfly, a butterfly flitting and fluttering around, happy with himself and doing as he pleased. He didn't know he was Zhuang Zhou. Suddenly he woke up and there he was, solid and unmistakable Zhuang Zhou. But he didn't know if he was Zhuang Zhou who had dreamed he was a butterfly, or a butterfly dreaming he was Zhuang Zhou.\"),\n        (\"吾生也有涯，而知也無涯。以有涯隨無涯，殆已！\",\n         \"Your life has a limit but knowledge has none. If you use what is limited to pursue what has no limit, you will be in danger.\"),\n        (\"泉涸，魚相與處於陸，相呴以濕，相濡以沫，不如相忘於江湖。\",\n         \"When the springs dry up and the fish are left stranded on the ground, they spew each other with moisture and wet each other down with spit - but it would be much better if they could forget each other in the rivers and lakes.\"),\n        (\"人皆知有用之用，而莫知無用之用也。\",\n         \"Everyone knows the usefulness of what is useful, but few know the usefulness of what is useless.\"),\n    ]\n    \n    passages = []\n    \n    for i, (chinese, english) in enumerate(analects):\n        passages.append({\n            'id': f'chinese_analects_{i}',\n            'text_original': chinese,\n            'text_english': english,\n            'source': 'Analects of Confucius (論語)',\n            'period': 'CONFUCIAN',\n            'century': -5\n        })\n    \n    for i, (chinese, english) in enumerate(daodejing):\n        passages.append({\n            'id': f'chinese_daodejing_{i}',\n            'text_original': chinese,\n            'text_english': english,\n            'source': 'Dao De Jing (道德經)',\n            'period': 'DAOIST',\n            'century': -6\n        })\n    \n    for i, (chinese, english) in enumerate(mencius):\n        passages.append({\n            'id': f'chinese_mencius_{i}',\n            'text_original': chinese,\n            'text_english': english,\n            'source': 'Mencius (孟子)',\n            'period': 'CONFUCIAN',\n            'century': -4\n        })\n    \n    for i, (chinese, english) in enumerate(zhuangzi):\n        passages.append({\n            'id': f'chinese_zhuangzi_{i}',\n            'text_original': chinese,\n            'text_english': english,\n            'source': 'Zhuangzi (莊子)',\n            'period': 'DAOIST',\n            'century': -4\n        })\n    \n    return passages\n\n# Download/generate Chinese corpus\nchinese_passages = download_chinese_texts()\n\nprint(f\"\\nTotal Chinese passages: {len(chinese_passages)}\")\n\n# Save to file\nwith open(chinese_dir / 'chinese_classics.json', 'w', encoding='utf-8') as f:\n    json.dump(chinese_passages, f, ensure_ascii=False, indent=2)\n\nprint(f\"Saved to {chinese_dir / 'chinese_classics.json'}\")\n\nprint_resources(\"After Chinese\")\n\nmark_task(\"Download Chinese classics\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 4. Download Islamic Texts (Quran + Hadith) { display-mode: \"form\" }\n#@markdown Downloads Quran and major Hadith collections with English translations.\n\nimport os\nimport json\nimport requests\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\nmark_task(\"Download Islamic texts (Quran + Hadith)\", \"running\")\n\nislamic_dir = Path('data/raw/islamic_texts')\nislamic_dir.mkdir(parents=True, exist_ok=True)\n\nprint(\"=\"*60)\nprint(\"DOWNLOADING ISLAMIC TEXTS\")\nprint(\"=\"*60)\nprint()\n\ndef download_quran():\n    \"\"\"Download Quran with English translation from tanzil.net or similar.\"\"\"\n    passages = []\n    \n    # Try quran.com API or tanzil.net\n    print(\"Downloading Quran...\")\n    \n    # Using the Quran.com API v4\n    try:\n        # Get all surahs\n        for surah_num in tqdm(range(1, 115), desc=\"Surahs\"):\n            # Arabic text\n            arabic_url = f\"https://api.quran.com/api/v4/quran/verses/uthmani?chapter_number={surah_num}\"\n            # English translation (Sahih International)\n            english_url = f\"https://api.quran.com/api/v4/quran/translations/131?chapter_number={surah_num}\"\n            \n            try:\n                arabic_resp = requests.get(arabic_url, timeout=30)\n                english_resp = requests.get(english_url, timeout=30)\n                \n                if arabic_resp.status_code == 200 and english_resp.status_code == 200:\n                    arabic_data = arabic_resp.json()\n                    english_data = english_resp.json()\n                    \n                    arabic_verses = {v['verse_key']: v['text_uthmani'] for v in arabic_data.get('verses', [])}\n                    english_verses = {t['verse_key']: t['text'] for t in english_data.get('translations', [])}\n                    \n                    for verse_key in arabic_verses:\n                        if verse_key in english_verses:\n                            arabic = arabic_verses[verse_key]\n                            english = english_verses[verse_key]\n                            # Clean HTML from translation\n                            english = english.replace('<sup', ' <sup').replace('</sup>', '')\n                            import re\n                            english = re.sub(r'<[^>]+>', '', english).strip()\n                            \n                            if len(english) >= 20:\n                                passages.append({\n                                    'id': f'quran_{verse_key.replace(\":\", \"_\")}',\n                                    'text_original': arabic,\n                                    'text_english': english,\n                                    'source': f'Quran {verse_key}',\n                                    'period': 'QURANIC',\n                                    'century': 7\n                                })\n            except Exception as e:\n                continue\n            \n            # Rate limiting\n            import time\n            time.sleep(0.1)\n    \n    except Exception as e:\n        print(f\"Quran API failed: {e}\")\n    \n    print(f\"  Downloaded {len(passages)} Quran verses\")\n    return passages\n\ndef download_hadith():\n    \"\"\"Download Hadith from sunnah.com API.\"\"\"\n    passages = []\n    \n    print(\"\\nDownloading Hadith collections...\")\n    \n    # Major collections to download\n    collections = [\n        ('bukhari', 'Sahih Bukhari', 97),  # ~97 books\n        ('muslim', 'Sahih Muslim', 56),    # ~56 books  \n        ('abudawud', 'Sunan Abu Dawud', 43),\n        ('tirmidhi', 'Jami at-Tirmidhi', 49),\n    ]\n    \n    for collection_name, collection_title, n_books in collections:\n        print(f\"  {collection_title}...\")\n        collection_passages = []\n        \n        # Try sunnah.com API\n        try:\n            for book_num in range(1, min(n_books + 1, 20)):  # Limit to first 20 books per collection\n                url = f\"https://api.sunnah.com/v1/collections/{collection_name}/books/{book_num}/hadiths\"\n                headers = {'X-API-Key': 'SqD712P3E82xnwOAEOkGd5JZH8s9wRR24TqNFzjk'}  # Public demo key\n                \n                try:\n                    resp = requests.get(url, headers=headers, timeout=30)\n                    if resp.status_code == 200:\n                        data = resp.json()\n                        for hadith in data.get('data', []):\n                            arabic = hadith.get('arabicText', '')\n                            english = hadith.get('englishText', '')\n                            \n                            if arabic and english and len(english) >= 50:\n                                collection_passages.append({\n                                    'id': f'hadith_{collection_name}_{hadith.get(\"hadithNumber\", len(collection_passages))}',\n                                    'text_original': arabic,\n                                    'text_english': english,\n                                    'source': f'{collection_title} {hadith.get(\"hadithNumber\", \"\")}',\n                                    'period': 'HADITH',\n                                    'century': 9  # Most compiled ~9th century\n                                })\n                except:\n                    continue\n                \n                import time\n                time.sleep(0.2)\n        \n        except Exception as e:\n            print(f\"    API failed: {e}\")\n        \n        passages.extend(collection_passages)\n        print(f\"    Got {len(collection_passages)} hadith\")\n    \n    return passages\n\ndef generate_embedded_islamic_corpus():\n    \"\"\"Fallback: embedded Islamic text samples.\"\"\"\n    passages = []\n    \n    # Key Quranic verses on moral themes\n    quran_samples = [\n        (\"بِسْمِ اللَّهِ الرَّحْمَٰنِ الرَّحِيمِ\", \"In the name of Allah, the Entirely Merciful, the Especially Merciful.\", \"1:1\"),\n        (\"وَقَضَىٰ رَبُّكَ أَلَّا تَعْبُدُوا إِلَّا إِيَّاهُ وَبِالْوَالِدَيْنِ إِحْسَانًا\", \"And your Lord has decreed that you not worship except Him, and to parents, good treatment.\", \"17:23\"),\n        (\"وَلَا تَقْتُلُوا النَّفْسَ الَّتِي حَرَّمَ اللَّهُ إِلَّا بِالْحَقِّ\", \"And do not kill the soul which Allah has forbidden, except by right.\", \"17:33\"),\n        (\"وَأَوْفُوا بِالْعَهْدِ إِنَّ الْعَهْدَ كَانَ مَسْئُولًا\", \"And fulfill every commitment. Indeed, the commitment is ever that about which one will be questioned.\", \"17:34\"),\n        (\"وَلَا تَقْرَبُوا مَالَ الْيَتِيمِ إِلَّا بِالَّتِي هِيَ أَحْسَنُ\", \"And do not approach the property of an orphan, except in the way that is best.\", \"17:34\"),\n        (\"إِنَّ اللَّهَ يَأْمُرُ بِالْعَدْلِ وَالْإِحْسَانِ وَإِيتَاءِ ذِي الْقُرْبَىٰ\", \"Indeed, Allah orders justice and good conduct and giving to relatives.\", \"16:90\"),\n        (\"يَا أَيُّهَا الَّذِينَ آمَنُوا كُونُوا قَوَّامِينَ بِالْقِسْطِ شُهَدَاءَ لِلَّهِ\", \"O you who have believed, be persistently standing firm in justice, witnesses for Allah.\", \"4:135\"),\n        (\"وَتَعَاوَنُوا عَلَى الْبِرِّ وَالتَّقْوَىٰ وَلَا تَعَاوَنُوا عَلَى الْإِثْمِ وَالْعُدْوَانِ\", \"And cooperate in righteousness and piety, but do not cooperate in sin and aggression.\", \"5:2\"),\n        (\"مَنْ قَتَلَ نَفْسًا بِغَيْرِ نَفْسٍ أَوْ فَسَادٍ فِي الْأَرْضِ فَكَأَنَّمَا قَتَلَ النَّاسَ جَمِيعًا\", \"Whoever kills a soul unless for a soul or for corruption done in the land - it is as if he had slain mankind entirely.\", \"5:32\"),\n        (\"وَمَنْ أَحْيَاهَا فَكَأَنَّمَا أَحْيَا النَّاسَ جَمِيعًا\", \"And whoever saves one - it is as if he had saved mankind entirely.\", \"5:32\"),\n    ]\n    \n    for arabic, english, ref in quran_samples:\n        passages.append({\n            'id': f'quran_{ref.replace(\":\", \"_\")}',\n            'text_original': arabic,\n            'text_english': english,\n            'source': f'Quran {ref}',\n            'period': 'QURANIC',\n            'century': 7\n        })\n    \n    # Key Hadith on moral themes\n    hadith_samples = [\n        (\"إنما الأعمال بالنيات وإنما لكل امرئ ما نوى\", \"Actions are judged by intentions, and everyone will be rewarded according to what he intended.\", \"Bukhari 1\"),\n        (\"لا يؤمن أحدكم حتى يحب لأخيه ما يحب لنفسه\", \"None of you truly believes until he loves for his brother what he loves for himself.\", \"Bukhari 13\"),\n        (\"من كان يؤمن بالله واليوم الآخر فليقل خيرا أو ليصمت\", \"Whoever believes in Allah and the Last Day should speak good or remain silent.\", \"Bukhari 6018\"),\n        (\"المسلم من سلم المسلمون من لسانه ويده\", \"A Muslim is one from whose tongue and hand other Muslims are safe.\", \"Bukhari 10\"),\n        (\"لا ضرر ولا ضرار\", \"There should be neither harm nor reciprocal harm.\", \"Ibn Majah 2341\"),\n        (\"ارحموا من في الأرض يرحمكم من في السماء\", \"Be merciful to those on earth and the One in the heavens will be merciful to you.\", \"Tirmidhi 1924\"),\n        (\"الدين النصيحة\", \"The religion is sincerity and sincere advice.\", \"Muslim 55\"),\n        (\"من رأى منكم منكرا فليغيره بيده\", \"Whoever among you sees an evil, let him change it with his hand.\", \"Muslim 49\"),\n        (\"لا يحل مال امرئ مسلم إلا بطيب نفس منه\", \"It is not permissible to take the property of a Muslim except with his willing consent.\", \"Ahmad 20172\"),\n        (\"كلكم راع وكلكم مسؤول عن رعيته\", \"Each of you is a shepherd and each of you is responsible for his flock.\", \"Bukhari 7138\"),\n    ]\n    \n    for arabic, english, ref in hadith_samples:\n        passages.append({\n            'id': f'hadith_{ref.replace(\" \", \"_\").lower()}',\n            'text_original': arabic,\n            'text_english': english,\n            'source': ref,\n            'period': 'HADITH',\n            'century': 9\n        })\n    \n    return passages\n\n# Try downloading, fall back to embedded\nquran_passages = download_quran()\nhadith_passages = download_hadith()\n\nislamic_passages = quran_passages + hadith_passages\n\n# If not enough, use embedded\nif len(islamic_passages) < 50:\n    print(\"\\nUsing embedded Islamic text samples...\")\n    islamic_passages = generate_embedded_islamic_corpus()\n\nprint(f\"\\nTotal Islamic passages: {len(islamic_passages)}\")\nprint(f\"  Quranic: {len([p for p in islamic_passages if p['period'] == 'QURANIC'])}\")\nprint(f\"  Hadith: {len([p for p in islamic_passages if p['period'] == 'HADITH'])}\")\n\n# Save\nwith open(islamic_dir / 'islamic_texts.json', 'w', encoding='utf-8') as f:\n    json.dump(islamic_passages, f, ensure_ascii=False, indent=2)\n\nprint(f\"Saved to {islamic_dir / 'islamic_texts.json'}\")\n\nprint_resources(\"After Islamic\")\n\nmark_task(\"Download Islamic texts (Quran + Hadith)\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 5. Download Dear Abby Dataset (English) { display-mode: \"form\" }\n#@markdown Downloads the Dear Abby advice column dataset.\n\nimport subprocess\nimport os\nimport pandas as pd\nfrom pathlib import Path\n\nmark_task(\"Download Dear Abby (English)\", \"running\")\n\nsqnd_path = 'sqnd-probe-data'\nif not os.path.exists(sqnd_path):\n    print(\"Cloning sqnd-probe repo...\")\n    process = subprocess.Popen(\n        ['git', 'clone', '--depth', '1', '--progress',\n         'https://github.com/ahb-sjsu/sqnd-probe.git', sqnd_path],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True,\n        bufsize=1\n    )\n    for line in process.stdout:\n        print(line, end='', flush=True)\n    process.wait()\nelse:\n    print(\"Repo already cloned.\")\n\ndear_abby_source = Path('sqnd-probe-data/dear_abby_data/raw_da_qs.csv')\ndear_abby_path = Path('data/raw/dear_abby.csv')\n\nif dear_abby_source.exists():\n    !cp \"{dear_abby_source}\" \"{dear_abby_path}\"\n    print(f\"Copied Dear Abby data\")\nelif not dear_abby_path.exists():\n    raise FileNotFoundError(\"Dear Abby dataset not found!\")\n\ndf_check = pd.read_csv(dear_abby_path)\nprint(f\"\\nDear Abby: {len(df_check):,} entries\")\nprint(f\"Year range: {df_check['year'].min():.0f} - {df_check['year'].max():.0f}\")\n\nprint_resources(\"After Dear Abby\")\n\nmark_task(\"Download Dear Abby (English)\", \"done\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 6. Define Data Structures and Load All Corpora { display-mode: \"form\" }\n#@markdown Unified data structures for multilingual corpus.\n\nimport json\nimport hashlib\nimport re\nimport os\nimport gc\nimport pandas as pd\nfrom pathlib import Path\nfrom dataclasses import dataclass, field, asdict\nfrom typing import List, Dict, Set, Optional\nfrom enum import Enum\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\n\nmark_task(\"Preprocess all corpora\", \"running\")\n\nprint(\"=\"*60)\nprint(\"DEFINING DATA STRUCTURES\")\nprint(\"=\"*60)\n\nclass TimePeriod(Enum):\n    # Jewish texts\n    BIBLICAL = 0        # ~1000-500 BCE\n    SECOND_TEMPLE = 1   # ~500 BCE - 70 CE  \n    TANNAITIC = 2       # ~70-200 CE\n    AMORAIC = 3         # ~200-500 CE\n    GEONIC = 4          # ~600-1000 CE\n    RISHONIM = 5        # ~1000-1500 CE\n    ACHRONIM = 6        # ~1500-1800 CE\n    MODERN_HEBREW = 7   # ~1800-present\n    # Chinese texts\n    CONFUCIAN = 8       # ~500-200 BCE\n    DAOIST = 9          # ~600-200 BCE\n    # Islamic texts\n    QURANIC = 10        # ~610-632 CE\n    HADITH = 11         # ~700-900 CE\n    # Modern English\n    DEAR_ABBY = 12      # 1956-2020\n\nclass Language(Enum):\n    HEBREW = 0\n    ARAMAIC = 1\n    JUDEO_ARABIC = 2\n    CLASSICAL_CHINESE = 3\n    ARABIC = 4\n    ENGLISH = 5\n\nclass BondType(Enum):\n    HARM_PREVENTION = 0\n    RECIPROCITY = 1\n    AUTONOMY = 2\n    PROPERTY = 3\n    FAMILY = 4\n    AUTHORITY = 5\n    EMERGENCY = 6\n    CONTRACT = 7\n    CARE = 8\n    FAIRNESS = 9\n    NONE = 10\n\nclass HohfeldianState(Enum):\n    RIGHT = 0\n    OBLIGATION = 1\n    LIBERTY = 2\n    NO_RIGHT = 3\n\n@dataclass\nclass Passage:\n    id: str\n    text_original: str\n    text_english: str\n    time_period: str\n    century: int\n    source: str\n    source_type: str  # sefaria, chinese, islamic, dear_abby\n    category: str\n    language: str\n    word_count: int = 0\n    bond_types: List[str] = field(default_factory=list)\n    \n    def to_dict(self):\n        return asdict(self)\n\n# Language detection for Sefaria\ndef detect_sefaria_language(text: str, category: str) -> str:\n    \"\"\"Detect language of Sefaria text.\"\"\"\n    if not text:\n        return 'hebrew'\n    \n    # Aramaic indicators (Talmud, Zohar)\n    aramaic_categories = {'Talmud', 'Bavli', 'Yerushalmi', 'Zohar'}\n    if category in aramaic_categories:\n        return 'aramaic'\n    \n    # Check for Arabic script (Judeo-Arabic)\n    arabic_chars = sum(1 for c in text if '\\u0600' <= c <= '\\u06FF')\n    if arabic_chars > len(text) * 0.3:\n        return 'judeo_arabic'\n    \n    return 'hebrew'\n\nCATEGORY_TO_PERIOD = {\n    'Tanakh': TimePeriod.BIBLICAL,\n    'Torah': TimePeriod.BIBLICAL,\n    'Prophets': TimePeriod.BIBLICAL,\n    'Writings': TimePeriod.BIBLICAL,\n    'Mishnah': TimePeriod.TANNAITIC,\n    'Tosefta': TimePeriod.TANNAITIC,\n    'Talmud': TimePeriod.AMORAIC,\n    'Bavli': TimePeriod.AMORAIC,\n    'Yerushalmi': TimePeriod.AMORAIC,\n    'Midrash': TimePeriod.AMORAIC,\n    'Halakhah': TimePeriod.RISHONIM,\n    'Kabbalah': TimePeriod.RISHONIM,\n    'Liturgy': TimePeriod.GEONIC,\n    'Philosophy': TimePeriod.RISHONIM,\n    'Chasidut': TimePeriod.ACHRONIM,\n    'Musar': TimePeriod.ACHRONIM,\n    'Responsa': TimePeriod.ACHRONIM,\n    'Modern': TimePeriod.MODERN_HEBREW,\n}\n\nPERIOD_TO_CENTURY = {\n    TimePeriod.BIBLICAL: -6,\n    TimePeriod.SECOND_TEMPLE: -2,\n    TimePeriod.TANNAITIC: 2,\n    TimePeriod.AMORAIC: 4,\n    TimePeriod.GEONIC: 8,\n    TimePeriod.RISHONIM: 12,\n    TimePeriod.ACHRONIM: 17,\n    TimePeriod.MODERN_HEBREW: 20,\n    TimePeriod.CONFUCIAN: -4,\n    TimePeriod.DAOIST: -5,\n    TimePeriod.QURANIC: 7,\n    TimePeriod.HADITH: 9,\n    TimePeriod.DEAR_ABBY: 20,\n}\n\ndef load_sefaria_full(base_path: str) -> List[Passage]:\n    \"\"\"Load FULL Sefaria corpus with language detection.\"\"\"\n    passages = []\n    json_path = Path(base_path) / \"json\"\n    \n    if not json_path.exists():\n        print(f\"Warning: {json_path} not found\")\n        return []\n    \n    json_files = list(json_path.rglob(\"*.json\"))\n    print(f\"Found {len(json_files):,} JSON files\")\n    print(\"Processing ALL files...\")\n    \n    for json_file in tqdm(json_files, desc=\"Sefaria\", unit=\"file\"):\n        try:\n            with open(json_file, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n        except:\n            continue\n        \n        rel_path = json_file.relative_to(json_path)\n        category = str(rel_path.parts[0]) if rel_path.parts else \"unknown\"\n        time_period = CATEGORY_TO_PERIOD.get(category, TimePeriod.AMORAIC)\n        century = PERIOD_TO_CENTURY.get(time_period, 0)\n        \n        if isinstance(data, dict):\n            hebrew = data.get('he', data.get('text', []))\n            english = data.get('text', data.get('en', []))\n            \n            def flatten(h, e, ref=\"\"):\n                if isinstance(h, str) and isinstance(e, str):\n                    h_clean = re.sub(r'<[^>]+>', '', h).strip()\n                    e_clean = re.sub(r'<[^>]+>', '', e).strip()\n                    if 50 <= len(e_clean) <= 2000:\n                        lang = detect_sefaria_language(h_clean, category)\n                        pid = hashlib.md5(f\"{json_file.stem}:{ref}:{h_clean[:50]}\".encode()).hexdigest()[:12]\n                        return [Passage(\n                            id=f\"sefaria_{pid}\",\n                            text_original=h_clean,\n                            text_english=e_clean,\n                            time_period=time_period.name,\n                            century=century,\n                            source=f\"{json_file.stem} {ref}\".strip(),\n                            source_type=\"sefaria\",\n                            category=category,\n                            language=lang,\n                            word_count=len(e_clean.split())\n                        )]\n                    return []\n                elif isinstance(h, list) and isinstance(e, list):\n                    result = []\n                    for i, (hh, ee) in enumerate(zip(h, e)):\n                        result.extend(flatten(hh, ee, f\"{ref}.{i+1}\" if ref else str(i+1)))\n                    return result\n                return []\n            \n            passages.extend(flatten(hebrew, english))\n    \n    return passages\n\ndef load_chinese(path: str) -> List[Passage]:\n    \"\"\"Load Chinese classics.\"\"\"\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    passages = []\n    for item in data:\n        passages.append(Passage(\n            id=item['id'],\n            text_original=item['text_original'],\n            text_english=item['text_english'],\n            time_period=item['period'],\n            century=item['century'],\n            source=item['source'],\n            source_type='chinese',\n            category=item['period'],\n            language='classical_chinese',\n            word_count=len(item['text_english'].split())\n        ))\n    return passages\n\ndef load_islamic(path: str) -> List[Passage]:\n    \"\"\"Load Islamic texts.\"\"\"\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    passages = []\n    for item in data:\n        passages.append(Passage(\n            id=item['id'],\n            text_original=item['text_original'],\n            text_english=item['text_english'],\n            time_period=item['period'],\n            century=item['century'],\n            source=item['source'],\n            source_type='islamic',\n            category=item['period'],\n            language='arabic',\n            word_count=len(item['text_english'].split())\n        ))\n    return passages\n\ndef load_dear_abby(path: str, max_passages: int = None) -> List[Passage]:\n    \"\"\"Load Dear Abby corpus.\"\"\"\n    passages = []\n    df = pd.read_csv(path)\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Dear Abby\", unit=\"row\"):\n        question = str(row.get('question_only', ''))\n        if not question or question == 'nan' or len(question) < 50 or len(question) > 2000:\n            continue\n        \n        year = int(row.get('year', 1990))\n        pid = hashlib.md5(f\"abby:{idx}:{question[:50]}\".encode()).hexdigest()[:12]\n        \n        passages.append(Passage(\n            id=f\"abby_{pid}\",\n            text_original=question,\n            text_english=question,\n            time_period=TimePeriod.DEAR_ABBY.name,\n            century=20 if year < 2000 else 21,\n            source=f\"Dear Abby {year}\",\n            source_type=\"dear_abby\",\n            category=\"advice\",\n            language=\"english\",\n            word_count=len(question.split())\n        ))\n        \n        if max_passages and len(passages) >= max_passages:\n            break\n    \n    return passages\n\n# Load all corpora\nprint()\nprint(\"=\"*60)\nprint(\"LOADING ALL CORPORA\")\nprint(\"=\"*60)\nprint()\n\nall_passages = []\n\n# 1. Sefaria (full)\nprint(\"1. Loading Sefaria (full)...\")\nsefaria = load_sefaria_full(\"data/raw/Sefaria-Export\")\nprint(f\"   Loaded: {len(sefaria):,} passages\")\nall_passages.extend(sefaria)\ndel sefaria\ngc.collect()\n\n# Language breakdown for Sefaria\nsefaria_langs = defaultdict(int)\nfor p in [p for p in all_passages if p.source_type == 'sefaria']:\n    sefaria_langs[p.language] += 1\nprint(f\"   Languages: {dict(sefaria_langs)}\")\n\n# 2. Chinese classics\nprint(\"\\n2. Loading Chinese classics...\")\nchinese = load_chinese(\"data/raw/chinese_classics/chinese_classics.json\")\nprint(f\"   Loaded: {len(chinese):,} passages\")\nall_passages.extend(chinese)\ndel chinese\n\n# 3. Islamic texts\nprint(\"\\n3. Loading Islamic texts...\")\nislamic = load_islamic(\"data/raw/islamic_texts/islamic_texts.json\")\nprint(f\"   Loaded: {len(islamic):,} passages\")\nall_passages.extend(islamic)\ndel islamic\n\n# 4. Dear Abby\nprint(\"\\n4. Loading Dear Abby...\")\nabby = load_dear_abby(\"data/raw/dear_abby.csv\")\nprint(f\"   Loaded: {len(abby):,} passages\")\nall_passages.extend(abby)\ndel abby\n\ngc.collect()\n\nprint()\nprint(\"=\"*60)\nprint(f\"TOTAL PASSAGES: {len(all_passages):,}\")\nprint(\"=\"*60)\n\n# Summary by source\nby_source = defaultdict(int)\nby_language = defaultdict(int)\nby_period = defaultdict(int)\nfor p in all_passages:\n    by_source[p.source_type] += 1\n    by_language[p.language] += 1\n    by_period[p.time_period] += 1\n\nprint(\"\\nBy source:\")\nfor src, cnt in sorted(by_source.items(), key=lambda x: -x[1]):\n    print(f\"  {src:15s}: {cnt:>8,}\")\n\nprint(\"\\nBy language:\")\nfor lang, cnt in sorted(by_language.items(), key=lambda x: -x[1]):\n    print(f\"  {lang:20s}: {cnt:>8,}\")\n\nprint(\"\\nBy time period:\")\nfor period, cnt in sorted(by_period.items(), key=lambda x: -x[1])[:10]:\n    print(f\"  {period:20s}: {cnt:>8,}\")\n\nprint_resources(\"After loading\")\n\nmark_task(\"Preprocess all corpora\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 7. Extract Bond Structures { display-mode: \"form\" }\n#@markdown Labels extracted from English translations for all languages.\n\nimport gc\nimport json\nimport re\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\n\nmark_task(\"Extract bond structures\", \"running\")\n\nprint(\"=\"*60)\nprint(\"EXTRACTING BOND STRUCTURES\")\nprint(\"=\"*60)\nprint()\nprint(\"Labels derived from English translations for all corpora.\")\nprint()\n\nRELATION_PATTERNS = {\n    BondType.HARM_PREVENTION: [r'\\b(kill|murder|harm|hurt|save|rescue|protect|danger|attack|injure|wound|destroy|blood|death|violence)\\b'],\n    BondType.RECIPROCITY: [r'\\b(return|repay|owe|debt|mutual|exchange|give back|pay back|reciprocate|reward|recompense)\\b'],\n    BondType.AUTONOMY: [r'\\b(choose|decision|consent|agree|force|coerce|right|freedom|liberty|self-determination|free will)\\b'],\n    BondType.PROPERTY: [r'\\b(property|own|steal|theft|buy|sell|land|possess|belong|asset|wealth|money|inheritance)\\b'],\n    BondType.FAMILY: [r'\\b(honor|parent|marry|divorce|inherit|family|mother|father|child|son|daughter|spouse|husband|wife|brother|sister|filial)\\b'],\n    BondType.AUTHORITY: [r'\\b(obey|command|law|judge|rule|teach|leader|king|master|servant|subject|authority|govern|submit)\\b'],\n    BondType.CARE: [r'\\b(care|help|assist|feed|clothe|visit|nurture|tend|support|comfort|mercy|compassion|kindness)\\b'],\n    BondType.FAIRNESS: [r'\\b(fair|just|equal|deserve|bias|impartial|equity|discrimination|justice|righteous)\\b'],\n    BondType.EMERGENCY: [r'\\b(emergency|urgent|crisis|danger|life-threatening|immediate|desperate|dire|peril|rescue)\\b'],\n    BondType.CONTRACT: [r'\\b(contract|agreement|promise|vow|oath|covenant|pledge|commit|bind|treaty|negotiate|witness)\\b'],\n}\n\nHOHFELD_PATTERNS = {\n    HohfeldianState.OBLIGATION: [r'\\b(must|shall|duty|require|should|ought|obligated|commanded)\\b'],\n    HohfeldianState.RIGHT: [r'\\b(right to|entitled|deserve|claim|due)\\b'],\n    HohfeldianState.LIBERTY: [r'\\b(may|can|permitted|allowed|free to|at liberty)\\b'],\n}\n\ndef extract_bond_structure(passage: Passage) -> Dict:\n    text = passage.text_english.lower()\n    \n    relations = []\n    for rel_type, patterns in RELATION_PATTERNS.items():\n        for pattern in patterns:\n            if re.search(pattern, text, re.IGNORECASE):\n                relations.append(rel_type.name)\n                break\n    \n    if not relations:\n        relations = ['NONE']\n    \n    hohfeld = None\n    for state, patterns in HOHFELD_PATTERNS.items():\n        for pattern in patterns:\n            if re.search(pattern, text, re.IGNORECASE):\n                hohfeld = state.name\n                break\n        if hohfeld:\n            break\n    \n    return {\n        'bonds': [{'relation': r} for r in relations],\n        'primary_relation': relations[0],\n        'hohfeld_state': hohfeld,\n        'signature': \"|\".join(sorted(set(relations)))\n    }\n\nprint(\"Writing to disk...\")\n\nbond_counts = defaultdict(int)\n\nwith open(\"data/processed/passages.jsonl\", 'w') as f_pass, \\\n     open(\"data/processed/bond_structures.jsonl\", 'w') as f_bond:\n    \n    for passage in tqdm(all_passages, desc=\"Extracting\", unit=\"passage\"):\n        bond_struct = extract_bond_structure(passage)\n        passage.bond_types = [b['relation'] for b in bond_struct['bonds']]\n        \n        for bond in bond_struct['bonds']:\n            bond_counts[bond['relation']] += 1\n        \n        f_pass.write(json.dumps(passage.to_dict()) + '\\n')\n        f_bond.write(json.dumps({\n            'passage_id': passage.id,\n            'bond_structure': bond_struct\n        }) + '\\n')\n\nn_passages = len(all_passages)\ndel all_passages\ngc.collect()\n\nprint(f\"\\nSaved {n_passages:,} passages\")\n\nprint(\"\\nBond distribution:\")\nfor bond, cnt in sorted(bond_counts.items(), key=lambda x: -x[1]):\n    pct = 100 * cnt / sum(bond_counts.values())\n    print(f\"  {bond:20s}: {cnt:>8,} ({pct:5.1f}%)\")\n\nprint_resources(\"After extraction\")\n\nmark_task(\"Extract bond structures\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 8. Generate Splits (Temporal + Lingual) { display-mode: \"form\" }\n#@markdown Creates both temporal and language-based splits.\n\nimport random\nimport gc\nimport json\nimport shutil\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\n\nrandom.seed(42)\n\nmark_task(\"Generate splits (temporal + lingual)\", \"running\")\n\nprint(\"=\"*60)\nprint(\"GENERATING SPLITS\")\nprint(\"=\"*60)\nprint()\n\n# Read all passage metadata\nprint(\"Reading passage metadata...\")\npassage_meta = []\nwith open(\"data/processed/passages.jsonl\", 'r') as f:\n    for line in tqdm(f, desc=\"Reading\", unit=\"line\"):\n        p = json.loads(line)\n        passage_meta.append({\n            'id': p['id'],\n            'language': p['language'],\n            'source_type': p['source_type'],\n            'time_period': p['time_period'],\n            'century': p['century']\n        })\n\nprint(f\"Total passages: {len(passage_meta):,}\")\n\n# Group by various attributes\nby_language = defaultdict(list)\nby_source = defaultdict(list)\nby_period = defaultdict(list)\n\nfor p in passage_meta:\n    by_language[p['language']].append(p['id'])\n    by_source[p['source_type']].append(p['id'])\n    by_period[p['time_period']].append(p['id'])\n\nprint(\"\\nBy language:\")\nfor lang, ids in sorted(by_language.items(), key=lambda x: -len(x[1])):\n    print(f\"  {lang:20s}: {len(ids):>8,}\")\n\nprint(\"\\nBy source:\")\nfor src, ids in sorted(by_source.items(), key=lambda x: -len(x[1])):\n    print(f\"  {src:15s}: {len(ids):>8,}\")\n\n# ==========================================\n# SPLIT 1: Temporal (Ancient -> Modern)\n# ==========================================\nprint(\"\\n\" + \"-\"*60)\nprint(\"SPLIT 1: TEMPORAL (Ancient → Modern)\")\nprint(\"-\"*60)\n\nancient_periods = {'BIBLICAL', 'SECOND_TEMPLE', 'TANNAITIC', 'AMORAIC', 'GEONIC', 'RISHONIM',\n                   'CONFUCIAN', 'DAOIST', 'QURANIC', 'HADITH'}\nmodern_periods = {'ACHRONIM', 'MODERN_HEBREW', 'DEAR_ABBY'}\n\nancient_ids = [p['id'] for p in passage_meta if p['time_period'] in ancient_periods]\nmodern_ids = [p['id'] for p in passage_meta if p['time_period'] in modern_periods]\n\nrandom.shuffle(ancient_ids)\nrandom.shuffle(modern_ids)\n\n# Use 10% of modern for validation\nn_valid = len(modern_ids) // 10\n\nsplit_temporal = {\n    'name': 'temporal_ancient_to_modern',\n    'train_ids': ancient_ids,\n    'valid_ids': modern_ids[:n_valid],\n    'test_ids': modern_ids[n_valid:],\n    'train_size': len(ancient_ids),\n    'valid_size': n_valid,\n    'test_size': len(modern_ids) - n_valid,\n    'description': 'Train on ancient texts (all languages), test on modern'\n}\nprint(f\"  Train: {split_temporal['train_size']:,} | Valid: {split_temporal['valid_size']:,} | Test: {split_temporal['test_size']:,}\")\n\n# ==========================================\n# SPLIT 2: Semitic -> English\n# ==========================================\nprint(\"\\n\" + \"-\"*60)\nprint(\"SPLIT 2: SEMITIC → ENGLISH\")\nprint(\"-\"*60)\n\nsemitic_ids = by_language['hebrew'] + by_language['aramaic'] + by_language.get('judeo_arabic', []) + by_language['arabic']\nenglish_ids = by_language['english']\n\nrandom.shuffle(semitic_ids)\nrandom.shuffle(english_ids)\n\nn_valid = len(english_ids) // 10\n\nsplit_semitic_to_english = {\n    'name': 'semitic_to_english',\n    'train_ids': semitic_ids,\n    'valid_ids': english_ids[:n_valid],\n    'test_ids': english_ids[n_valid:],\n    'train_size': len(semitic_ids),\n    'valid_size': n_valid,\n    'test_size': len(english_ids) - n_valid,\n    'description': 'Train on Hebrew/Aramaic/Arabic, test on English'\n}\nprint(f\"  Train: {split_semitic_to_english['train_size']:,} | Valid: {split_semitic_to_english['valid_size']:,} | Test: {split_semitic_to_english['test_size']:,}\")\n\n# ==========================================\n# SPLIT 3: Chinese -> All Others\n# ==========================================\nprint(\"\\n\" + \"-\"*60)\nprint(\"SPLIT 3: CHINESE → ALL OTHERS\")\nprint(\"-\"*60)\n\nchinese_ids = by_language['classical_chinese']\nother_ids = [p['id'] for p in passage_meta if p['language'] != 'classical_chinese']\n\nrandom.shuffle(other_ids)\nn_valid = min(len(chinese_ids) // 2, 500)\nn_test = min(len(other_ids), 10000)\n\nsplit_chinese_to_others = {\n    'name': 'chinese_to_others',\n    'train_ids': chinese_ids,\n    'valid_ids': other_ids[:n_valid],\n    'test_ids': other_ids[n_valid:n_valid+n_test],\n    'train_size': len(chinese_ids),\n    'valid_size': n_valid,\n    'test_size': n_test,\n    'description': 'Train on Classical Chinese, test on other languages'\n}\nprint(f\"  Train: {split_chinese_to_others['train_size']:,} | Valid: {split_chinese_to_others['valid_size']:,} | Test: {split_chinese_to_others['test_size']:,}\")\n\n# ==========================================\n# SPLIT 4: Mixed (In-Domain Baseline)\n# ==========================================\nprint(\"\\n\" + \"-\"*60)\nprint(\"SPLIT 4: MIXED (In-Domain Baseline)\")\nprint(\"-\"*60)\n\nall_ids = [p['id'] for p in passage_meta]\nrandom.shuffle(all_ids)\n\nn = len(all_ids)\nn_train = int(0.7 * n)\nn_valid = int(0.15 * n)\n\nsplit_mixed = {\n    'name': 'mixed_baseline',\n    'train_ids': all_ids[:n_train],\n    'valid_ids': all_ids[n_train:n_train+n_valid],\n    'test_ids': all_ids[n_train+n_valid:],\n    'train_size': n_train,\n    'valid_size': n_valid,\n    'test_size': n - n_train - n_valid,\n    'description': 'Random split across all corpora (in-domain baseline)'\n}\nprint(f\"  Train: {split_mixed['train_size']:,} | Valid: {split_mixed['valid_size']:,} | Test: {split_mixed['test_size']:,}\")\n\n# ==========================================\n# SPLIT 5: Hebrew-only -> Others\n# ==========================================\nprint(\"\\n\" + \"-\"*60)\nprint(\"SPLIT 5: HEBREW → NON-HEBREW\")\nprint(\"-\"*60)\n\nhebrew_ids = by_language['hebrew']\nnon_hebrew_ids = [p['id'] for p in passage_meta if p['language'] != 'hebrew']\n\nrandom.shuffle(hebrew_ids)\nrandom.shuffle(non_hebrew_ids)\n\nn_valid = min(len(non_hebrew_ids) // 10, 5000)\nn_test = min(len(non_hebrew_ids) - n_valid, 20000)\n\nsplit_hebrew_to_others = {\n    'name': 'hebrew_to_others',\n    'train_ids': hebrew_ids,\n    'valid_ids': non_hebrew_ids[:n_valid],\n    'test_ids': non_hebrew_ids[n_valid:n_valid+n_test],\n    'train_size': len(hebrew_ids),\n    'valid_size': n_valid,\n    'test_size': n_test,\n    'description': 'Train on Hebrew only, test on all other languages'\n}\nprint(f\"  Train: {split_hebrew_to_others['train_size']:,} | Valid: {split_hebrew_to_others['valid_size']:,} | Test: {split_hebrew_to_others['test_size']:,}\")\n\n# Save all splits\nall_splits = {\n    'temporal_ancient_to_modern': split_temporal,\n    'semitic_to_english': split_semitic_to_english,\n    'chinese_to_others': split_chinese_to_others,\n    'hebrew_to_others': split_hebrew_to_others,\n    'mixed_baseline': split_mixed,\n}\n\nwith open(\"data/splits/all_splits.json\", 'w') as f:\n    json.dump(all_splits, f, indent=2)\n\n# Compute baselines\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPUTING BASELINES\")\nprint(\"=\"*60)\n\nbond_counts = defaultdict(int)\ntime_counts = defaultdict(int)\nlang_counts = defaultdict(int)\n\nwith open(\"data/processed/passages.jsonl\", 'r') as fp, \\\n     open(\"data/processed/bond_structures.jsonl\", 'r') as fb:\n    for p_line, b_line in zip(fp, fb):\n        p = json.loads(p_line)\n        b = json.loads(b_line)\n        \n        # ID integrity check\n        assert b['passage_id'] == p['id'], f\"ID mismatch: {b['passage_id']} != {p['id']}\"\n        \n        bond_counts[b['bond_structure']['primary_relation']] += 1\n        time_counts[p['time_period']] += 1\n        lang_counts[p['language']] += 1\n\nprint(\"ID integrity check: PASSED ✓\")\n\nN_BOND = len(bond_counts)\nN_TIME = len(time_counts)\nN_LANG = len(lang_counts)\n\nbaselines = {\n    'bond_counts': dict(bond_counts),\n    'time_counts': dict(time_counts),\n    'language_counts': dict(lang_counts),\n    'chance_bond': 1.0 / N_BOND,\n    'chance_time': 1.0 / N_TIME,\n    'chance_language': 1.0 / N_LANG,\n    'n_bond_classes': N_BOND,\n    'n_time_classes': N_TIME,\n    'n_language_classes': N_LANG,\n}\n\nwith open(\"data/splits/baselines.json\", 'w') as f:\n    json.dump(baselines, f, indent=2)\n\nprint(f\"\\nChance baselines:\")\nprint(f\"  Bond:     {baselines['chance_bond']:.1%} ({N_BOND} classes)\")\nprint(f\"  Time:     {baselines['chance_time']:.1%} ({N_TIME} classes)\")\nprint(f\"  Language: {baselines['chance_language']:.1%} ({N_LANG} classes)\")\n\n# Save to Drive\nprint(\"\\nSaving to Google Drive...\")\nshutil.copytree(\"data/processed\", f\"{SAVE_DIR}/processed\", dirs_exist_ok=True)\nshutil.copytree(\"data/splits\", f\"{SAVE_DIR}/splits\", dirs_exist_ok=True)\n\ndel passage_meta\ngc.collect()\n\nprint_resources(\"After splits\")\n\nmark_task(\"Generate splits (temporal + lingual)\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 9. Define Model Architecture { display-mode: \"form\" }\n#@markdown BIP model with multilingual encoder.\n\nimport gc\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoTokenizer\nfrom tqdm.auto import tqdm\n\nprint(\"=\"*60)\nprint(\"MODEL ARCHITECTURE\")\nprint(\"=\"*60)\nprint()\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nclass GradientReversal(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, lambda_):\n        ctx.lambda_ = lambda_\n        return x.clone()\n    @staticmethod\n    def backward(ctx, grad_output):\n        return -ctx.lambda_ * grad_output, None\n\ndef gradient_reversal(x, lambda_=1.0):\n    return GradientReversal.apply(x, lambda_)\n\nclass BIPEncoder(nn.Module):\n    def __init__(self, model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        self.d_model = 384\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        hidden = outputs.last_hidden_state\n        mask = attention_mask.unsqueeze(-1).float()\n        return (hidden * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n\nclass BIPModel(nn.Module):\n    def __init__(self, d_model=384, d_bond=64, d_label=32, n_periods=13, n_languages=6, n_hohfeld=4, n_bonds=11):\n        super().__init__()\n        self.encoder = BIPEncoder()\n        \n        self.bond_proj = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, d_bond)\n        )\n        \n        self.label_proj = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, d_label)\n        )\n        \n        # Classifiers\n        self.time_classifier = nn.Linear(d_bond, n_periods)\n        self.language_classifier = nn.Linear(d_bond, n_languages)\n        self.hohfeld_classifier = nn.Linear(d_bond, n_hohfeld)\n        self.bond_classifier = nn.Linear(d_bond, n_bonds)\n    \n    def forward(self, input_ids, attention_mask, adversarial_lambda=1.0):\n        h = self.encoder(input_ids, attention_mask)\n        z_bond = self.bond_proj(h)\n        z_label = self.label_proj(h)\n        \n        z_bond_adv = gradient_reversal(z_bond, adversarial_lambda)\n        \n        return {\n            'z_bond': z_bond,\n            'z_label': z_label,\n            'time_pred': self.time_classifier(z_bond_adv),\n            'language_pred': self.language_classifier(z_bond_adv),\n            'hohfeld_pred': self.hohfeld_classifier(z_bond),\n            'bond_pred': self.bond_classifier(z_bond)\n        }\n    \n    def extract_z_bond(self, input_ids, attention_mask):\n        with torch.no_grad():\n            h = self.encoder(input_ids, attention_mask)\n            return self.bond_proj(h)\n\n# Index mappings\nTIME_PERIOD_TO_IDX = {\n    'BIBLICAL': 0, 'SECOND_TEMPLE': 1, 'TANNAITIC': 2, 'AMORAIC': 3,\n    'GEONIC': 4, 'RISHONIM': 5, 'ACHRONIM': 6, 'MODERN_HEBREW': 7,\n    'CONFUCIAN': 8, 'DAOIST': 9, 'QURANIC': 10, 'HADITH': 11, 'DEAR_ABBY': 12\n}\n\nLANGUAGE_TO_IDX = {\n    'hebrew': 0, 'aramaic': 1, 'judeo_arabic': 2,\n    'classical_chinese': 3, 'arabic': 4, 'english': 5\n}\n\nHOHFELD_TO_IDX = {'OBLIGATION': 0, 'RIGHT': 1, 'LIBERTY': 2, None: 3}\n\nBOND_TYPE_TO_IDX = {\n    'HARM_PREVENTION': 0, 'RECIPROCITY': 1, 'AUTONOMY': 2, 'PROPERTY': 3,\n    'FAMILY': 4, 'AUTHORITY': 5, 'EMERGENCY': 6, 'CONTRACT': 7,\n    'CARE': 8, 'FAIRNESS': 9, 'NONE': 10\n}\n\nclass MoralDataset(Dataset):\n    def __init__(self, passage_ids: set, passages_file: str, bonds_file: str, tokenizer, max_len=64):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.data = []\n        \n        with open(passages_file, 'r') as fp, open(bonds_file, 'r') as fb:\n            for p_line, b_line in tqdm(zip(fp, fb), desc=\"Loading\", unit=\"line\"):\n                p = json.loads(p_line)\n                b = json.loads(b_line)\n                \n                if b['passage_id'] != p['id']:\n                    continue\n                \n                if p['id'] in passage_ids:\n                    # Use original text for non-English, English text for English\n                    if p['language'] == 'english':\n                        text = p['text_english']\n                    else:\n                        text = p['text_original']\n                    \n                    self.data.append({\n                        'text': text[:1000],\n                        'time_period': p['time_period'],\n                        'language': p['language'],\n                        'source_type': p['source_type'],\n                        'hohfeld': b['bond_structure']['hohfeld_state'],\n                        'bond': b['bond_structure']['primary_relation']\n                    })\n        \n        print(f\"  Loaded {len(self.data):,} samples\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        enc = self.tokenizer(item['text'], truncation=True, max_length=self.max_len,\n                            padding='max_length', return_tensors='pt')\n        return {\n            'input_ids': enc['input_ids'].squeeze(0),\n            'attention_mask': enc['attention_mask'].squeeze(0),\n            'time_label': TIME_PERIOD_TO_IDX.get(item['time_period'], 12),\n            'language_label': LANGUAGE_TO_IDX.get(item['language'], 5),\n            'hohfeld_label': HOHFELD_TO_IDX.get(item['hohfeld'], 3),\n            'bond_label': BOND_TYPE_TO_IDX.get(item['bond'], 10),\n            'source_type': item['source_type'],\n            'language': item['language']\n        }\n\ndef collate_fn(batch):\n    return {\n        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n        'time_labels': torch.tensor([x['time_label'] for x in batch]),\n        'language_labels': torch.tensor([x['language_label'] for x in batch]),\n        'hohfeld_labels': torch.tensor([x['hohfeld_label'] for x in batch]),\n        'bond_labels': torch.tensor([x['bond_label'] for x in batch]),\n        'source_types': [x['source_type'] for x in batch],\n        'languages': [x['language'] for x in batch]\n    }\n\nprint(\"Model defined!\")\nprint(f\"  Time periods: {len(TIME_PERIOD_TO_IDX)}\")\nprint(f\"  Languages: {len(LANGUAGE_TO_IDX)}\")\nprint(f\"  Bond types: {len(BOND_TYPE_TO_IDX)}\")\nprint(f\"  Base batch: {BASE_BATCH_SIZE}\")\n\nprint_resources(\"Model defined\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 10. Train BIP Model { display-mode: \"form\" }\n#@markdown Trains on selected splits with adversarial time/language invariance.\n\n#@markdown **Select splits to train:**\nTRAIN_TEMPORAL = True  #@param {type:\"boolean\"}\nTRAIN_SEMITIC_TO_ENGLISH = True  #@param {type:\"boolean\"}\nTRAIN_HEBREW_TO_OTHERS = True  #@param {type:\"boolean\"}\nTRAIN_MIXED_BASELINE = True  #@param {type:\"boolean\"}\n\nimport gc\nimport json\nimport time\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\nfrom sklearn.metrics import f1_score\nfrom tqdm.auto import tqdm\n\nmark_task(\"Train BIP model\", \"running\")\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(\"=\"*60)\nprint(\"TRAINING BIP MODEL\")\nprint(\"=\"*60)\nprint()\n\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n\n# Select splits to train\nsplits_to_train = []\nif TRAIN_TEMPORAL:\n    splits_to_train.append('temporal_ancient_to_modern')\nif TRAIN_SEMITIC_TO_ENGLISH:\n    splits_to_train.append('semitic_to_english')\nif TRAIN_HEBREW_TO_OTHERS:\n    splits_to_train.append('hebrew_to_others')\nif TRAIN_MIXED_BASELINE:\n    splits_to_train.append('mixed_baseline')\n\nprint(f\"Training {len(splits_to_train)} splits: {splits_to_train}\")\n\nall_results = {}\n\nfor split_idx, split_name in enumerate(splits_to_train):\n    split_start = time.time()\n    print()\n    print(\"=\"*60)\n    print(f\"TRAINING [{split_idx+1}/{len(splits_to_train)}]: {split_name}\")\n    print(\"=\"*60)\n    \n    with open(\"data/splits/all_splits.json\", 'r') as f:\n        splits = json.load(f)\n    split = splits[split_name]\n    \n    print(f\"Description: {split.get('description', '')}\")\n    print(f\"Train: {split['train_size']:,} | Valid: {split['valid_size']:,} | Test: {split['test_size']:,}\")\n    print()\n    \n    # Create model\n    model = BIPModel().to(device)\n    \n    # Create datasets\n    print(\"Creating datasets...\")\n    train_dataset = MoralDataset(set(split['train_ids']), \"data/processed/passages.jsonl\",\n                                  \"data/processed/bond_structures.jsonl\", tokenizer)\n    valid_dataset = MoralDataset(set(split['valid_ids']), \"data/processed/passages.jsonl\",\n                                  \"data/processed/bond_structures.jsonl\", tokenizer)\n    test_dataset = MoralDataset(set(split['test_ids']), \"data/processed/passages.jsonl\",\n                                 \"data/processed/bond_structures.jsonl\", tokenizer)\n    \n    if len(train_dataset) == 0:\n        print(\"ERROR: No training data!\")\n        continue\n    \n    # Batch size scaling\n    batch_size = min(BASE_BATCH_SIZE, max(32, len(train_dataset) // 20))\n    num_workers = 4 if IS_L4 or IS_A100 else 2\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                              collate_fn=collate_fn, drop_last=True, num_workers=num_workers,\n                              pin_memory=True, prefetch_factor=4, persistent_workers=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size*2, shuffle=False,\n                              collate_fn=collate_fn, num_workers=num_workers, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False,\n                             collate_fn=collate_fn, num_workers=num_workers, pin_memory=True)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    \n    n_epochs = 5\n    best_valid_loss = float('inf')\n    patience_counter = 0\n    \n    print(f\"Training {n_epochs} epochs (batch={batch_size})...\")\n    \n    for epoch in range(1, n_epochs + 1):\n        epoch_start = time.time()\n        model.train()\n        total_loss = 0\n        n_batches = 0\n        \n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", unit=\"batch\", leave=False):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            time_labels = batch['time_labels'].to(device)\n            language_labels = batch['language_labels'].to(device)\n            hohfeld_labels = batch['hohfeld_labels'].to(device)\n            bond_labels = batch['bond_labels'].to(device)\n            \n            with torch.cuda.amp.autocast(enabled=USE_AMP):\n                outputs = model(input_ids, attention_mask, adversarial_lambda=1.0)\n                \n                loss_time = F.cross_entropy(outputs['time_pred'], time_labels)\n                loss_lang = F.cross_entropy(outputs['language_pred'], language_labels)\n                loss_hohfeld = F.cross_entropy(outputs['hohfeld_pred'], hohfeld_labels)\n                loss_bond = F.cross_entropy(outputs['bond_pred'], bond_labels)\n            \n            loss = loss_hohfeld + loss_bond + loss_time + loss_lang\n            \n            optimizer.zero_grad()\n            if USE_AMP and scaler:\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n            \n            total_loss += loss.item()\n            n_batches += 1\n        \n        # Validation\n        model.eval()\n        valid_loss = 0\n        with torch.no_grad():\n            for batch in valid_loader:\n                outputs = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n                valid_loss += F.cross_entropy(outputs['bond_pred'], batch['bond_labels'].to(device)).item()\n        valid_loss /= len(valid_loader)\n        \n        epoch_time = time.time() - epoch_start\n        print(f\"Epoch {epoch}: Loss={total_loss/n_batches:.4f}/{valid_loss:.4f} | {epoch_time:.1f}s\")\n        \n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), f\"models/checkpoints/best_model_{split_name}.pt\")\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= 3:\n                print(\"Early stopping\")\n                break\n    \n    # Evaluate\n    print(\"\\nEvaluating...\")\n    model.load_state_dict(torch.load(f\"models/checkpoints/best_model_{split_name}.pt\"))\n    model.eval()\n    \n    all_preds = {'time': [], 'lang': [], 'bond': []}\n    all_labels = {'time': [], 'lang': [], 'bond': []}\n    all_languages = []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n            outputs = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n            \n            all_preds['time'].extend(outputs['time_pred'].argmax(-1).cpu().tolist())\n            all_preds['lang'].extend(outputs['language_pred'].argmax(-1).cpu().tolist())\n            all_preds['bond'].extend(outputs['bond_pred'].argmax(-1).cpu().tolist())\n            all_labels['time'].extend(batch['time_labels'].tolist())\n            all_labels['lang'].extend(batch['language_labels'].tolist())\n            all_labels['bond'].extend(batch['bond_labels'].tolist())\n            all_languages.extend(batch['languages'])\n    \n    # Metrics\n    time_acc = sum(p == l for p, l in zip(all_preds['time'], all_labels['time'])) / len(all_preds['time'])\n    lang_acc = sum(p == l for p, l in zip(all_preds['lang'], all_labels['lang'])) / len(all_preds['lang'])\n    bond_acc = sum(p == l for p, l in zip(all_preds['bond'], all_labels['bond'])) / len(all_preds['bond'])\n    bond_f1 = f1_score(all_labels['bond'], all_preds['bond'], average='macro', zero_division=0)\n    \n    # Per-language bond F1\n    lang_bond_f1 = {}\n    for lang in set(all_languages):\n        mask = [l == lang for l in all_languages]\n        if sum(mask) > 0:\n            preds = [p for p, m in zip(all_preds['bond'], mask) if m]\n            labels = [l for l, m in zip(all_labels['bond'], mask) if m]\n            lang_bond_f1[lang] = {\n                'f1': f1_score(labels, preds, average='macro', zero_division=0),\n                'n': sum(mask)\n            }\n    \n    split_time = time.time() - split_start\n    \n    all_results[split_name] = {\n        'time_acc': time_acc,\n        'language_acc': lang_acc,\n        'bond_acc': bond_acc,\n        'bond_f1_macro': bond_f1,\n        'per_language_f1': lang_bond_f1,\n        'training_time': split_time\n    }\n    \n    print(f\"\\n{split_name} RESULTS ({split_time/60:.1f} min):\")\n    print(f\"  Time accuracy (adversary): {time_acc:.1%}\")\n    print(f\"  Language accuracy (adversary): {lang_acc:.1%}\")\n    print(f\"  Bond F1 (macro): {bond_f1:.3f}\")\n    print(\"  Per-language Bond F1:\")\n    for lang, metrics in sorted(lang_bond_f1.items(), key=lambda x: -x[1]['n']):\n        print(f\"    {lang:20s}: F1={metrics['f1']:.3f} (n={metrics['n']:,})\")\n    \n    # Cleanup\n    del model, train_dataset, valid_dataset, test_dataset\n    gc.collect()\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n\nprint()\nprint(\"=\"*60)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*60)\n\nmark_task(\"Train BIP model\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 11. Linear Probe Test { display-mode: \"form\" }\n#@markdown Tests if time/language can be decoded from frozen z_bond.\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nmark_task(\"Linear probe test\", \"running\")\n\nprint(\"=\"*60)\nprint(\"LINEAR PROBE TEST\")\nprint(\"=\"*60)\nprint()\n\nlinear_probe_results = {}\n\nfor split_name in all_results.keys():\n    if split_name == 'mixed_baseline':\n        continue  # Skip baseline for probe test\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"PROBE: {split_name}\")\n    print(f\"{'='*50}\")\n    \n    model = BIPModel().to(device)\n    model.load_state_dict(torch.load(f\"models/checkpoints/best_model_{split_name}.pt\"))\n    model.eval()\n    \n    for param in model.parameters():\n        param.requires_grad = False\n    \n    with open(\"data/splits/all_splits.json\", 'r') as f:\n        split = json.load(f)[split_name]\n    \n    test_dataset = MoralDataset(set(split['test_ids']), \"data/processed/passages.jsonl\",\n                                 \"data/processed/bond_structures.jsonl\", tokenizer)\n    test_loader = DataLoader(test_dataset, batch_size=BASE_BATCH_SIZE, collate_fn=collate_fn, num_workers=4)\n    \n    print(\"Extracting z_bond...\")\n    all_z = []\n    all_time = []\n    all_lang = []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Extract\", unit=\"batch\"):\n            z = model.extract_z_bond(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n            all_z.append(z.cpu().numpy())\n            all_time.extend(batch['time_labels'].tolist())\n            all_lang.extend(batch['language_labels'].tolist())\n    \n    X = np.vstack(all_z)\n    y_time = np.array(all_time)\n    y_lang = np.array(all_lang)\n    \n    # Standardize\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # 50/50 split\n    np.random.seed(42)\n    idx = np.random.permutation(len(X_scaled))\n    train_idx, test_idx = idx[:len(idx)//2], idx[len(idx)//2:]\n    \n    # Time probe\n    print(\"\\nFitting time probe...\")\n    time_probe = LogisticRegression(max_iter=1000, multi_class='multinomial', n_jobs=-1)\n    time_probe.fit(X_scaled[train_idx], y_time[train_idx])\n    time_probe_acc = (time_probe.predict(X_scaled[test_idx]) == y_time[test_idx]).mean()\n    time_chance = 1.0 / len(np.unique(y_time[test_idx]))\n    \n    # Language probe\n    print(\"Fitting language probe...\")\n    lang_probe = LogisticRegression(max_iter=1000, multi_class='multinomial', n_jobs=-1)\n    lang_probe.fit(X_scaled[train_idx], y_lang[train_idx])\n    lang_probe_acc = (lang_probe.predict(X_scaled[test_idx]) == y_lang[test_idx]).mean()\n    lang_chance = 1.0 / len(np.unique(y_lang[test_idx]))\n    \n    time_invariant = time_probe_acc < (time_chance + 0.10)\n    lang_invariant = lang_probe_acc < (lang_chance + 0.10)\n    \n    print(f\"\\nRESULTS:\")\n    print(f\"  Time probe:     {time_probe_acc:.1%} (chance: {time_chance:.1%}) {'✓ INVARIANT' if time_invariant else '✗ LEAKAGE'}\")\n    print(f\"  Language probe: {lang_probe_acc:.1%} (chance: {lang_chance:.1%}) {'✓ INVARIANT' if lang_invariant else '✗ LEAKAGE'}\")\n    \n    linear_probe_results[split_name] = {\n        'time_probe_acc': float(time_probe_acc),\n        'time_chance': float(time_chance),\n        'time_invariant': bool(time_invariant),\n        'lang_probe_acc': float(lang_probe_acc),\n        'lang_chance': float(lang_chance),\n        'lang_invariant': bool(lang_invariant)\n    }\n    \n    del model\n    gc.collect()\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n\nwith open('results/linear_probe_results.json', 'w') as f:\n    json.dump(linear_probe_results, f, indent=2)\n\nprint()\nprint(\"=\"*60)\nprint(\"PROBE SUMMARY\")\nprint(\"=\"*60)\nfor name, res in linear_probe_results.items():\n    time_status = \"✓\" if res['time_invariant'] else \"✗\"\n    lang_status = \"✓\" if res['lang_invariant'] else \"✗\"\n    print(f\"{name}:\")\n    print(f\"  Time: {res['time_probe_acc']:.1%} vs {res['time_chance']:.1%} {time_status}\")\n    print(f\"  Lang: {res['lang_probe_acc']:.1%} vs {res['lang_chance']:.1%} {lang_status}\")\n\nmark_task(\"Linear probe test\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 12. Final Evaluation { display-mode: \"form\" }\n#@markdown Comprehensive results summary.\n\nimport json\nimport time\n\nmark_task(\"Evaluate results\", \"running\")\n\nprint(\"=\"*60)\nprint(\"FINAL BIP EVALUATION (v8 Multilingual)\")\nprint(\"=\"*60)\nprint()\n\n# Load baselines\nwith open(\"data/splits/baselines.json\", 'r') as f:\n    baselines = json.load(f)\n\nchance_bond = baselines['chance_bond']\nchance_time = baselines['chance_time']\nchance_lang = baselines['chance_language']\n\nbaseline_f1 = all_results.get('mixed_baseline', {}).get('bond_f1_macro', 0)\n\nprint(\"=\"*60)\nprint(\"CROSS-DOMAIN TRANSFER RESULTS\")\nprint(\"=\"*60)\n\nfor split_name, res in all_results.items():\n    if split_name == 'mixed_baseline':\n        continue\n    \n    print(f\"\\n{split_name.upper()}\")\n    print(\"-\"*50)\n    \n    probe = linear_probe_results.get(split_name, {})\n    \n    # Invariance\n    time_inv = \"✓\" if probe.get('time_invariant', False) else \"✗\"\n    lang_inv = \"✓\" if probe.get('lang_invariant', False) else \"✗\"\n    print(f\"  Time invariant (probe):     {time_inv} ({probe.get('time_probe_acc', 0):.1%} vs {probe.get('time_chance', 0):.1%})\")\n    print(f\"  Language invariant (probe): {lang_inv} ({probe.get('lang_probe_acc', 0):.1%} vs {probe.get('lang_chance', 0):.1%})\")\n    \n    # Transfer\n    bond_f1 = res['bond_f1_macro']\n    degradation = baseline_f1 - bond_f1 if baseline_f1 > 0 else 0\n    print(f\"  Bond F1 (macro):            {bond_f1:.3f}\")\n    print(f\"  vs in-domain baseline:      {baseline_f1:.3f} (degradation: {degradation:+.3f})\")\n    print(f\"  vs chance ({chance_bond:.1%}):          {bond_f1/chance_bond:.1f}x\")\n    \n    # Per-language\n    if 'per_language_f1' in res:\n        print(\"  Per-language F1:\")\n        for lang, metrics in sorted(res['per_language_f1'].items(), key=lambda x: -x[1]['n']):\n            print(f\"    {lang:20s}: {metrics['f1']:.3f} (n={metrics['n']:,})\")\n\n# Verdict\nprint()\nprint(\"=\"*60)\nprint(\"VERDICT\")\nprint(\"=\"*60)\n\n# Count successes\nn_time_inv = sum(1 for r in linear_probe_results.values() if r.get('time_invariant', False))\nn_lang_inv = sum(1 for r in linear_probe_results.values() if r.get('lang_invariant', False))\nn_transfer = sum(1 for r in all_results.values() if r.get('bond_f1_macro', 0) > chance_bond * 1.5)\n\nprint(f\"\\nTime-invariant splits:     {n_time_inv}/{len(linear_probe_results)}\")\nprint(f\"Language-invariant splits: {n_lang_inv}/{len(linear_probe_results)}\")\nprint(f\"Transfer above 1.5x chance: {n_transfer}/{len(all_results)}\")\n\nif n_time_inv >= 2 and n_lang_inv >= 2 and n_transfer >= 3:\n    verdict = \"STRONGLY_SUPPORTED\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"   BIP: STRONGLY SUPPORTED\")\n    print(\"   Cross-temporal AND cross-lingual transfer demonstrated\")\n    print(\"=\"*60)\nelif n_transfer >= 2:\n    verdict = \"SUPPORTED\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"   BIP: SUPPORTED\")\n    print(\"   Transfer demonstrated with some confound leakage\")\n    print(\"=\"*60)\nelse:\n    verdict = \"INCONCLUSIVE\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"   BIP: INCONCLUSIVE\")\n    print(\"=\"*60)\n\n# Save\ntotal_time = time.time() - EXPERIMENT_START\n\nfinal_results = {\n    'model_results': all_results,\n    'linear_probe_results': linear_probe_results,\n    'verdict': verdict,\n    'total_time_minutes': total_time / 60,\n    'baselines': baselines,\n    'methodology': {\n        'languages': ['hebrew', 'aramaic', 'judeo_arabic', 'classical_chinese', 'arabic', 'english'],\n        'label_source': 'English translations',\n        'encoder': 'paraphrase-multilingual-MiniLM-L12-v2'\n    }\n}\n\nwith open('results/final_results.json', 'w') as f:\n    json.dump(final_results, f, indent=2, default=str)\n\nwith open(f\"{SAVE_DIR}/final_results.json\", 'w') as f:\n    json.dump(final_results, f, indent=2, default=str)\n\nprint(f\"\\nTotal time: {total_time/60:.1f} minutes\")\nprint(f\"Results saved to {SAVE_DIR}\")\n\nmark_task(\"Evaluate results\", \"done\")\n\nprint()\nprint_progress()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 13. Download Results { display-mode: \"form\" }\n\nimport shutil\nfrom google.colab import files\n\n!mkdir -p results\n\nfor split_name in all_results.keys():\n    src = f\"models/checkpoints/best_model_{split_name}.pt\"\n    if os.path.exists(src):\n        !cp \"{src}\" results/\n\n!cp data/splits/*.json results/ 2>/dev/null || true\n!cp results/*.json results/ 2>/dev/null || true\n\nshutil.make_archive('bip_multilingual_v8', 'zip', 'results')\n!ls -la results/\n\nfiles.download('bip_multilingual_v8.zip')",
      "execution_count": null,
      "outputs": []
    }
  ]
}
