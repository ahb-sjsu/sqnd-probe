{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BIP Temporal Invariance Experiment\n",
        "\n",
        "**Testing the Bond Invariance Principle across 2000+ years of moral reasoning**\n",
        "\n",
        "This experiment tests whether moral cognition has invariant structure by:\n",
        "1. Training on ancient Hebrew ethical texts (Sefaria corpus, ~500 BCE - 1800 CE)\n",
        "2. Testing transfer to modern American advice columns (Dear Abby, 1956-2020)\n",
        "\n",
        "**Hypothesis**: If BIP holds, bond-level features should transfer across 2000 years with no accuracy drop.\n",
        "\n",
        "---\n",
        "\n",
        "## Setup Instructions\n",
        "1. **Runtime -> Change runtime type -> GPU (T4) or TPU (v5e)**\n",
        "2. Run cells in order - each shows progress in real-time\n",
        "3. Expected runtime: ~1-2 hours (TPU) or ~2-4 hours (GPU)\n",
        "\n",
        "**Supported Accelerators:**\n",
        "- NVIDIA GPU (T4, V100, A100)\n",
        "- Google TPU (v2, v3, v4, v5e)\n",
        "- CPU (slow, not recommended)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 1. Setup and Install Dependencies { display-mode: \"form\" }\n",
        "#@markdown Installs packages and detects GPU/TPU.\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BIP TEMPORAL INVARIANCE EXPERIMENT\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Progress tracker - persists across cells\n",
        "TASKS = [\n",
        "    \"Install dependencies\",\n",
        "    \"Clone Sefaria corpus (~8GB)\",\n",
        "    \"Clone sqnd-probe repo (Dear Abby data)\",\n",
        "    \"Preprocess corpora\",\n",
        "    \"Extract bond structures\",\n",
        "    \"Generate train/test splits\",\n",
        "    \"Train BIP model\",\n",
        "    \"Evaluate results\"\n",
        "]\n",
        "task_status = {task: \"pending\" for task in TASKS}\n",
        "\n",
        "def print_progress():\n",
        "    \"\"\"Print current progress checklist.\"\"\"\n",
        "    print()\n",
        "    print(\"-\" * 50)\n",
        "    print(\"EXPERIMENT PROGRESS:\")\n",
        "    print(\"-\" * 50)\n",
        "    for task in TASKS:\n",
        "        status = task_status[task]\n",
        "        if status == \"done\":\n",
        "            mark = \"[X]\"\n",
        "        elif status == \"running\":\n",
        "            mark = \"[>]\"\n",
        "        else:\n",
        "            mark = \"[ ]\"\n",
        "        print(f\"  {mark} {task}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(flush=True)\n",
        "\n",
        "def mark_task(task, status):\n",
        "    \"\"\"Update task status and print progress.\"\"\"\n",
        "    task_status[task] = status\n",
        "    print_progress()\n",
        "\n",
        "print_progress()\n",
        "\n",
        "# Detect accelerator type\n",
        "mark_task(\"Install dependencies\", \"running\")\n",
        "\n",
        "import os\n",
        "USE_TPU = False\n",
        "TPU_TYPE = None\n",
        "\n",
        "# Check for TPU\n",
        "if 'COLAB_TPU_ADDR' in os.environ or os.path.exists('/dev/accel0'):\n",
        "    print(\"TPU detected! Installing torch_xla...\")\n",
        "    USE_TPU = True\n",
        "    # Install PyTorch XLA for TPU support\n",
        "    !pip install -q torch~=2.4.0 torch_xla[tpu]~=2.4.0 -f https://storage.googleapis.com/libtpu-releases/index.html\n",
        "    !pip install -q transformers sentence-transformers scipy scikit-learn pandas numpy tqdm pyyaml\n",
        "else:\n",
        "    print(\"No TPU detected, using GPU/CPU...\")\n",
        "    !pip install -q torch transformers sentence-transformers scipy scikit-learn pandas numpy tqdm pyyaml\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Setup device\n",
        "if USE_TPU:\n",
        "    try:\n",
        "        import torch_xla\n",
        "        import torch_xla.core.xla_model as xm\n",
        "        import torch_xla.distributed.parallel_loader as pl\n",
        "        \n",
        "        device = xm.xla_device()\n",
        "        TPU_TYPE = str(device)\n",
        "        print(f\"\\nTPU initialized successfully!\")\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"TPU cores available: {xm.xrt_world_size()}\")\n",
        "        ACCELERATOR = \"TPU\"\n",
        "    except Exception as e:\n",
        "        print(f\"TPU initialization failed: {e}\")\n",
        "        print(\"Falling back to GPU/CPU...\")\n",
        "        USE_TPU = False\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        ACCELERATOR = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
        "else:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    ACCELERATOR = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
        "\n",
        "if ACCELERATOR == \"GPU\":\n",
        "    print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "elif ACCELERATOR == \"CPU\":\n",
        "    print(\"\\nWARNING: No accelerator detected! Training will be slow.\")\n",
        "    print(\"Go to Runtime -> Change runtime type -> GPU or TPU\")\n",
        "\n",
        "print(f\"\\n>>> Using: {ACCELERATOR} <<<\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('data/raw', exist_ok=True)\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "os.makedirs('data/splits', exist_ok=True)\n",
        "os.makedirs('models/checkpoints', exist_ok=True)\n",
        "\n",
        "mark_task(\"Install dependencies\", \"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 2. Download Sefaria Corpus (~8GB) { display-mode: \"form\" }\n",
        "#@markdown Downloads the complete Sefaria corpus with real-time git progress.\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "mark_task(\"Clone Sefaria corpus (~8GB)\", \"running\")\n",
        "\n",
        "sefaria_path = 'data/raw/Sefaria-Export'\n",
        "\n",
        "if not os.path.exists(sefaria_path) or not os.path.exists(f\"{sefaria_path}/json\"):\n",
        "    print(\"=\"*60)\n",
        "    print(\"CLONING SEFARIA CORPUS\")\n",
        "    print(\"=\"*60)\n",
        "    print()\n",
        "    print(\"This downloads ~3.5GB and takes 5-15 minutes.\")\n",
        "    print(\"Git's native progress will display below:\")\n",
        "    print(\"-\"*60)\n",
        "    print(flush=True)\n",
        "    \n",
        "    # Use subprocess.Popen for real-time output streaming\n",
        "    process = subprocess.Popen(\n",
        "        ['git', 'clone', '--depth', '1', '--progress',\n",
        "         'https://github.com/Sefaria/Sefaria-Export.git', sefaria_path],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,  # Git writes progress to stderr\n",
        "        text=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    \n",
        "    # Stream output in real-time\n",
        "    for line in process.stdout:\n",
        "        print(line, end='', flush=True)\n",
        "    \n",
        "    process.wait()\n",
        "    \n",
        "    print(\"-\"*60)\n",
        "    if process.returncode == 0:\n",
        "        print(\"\\nSefaria clone COMPLETE!\")\n",
        "    else:\n",
        "        print(f\"\\nERROR: Git clone failed with code {process.returncode}\")\n",
        "        print(\"Try running this cell again, or check your internet connection.\")\n",
        "else:\n",
        "    print(\"Sefaria already exists, skipping download.\")\n",
        "\n",
        "# Verify and count files\n",
        "print()\n",
        "print(\"Verifying download...\")\n",
        "!du -sh {sefaria_path} 2>/dev/null || echo \"Directory not found\"\n",
        "json_count = !find {sefaria_path}/json -name \"*.json\" 2>/dev/null | wc -l\n",
        "print(f\"Sefaria JSON files found: {json_count[0]}\")\n",
        "\n",
        "mark_task(\"Clone Sefaria corpus (~8GB)\", \"done\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 3. Download Dear Abby Dataset { display-mode: \"form\" }\n",
        "#@markdown Downloads the Dear Abby advice column dataset (68,330 entries).\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "mark_task(\"Clone sqnd-probe repo (Dear Abby data)\", \"running\")\n",
        "\n",
        "sqnd_path = 'sqnd-probe-data'\n",
        "if not os.path.exists(sqnd_path):\n",
        "    print(\"Cloning sqnd-probe repo...\")\n",
        "    process = subprocess.Popen(\n",
        "        ['git', 'clone', '--depth', '1', '--progress',\n",
        "         'https://github.com/ahb-sjsu/sqnd-probe.git', sqnd_path],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    for line in process.stdout:\n",
        "        print(line, end='', flush=True)\n",
        "    process.wait()\n",
        "else:\n",
        "    print(\"Repo already cloned.\")\n",
        "\n",
        "# Copy Dear Abby data\n",
        "dear_abby_source = Path('sqnd-probe-data/dear_abby_data/raw_da_qs.csv')\n",
        "dear_abby_path = Path('data/raw/dear_abby.csv')\n",
        "\n",
        "if dear_abby_source.exists():\n",
        "    !cp \"{dear_abby_source}\" \"{dear_abby_path}\"\n",
        "    print(f\"\\nCopied Dear Abby data\")\n",
        "elif not dear_abby_path.exists():\n",
        "    raise FileNotFoundError(\"Dear Abby dataset not found!\")\n",
        "\n",
        "# Verify\n",
        "df_check = pd.read_csv(dear_abby_path)\n",
        "print(f\"\\n\" + \"=\" * 50)\n",
        "print(f\"Dear Abby dataset: {len(df_check):,} entries\")\n",
        "print(f\"Columns: {list(df_check.columns)}\")\n",
        "print(f\"Year range: {df_check['year'].min():.0f} - {df_check['year'].max():.0f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "mark_task(\"Clone sqnd-probe repo (Dear Abby data)\", \"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 4. Define Data Classes and Loaders { display-mode: \"form\" }\n",
        "#@markdown Defines enums, dataclasses, and corpus loaders.\n",
        "\n",
        "import json\n",
        "import hashlib\n",
        "import re\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import List, Dict\n",
        "from enum import Enum\n",
        "from collections import defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"Defining data structures...\")\n",
        "\n",
        "class TimePeriod(Enum):\n",
        "    BIBLICAL = 0        # ~1000-500 BCE\n",
        "    SECOND_TEMPLE = 1   # ~500 BCE - 70 CE\n",
        "    TANNAITIC = 2       # ~70-200 CE\n",
        "    AMORAIC = 3         # ~200-500 CE\n",
        "    GEONIC = 4          # ~600-1000 CE\n",
        "    RISHONIM = 5        # ~1000-1500 CE\n",
        "    ACHRONIM = 6        # ~1500-1800 CE\n",
        "    MODERN_HEBREW = 7   # ~1800-present\n",
        "    DEAR_ABBY = 8       # 1956-2020\n",
        "\n",
        "class BondType(Enum):\n",
        "    HARM_PREVENTION = 0\n",
        "    RECIPROCITY = 1\n",
        "    AUTONOMY = 2\n",
        "    PROPERTY = 3\n",
        "    FAMILY = 4\n",
        "    AUTHORITY = 5\n",
        "    EMERGENCY = 6\n",
        "    CONTRACT = 7\n",
        "    CARE = 8\n",
        "    FAIRNESS = 9\n",
        "\n",
        "class HohfeldianState(Enum):\n",
        "    RIGHT = 0\n",
        "    OBLIGATION = 1\n",
        "    LIBERTY = 2\n",
        "    NO_RIGHT = 3\n",
        "\n",
        "@dataclass\n",
        "class Passage:\n",
        "    id: str\n",
        "    text_original: str\n",
        "    text_english: str\n",
        "    time_period: str\n",
        "    century: int\n",
        "    source: str\n",
        "    source_type: str\n",
        "    category: str\n",
        "    language: str = \"hebrew\"\n",
        "    word_count: int = 0\n",
        "    has_dispute: bool = False\n",
        "    consensus_tier: str = \"unknown\"\n",
        "    bond_types: List[str] = field(default_factory=list)\n",
        "    \n",
        "    def to_dict(self):\n",
        "        return asdict(self)\n",
        "\n",
        "CATEGORY_TO_PERIOD = {\n",
        "    'Tanakh': TimePeriod.BIBLICAL,\n",
        "    'Torah': TimePeriod.BIBLICAL,\n",
        "    'Mishnah': TimePeriod.TANNAITIC,\n",
        "    'Tosefta': TimePeriod.TANNAITIC,\n",
        "    'Talmud': TimePeriod.AMORAIC,\n",
        "    'Bavli': TimePeriod.AMORAIC,\n",
        "    'Midrash': TimePeriod.AMORAIC,\n",
        "    'Halakhah': TimePeriod.RISHONIM,\n",
        "    'Chasidut': TimePeriod.ACHRONIM,\n",
        "}\n",
        "\n",
        "PERIOD_TO_CENTURY = {\n",
        "    TimePeriod.BIBLICAL: -6,\n",
        "    TimePeriod.SECOND_TEMPLE: -2,\n",
        "    TimePeriod.TANNAITIC: 2,\n",
        "    TimePeriod.AMORAIC: 4,\n",
        "    TimePeriod.GEONIC: 8,\n",
        "    TimePeriod.RISHONIM: 12,\n",
        "    TimePeriod.ACHRONIM: 17,\n",
        "    TimePeriod.MODERN_HEBREW: 20,\n",
        "}\n",
        "\n",
        "def load_sefaria(base_path: str, max_passages: int = None) -> List[Passage]:\n",
        "    \"\"\"Load Sefaria corpus with progress bar.\"\"\"\n",
        "    passages = []\n",
        "    json_path = Path(base_path) / \"json\"\n",
        "    \n",
        "    if not json_path.exists():\n",
        "        print(f\"Warning: {json_path} not found\")\n",
        "        return []\n",
        "    \n",
        "    json_files = list(json_path.rglob(\"*.json\"))\n",
        "    print(f\"Processing {len(json_files):,} JSON files...\")\n",
        "    \n",
        "    for json_file in tqdm(json_files[:max_passages] if max_passages else json_files,\n",
        "                          desc=\"Loading Sefaria\", unit=\"file\"):\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "        except:\n",
        "            continue\n",
        "        \n",
        "        rel_path = json_file.relative_to(json_path)\n",
        "        category = str(rel_path.parts[0]) if rel_path.parts else \"unknown\"\n",
        "        time_period = CATEGORY_TO_PERIOD.get(category, TimePeriod.AMORAIC)\n",
        "        century = PERIOD_TO_CENTURY.get(time_period, 0)\n",
        "        \n",
        "        if isinstance(data, dict):\n",
        "            hebrew = data.get('he', data.get('text', []))\n",
        "            english = data.get('text', data.get('en', []))\n",
        "            \n",
        "            def flatten(h, e, ref=\"\"):\n",
        "                if isinstance(h, str) and isinstance(e, str):\n",
        "                    h_clean = re.sub(r'<[^>]+>', '', h).strip()\n",
        "                    e_clean = re.sub(r'<[^>]+>', '', e).strip()\n",
        "                    if 50 <= len(e_clean) <= 2000:\n",
        "                        pid = hashlib.md5(f\"{json_file.stem}:{ref}:{h_clean[:50]}\".encode()).hexdigest()[:12]\n",
        "                        return [Passage(\n",
        "                            id=f\"sefaria_{pid}\",\n",
        "                            text_original=h_clean,\n",
        "                            text_english=e_clean,\n",
        "                            time_period=time_period.name,\n",
        "                            century=century,\n",
        "                            source=f\"{json_file.stem} {ref}\".strip(),\n",
        "                            source_type=\"sefaria\",\n",
        "                            category=category,\n",
        "                            language=\"hebrew\",\n",
        "                            word_count=len(e_clean.split())\n",
        "                        )]\n",
        "                    return []\n",
        "                elif isinstance(h, list) and isinstance(e, list):\n",
        "                    result = []\n",
        "                    for i, (hh, ee) in enumerate(zip(h, e)):\n",
        "                        result.extend(flatten(hh, ee, f\"{ref}.{i+1}\" if ref else str(i+1)))\n",
        "                    return result\n",
        "                return []\n",
        "            \n",
        "            passages.extend(flatten(hebrew, english))\n",
        "    \n",
        "    return passages\n",
        "\n",
        "def load_dear_abby(path: str, max_passages: int = None) -> List[Passage]:\n",
        "    \"\"\"Load Dear Abby corpus with progress bar.\"\"\"\n",
        "    passages = []\n",
        "    df = pd.read_csv(path)\n",
        "    \n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading Dear Abby\", unit=\"row\"):\n",
        "        question = str(row.get('question_only', ''))\n",
        "        if not question or question == 'nan' or len(question) < 50 or len(question) > 2000:\n",
        "            continue\n",
        "        \n",
        "        year = int(row.get('year', 1990))\n",
        "        pid = hashlib.md5(f\"abby:{idx}:{question[:50]}\".encode()).hexdigest()[:12]\n",
        "        \n",
        "        passages.append(Passage(\n",
        "            id=f\"abby_{pid}\",\n",
        "            text_original=question,\n",
        "            text_english=question,\n",
        "            time_period=TimePeriod.DEAR_ABBY.name,\n",
        "            century=20 if year < 2000 else 21,\n",
        "            source=f\"Dear Abby {year}\",\n",
        "            source_type=\"dear_abby\",\n",
        "            category=\"general\",\n",
        "            language=\"english\",\n",
        "            word_count=len(question.split())\n",
        "        ))\n",
        "        \n",
        "        if max_passages and len(passages) >= max_passages:\n",
        "            break\n",
        "    \n",
        "    return passages\n",
        "\n",
        "print(\"Data structures defined!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 5. Load and Preprocess Corpora { display-mode: \"form\" }\n",
        "#@markdown Loads both corpora and shows statistics.\n",
        "\n",
        "mark_task(\"Preprocess corpora\", \"running\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING CORPORA\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Load Sefaria\n",
        "sefaria_passages = load_sefaria(\"data/raw/Sefaria-Export\")\n",
        "print(f\"\\nSefaria passages loaded: {len(sefaria_passages):,}\")\n",
        "\n",
        "# Load Dear Abby\n",
        "print()\n",
        "abby_passages = load_dear_abby(\"data/raw/dear_abby.csv\")\n",
        "print(f\"\\nDear Abby passages loaded: {len(abby_passages):,}\")\n",
        "\n",
        "# Combine\n",
        "all_passages = sefaria_passages + abby_passages\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(f\"TOTAL PASSAGES: {len(all_passages):,}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Statistics\n",
        "by_period = defaultdict(int)\n",
        "by_source = defaultdict(int)\n",
        "for p in all_passages:\n",
        "    by_period[p.time_period] += 1\n",
        "    by_source[p.source_type] += 1\n",
        "\n",
        "print(\"\\nBy source:\")\n",
        "for source, count in sorted(by_source.items()):\n",
        "    print(f\"  {source}: {count:,}\")\n",
        "\n",
        "print(\"\\nBy time period:\")\n",
        "for period, count in sorted(by_period.items()):\n",
        "    pct = count / len(all_passages) * 100\n",
        "    bar = '#' * int(pct / 2)\n",
        "    print(f\"  {period:20s}: {count:6,} ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "mark_task(\"Preprocess corpora\", \"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 6. Extract Bond Structures { display-mode: \"form\" }\n",
        "#@markdown Extracts moral bond structures from each passage.\n",
        "\n",
        "mark_task(\"Extract bond structures\", \"running\")\n",
        "\n",
        "RELATION_PATTERNS = {\n",
        "    BondType.HARM_PREVENTION: [r'\\b(kill|murder|harm|hurt|save|rescue|protect|danger)\\b'],\n",
        "    BondType.RECIPROCITY: [r'\\b(return|repay|owe|debt|mutual|exchange)\\b'],\n",
        "    BondType.AUTONOMY: [r'\\b(choose|decision|consent|agree|force|coerce|right)\\b'],\n",
        "    BondType.PROPERTY: [r'\\b(property|own|steal|theft|buy|sell|land)\\b'],\n",
        "    BondType.FAMILY: [r'\\b(honor|parent|marry|divorce|inherit|family)\\b'],\n",
        "    BondType.AUTHORITY: [r'\\b(obey|command|law|judge|rule|teach)\\b'],\n",
        "    BondType.CARE: [r'\\b(care|help|assist|feed|clothe|visit)\\b'],\n",
        "    BondType.FAIRNESS: [r'\\b(fair|just|equal|deserve|bias)\\b'],\n",
        "}\n",
        "\n",
        "HOHFELD_PATTERNS = {\n",
        "    HohfeldianState.OBLIGATION: [r'\\b(must|shall|duty|require|should)\\b'],\n",
        "    HohfeldianState.RIGHT: [r'\\b(right to|entitled|deserve)\\b'],\n",
        "    HohfeldianState.LIBERTY: [r'\\b(may|can|permitted|allowed)\\b'],\n",
        "}\n",
        "\n",
        "def extract_bond_structure(passage: Passage) -> Dict:\n",
        "    \"\"\"Extract bond structure from passage.\"\"\"\n",
        "    text = passage.text_english.lower()\n",
        "    \n",
        "    relations = []\n",
        "    for rel_type, patterns in RELATION_PATTERNS.items():\n",
        "        for pattern in patterns:\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                relations.append(rel_type.name)\n",
        "                break\n",
        "    \n",
        "    if not relations:\n",
        "        relations = ['CARE']\n",
        "    \n",
        "    hohfeld = None\n",
        "    for state, patterns in HOHFELD_PATTERNS.items():\n",
        "        for pattern in patterns:\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                hohfeld = state.name\n",
        "                break\n",
        "        if hohfeld:\n",
        "            break\n",
        "    \n",
        "    signature = \"|\".join(sorted(set(relations)))\n",
        "    \n",
        "    return {\n",
        "        'bonds': [{'relation': r} for r in relations],\n",
        "        'primary_relation': relations[0],\n",
        "        'hohfeld_state': hohfeld,\n",
        "        'signature': signature\n",
        "    }\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXTRACTING BOND STRUCTURES\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "bond_structures = []\n",
        "for passage in tqdm(all_passages, desc=\"Extracting bonds\", unit=\"passage\"):\n",
        "    bond_struct = extract_bond_structure(passage)\n",
        "    passage.bond_types = [b['relation'] for b in bond_struct['bonds']]\n",
        "    bond_structures.append({\n",
        "        'passage_id': passage.id,\n",
        "        'bond_structure': bond_struct\n",
        "    })\n",
        "\n",
        "# Save\n",
        "print(\"\\nSaving processed data...\")\n",
        "with open(\"data/processed/passages.jsonl\", 'w') as f:\n",
        "    for p in all_passages:\n",
        "        f.write(json.dumps(p.to_dict()) + '\\n')\n",
        "\n",
        "with open(\"data/processed/bond_structures.jsonl\", 'w') as f:\n",
        "    for bs in bond_structures:\n",
        "        f.write(json.dumps(bs) + '\\n')\n",
        "\n",
        "# Bond type distribution\n",
        "bond_counts = defaultdict(int)\n",
        "for bs in bond_structures:\n",
        "    for bond in bs['bond_structure']['bonds']:\n",
        "        bond_counts[bond['relation']] += 1\n",
        "\n",
        "print(\"\\nBond type distribution:\")\n",
        "for bond_type, count in sorted(bond_counts.items(), key=lambda x: -x[1]):\n",
        "    pct = count / sum(bond_counts.values()) * 100\n",
        "    bar = '#' * int(pct)\n",
        "    print(f\"  {bond_type:20s}: {count:6,} ({pct:5.1f}%) {bar}\")\n",
        "\n",
        "mark_task(\"Extract bond structures\", \"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 7. Generate Train/Test Splits { display-mode: \"form\" }\n",
        "#@markdown Creates temporal holdout split: train on ancient, test on modern.\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "mark_task(\"Generate train/test splits\", \"running\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GENERATING SPLITS\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# TEMPORAL HOLDOUT SPLIT (Primary BIP Test)\n",
        "# Train: Ancient/Medieval Hebrew texts\n",
        "# Test: Modern (Dear Abby)\n",
        "train_periods = {'BIBLICAL', 'SECOND_TEMPLE', 'TANNAITIC', 'AMORAIC', 'GEONIC', 'RISHONIM'}\n",
        "valid_periods = {'ACHRONIM'}\n",
        "test_periods = {'MODERN_HEBREW', 'DEAR_ABBY'}\n",
        "\n",
        "train = [p for p in all_passages if p.time_period in train_periods]\n",
        "valid = [p for p in all_passages if p.time_period in valid_periods]\n",
        "test = [p for p in all_passages if p.time_period in test_periods]\n",
        "\n",
        "random.shuffle(train)\n",
        "random.shuffle(valid)\n",
        "random.shuffle(test)\n",
        "\n",
        "temporal_holdout = {\n",
        "    'name': 'temporal_holdout',\n",
        "    'train_ids': [p.id for p in train],\n",
        "    'valid_ids': [p.id for p in valid],\n",
        "    'test_ids': [p.id for p in test],\n",
        "    'train_size': len(train),\n",
        "    'valid_size': len(valid),\n",
        "    'test_size': len(test)\n",
        "}\n",
        "\n",
        "print(\"TEMPORAL HOLDOUT SPLIT (Primary BIP Test):\")\n",
        "print(f\"  Train (ancient/medieval, ~500 BCE - 1500 CE): {len(train):,}\")\n",
        "print(f\"  Valid (early modern, ~1500 - 1800 CE):        {len(valid):,}\")\n",
        "print(f\"  Test (modern, 1956 - 2020):                   {len(test):,}\")\n",
        "print()\n",
        "print(f\"  Temporal gap: ~500 years between train and test\")\n",
        "\n",
        "# Also create stratified random for comparison\n",
        "random.shuffle(all_passages)\n",
        "n = len(all_passages)\n",
        "n_train = int(0.7 * n)\n",
        "n_valid = int(0.15 * n)\n",
        "\n",
        "stratified = {\n",
        "    'name': 'stratified_random',\n",
        "    'train_ids': [p.id for p in all_passages[:n_train]],\n",
        "    'valid_ids': [p.id for p in all_passages[n_train:n_train+n_valid]],\n",
        "    'test_ids': [p.id for p in all_passages[n_train+n_valid:]],\n",
        "    'train_size': n_train,\n",
        "    'valid_size': n_valid,\n",
        "    'test_size': n - n_train - n_valid\n",
        "}\n",
        "\n",
        "print()\n",
        "print(\"STRATIFIED RANDOM SPLIT (Control):\")\n",
        "print(f\"  Train: {stratified['train_size']:,}\")\n",
        "print(f\"  Valid: {stratified['valid_size']:,}\")\n",
        "print(f\"  Test:  {stratified['test_size']:,}\")\n",
        "\n",
        "# Save\n",
        "splits = {\n",
        "    'temporal_holdout': temporal_holdout,\n",
        "    'stratified_random': stratified\n",
        "}\n",
        "\n",
        "with open(\"data/splits/all_splits.json\", 'w') as f:\n",
        "    json.dump(splits, f, indent=2)\n",
        "\n",
        "print(\"\\nSplits saved!\")\n",
        "\n",
        "mark_task(\"Generate train/test splits\", \"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 8. Define BIP Model Architecture { display-mode: \"form\" }\n",
        "#@markdown Defines the adversarial disentanglement model (GPU/TPU compatible).\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DEFINING MODEL ARCHITECTURE\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "class GradientReversal(torch.autograd.Function):\n",
        "    \"\"\"Gradient reversal layer for adversarial training.\"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambda_):\n",
        "        ctx.lambda_ = lambda_\n",
        "        return x.clone()\n",
        "    \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return -ctx.lambda_ * grad_output, None\n",
        "\n",
        "def gradient_reversal(x, lambda_=1.0):\n",
        "    return GradientReversal.apply(x, lambda_)\n",
        "\n",
        "class BIPEncoder(nn.Module):\n",
        "    \"\"\"Sentence encoder using pretrained transformer.\"\"\"\n",
        "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", d_model=384):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.d_model = d_model\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden = outputs.last_hidden_state\n",
        "        mask = attention_mask.unsqueeze(-1).float()\n",
        "        pooled = (hidden * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
        "        return pooled\n",
        "\n",
        "class BIPModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Bond Invariance Principle Model.\n",
        "    \n",
        "    Disentangles representations into:\n",
        "    - z_bond: Time-invariant moral structure (should NOT predict time period)\n",
        "    - z_label: Temporal/cultural context (CAN predict time period)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model=384, d_bond=64, d_label=32, n_periods=9, n_hohfeld=4):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = BIPEncoder()\n",
        "        \n",
        "        # Disentanglement heads\n",
        "        self.bond_proj = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(d_model // 2, d_bond)\n",
        "        )\n",
        "        \n",
        "        self.label_proj = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(d_model // 2, d_label)\n",
        "        )\n",
        "        \n",
        "        # Classifiers\n",
        "        self.time_from_bond = nn.Linear(d_bond, n_periods)   # Should fail (adversarial)\n",
        "        self.time_from_label = nn.Linear(d_label, n_periods) # Should succeed\n",
        "        self.hohfeld_classifier = nn.Linear(d_bond, n_hohfeld)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, adversarial_lambda=1.0):\n",
        "        h = self.encoder(input_ids, attention_mask)\n",
        "        \n",
        "        z_bond = self.bond_proj(h)\n",
        "        z_label = self.label_proj(h)\n",
        "        \n",
        "        # Time prediction with gradient reversal on z_bond\n",
        "        if adversarial_lambda > 0:\n",
        "            z_bond_adv = gradient_reversal(z_bond, adversarial_lambda)\n",
        "            time_pred_bond = self.time_from_bond(z_bond_adv)\n",
        "        else:\n",
        "            time_pred_bond = self.time_from_bond(z_bond)\n",
        "        \n",
        "        time_pred_label = self.time_from_label(z_label)\n",
        "        hohfeld_pred = self.hohfeld_classifier(z_bond)\n",
        "        \n",
        "        return {\n",
        "            'z_bond': z_bond,\n",
        "            'z_label': z_label,\n",
        "            'time_pred_bond': time_pred_bond,\n",
        "            'time_pred_label': time_pred_label,\n",
        "            'hohfeld_pred': hohfeld_pred\n",
        "        }\n",
        "\n",
        "# Dataset class\n",
        "PERIOD_TO_IDX = {p.name: i for i, p in enumerate(TimePeriod)}\n",
        "HOHFELD_TO_IDX = {h.name: i for i, h in enumerate(HohfeldianState)}\n",
        "\n",
        "class MoralDataset(Dataset):\n",
        "    def __init__(self, passage_ids, passages_file, bonds_file, tokenizer, max_length=256):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        self.passages = {}\n",
        "        with open(passages_file) as f:\n",
        "            for line in f:\n",
        "                p = json.loads(line)\n",
        "                if p['id'] in passage_ids:\n",
        "                    self.passages[p['id']] = p\n",
        "        \n",
        "        self.bonds = {}\n",
        "        with open(bonds_file) as f:\n",
        "            for line in f:\n",
        "                b = json.loads(line)\n",
        "                self.bonds[b['passage_id']] = b['bond_structure']\n",
        "        \n",
        "        self.ids = [pid for pid in passage_ids if pid in self.passages]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        pid = self.ids[idx]\n",
        "        passage = self.passages[pid]\n",
        "        \n",
        "        encoded = self.tokenizer(\n",
        "            passage['text_english'],\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        time_label = PERIOD_TO_IDX.get(passage['time_period'], 0)\n",
        "        hohfeld = self.bonds.get(pid, {}).get('hohfeld_state')\n",
        "        hohfeld_label = HOHFELD_TO_IDX.get(hohfeld, 0) if hohfeld else 0\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'time_label': time_label,\n",
        "            'hohfeld_label': hohfeld_label\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
        "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
        "        'time_labels': torch.tensor([b['time_label'] for b in batch]),\n",
        "        'hohfeld_labels': torch.tensor([b['hohfeld_label'] for b in batch])\n",
        "    }\n",
        "\n",
        "print(\"Model architecture defined!\")\n",
        "print()\n",
        "print(\"Key components:\")\n",
        "print(\"  - BIPEncoder: Sentence transformer (MiniLM-L6)\")\n",
        "print(\"  - bond_proj: Projects to time-invariant z_bond\")\n",
        "print(\"  - label_proj: Projects to temporal z_label\")\n",
        "print(\"  - Gradient reversal: Forces z_bond to be time-agnostic\")\n",
        "print(f\"\\nTarget device: {ACCELERATOR}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 9. Train BIP Model (GPU/TPU) { display-mode: \"form\" }\n",
        "#@markdown Trains the model with adversarial disentanglement. Auto-detects GPU or TPU.\n",
        "\n",
        "mark_task(\"Train BIP model\", \"running\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING BIP MODEL\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(f\"Accelerator: {ACCELERATOR}\")\n",
        "print(f\"Device: {device}\")\n",
        "print()\n",
        "\n",
        "# Use temporal holdout split\n",
        "split = temporal_holdout\n",
        "print(f\"Using TEMPORAL HOLDOUT split\")\n",
        "print(f\"  Train: {split['train_size']:,} (ancient/medieval)\")\n",
        "print(f\"  Valid: {split['valid_size']:,} (early modern)\")\n",
        "print(f\"  Test:  {split['test_size']:,} (modern)\")\n",
        "print()\n",
        "\n",
        "# Initialize\n",
        "print(\"Loading tokenizer and model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model = BIPModel().to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print()\n",
        "\n",
        "# Create datasets\n",
        "print(\"Creating datasets...\")\n",
        "train_dataset = MoralDataset(\n",
        "    set(split['train_ids']),\n",
        "    \"data/processed/passages.jsonl\",\n",
        "    \"data/processed/bond_structures.jsonl\",\n",
        "    tokenizer\n",
        ")\n",
        "valid_dataset = MoralDataset(\n",
        "    set(split['valid_ids']),\n",
        "    \"data/processed/passages.jsonl\",\n",
        "    \"data/processed/bond_structures.jsonl\",\n",
        "    tokenizer\n",
        ")\n",
        "test_dataset = MoralDataset(\n",
        "    set(split['test_ids']),\n",
        "    \"data/processed/passages.jsonl\",\n",
        "    \"data/processed/bond_structures.jsonl\",\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset):,}\")\n",
        "print(f\"Valid samples: {len(valid_dataset):,}\")\n",
        "print(f\"Test samples:  {len(test_dataset):,}\")\n",
        "print()\n",
        "\n",
        "if len(train_dataset) == 0:\n",
        "    print(\"ERROR: No training data! Check if Sefaria loaded correctly.\")\n",
        "else:\n",
        "    # Adjust batch size for TPU (larger batches work better)\n",
        "    batch_size = 64 if USE_TPU else 32\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
        "                              collate_fn=collate_fn, drop_last=True, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size*2, shuffle=False, \n",
        "                              collate_fn=collate_fn, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False, \n",
        "                             collate_fn=collate_fn, num_workers=2)\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "    \n",
        "    n_epochs = 10\n",
        "    best_valid_loss = float('inf')\n",
        "    \n",
        "    print(f\"Training for {n_epochs} epochs (batch_size={batch_size})...\")\n",
        "    print(\"=\" * 60)\n",
        "    print()\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        n_batches = 0\n",
        "        \n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{n_epochs}\", unit=\"batch\")\n",
        "        for batch in pbar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            time_labels = batch['time_labels'].to(device)\n",
        "            hohfeld_labels = batch['hohfeld_labels'].to(device)\n",
        "            \n",
        "            outputs = model(input_ids, attention_mask, adversarial_lambda=1.0)\n",
        "            \n",
        "            # Losses\n",
        "            # 1. Adversarial: maximize entropy of time prediction from z_bond\n",
        "            time_probs = F.softmax(outputs['time_pred_bond'], dim=-1)\n",
        "            entropy = -torch.sum(time_probs * torch.log(time_probs + 1e-8), dim=-1)\n",
        "            loss_adv = -entropy.mean()\n",
        "            \n",
        "            # 2. Time prediction from z_label should work\n",
        "            loss_time = F.cross_entropy(outputs['time_pred_label'], time_labels)\n",
        "            \n",
        "            # 3. Hohfeldian classification from z_bond\n",
        "            loss_hohfeld = F.cross_entropy(outputs['hohfeld_pred'], hohfeld_labels)\n",
        "            \n",
        "            loss = loss_adv + loss_time + loss_hohfeld\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            \n",
        "            # TPU vs GPU optimizer step\n",
        "            if USE_TPU:\n",
        "                xm.optimizer_step(optimizer)\n",
        "                xm.mark_step()  # Sync TPU\n",
        "            else:\n",
        "                optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            n_batches += 1\n",
        "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'adv': f\"{loss_adv.item():.3f}\"})\n",
        "        \n",
        "        avg_train_loss = total_loss / n_batches\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        valid_batches = 0\n",
        "        time_correct = 0\n",
        "        time_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in valid_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                time_labels = batch['time_labels'].to(device)\n",
        "                hohfeld_labels = batch['hohfeld_labels'].to(device)\n",
        "                \n",
        "                outputs = model(input_ids, attention_mask, adversarial_lambda=0)\n",
        "                loss = F.cross_entropy(outputs['hohfeld_pred'], hohfeld_labels)\n",
        "                valid_loss += loss.item()\n",
        "                valid_batches += 1\n",
        "                \n",
        "                # Check if z_bond predicts time (should be near chance = 11%)\n",
        "                time_preds = outputs['time_pred_bond'].argmax(dim=-1)\n",
        "                time_correct += (time_preds == time_labels).sum().item()\n",
        "                time_total += len(time_labels)\n",
        "                \n",
        "                if USE_TPU:\n",
        "                    xm.mark_step()\n",
        "        \n",
        "        avg_valid_loss = valid_loss / valid_batches if valid_batches > 0 else 0\n",
        "        time_acc = time_correct / time_total if time_total > 0 else 0\n",
        "        \n",
        "        # Print epoch summary\n",
        "        print(f\"\\nEpoch {epoch} Summary:\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"  Valid Loss: {avg_valid_loss:.4f}\")\n",
        "        print(f\"  Time Acc from z_bond: {time_acc:.1%} (target: ~11% = chance)\")\n",
        "        \n",
        "        if avg_valid_loss < best_valid_loss:\n",
        "            best_valid_loss = avg_valid_loss\n",
        "            # For TPU, need to save from CPU\n",
        "            if USE_TPU:\n",
        "                xm.save(model.state_dict(), \"models/checkpoints/best_model.pt\")\n",
        "            else:\n",
        "                torch.save(model.state_dict(), \"models/checkpoints/best_model.pt\")\n",
        "            print(f\"  -> Saved best model!\")\n",
        "        print()\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "mark_task(\"Train BIP model\", \"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 10. Evaluate and Interpret Results { display-mode: \"form\" }\n",
        "#@markdown Tests the BIP hypothesis: Does z_bond transfer across 2000 years?\n",
        "\n",
        "mark_task(\"Evaluate results\", \"running\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BIP TEMPORAL INVARIANCE TEST\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "if len(train_dataset) > 0:\n",
        "    # Load best model\n",
        "    if USE_TPU:\n",
        "        model.load_state_dict(torch.load(\"models/checkpoints/best_model.pt\", map_location='cpu'))\n",
        "        model = model.to(device)\n",
        "    else:\n",
        "        model.load_state_dict(torch.load(\"models/checkpoints/best_model.pt\"))\n",
        "    model.eval()\n",
        "    \n",
        "    # Evaluate on test set (modern data)\n",
        "    print(\"Evaluating on TEST SET (modern passages)...\")\n",
        "    print()\n",
        "    \n",
        "    all_time_preds = []\n",
        "    all_time_labels = []\n",
        "    all_hohfeld_preds = []\n",
        "    all_hohfeld_labels = []\n",
        "    all_z_bonds = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            \n",
        "            outputs = model(input_ids, attention_mask, adversarial_lambda=0)\n",
        "            \n",
        "            all_time_preds.extend(outputs['time_pred_bond'].argmax(dim=-1).cpu().tolist())\n",
        "            all_time_labels.extend(batch['time_labels'].tolist())\n",
        "            all_hohfeld_preds.extend(outputs['hohfeld_pred'].argmax(dim=-1).cpu().tolist())\n",
        "            all_hohfeld_labels.extend(batch['hohfeld_labels'].tolist())\n",
        "            all_z_bonds.append(outputs['z_bond'].cpu())\n",
        "            \n",
        "            if USE_TPU:\n",
        "                xm.mark_step()\n",
        "    \n",
        "    # Calculate metrics\n",
        "    time_acc = sum(p == l for p, l in zip(all_time_preds, all_time_labels)) / len(all_time_preds)\n",
        "    hohfeld_acc = sum(p == l for p, l in zip(all_hohfeld_preds, all_hohfeld_labels)) / len(all_hohfeld_preds)\n",
        "    chance_level = 1/9  # 9 time periods\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print()\n",
        "    \n",
        "    print(\"TEST 1: Time Prediction from z_bond\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Accuracy: {time_acc:.1%}\")\n",
        "    print(f\"  Chance level: {chance_level:.1%}\")\n",
        "    print(f\"  Difference from chance: {abs(time_acc - chance_level):.1%}\")\n",
        "    print()\n",
        "    \n",
        "    if abs(time_acc - chance_level) < 0.05:\n",
        "        print(\"  RESULT: z_bond IS time-invariant!\")\n",
        "        print(\"  The bond representation cannot predict temporal origin.\")\n",
        "        time_invariant = True\n",
        "    else:\n",
        "        print(\"  RESULT: z_bond retains some temporal information.\")\n",
        "        time_invariant = False\n",
        "    print()\n",
        "    \n",
        "    print(\"TEST 2: Hohfeldian Classification from z_bond\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Accuracy: {hohfeld_acc:.1%}\")\n",
        "    print(f\"  Random baseline: 25%\")\n",
        "    print()\n",
        "    \n",
        "    if hohfeld_acc > 0.4:\n",
        "        print(\"  RESULT: z_bond captures moral structure!\")\n",
        "        moral_structure = True\n",
        "    else:\n",
        "        print(\"  RESULT: Weak moral structure encoding.\")\n",
        "        moral_structure = False\n",
        "    print()\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"INTERPRETATION\")\n",
        "    print(\"=\" * 60)\n",
        "    print()\n",
        "    \n",
        "    if time_invariant and moral_structure:\n",
        "        print(\"\"\"\n",
        "    ******************************************************\n",
        "    *                                                    *\n",
        "    *     BIP TEMPORAL INVARIANCE: SUPPORTED             *\n",
        "    *                                                    *\n",
        "    ******************************************************\n",
        "    \n",
        "    The bond embedding (z_bond) successfully captured moral \n",
        "    structure while remaining invariant to temporal context.\n",
        "    \n",
        "    This suggests that moral cognition has a geometry that \n",
        "    is STABLE ACROSS 2000+ YEARS of human ethical reasoning.\n",
        "    \n",
        "    The same abstract patterns appear in:\n",
        "    - Ancient Hebrew texts (~500 BCE - 500 CE)\n",
        "    - Medieval rabbinical commentary (~500 - 1500 CE)  \n",
        "    - Modern American advice columns (1956 - 2020)\n",
        "    \n",
        "    \"\"\")\n",
        "    elif moral_structure and not time_invariant:\n",
        "        print(\"\"\"\n",
        "    BIP TEST: PARTIAL SUPPORT\n",
        "    \n",
        "    The model captures moral structure but also retains\n",
        "    some temporal information. This could indicate:\n",
        "    - Need for stronger adversarial training\n",
        "    - Genuine temporal variation in moral concepts\n",
        "    - Artifact of linguistic differences\n",
        "    \n",
        "    \"\"\")\n",
        "    else:\n",
        "        print(\"\"\"\n",
        "    BIP TEST: INCONCLUSIVE\n",
        "    \n",
        "    For definitive results, ensure:\n",
        "    1. Sefaria corpus loaded successfully  \n",
        "    2. At least 10,000 passages per time period\n",
        "    3. Model trained for 10+ epochs with GPU/TPU\n",
        "    \n",
        "    \"\"\")\n",
        "\n",
        "mark_task(\"Evaluate results\", \"done\")\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(\"EXPERIMENT COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print_progress()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Save Results\n",
        "\n",
        "Run the cell below to download your trained model and results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 11. Download Results (Optional) { display-mode: \"form\" }\n",
        "#@markdown Creates a zip file with model checkpoint and metrics.\n",
        "\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create results directory\n",
        "!mkdir -p results\n",
        "!cp models/checkpoints/best_model.pt results/\n",
        "!cp data/splits/all_splits.json results/\n",
        "\n",
        "# Save metrics\n",
        "if len(train_dataset) > 0:\n",
        "    metrics = {\n",
        "        'accelerator': ACCELERATOR,\n",
        "        'time_acc_from_bond': time_acc,\n",
        "        'hohfeld_acc': hohfeld_acc,\n",
        "        'chance_level': chance_level,\n",
        "        'time_invariant': time_invariant,\n",
        "        'moral_structure': moral_structure,\n",
        "        'bip_supported': time_invariant and moral_structure\n",
        "    }\n",
        "    with open('results/metrics.json', 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "# Zip\n",
        "shutil.make_archive('bip_results', 'zip', 'results')\n",
        "print(\"Results saved to bip_results.zip\")\n",
        "print()\n",
        "print(\"Contents:\")\n",
        "!ls -la results/\n",
        "\n",
        "# Download\n",
        "files.download('bip_results.zip')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}