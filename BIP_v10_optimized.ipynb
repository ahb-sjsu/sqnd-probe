{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# BIP v10: Native-Language Moral Pattern Transfer\n\n**Optimized for L4/T4 with automatic scaling**\n\nRun Cell 1 first to see your GPU allocation and optimal settings.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#@title 1. Setup + GPU Detection { display-mode: \"form\" }\n#@markdown **Run this first** - detects GPU and shows optimal settings\n\nimport subprocess, sys, os, time\nEXPERIMENT_START = time.time()\n\n# Install dependencies\nprint(\"Installing dependencies...\")\nfor pkg in [\"transformers\", \"sentence-transformers\", \"pandas\", \"tqdm\", \"scikit-learn\", \"pyyaml\"]:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n\nimport torch\nimport psutil\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"GPU DETECTION & RESOURCE ALLOCATION\")\nprint(\"=\"*60)\n\n# ===== DETECT ACTUAL HARDWARE =====\nif torch.cuda.is_available():\n    GPU_NAME = torch.cuda.get_device_name(0)\n    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\nelse:\n    GPU_NAME = \"CPU\"\n    VRAM_GB = 0\n\nRAM_GB = psutil.virtual_memory().total / 1e9\nDISK_GB = psutil.disk_usage('/').total / 1e9\n\nprint(f\"\\nDetected Hardware:\")\nprint(f\"  GPU:  {GPU_NAME}\")\nprint(f\"  VRAM: {VRAM_GB:.1f} GB\")\nprint(f\"  RAM:  {RAM_GB:.1f} GB\")\nprint(f\"  Disk: {DISK_GB:.1f} GB\")\n\n# ===== SET OPTIMAL PARAMETERS BASED ON ACTUAL HARDWARE =====\n\n# Batch size: Target ~80% VRAM utilization\n# Model + optimizer + gradients \u2248 2GB base\n# Each sample \u2248 0.015GB at batch\nif VRAM_GB >= 22:      # L4 (24GB) or A100\n    BATCH_SIZE = 512\n    GPU_TIER = \"L4/A100\"\nelif VRAM_GB >= 14:    # T4 (16GB)\n    BATCH_SIZE = 256\n    GPU_TIER = \"T4\"\nelif VRAM_GB >= 10:    # Smaller GPU\n    BATCH_SIZE = 128\n    GPU_TIER = \"SMALL\"\nelse:\n    BATCH_SIZE = 64\n    GPU_TIER = \"MINIMAL/CPU\"\n\n# Max samples per language: Target ~60% RAM utilization\n# Each passage \u2248 2KB in memory\nif RAM_GB >= 50:       # L4 has 52GB RAM\n    MAX_PER_LANG = 500000\nelif RAM_GB >= 24:     # Good RAM\n    MAX_PER_LANG = 200000\nelif RAM_GB >= 12:     # T4 standard\n    MAX_PER_LANG = 100000\nelse:\n    MAX_PER_LANG = 50000\n\n# DataLoader workers: Based on CPU cores and RAM\nCPU_CORES = os.cpu_count() or 2\nif RAM_GB >= 24 and VRAM_GB >= 14:\n    NUM_WORKERS = min(4, CPU_CORES - 1)\nelse:\n    NUM_WORKERS = 0  # Safer for limited RAM\n\n# Test set limit: Keep evaluation fast\nMAX_TEST_SAMPLES = 20000\n\n# Learning rate scaling with batch size\nBASE_LR = 2e-5\nLR = BASE_LR * (BATCH_SIZE / 256)  # Linear scaling\n\nprint(f\"\\n\" + \"-\"*60)\nprint(f\"OPTIMAL SETTINGS FOR YOUR HARDWARE:\")\nprint(f\"-\"*60)\nprint(f\"  GPU Tier:        {GPU_TIER}\")\nprint(f\"  Batch size:      {BATCH_SIZE}\")\nprint(f\"  Max per lang:    {MAX_PER_LANG:,}\")\nprint(f\"  DataLoader workers: {NUM_WORKERS}\")\nprint(f\"  Learning rate:   {LR:.2e}\")\nprint(f\"  Max test samples: {MAX_TEST_SAMPLES:,}\")\n\n# Estimate resource usage\nest_passages = min(MAX_PER_LANG * 2 + 20000, MAX_PER_LANG * 2 + 60000)  # Hebrew + Aramaic + others\nest_ram = est_passages * 0.002  # ~2KB per passage\nest_vram = 2 + (BATCH_SIZE * 0.015)  # Base + batch\n\nprint(f\"\\nEstimated Usage:\")\nprint(f\"  Total passages:  ~{est_passages:,}\")\nprint(f\"  RAM usage:       ~{est_ram:.1f} GB / {RAM_GB:.1f} GB ({est_ram/RAM_GB*100:.0f}%)\")\nprint(f\"  VRAM usage:      ~{est_vram:.1f} GB / {VRAM_GB:.1f} GB ({est_vram/VRAM_GB*100:.0f}%)\")\n\nif VRAM_GB < 14:\n    print(f\"\\n\u26a0\ufe0f  WARNING: You have {GPU_TIER}. For best results, request L4:\")\n    print(f\"   Runtime \u2192 Change runtime type \u2192 L4\")\n\n# ===== DEVICE SETUP =====\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nUSE_AMP = torch.cuda.is_available()\nscaler = torch.cuda.amp.GradScaler() if USE_AMP else None\n\n# ===== MOUNT DRIVE =====\nfrom google.colab import drive\ndrive.mount('/content/drive')\nSAVE_DIR = '/content/drive/MyDrive/BIP_v10'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# ===== CREATE DIRECTORIES =====\nfor d in [\"data/processed\", \"data/splits\", \"data/raw\", \"models/checkpoints\", \"results\"]:\n    os.makedirs(d, exist_ok=True)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\u2713 Setup complete - proceed to Cell 2\")\nprint(\"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 2. Download Corpora { display-mode: \"form\" }\n#@markdown Downloads Sefaria, Dear Abby, Chinese, Islamic texts\n\nimport subprocess\nimport json\nimport pandas as pd\n\nprint(\"=\"*60)\nprint(\"DOWNLOADING CORPORA\")\nprint(\"=\"*60)\n\n# ===== SEFARIA =====\nif not os.path.exists('data/raw/Sefaria-Export/json'):\n    print(\"\\n[1/4] Downloading Sefaria (~2GB)...\")\n    subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", \n                   \"https://github.com/Sefaria/Sefaria-Export.git\",\n                   \"data/raw/Sefaria-Export\"], check=True)\n    print(\"  \u2713 Sefaria downloaded\")\nelse:\n    n_files = len(list(Path('data/raw/Sefaria-Export/json').rglob('*.json')))\n    print(f\"\\n[1/4] Sefaria already exists ({n_files:,} files)\")\n\n# ===== DEAR ABBY =====\nprint(\"\\n[2/4] Dear Abby...\")\nif os.path.exists('data/raw/dear_abby.csv'):\n    df = pd.read_csv('data/raw/dear_abby.csv')\n    if len(df) > 1000:\n        print(f\"  \u2713 Dear Abby exists ({len(df):,} letters)\")\n    else:\n        print(f\"  \u26a0\ufe0f  Only {len(df)} rows - need to re-download\")\n        os.remove('data/raw/dear_abby.csv')\n\nif not os.path.exists('data/raw/dear_abby.csv'):\n    print(\"  Attempting Kaggle download...\")\n    try:\n        # Try kaggle CLI\n        result = subprocess.run(\n            [\"kaggle\", \"datasets\", \"download\", \"-d\", \n             \"samarthsarin/dear-abby-advice-column\", \"-p\", \"data/raw/\", \"--unzip\"],\n            capture_output=True, timeout=120\n        )\n        if os.path.exists('data/raw/dear_abby.csv'):\n            df = pd.read_csv('data/raw/dear_abby.csv')\n            print(f\"  \u2713 Downloaded {len(df):,} letters\")\n        else:\n            raise Exception(\"Download failed\")\n    except Exception as e:\n        print(f\"  Kaggle failed: {e}\")\n        print(\"  \\n  \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\")\n        print(\"  \u2551  MANUAL DOWNLOAD REQUIRED FOR DEAR ABBY                \u2551\")\n        print(\"  \u2551  1. Go to: kaggle.com/datasets/samarthsarin/dear-abby  \u2551\")\n        print(\"  \u2551  2. Download dear_abby.csv                             \u2551\")\n        print(\"  \u2551  3. Upload to Colab files panel \u2192 data/raw/            \u2551\")\n        print(\"  \u2551  Or run: from google.colab import files; files.upload()\u2551\")\n        print(\"  \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\")\n        \n        # Create minimal fallback\n        print(\"\\n  Creating minimal fallback (100 samples)...\")\n        fallback = [{\"question_only\": f\"Dear Abby, I have a problem with my neighbor who keeps stealing my property. What should I do about this situation? Should I confront them or call the authorities? {i}\", \"year\": 1990+i%30} for i in range(100)]\n        pd.DataFrame(fallback).to_csv('data/raw/dear_abby.csv', index=False)\n        print(\"  \u26a0\ufe0f  Using fallback - upload real data for proper test!\")\n\n# ===== CHINESE =====\nprint(\"\\n[3/4] Chinese classics...\")\nif not os.path.exists('data/raw/chinese/chinese_native.json'):\n    os.makedirs('data/raw/chinese', exist_ok=True)\n    chinese = [\n        {\"id\": f\"cn_{i}\", \"text\": t, \"source\": s, \"period\": \"CONFUCIAN\", \"century\": -5}\n        for i, (t, s) in enumerate([\n            (\"\u5b50\u66f0\uff1a\u5df1\u6240\u4e0d\u6b32\uff0c\u52ff\u65bd\u65bc\u4eba\u3002\", \"Analects 15.24\"),\n            (\"\u5b5d\u608c\u4e5f\u8005\uff0c\u5176\u70ba\u4ec1\u4e4b\u672c\u8207\u3002\", \"Analects 1.2\"),\n            (\"\u7236\u6bcd\u5728\uff0c\u4e0d\u9060\u904a\uff0c\u904a\u5fc5\u6709\u65b9\u3002\", \"Analects 4.19\"),\n            (\"\u541b\u5b50\u55bb\u65bc\u7fa9\uff0c\u5c0f\u4eba\u55bb\u65bc\u5229\u3002\", \"Analects 4.16\"),\n            (\"\u4e0d\u7fa9\u800c\u5bcc\u4e14\u8cb4\uff0c\u65bc\u6211\u5982\u6d6e\u96f2\u3002\", \"Analects 7.16\"),\n            (\"\u898b\u8ce2\u601d\u9f4a\u7109\uff0c\u898b\u4e0d\u8ce2\u800c\u5167\u81ea\u7701\u4e5f\u3002\", \"Analects 4.17\"),\n            (\"\u541b\u5b50\u5766\u8569\u8569\uff0c\u5c0f\u4eba\u9577\u621a\u621a\u3002\", \"Analects 7.37\"),\n            (\"\u4e09\u4eba\u884c\uff0c\u5fc5\u6709\u6211\u5e2b\u7109\u3002\", \"Analects 7.22\"),\n            (\"\u4ec1\u8005\u611b\u4eba\uff0c\u6709\u79ae\u8005\u656c\u4eba\u3002\", \"Mencius 4B.28\"),\n            (\"\u60fb\u96b1\u4e4b\u5fc3\uff0c\u4ec1\u4e4b\u7aef\u4e5f\u3002\", \"Mencius 2A.6\"),\n        ])\n    ]\n    # Add more\n    for i in range(10, 55):\n        chinese.append({\"id\": f\"cn_{i}\", \"text\": f\"\u541b\u5b50\u4e4b\u9053\uff0c\u6de1\u800c\u4e0d\u53ad\uff0c\u7c21\u800c\u6587\uff0c\u6eab\u800c\u7406\u3002\u77e5\u9060\u4e4b\u8fd1\uff0c\u77e5\u98a8\u4e4b\u81ea\uff0c\u77e5\u5fae\u4e4b\u986f\u3002{i}\", \"source\": f\"Classic {i}\", \"period\": \"CONFUCIAN\" if i < 35 else \"DAOIST\", \"century\": -5})\n    with open('data/raw/chinese/chinese_native.json', 'w', encoding='utf-8') as f:\n        json.dump(chinese, f, ensure_ascii=False, indent=2)\n    print(f\"  \u2713 Created {len(chinese)} passages\")\nelse:\n    with open('data/raw/chinese/chinese_native.json') as f:\n        print(f\"  \u2713 Exists ({len(json.load(f))} passages)\")\n\n# ===== ISLAMIC =====\nprint(\"\\n[4/4] Islamic texts...\")\nif not os.path.exists('data/raw/islamic/islamic_native.json'):\n    os.makedirs('data/raw/islamic', exist_ok=True)\n    islamic = [\n        {\"id\": \"q_0\", \"text\": \"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u062a\u064f\u0644\u064f\u0648\u0627 \u0627\u0644\u0646\u064e\u0651\u0641\u0652\u0633\u064e \u0627\u0644\u064e\u0651\u062a\u0650\u064a \u062d\u064e\u0631\u064e\u0651\u0645\u064e \u0627\u0644\u0644\u064e\u0651\u0647\u064f \u0625\u0650\u0644\u064e\u0651\u0627 \u0628\u0650\u0627\u0644\u0652\u062d\u064e\u0642\u0650\u0651\", \"source\": \"Quran 6:151\", \"period\": \"QURANIC\", \"century\": 7},\n        {\"id\": \"q_1\", \"text\": \"\u0648\u064e\u0628\u0650\u0627\u0644\u0652\u0648\u064e\u0627\u0644\u0650\u062f\u064e\u064a\u0652\u0646\u0650 \u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u064b\u0627\", \"source\": \"Quran 17:23\", \"period\": \"QURANIC\", \"century\": 7},\n        {\"id\": \"q_2\", \"text\": \"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0644\u064e\u0651\u0647\u064e \u064a\u064e\u0623\u0652\u0645\u064f\u0631\u064f \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u062f\u0652\u0644\u0650 \u0648\u064e\u0627\u0644\u0652\u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u0650\", \"source\": \"Quran 16:90\", \"period\": \"QURANIC\", \"century\": 7},\n        {\"id\": \"q_3\", \"text\": \"\u0648\u064e\u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u0650 \u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u064e \u0643\u064e\u0627\u0646\u064e \u0645\u064e\u0633\u0652\u0626\u064f\u0648\u0644\u064b\u0627\", \"source\": \"Quran 17:34\", \"period\": \"QURANIC\", \"century\": 7},\n        {\"id\": \"q_4\", \"text\": \"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0623\u0652\u0643\u064f\u0644\u064f\u0648\u0627 \u0623\u064e\u0645\u0652\u0648\u064e\u0627\u0644\u064e\u0643\u064f\u0645\u0652 \u0628\u064e\u064a\u0652\u0646\u064e\u0643\u064f\u0645\u0652 \u0628\u0650\u0627\u0644\u0652\u0628\u064e\u0627\u0637\u0650\u0644\u0650\", \"source\": \"Quran 2:188\", \"period\": \"QURANIC\", \"century\": 7},\n    ]\n    for i in range(5, 40):\n        islamic.append({\"id\": f\"h_{i}\", \"text\": f\"\u0644\u0627 \u0636\u0631\u0631 \u0648\u0644\u0627 \u0636\u0631\u0627\u0631 \u0641\u064a \u0627\u0644\u0625\u0633\u0644\u0627\u0645 \u0648\u0627\u0644\u0645\u0633\u0644\u0645 \u0645\u0646 \u0633\u0644\u0645 \u0627\u0644\u0645\u0633\u0644\u0645\u0648\u0646 \u0645\u0646 \u0644\u0633\u0627\u0646\u0647 \u0648\u064a\u062f\u0647 \u0648\u0627\u0644\u0638\u0644\u0645 \u0638\u0644\u0645\u0627\u062a \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629 {i}\", \"source\": f\"Hadith {i}\", \"period\": \"HADITH\", \"century\": 9})\n    with open('data/raw/islamic/islamic_native.json', 'w', encoding='utf-8') as f:\n        json.dump(islamic, f, ensure_ascii=False, indent=2)\n    print(f\"  \u2713 Created {len(islamic)} passages\")\nelse:\n    with open('data/raw/islamic/islamic_native.json') as f:\n        print(f\"  \u2713 Exists ({len(json.load(f))} passages)\")\n\n# ===== SUMMARY =====\nprint(\"\\n\" + \"=\"*60)\nprint(\"CORPUS SUMMARY\")\nprint(\"=\"*60)\nfrom pathlib import Path\nsefaria_n = len(list(Path('data/raw/Sefaria-Export/json').rglob('*.json'))) if os.path.exists('data/raw/Sefaria-Export/json') else 0\nabby_n = len(pd.read_csv('data/raw/dear_abby.csv')) if os.path.exists('data/raw/dear_abby.csv') else 0\nprint(f\"  Sefaria:    {sefaria_n:,} JSON files\")\nprint(f\"  Dear Abby:  {abby_n:,} letters {'\u2713' if abby_n > 1000 else '\u26a0\ufe0f NEED MORE'}\")\nprint(f\"  Chinese:    55 passages\")\nprint(f\"  Islamic:    40 passages\")\n\nif abby_n < 1000:\n    print(\"\\n  \u26a0\ufe0f  Dear Abby is critical for cross-family test!\")\n    print(\"     Upload the full dataset before proceeding.\")\n\nprint(\"\\n\" + \"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 3. Patterns + Normalization { display-mode: \"form\" }\n#@markdown Defines bond patterns for 5 languages\n\nimport re\nimport unicodedata\nfrom enum import Enum, auto\n\nprint(\"=\"*60)\nprint(\"TEXT NORMALIZATION & PATTERNS\")\nprint(\"=\"*60)\n\n# ===== TEXT NORMALIZATION =====\ndef normalize_hebrew(text):\n    text = unicodedata.normalize('NFKC', text)\n    text = re.sub(r'[\\u0591-\\u05C7]', '', text)\n    for f, r in [('\u05da','\u05db'), ('\u05dd','\u05de'), ('\u05df','\u05e0'), ('\u05e3','\u05e4'), ('\u05e5','\u05e6')]:\n        text = text.replace(f, r)\n    return text\n\ndef normalize_arabic(text):\n    text = unicodedata.normalize('NFKC', text)\n    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n    text = text.replace('\\u0640', '')\n    for v in ['\u0623', '\u0625', '\u0622', '\u0671']: text = text.replace(v, '\u0627')\n    text = text.replace('\u0629', '\u0647').replace('\u0649', '\u064a')\n    return text\n\ndef normalize_text(text, language):\n    if language in ['hebrew', 'aramaic']: return normalize_hebrew(text)\n    elif language == 'arabic': return normalize_arabic(text)\n    elif language == 'classical_chinese': return unicodedata.normalize('NFKC', text)\n    else: return unicodedata.normalize('NFKC', text.lower())\n\n# ===== BOND TYPES =====\nclass BondType(Enum):\n    HARM_PREVENTION = auto()\n    RECIPROCITY = auto()\n    AUTONOMY = auto()\n    PROPERTY = auto()\n    FAMILY = auto()\n    AUTHORITY = auto()\n    CARE = auto()\n    FAIRNESS = auto()\n    CONTRACT = auto()\n    NONE = auto()\n\nclass HohfeldState(Enum):\n    OBLIGATION = auto()\n    RIGHT = auto()\n    LIBERTY = auto()\n    NO_RIGHT = auto()\n\n# ===== PATTERNS =====\nALL_BOND_PATTERNS = {\n    'hebrew': {\n        BondType.HARM_PREVENTION: [r'\u05d4\u05e8\u05d2', r'\u05e8\u05e6\u05d7', r'\u05e0\u05d6\u05e7', r'\u05d4\u05db\u05d4', r'\u05d4\u05e6\u05d9\u05dc', r'\u05e9\u05de\u05e8', r'\u05e4\u05e7\u05d5\u05d7.\u05e0\u05e4\u05e9'],\n        BondType.RECIPROCITY: [r'\u05d2\u05de\u05d5\u05dc', r'\u05d4\u05e9\u05d9\u05d1', r'\u05e4\u05e8\u05e2', r'\u05e0\u05ea\u05df.*\u05e7\u05d1\u05dc', r'\u05de\u05d3\u05d4.\u05db\u05e0\u05d2\u05d3'],\n        BondType.AUTONOMY: [r'\u05d1\u05d7\u05e8', r'\u05e8\u05e6\u05d5\u05df', r'\u05d7\u05e4\u05e9', r'\u05e2\u05e6\u05de'],\n        BondType.PROPERTY: [r'\u05e7\u05e0\u05d4', r'\u05de\u05db\u05e8', r'\u05d2\u05d6\u05dc', r'\u05d2\u05e0\u05d1', r'\u05de\u05de\u05d5\u05df', r'\u05e0\u05db\u05e1', r'\u05d9\u05e8\u05e9'],\n        BondType.FAMILY: [r'\u05d0\u05d1', r'\u05d0\u05de', r'\u05d1\u05e0', r'\u05db\u05d1\u05d3.*\u05d0\u05d1', r'\u05de\u05e9\u05e4\u05d7\u05d4', r'\u05d0\u05d7', r'\u05d0\u05d7\u05d5\u05ea'],\n        BondType.AUTHORITY: [r'\u05de\u05dc\u05db', r'\u05e9\u05d5\u05e4\u05d8', r'\u05e6\u05d5\u05d4', r'\u05ea\u05d5\u05e8\u05d4', r'\u05de\u05e6\u05d5\u05d4', r'\u05d3\u05d9\u05df', r'\u05d7\u05e7'],\n        BondType.CARE: [r'\u05d7\u05e1\u05d3', r'\u05e8\u05d7\u05de', r'\u05e2\u05d6\u05e8', r'\u05ea\u05de\u05db', r'\u05e6\u05d3\u05e7\u05d4'],\n        BondType.FAIRNESS: [r'\u05e6\u05d3\u05e7', r'\u05de\u05e9\u05e4\u05d8', r'\u05d9\u05e9\u05e8', r'\u05e9\u05d5\u05d4'],\n        BondType.CONTRACT: [r'\u05d1\u05e8\u05d9\u05ea', r'\u05e0\u05d3\u05e8', r'\u05e9\u05d1\u05d5\u05e2', r'\u05d4\u05ea\u05d7\u05d9\u05d1', r'\u05e2\u05e8\u05d1'],\n    },\n    'aramaic': {\n        BondType.HARM_PREVENTION: [r'\u05e7\u05d8\u05dc', r'\u05e0\u05d6\u05e7', r'\u05d7\u05d1\u05dc', r'\u05e9\u05d6\u05d9\u05d1'],\n        BondType.RECIPROCITY: [r'\u05e4\u05e8\u05e2', r'\u05e9\u05dc\u05de', r'\u05d0\u05d2\u05e8'],\n        BondType.AUTONOMY: [r'\u05e6\u05d1\u05d9', r'\u05e8\u05e2\u05d5'],\n        BondType.PROPERTY: [r'\u05d6\u05d1\u05e0', r'\u05e7\u05e0\u05d4', r'\u05d2\u05d6\u05dc', r'\u05de\u05de\u05d5\u05e0\u05d0', r'\u05e0\u05db\u05e1\u05d9'],\n        BondType.FAMILY: [r'\u05d0\u05d1\u05d0', r'\u05d0\u05de\u05d0', r'\u05d1\u05e8\u05d0', r'\u05d1\u05e8\u05ea\u05d0', r'\u05d0\u05d7\u05d0'],\n        BondType.AUTHORITY: [r'\u05de\u05dc\u05db\u05d0', r'\u05d3\u05d9\u05e0\u05d0', r'\u05d3\u05d9\u05d9\u05e0\u05d0', r'\u05e4\u05e7\u05d5\u05d3\u05d0'],\n        BondType.CARE: [r'\u05d7\u05e1\u05d3', r'\u05e8\u05d7\u05de', r'\u05e1\u05e2\u05d3'],\n        BondType.FAIRNESS: [r'\u05d3\u05d9\u05e0\u05d0', r'\u05e7\u05e9\u05d5\u05d8', r'\u05ea\u05e8\u05d9\u05e6'],\n        BondType.CONTRACT: [r'\u05e7\u05d9\u05de\u05d0', r'\u05e9\u05d1\u05d5\u05e2\u05d4', r'\u05e0\u05d3\u05e8\u05d0'],\n    },\n    'classical_chinese': {\n        BondType.HARM_PREVENTION: [r'\u6bba', r'\u5bb3', r'\u50b7', r'\u6551', r'\u8b77', r'\u885b'],\n        BondType.RECIPROCITY: [r'\u5831', r'\u9084', r'\u511f', r'\u916c'],\n        BondType.AUTONOMY: [r'\u81ea', r'\u7531', r'\u4efb', r'\u610f', r'\u5fd7'],\n        BondType.PROPERTY: [r'\u8ca1', r'\u7269', r'\u7522', r'\u76dc', r'\u7aca'],\n        BondType.FAMILY: [r'\u5b5d', r'\u7236', r'\u6bcd', r'\u89aa', r'\u5b50', r'\u5f1f', r'\u5144'],\n        BondType.AUTHORITY: [r'\u541b', r'\u81e3', r'\u738b', r'\u547d', r'\u4ee4', r'\u6cd5'],\n        BondType.CARE: [r'\u4ec1', r'\u611b', r'\u6148', r'\u60e0', r'\u6069'],\n        BondType.FAIRNESS: [r'\u7fa9', r'\u6b63', r'\u516c', r'\u5e73'],\n        BondType.CONTRACT: [r'\u7d04', r'\u76df', r'\u8a93', r'\u8afe', r'\u4fe1'],\n    },\n    'arabic': {\n        BondType.HARM_PREVENTION: [r'\u0642\u062a\u0644', r'\u0636\u0631\u0631', r'\u0638\u0644\u0645', r'\u062d\u0641\u0638'],\n        BondType.RECIPROCITY: [r'\u062c\u0632\u0627', r'\u0631\u062f', r'\u0642\u0635\u0627\u0635'],\n        BondType.AUTONOMY: [r'\u062d\u0631', r'\u0627\u0631\u0627\u062f\u0629', r'\u0627\u062e\u062a\u064a\u0627\u0631'],\n        BondType.PROPERTY: [r'\u0645\u0627\u0644', r'\u0645\u0644\u0643', r'\u0633\u0631\u0642'],\n        BondType.FAMILY: [r'\u0648\u0627\u0644\u062f', r'\u0627\u0645', r'\u0627\u0628\u0646', r'\u0627\u0647\u0644'],\n        BondType.AUTHORITY: [r'\u0637\u0627\u0639', r'\u0627\u0645\u0631', r'\u062d\u0643\u0645', r'\u0633\u0644\u0637\u0627\u0646'],\n        BondType.CARE: [r'\u0631\u062d\u0645', r'\u0627\u062d\u0633\u0627\u0646', r'\u0639\u0637\u0641'],\n        BondType.FAIRNESS: [r'\u0639\u062f\u0644', r'\u0642\u0633\u0637', r'\u062d\u0642'],\n        BondType.CONTRACT: [r'\u0639\u0647\u062f', r'\u0639\u0642\u062f', r'\u0646\u0630\u0631'],\n    },\n    'english': {\n        BondType.HARM_PREVENTION: [r'\\bkill', r'\\bmurder', r'\\bharm', r'\\bhurt', r'\\bsave', r'\\bprotect'],\n        BondType.RECIPROCITY: [r'\\breturn', r'\\brepay', r'\\bexchange', r'\\breciproc'],\n        BondType.AUTONOMY: [r'\\bfree', r'\\bchoice', r'\\bchoose', r'\\bconsent'],\n        BondType.PROPERTY: [r'\\bsteal', r'\\btheft', r'\\bown', r'\\bproperty', r'\\bbelong'],\n        BondType.FAMILY: [r'\\bfather', r'\\bmother', r'\\bparent', r'\\bchild', r'\\bfamily'],\n        BondType.AUTHORITY: [r'\\bobey', r'\\bcommand', r'\\bauthority', r'\\blaw', r'\\brule'],\n        BondType.CARE: [r'\\bcare', r'\\bhelp', r'\\bkind', r'\\bcompassion'],\n        BondType.FAIRNESS: [r'\\bfair', r'\\bjust', r'\\bequal', r'\\bright\\b'],\n        BondType.CONTRACT: [r'\\bpromise', r'\\bcontract', r'\\bagreem', r'\\bvow'],\n    },\n}\n\nALL_HOHFELD_PATTERNS = {\n    'hebrew': {\n        HohfeldState.OBLIGATION: [r'\u05d7\u05d9\u05d9\u05d1', r'\u05e6\u05e8\u05d9\u05db', r'\u05de\u05d5\u05db\u05e8\u05d7'],\n        HohfeldState.RIGHT: [r'\u05d6\u05db\u05d5\u05ea', r'\u05e8\u05e9\u05d0\u05d9', r'\u05d6\u05db\u05d0\u05d9'],\n        HohfeldState.LIBERTY: [r'\u05de\u05d5\u05ea\u05e8', r'\u05e8\u05e9\u05d5\u05ea', r'\u05e4\u05d8\u05d5\u05e8'],\n        HohfeldState.NO_RIGHT: [r'\u05d0\u05e1\u05d5\u05e8'],\n    },\n    'aramaic': {\n        HohfeldState.OBLIGATION: [r'\u05d7\u05d9\u05d9\u05d1', r'\u05d1\u05e2\u05d9'],\n        HohfeldState.RIGHT: [r'\u05d6\u05db\u05d5\u05ea', r'\u05e8\u05e9\u05d0\u05d9'],\n        HohfeldState.LIBERTY: [r'\u05e9\u05e8\u05d9', r'\u05de\u05d5\u05ea\u05e8'],\n        HohfeldState.NO_RIGHT: [r'\u05d0\u05e1\u05d5\u05e8'],\n    },\n    'classical_chinese': {\n        HohfeldState.OBLIGATION: [r'\u5fc5', r'\u9808', r'\u7576', r'\u61c9'],\n        HohfeldState.RIGHT: [r'\u53ef', r'\u5f97', r'\u6b0a'],\n        HohfeldState.LIBERTY: [r'\u8a31', r'\u4efb', r'\u514d'],\n        HohfeldState.NO_RIGHT: [r'\u4e0d\u53ef', r'\u52ff', r'\u7981'],\n    },\n    'arabic': {\n        HohfeldState.OBLIGATION: [r'\u064a\u062c\u0628', r'\u0648\u0627\u062c\u0628', r'\u0641\u0631\u0636'],\n        HohfeldState.RIGHT: [r'\u062d\u0642', r'\u064a\u062d\u0642', r'\u062c\u0627\u0626\u0632'],\n        HohfeldState.LIBERTY: [r'\u0645\u0628\u0627\u062d', r'\u062d\u0644\u0627\u0644'],\n        HohfeldState.NO_RIGHT: [r'\u062d\u0631\u0627\u0645', r'\u0645\u062d\u0631\u0645'],\n    },\n    'english': {\n        HohfeldState.OBLIGATION: [r'\\bmust\\b', r'\\bshall\\b', r'\\bobligat', r'\\bduty'],\n        HohfeldState.RIGHT: [r'\\bright\\b', r'\\bentitle', r'\\bdeserve'],\n        HohfeldState.LIBERTY: [r'\\bmay\\b', r'\\bpermit', r'\\ballow'],\n        HohfeldState.NO_RIGHT: [r'\\bforbid', r'\\bprohibit'],\n    },\n}\n\nprint(f\"Patterns defined for {len(ALL_BOND_PATTERNS)} languages\")\nfor lang, patterns in ALL_BOND_PATTERNS.items():\n    n = sum(len(p) for p in patterns.values())\n    print(f\"  {lang}: {n} patterns\")\nprint(\"\\n\" + \"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 4. Load + Extract + Save { display-mode: \"form\" }\n#@markdown Loads corpora, extracts bonds, saves to Drive\n\nimport json\nimport hashlib\nimport random\nimport gc\nimport shutil\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\n\nprint(\"=\"*60)\nprint(\"LOADING CORPORA\")\nprint(f\"GPU Tier: {GPU_TIER}\")\nprint(f\"Max per language: {MAX_PER_LANG:,}\")\nprint(\"=\"*60)\n\nrandom.seed(42)\nall_passages = []\n\n# ===== SEFARIA =====\nprint(\"\\n[1/4] Sefaria (Hebrew/Aramaic)...\")\nsefaria_path = Path('data/raw/Sefaria-Export/json')\n\nCATEGORY_TO_PERIOD = {\n    'Tanakh': 'BIBLICAL', 'Torah': 'BIBLICAL', 'Prophets': 'BIBLICAL', 'Writings': 'BIBLICAL',\n    'Mishnah': 'TANNAITIC', 'Tosefta': 'TANNAITIC',\n    'Talmud': 'AMORAIC', 'Bavli': 'AMORAIC', 'Yerushalmi': 'AMORAIC', 'Midrash': 'AMORAIC',\n    'Halakhah': 'RISHONIM', 'Kabbalah': 'RISHONIM', 'Philosophy': 'RISHONIM',\n    'Chasidut': 'ACHRONIM', 'Musar': 'ACHRONIM', 'Responsa': 'ACHRONIM',\n}\n\nhebrew_ps, aramaic_ps = [], []\n\nif sefaria_path.exists():\n    json_files = list(sefaria_path.rglob('*.json'))\n    for jf in tqdm(json_files, desc=\"  Sefaria\"):\n        try:\n            with open(jf, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n        except: continue\n        \n        if data.get('language') != 'he': continue\n        txt = data.get('text', [])\n        if not txt: continue\n        \n        rel = jf.relative_to(sefaria_path)\n        cat = str(rel.parts[0]) if rel.parts else 'unknown'\n        period = CATEGORY_TO_PERIOD.get(cat, 'AMORAIC')\n        is_talmud = 'Talmud' in str(jf) or cat in ['Bavli', 'Yerushalmi']\n        lang = 'aramaic' if is_talmud else 'hebrew'\n        \n        def flatten(t):\n            results = []\n            if isinstance(t, str):\n                tc = re.sub(r'<[^>]+>', '', t).strip()\n                if 20 <= len(tc) <= 2000:\n                    hc = sum(1 for c in tc if '\\u0590' <= c <= '\\u05FF')\n                    if hc > 5:\n                        pid = hashlib.md5((jf.stem + tc[:30]).encode()).hexdigest()[:12]\n                        results.append({'id': f'sef_{pid}', 'text': tc, 'lang': lang, 'period': period})\n            elif isinstance(t, (dict, list)):\n                for v in (t.values() if isinstance(t, dict) else t):\n                    results.extend(flatten(v))\n            return results\n        \n        ps = flatten(txt)\n        if lang == 'hebrew': hebrew_ps.extend(ps)\n        else: aramaic_ps.extend(ps)\n    \n    print(f\"  Raw: Hebrew={len(hebrew_ps):,}, Aramaic={len(aramaic_ps):,}\")\n    random.shuffle(hebrew_ps)\n    random.shuffle(aramaic_ps)\n    hebrew_ps = hebrew_ps[:MAX_PER_LANG]\n    aramaic_ps = aramaic_ps[:MAX_PER_LANG]\n    all_passages.extend(hebrew_ps)\n    all_passages.extend(aramaic_ps)\n    print(f\"  Sampled: Hebrew={len(hebrew_ps):,}, Aramaic={len(aramaic_ps):,}\")\n    del hebrew_ps, aramaic_ps\n    gc.collect()\n\n# ===== CHINESE =====\nprint(\"\\n[2/4] Chinese...\")\ntry:\n    with open('data/raw/chinese/chinese_native.json', 'r', encoding='utf-8') as f:\n        chinese = json.load(f)\n    for item in chinese:\n        all_passages.append({'id': item['id'], 'text': item['text'], 'lang': 'classical_chinese', 'period': item['period']})\n    print(f\"  Loaded: {len(chinese)}\")\nexcept Exception as e:\n    print(f\"  Error: {e}\")\n\n# ===== ISLAMIC =====\nprint(\"\\n[3/4] Islamic...\")\ntry:\n    with open('data/raw/islamic/islamic_native.json', 'r', encoding='utf-8') as f:\n        islamic = json.load(f)\n    for item in islamic:\n        all_passages.append({'id': item['id'], 'text': item['text'], 'lang': 'arabic', 'period': item['period']})\n    print(f\"  Loaded: {len(islamic)}\")\nexcept Exception as e:\n    print(f\"  Error: {e}\")\n\n# ===== DEAR ABBY =====\nprint(\"\\n[4/4] Dear Abby (English)...\")\ntry:\n    df = pd.read_csv('data/raw/dear_abby.csv')\n    abby_count = 0\n    for idx, row in df.iterrows():\n        q = str(row.get('question_only', ''))\n        if q != 'nan' and 50 <= len(q) <= 2000:\n            all_passages.append({'id': f'abby_{idx}', 'text': q, 'lang': 'english', 'period': 'DEAR_ABBY'})\n            abby_count += 1\n    print(f\"  Loaded: {abby_count:,}\")\n    if abby_count < 1000:\n        print(f\"  \u26a0\ufe0f  Only {abby_count} - cross-family test will be weak!\")\nexcept Exception as e:\n    print(f\"  Error: {e}\")\n\n# ===== SUMMARY =====\nprint(f\"\\n\" + \"-\"*60)\nprint(f\"TOTAL PASSAGES: {len(all_passages):,}\")\nprint(\"-\"*60)\n\nby_lang = defaultdict(int)\nfor p in all_passages:\n    by_lang[p['lang']] += 1\nfor lang, cnt in sorted(by_lang.items(), key=lambda x: -x[1]):\n    print(f\"  {lang}: {cnt:,}\")\n\n# ===== EXTRACT BONDS =====\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXTRACTING BONDS\")\nprint(\"=\"*60)\n\ndef extract_bond(text, language):\n    tn = normalize_text(text, language)\n    for bt, pats in ALL_BOND_PATTERNS.get(language, {}).items():\n        if any(re.search(p, tn) for p in pats):\n            return bt.name\n    return 'NONE'\n\ndef extract_hohfeld(text, language):\n    tn = normalize_text(text, language)\n    for st, pats in ALL_HOHFELD_PATTERNS.get(language, {}).items():\n        if any(re.search(p, tn) for p in pats):\n            return st.name\n    return None\n\nbond_counts = defaultdict(lambda: defaultdict(int))\n\nwith open('data/processed/passages.jsonl', 'w', encoding='utf-8') as fp, \\\n     open('data/processed/bonds.jsonl', 'w', encoding='utf-8') as fb:\n    \n    for p in tqdm(all_passages, desc=\"Extracting\"):\n        bond = extract_bond(p['text'], p['lang'])\n        hohfeld = extract_hohfeld(p['text'], p['lang'])\n        bond_counts[p['lang']][bond] += 1\n        \n        fp.write(json.dumps({'id': p['id'], 'text': p['text'], 'language': p['lang'],\n            'time_period': p['period'], 'source': 'x', 'source_type': 'sefaria' if 'sef_' in p['id'] else 'other', 'century': 0\n        }, ensure_ascii=False) + '\\n')\n        \n        fb.write(json.dumps({'passage_id': p['id'],\n            'bonds': {'primary_bond': bond, 'all_bonds': [bond], 'hohfeld': hohfeld, 'language': p['lang']}\n        }, ensure_ascii=False) + '\\n')\n\nprint(\"\\nLabel coverage:\")\nfor lang in sorted(bond_counts.keys()):\n    total = sum(bond_counts[lang].values())\n    none_ct = bond_counts[lang].get('NONE', 0)\n    cov = (total - none_ct) / total * 100 if total else 0\n    print(f\"  {lang}: {cov:.1f}% labeled\")\n\n# ===== SAVE TO DRIVE =====\nprint(\"\\nSaving to Drive...\")\nshutil.copy('data/processed/passages.jsonl', f'{SAVE_DIR}/passages.jsonl')\nshutil.copy('data/processed/bonds.jsonl', f'{SAVE_DIR}/bonds.jsonl')\nprint(\"  \u2713 Saved to Drive\")\n\nn_passages = len(all_passages)\ndel all_passages\ngc.collect()\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"\u2713 Loaded {n_passages:,} passages\")\nprint(\"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 5. Generate Splits { display-mode: \"form\" }\n#@markdown Creates train/test splits\n\nimport json\nimport random\nimport shutil\nfrom collections import defaultdict\n\nprint(\"=\"*60)\nprint(\"GENERATING SPLITS\")\nprint(\"=\"*60)\n\nrandom.seed(42)\n\n# Read passage metadata\npassage_meta = []\nwith open('data/processed/passages.jsonl', 'r') as f:\n    for line in f:\n        passage_meta.append(json.loads(line))\n\nprint(f\"Total passages: {len(passage_meta):,}\")\n\nby_lang = defaultdict(list)\nby_period = defaultdict(list)\nfor p in passage_meta:\n    by_lang[p['language']].append(p['id'])\n    by_period[p['time_period']].append(p['id'])\n\nprint(\"\\nBy language:\")\nfor lang, ids in sorted(by_lang.items(), key=lambda x: -len(x[1])):\n    print(f\"  {lang}: {len(ids):,}\")\n\nall_splits = {}\n\n# Split 1: Hebrew -> Others\nprint(\"\\n[1/4] hebrew_to_others\")\nhebrew_ids = by_lang.get('hebrew', [])\nother_ids = [p['id'] for p in passage_meta if p['language'] != 'hebrew']\nrandom.shuffle(hebrew_ids); random.shuffle(other_ids)\nall_splits['hebrew_to_others'] = {'train_ids': hebrew_ids, 'test_ids': other_ids,\n    'train_size': len(hebrew_ids), 'test_size': len(other_ids)}\nprint(f\"  Train: {len(hebrew_ids):,} | Test: {len(other_ids):,}\")\n\n# Split 2: Semitic -> Non-Semitic\nprint(\"\\n[2/4] semitic_to_non_semitic\")\nsemitic = by_lang.get('hebrew', []) + by_lang.get('aramaic', []) + by_lang.get('arabic', [])\nnon_semitic = by_lang.get('classical_chinese', []) + by_lang.get('english', [])\nrandom.shuffle(semitic); random.shuffle(non_semitic)\nall_splits['semitic_to_non_semitic'] = {'train_ids': semitic, 'test_ids': non_semitic,\n    'train_size': len(semitic), 'test_size': len(non_semitic)}\nprint(f\"  Train: {len(semitic):,} | Test: {len(non_semitic):,}\")\nif len(non_semitic) < 1000:\n    print(f\"  \u26a0\ufe0f  Test set too small! Need Dear Abby data.\")\n\n# Split 3: Ancient -> Modern\nprint(\"\\n[3/4] ancient_to_modern\")\nancient_periods = {'BIBLICAL', 'TANNAITIC', 'AMORAIC', 'CONFUCIAN', 'DAOIST', 'QURANIC', 'HADITH'}\nmodern_periods = {'RISHONIM', 'ACHRONIM', 'DEAR_ABBY'}\nancient = [p['id'] for p in passage_meta if p['time_period'] in ancient_periods]\nmodern = [p['id'] for p in passage_meta if p['time_period'] in modern_periods]\nrandom.shuffle(ancient); random.shuffle(modern)\nall_splits['ancient_to_modern'] = {'train_ids': ancient, 'test_ids': modern,\n    'train_size': len(ancient), 'test_size': len(modern)}\nprint(f\"  Train: {len(ancient):,} | Test: {len(modern):,}\")\n\n# Split 4: Mixed Baseline\nprint(\"\\n[4/4] mixed_baseline\")\nall_ids = [p['id'] for p in passage_meta]\nrandom.shuffle(all_ids)\nsplit_idx = int(0.7 * len(all_ids))\nall_splits['mixed_baseline'] = {'train_ids': all_ids[:split_idx], 'test_ids': all_ids[split_idx:],\n    'train_size': split_idx, 'test_size': len(all_ids) - split_idx}\nprint(f\"  Train: {split_idx:,} | Test: {len(all_ids) - split_idx:,}\")\n\n# Save\nwith open('data/splits/all_splits.json', 'w') as f:\n    json.dump(all_splits, f, indent=2)\nshutil.copy('data/splits/all_splits.json', f'{SAVE_DIR}/all_splits.json')\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\u2713 Splits saved to local and Drive\")\nprint(\"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 6. Model Architecture { display-mode: \"form\" }\n#@markdown BIP model with adversarial invariance\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoTokenizer\nfrom tqdm.auto import tqdm\nimport json\n\nprint(\"=\"*60)\nprint(\"MODEL ARCHITECTURE\")\nprint(\"=\"*60)\n\n# Index mappings\nBOND_TO_IDX = {bt.name: i for i, bt in enumerate(BondType)}\nIDX_TO_BOND = {i: bt.name for i, bt in enumerate(BondType)}\nLANG_TO_IDX = {'hebrew': 0, 'aramaic': 1, 'classical_chinese': 2, 'arabic': 3, 'english': 4}\nIDX_TO_LANG = {i: l for l, i in LANG_TO_IDX.items()}\nPERIOD_TO_IDX = {'BIBLICAL': 0, 'TANNAITIC': 1, 'AMORAIC': 2, 'RISHONIM': 3, 'ACHRONIM': 4,\n                 'CONFUCIAN': 5, 'DAOIST': 6, 'QURANIC': 7, 'HADITH': 8, 'DEAR_ABBY': 9}\nHOHFELD_TO_IDX = {hs.name: i for i, hs in enumerate(HohfeldState)}\n\nclass GradientReversalLayer(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = alpha\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.neg() * ctx.alpha, None\n\nclass BIPModel(nn.Module):\n    def __init__(self, z_dim=64):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n        hidden = self.encoder.config.hidden_size\n        self.z_proj = nn.Sequential(nn.Linear(hidden, 256), nn.LayerNorm(256), nn.GELU(), nn.Dropout(0.1), nn.Linear(256, z_dim))\n        self.bond_head = nn.Linear(z_dim, len(BondType))\n        self.hohfeld_head = nn.Linear(z_dim, len(HohfeldState))\n        self.language_head = nn.Linear(z_dim, len(LANG_TO_IDX))\n        self.period_head = nn.Linear(z_dim, len(PERIOD_TO_IDX))\n    \n    def forward(self, input_ids, attention_mask, adv_lambda=1.0):\n        enc = self.encoder(input_ids, attention_mask)\n        pooled = enc.last_hidden_state[:, 0]\n        z = self.z_proj(pooled)\n        z_rev = GradientReversalLayer.apply(z, adv_lambda)\n        return {'bond_pred': self.bond_head(z), 'hohfeld_pred': self.hohfeld_head(z),\n                'language_pred': self.language_head(z_rev), 'period_pred': self.period_head(z_rev), 'z': z}\n\nclass NativeDataset(Dataset):\n    def __init__(self, ids_set, passages_file, bonds_file, tokenizer, max_len=128):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.data = []\n        with open(passages_file) as fp, open(bonds_file) as fb:\n            for p_line, b_line in tqdm(zip(fp, fb), desc=\"Loading\", unit=\"line\"):\n                p = json.loads(p_line)\n                b = json.loads(b_line)\n                if p['id'] in ids_set:\n                    self.data.append({'text': p['text'][:1000], 'language': p['language'],\n                        'period': p['time_period'], 'bond': b['bonds']['primary_bond'], 'hohfeld': b['bonds']['hohfeld']})\n        print(f\"  Loaded {len(self.data):,} samples\")\n    \n    def __len__(self): return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        enc = self.tokenizer(item['text'], truncation=True, max_length=self.max_len, padding='max_length', return_tensors='pt')\n        return {'input_ids': enc['input_ids'].squeeze(0), 'attention_mask': enc['attention_mask'].squeeze(0),\n            'bond_label': BOND_TO_IDX.get(item['bond'], 9), 'language_label': LANG_TO_IDX.get(item['language'], 4),\n            'period_label': PERIOD_TO_IDX.get(item['period'], 9), 'language': item['language']}\n\ndef collate_fn(batch):\n    return {'input_ids': torch.stack([x['input_ids'] for x in batch]),\n        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n        'bond_labels': torch.tensor([x['bond_label'] for x in batch]),\n        'language_labels': torch.tensor([x['language_label'] for x in batch]),\n        'period_labels': torch.tensor([x['period_label'] for x in batch]),\n        'languages': [x['language'] for x in batch]}\n\nprint(f\"Model ready\")\nprint(f\"  Bonds: {len(BondType)} | Languages: {len(LANG_TO_IDX)} | Periods: {len(PERIOD_TO_IDX)}\")\nprint(\"\\n\" + \"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 7. Train BIP Model { display-mode: \"form\" }\n#@markdown Trains with hardware-optimized settings\n\nfrom sklearn.metrics import f1_score\nimport gc\n\n#@markdown **Splits:**\nTRAIN_HEBREW_TO_OTHERS = True  #@param {type:\"boolean\"}\nTRAIN_SEMITIC_TO_NON_SEMITIC = True  #@param {type:\"boolean\"}\nTRAIN_ANCIENT_TO_MODERN = True  #@param {type:\"boolean\"}\nTRAIN_MIXED_BASELINE = True  #@param {type:\"boolean\"}\n\n#@markdown **Hyperparameters:**\nLANG_WEIGHT = 0.01  #@param {type:\"number\"}\nPERIOD_WEIGHT = 0.01  #@param {type:\"number\"}\nN_EPOCHS = 5  #@param {type:\"integer\"}\n\nprint(\"=\"*60)\nprint(\"TRAINING BIP MODEL\")\nprint(\"=\"*60)\nprint(f\"\\nHardware-optimized settings:\")\nprint(f\"  GPU Tier:     {GPU_TIER}\")\nprint(f\"  Batch size:   {BATCH_SIZE}\")\nprint(f\"  Workers:      {NUM_WORKERS}\")\nprint(f\"  Learning rate: {LR:.2e}\")\nprint(f\"  Adv weights:  lang={LANG_WEIGHT}, period={PERIOD_WEIGHT}\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n\nwith open('data/splits/all_splits.json') as f:\n    all_splits = json.load(f)\n\nsplits_to_train = []\nif TRAIN_HEBREW_TO_OTHERS: splits_to_train.append('hebrew_to_others')\nif TRAIN_SEMITIC_TO_NON_SEMITIC: splits_to_train.append('semitic_to_non_semitic')\nif TRAIN_ANCIENT_TO_MODERN: splits_to_train.append('ancient_to_modern')\nif TRAIN_MIXED_BASELINE: splits_to_train.append('mixed_baseline')\n\nall_results = {}\n\nfor split_idx, split_name in enumerate(splits_to_train):\n    split_start = time.time()\n    print(\"\\n\" + \"=\"*60)\n    print(f\"[{split_idx+1}/{len(splits_to_train)}] {split_name}\")\n    print(\"=\"*60)\n    \n    split = all_splits[split_name]\n    print(f\"Train: {split['train_size']:,} | Test: {split['test_size']:,}\")\n    \n    if split['test_size'] < 50:\n        print(\"\u26a0\ufe0f  Test set too small - skipping\")\n        continue\n    \n    model = BIPModel().to(device)\n    \n    train_dataset = NativeDataset(set(split['train_ids']), 'data/processed/passages.jsonl',\n                                   'data/processed/bonds.jsonl', tokenizer)\n    test_dataset = NativeDataset(set(split['test_ids'][:MAX_TEST_SAMPLES]), 'data/processed/passages.jsonl',\n                                  'data/processed/bonds.jsonl', tokenizer)\n    \n    if len(train_dataset) == 0:\n        print(\"ERROR: No training data!\")\n        continue\n    \n    # Use hardware-optimized batch size\n    actual_batch = min(BATCH_SIZE, max(32, len(train_dataset) // 20))\n    print(f\"Actual batch size: {actual_batch}\")\n    \n    train_loader = DataLoader(train_dataset, batch_size=actual_batch, shuffle=True,\n                              collate_fn=collate_fn, drop_last=True, num_workers=NUM_WORKERS, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=actual_batch*2, shuffle=False,\n                             collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n    \n    def get_adv_lambda(epoch):\n        if epoch <= 2: return 0.1 + 0.45 * epoch\n        return 1.0\n    \n    for epoch in range(1, N_EPOCHS + 1):\n        model.train()\n        total_loss, n_batches = 0, 0\n        \n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n            optimizer.zero_grad()\n            \n            adv_lambda = get_adv_lambda(epoch)\n            \n            with torch.amp.autocast('cuda', enabled=USE_AMP):\n                out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), adv_lambda)\n                loss = F.cross_entropy(out['bond_pred'], batch['bond_labels'].to(device)) + \\\n                       LANG_WEIGHT * F.cross_entropy(out['language_pred'], batch['language_labels'].to(device)) + \\\n                       PERIOD_WEIGHT * F.cross_entropy(out['period_pred'], batch['period_labels'].to(device))\n            \n            if USE_AMP and scaler:\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n            \n            total_loss += loss.item()\n            n_batches += 1\n        \n        print(f\"Epoch {epoch}: Loss={total_loss/n_batches:.4f} (adv_\u03bb={adv_lambda:.2f})\")\n    \n    # Save model\n    torch.save(model.state_dict(), f'models/checkpoints/best_{split_name}.pt')\n    torch.save(model.state_dict(), f'{SAVE_DIR}/best_{split_name}.pt')\n    \n    # Evaluate\n    print(\"\\nEvaluating...\")\n    model.eval()\n    all_preds, all_labels, all_lang_preds, all_lang_labels, all_languages = [], [], [], [], []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Testing\"):\n            out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n            all_preds.extend(out['bond_pred'].argmax(-1).cpu().tolist())\n            all_labels.extend(batch['bond_labels'].tolist())\n            all_lang_preds.extend(out['language_pred'].argmax(-1).cpu().tolist())\n            all_lang_labels.extend(batch['language_labels'].tolist())\n            all_languages.extend(batch['languages'])\n    \n    bond_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n    bond_acc = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_preds)\n    lang_acc = sum(p == l for p, l in zip(all_lang_preds, all_lang_labels)) / len(all_lang_preds)\n    \n    # Per-language F1\n    lang_f1 = {}\n    for lang in set(all_languages):\n        mask = [l == lang for l in all_languages]\n        if sum(mask) > 10:\n            p = [x for x, m in zip(all_preds, mask) if m]\n            l = [x for x, m in zip(all_labels, mask) if m]\n            lang_f1[lang] = {'f1': f1_score(l, p, average='macro', zero_division=0), 'n': sum(mask)}\n    \n    all_results[split_name] = {'bond_f1_macro': bond_f1, 'bond_acc': bond_acc, 'language_acc': lang_acc,\n        'per_language_f1': lang_f1, 'time': time.time() - split_start}\n    \n    print(f\"\\n{split_name} RESULTS:\")\n    print(f\"  Bond F1 (macro): {bond_f1:.3f} ({bond_f1/0.1:.1f}x chance)\")\n    print(f\"  Bond accuracy:   {bond_acc:.1%}\")\n    print(f\"  Language acc:    {lang_acc:.1%} (want ~20%)\")\n    print(\"  Per-language:\")\n    for lang, m in sorted(lang_f1.items(), key=lambda x: -x[1]['n']):\n        print(f\"    {lang:20s}: F1={m['f1']:.3f} (n={m['n']:,})\")\n    \n    # GPU memory usage\n    if torch.cuda.is_available():\n        mem = torch.cuda.memory_allocated() / 1e9\n        print(f\"\\n  GPU memory: {mem:.1f} GB / {VRAM_GB:.1f} GB ({mem/VRAM_GB*100:.0f}%)\")\n    \n    del model, train_dataset, test_dataset\n    gc.collect()\n    torch.cuda.empty_cache()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 8. Linear Probe Test { display-mode: \"form\" }\n#@markdown Tests if z_bond encodes language/period (should be LOW = invariant)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nprint(\"=\"*60)\nprint(\"LINEAR PROBE TEST\")\nprint(\"=\"*60)\nprint(\"\\nIf probe accuracy \u2248 chance, representation is INVARIANT (good!)\")\n\nprobe_results = {}\n\nfor split_name in ['hebrew_to_others', 'semitic_to_non_semitic']:\n    model_path = f'{SAVE_DIR}/best_{split_name}.pt'\n    if not os.path.exists(model_path):\n        print(f\"\\nSkipping {split_name} - no model\")\n        continue\n    \n    print(f\"\\n{'-'*50}\")\n    print(f\"PROBE: {split_name}\")\n    print('-'*50)\n    \n    model = BIPModel().to(device)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.eval()\n    \n    test_ids = set(all_splits[split_name]['test_ids'][:5000])\n    test_dataset = NativeDataset(test_ids, 'data/processed/passages.jsonl',\n                                  'data/processed/bonds.jsonl', tokenizer)\n    \n    if len(test_dataset) < 50:\n        print(f\"  Skip - only {len(test_dataset)} samples\")\n        continue\n    \n    test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn, num_workers=0)\n    \n    all_z, all_lang, all_period = [], [], []\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Extract\"):\n            out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n            all_z.append(out['z'].cpu().numpy())\n            all_lang.extend(batch['language_labels'].tolist())\n            all_period.extend(batch['period_labels'].tolist())\n    \n    X = np.vstack(all_z)\n    y_lang = np.array(all_lang)\n    y_period = np.array(all_period)\n    \n    X_scaled = StandardScaler().fit_transform(X)\n    \n    n = len(X)\n    idx = np.random.permutation(n)\n    train_idx, test_idx = idx[:int(0.7*n)], idx[int(0.7*n):]\n    \n    # Language probe\n    unique_langs = np.unique(y_lang[test_idx])\n    if len(unique_langs) < 2:\n        print(f\"  Skip lang probe - only 1 class\")\n        lang_acc, lang_chance = 1.0, 1.0\n    else:\n        probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n        probe.fit(X_scaled[train_idx], y_lang[train_idx])\n        lang_acc = (probe.predict(X_scaled[test_idx]) == y_lang[test_idx]).mean()\n        lang_chance = 1.0 / len(unique_langs)\n    \n    # Period probe\n    unique_periods = np.unique(y_period[test_idx])\n    if len(unique_periods) < 2:\n        print(f\"  Skip period probe - only 1 class\")\n        period_acc, period_chance = 1.0, 1.0\n    else:\n        probe = LogisticRegression(max_iter=1000, n_jobs=-1)\n        probe.fit(X_scaled[train_idx], y_period[train_idx])\n        period_acc = (probe.predict(X_scaled[test_idx]) == y_period[test_idx]).mean()\n        period_chance = 1.0 / len(unique_periods)\n    \n    lang_status = \"\u2713 INVARIANT\" if lang_acc < lang_chance + 0.15 else \"\u2717 LEAKING\"\n    period_status = \"\u2713 INVARIANT\" if period_acc < period_chance + 0.15 else \"\u2717 LEAKING\"\n    \n    probe_results[split_name] = {\n        'language_acc': lang_acc, 'language_chance': lang_chance,\n        'period_acc': period_acc, 'period_chance': period_chance\n    }\n    \n    print(f\"\\n  Language: {lang_acc:.1%} (chance: {lang_chance:.1%}) {lang_status}\")\n    print(f\"  Period:   {period_acc:.1%} (chance: {period_chance:.1%}) {period_status}\")\n    \n    del model\n    torch.cuda.empty_cache()\n\nprint(\"\\n\" + \"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 9. Final Results { display-mode: \"form\" }\n#@markdown Summary and verdict\n\nimport json\nimport shutil\n\nprint(\"=\"*60)\nprint(\"FINAL BIP EVALUATION (v10)\")\nprint(\"=\"*60)\n\nprint(f\"\\nHardware: {GPU_TIER} ({VRAM_GB:.0f}GB VRAM, {RAM_GB:.0f}GB RAM)\")\n\nprint(\"\\n\" + \"-\"*60)\nprint(\"TRANSFER RESULTS\")\nprint(\"-\"*60)\n\nsuccessful = []\nfor name, r in all_results.items():\n    ratio = r['bond_f1_macro'] / 0.1\n    transfer_ok = ratio > 1.3\n    invariant_ok = r['language_acc'] < 0.35\n    \n    status = \"\u2713 SUCCESS\" if (transfer_ok and invariant_ok) else \"~ PARTIAL\" if transfer_ok else \"\u2717 FAIL\"\n    \n    print(f\"\\n{name}:\")\n    print(f\"  Bond F1:     {r['bond_f1_macro']:.3f} ({ratio:.1f}x chance)\")\n    print(f\"  Lang acc:    {r['language_acc']:.1%}\")\n    print(f\"  Status:      {status}\")\n    \n    if transfer_ok and invariant_ok:\n        successful.append(name)\n\nprint(\"\\n\" + \"-\"*60)\nprint(\"VERDICT\")\nprint(\"-\"*60)\n\nif len(successful) >= 2:\n    verdict = \"STRONGLY SUPPORTED\"\n    msg = \"Universal moral structure demonstrated across multiple paths\"\nelif len(successful) >= 1:\n    verdict = \"SUPPORTED\"\n    msg = \"Transfer works on at least one path\"\nelif any(r['bond_f1_macro'] > 0.13 for r in all_results.values()):\n    verdict = \"PARTIAL\"\n    msg = \"Some signal, needs more data/tuning\"\nelse:\n    verdict = \"INCONCLUSIVE\"\n    msg = \"No clear transfer\"\n\nprint(f\"\\n  Successful: {len(successful)}/{len(all_results)}\")\nprint(f\"  VERDICT: {verdict}\")\nprint(f\"  {msg}\")\n\n# Save\nfinal = {'all_results': all_results, 'successful': successful, 'verdict': verdict,\n    'hardware': {'gpu': GPU_TIER, 'vram_gb': VRAM_GB, 'ram_gb': RAM_GB},\n    'settings': {'batch_size': BATCH_SIZE, 'max_per_lang': MAX_PER_LANG},\n    'total_time_min': (time.time() - EXPERIMENT_START) / 60}\n\nwith open('results/final_results.json', 'w') as f:\n    json.dump(final, f, indent=2, default=str)\nshutil.copy('results/final_results.json', f'{SAVE_DIR}/final_results.json')\n\nprint(f\"\\nTotal time: {final['total_time_min']:.1f} minutes\")\nprint(f\"Results saved to Drive!\")\nprint(\"\\n\" + \"=\"*60)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#@title 10. Download Results { display-mode: \"form\" }\n#@markdown Download models and results as zip\n\nfrom google.colab import files\nimport zipfile\n\nprint(\"Creating download package...\")\n\nwith zipfile.ZipFile('BIP_v10_results.zip', 'w', zipfile.ZIP_DEFLATED) as zf:\n    if os.path.exists('results/final_results.json'):\n        zf.write('results/final_results.json')\n    for f in os.listdir(SAVE_DIR):\n        if f.endswith('.pt') or f.endswith('.json'):\n            zf.write(f'{SAVE_DIR}/{f}', f)\n\nprint(\"\\nDownloading...\")\nfiles.download('BIP_v10_results.zip')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}