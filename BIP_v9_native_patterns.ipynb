{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# BIP Native-Language Morality Experiment (v9)\n\n**Testing Universal Moral Structure with NO Translation Bridge**\n\nThis experiment tests whether moral cognition has invariant structure using a rigorous methodology:\n\n1. **Native-language pattern matching**: Each language uses its OWN patterns to extract moral labels\n2. **Native text input**: Model sees original Hebrew/Chinese/Arabic text\n3. **Mathematical alignment only**: The ONLY connection between languages is the learned latent space\n\n---\n\n## Why This Matters\n\n**v8 approach** (weaker): Extract labels from English translations \u2192 cheats via semantic bridge\n\n**v9 approach** (stronger): \n- Hebrew text \u2192 Hebrew patterns \u2192 Hebrew moral labels\n- Chinese text \u2192 Chinese patterns \u2192 Chinese moral labels  \n- Arabic text \u2192 Arabic patterns \u2192 Arabic moral labels\n- English text \u2192 English patterns \u2192 English moral labels\n\nIf the model can still transfer bond representations across languages, it means the **mathematical structure of morality is truly universal** - not just an artifact of translation.\n\n---\n\n## Native Pattern Examples\n\n| Concept | Hebrew | Chinese | Arabic | English |\n|---------|--------|---------|--------|--------|\n| Obligation | \u05d7\u05d9\u05d9\u05d1, \u05e6\u05e8\u05d9\u05da, \u05d0\u05e1\u05d5\u05e8 | \u5fc5\u9808, \u61c9\u8a72, \u7576 | \u064a\u062c\u0628\u060c \u0648\u0627\u062c\u0628\u060c \u0641\u0631\u0636 | must, shall, ought |\n| Harm | \u05d4\u05e8\u05d2, \u05e0\u05d6\u05e7, \u05e4\u05d2\u05e2 | \u6bba, \u5bb3, \u50b7 | \u0642\u062a\u0644\u060c \u0636\u0631\u0631\u060c \u0623\u0630\u0649 | kill, harm, hurt |\n| Family | \u05d0\u05d1, \u05d0\u05dd, \u05db\u05d1\u05d3 | \u7236, \u6bcd, \u5b5d | \u0648\u0627\u0644\u062f\u064a\u0646\u060c \u0623\u0628\u060c \u0623\u0645 | father, mother, honor |\n| Authority | \u05de\u05dc\u05da, \u05e9\u05d5\u05e4\u05d8, \u05e6\u05d5\u05d5\u05d4 | \u541b, \u81e3, \u547d | \u0637\u0627\u0639\u0629\u060c \u062d\u0643\u0645\u060c \u0623\u0645\u0631 | king, judge, command |\n\n---"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 1. Setup { display-mode: \"form\" }\n\nimport time\nEXPERIMENT_START = time.time()\n\nprint(\"=\"*60)\nprint(\"BIP NATIVE-LANGUAGE EXPERIMENT (v9)\")\nprint(\"No Translation Bridge - Pure Mathematical Alignment\")\nprint(\"=\"*60)\nprint()\n\nTASKS = [\n    \"Install dependencies\",\n    \"Download corpora\",\n    \"Define native patterns\",\n    \"Extract bonds (native)\",\n    \"Generate splits\",\n    \"Train BIP model\",\n    \"Linear probe test\",\n    \"Evaluate results\"\n]\ntask_status = {task: \"pending\" for task in TASKS}\ntask_times = {}\ntask_start_time = None\n\ndef print_progress():\n    print()\n    print(\"-\"*50)\n    for task in TASKS:\n        status = task_status[task]\n        mark = \"[X]\" if status == \"done\" else \"[>]\" if status == \"running\" else \"[ ]\"\n        time_str = f\" ({task_times.get(task, 0):.1f}s)\" if task in task_times else \"\"\n        print(f\"  {mark} {task}{time_str}\")\n    print(f\"  Elapsed: {(time.time() - EXPERIMENT_START)/60:.1f} min\")\n    print(\"-\"*50, flush=True)\n\ndef mark_task(task, status):\n    global task_start_time\n    if status == \"running\":\n        task_start_time = time.time()\n    elif status == \"done\" and task_start_time:\n        task_times[task] = time.time() - task_start_time\n    task_status[task] = status\n    print_progress()\n\nprint_progress()\nmark_task(\"Install dependencies\", \"running\")\n\nimport os, subprocess, sys\n\nfor dep in [\"transformers\", \"torch\", \"sentence-transformers\", \"pandas\", \"tqdm\", \"psutil\", \"scikit-learn\", \"requests\"]:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", dep])\n\nimport torch, json, psutil, shutil, gc\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nUSE_TPU = 'COLAB_TPU_ADDR' in os.environ\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    device = torch.device(\"cuda\")\n    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n    \n    # Adjust batch size based on GPU\n    if 'A100' in gpu_name:\n        BASE_BATCH_SIZE = 512\n        GPU_TIER = 'A100'\n    elif 'L4' in gpu_name:\n        BASE_BATCH_SIZE = 512\n        GPU_TIER = 'L4'\n    elif 'T4' in gpu_name:\n        BASE_BATCH_SIZE = 256  # T4 has 16GB, be conservative\n        GPU_TIER = 'T4'\n    elif vram_gb >= 20:\n        BASE_BATCH_SIZE = 512\n        GPU_TIER = f'HIGH ({vram_gb:.0f}GB)'\n    elif vram_gb >= 12:\n        BASE_BATCH_SIZE = 256\n        GPU_TIER = f'MED ({vram_gb:.0f}GB)'\n    else:\n        BASE_BATCH_SIZE = 128\n        GPU_TIER = f'LOW ({vram_gb:.0f}GB)'\n    \n    print(f\"GPU: {gpu_name}\")\n    print(f\"VRAM: {vram_gb:.1f} GB\")\n    print(f\"Tier: {GPU_TIER}\")\nelif USE_TPU:\n    import torch_xla.core.xla_model as xm\n    device = xm.xla_device()\n    IS_L4 = False\n    BASE_BATCH_SIZE = 256\nelse:\n    device = torch.device(\"cpu\")\n    IS_L4 = False\n    BASE_BATCH_SIZE = 64\n\nprint(f\"Device: {device}\")\nprint(f\"Batch size: {BASE_BATCH_SIZE} (optimized for {GPU_TIER})\")\n\nif torch.cuda.is_available():\n    from torch.cuda.amp import GradScaler\n    USE_AMP = True\n    scaler = GradScaler()\nelse:\n    USE_AMP = False\n    scaler = None\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\nSAVE_DIR = '/content/drive/MyDrive/BIP_native_v9'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\nfor d in [\"data/processed\", \"data/splits\", \"data/raw\", \"models/checkpoints\", \"results\"]:\n    os.makedirs(d, exist_ok=True)\n\ndef print_resources(label=\"\"):\n    mem = psutil.virtual_memory()\n    msg = f\"[{label}] RAM: {mem.used/1e9:.1f}/{mem.total/1e9:.1f}GB\"\n    if torch.cuda.is_available():\n        msg += f\" | GPU: {torch.cuda.memory_allocated()/1e9:.1f}GB\"\n    print(msg)\n\nmark_task(\"Install dependencies\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 2. Download All Corpora { display-mode: \"form\" }\n\nimport subprocess\nimport os\nimport json\nimport requests\nfrom pathlib import Path\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nmark_task(\"Download corpora\", \"running\")\n\n# ========== SEFARIA ==========\nprint(\"=\"*60)\nprint(\"1. SEFARIA (Hebrew/Aramaic)\")\nprint(\"=\"*60)\n\nsefaria_path = 'data/raw/Sefaria-Export'\nif not os.path.exists(f\"{sefaria_path}/json\"):\n    process = subprocess.Popen(\n        ['git', 'clone', '--depth', '1', '--progress',\n         'https://github.com/Sefaria/Sefaria-Export.git', sefaria_path],\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n    for line in process.stdout:\n        print(line, end='', flush=True)\n    process.wait()\nelse:\n    print(\"Already downloaded.\")\n\n# ========== CHINESE ==========\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. CHINESE CLASSICS\")\nprint(\"=\"*60)\n\nchinese_dir = Path('data/raw/chinese')\nchinese_dir.mkdir(parents=True, exist_ok=True)\n\n# Embedded bilingual Chinese classics (original + translation for reference only)\nCHINESE_TEXTS = [\n    # Analects - Confucian ethics\n    (\"\u5b78\u800c\u6642\u7fd2\u4e4b\uff0c\u4e0d\u4ea6\u8aaa\u4e4e\uff1f\u6709\u670b\u81ea\u9060\u65b9\u4f86\uff0c\u4e0d\u4ea6\u6a02\u4e4e\uff1f\u4eba\u4e0d\u77e5\u800c\u4e0d\u614d\uff0c\u4e0d\u4ea6\u541b\u5b50\u4e4e\uff1f\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u5176\u70ba\u4eba\u4e5f\u5b5d\u5f1f\uff0c\u800c\u597d\u72af\u4e0a\u8005\uff0c\u9bae\u77e3\uff1b\u4e0d\u597d\u72af\u4e0a\uff0c\u800c\u597d\u4f5c\u4e82\u8005\uff0c\u672a\u4e4b\u6709\u4e5f\u3002\u541b\u5b50\u52d9\u672c\uff0c\u672c\u7acb\u800c\u9053\u751f\u3002\u5b5d\u5f1f\u4e5f\u8005\uff0c\u5176\u70ba\u4ec1\u4e4b\u672c\u8207\uff01\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u5de7\u8a00\u4ee4\u8272\uff0c\u9bae\u77e3\u4ec1\uff01\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u543e\u65e5\u4e09\u7701\u543e\u8eab\uff1a\u70ba\u4eba\u8b00\u800c\u4e0d\u5fe0\u4e4e\uff1f\u8207\u670b\u53cb\u4ea4\u800c\u4e0d\u4fe1\u4e4e\uff1f\u50b3\u4e0d\u7fd2\u4e4e\uff1f\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u5f1f\u5b50\u5165\u5247\u5b5d\uff0c\u51fa\u5247\u5f1f\uff0c\u8b39\u800c\u4fe1\uff0c\u6c4e\u611b\u773e\uff0c\u800c\u89aa\u4ec1\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u541b\u5b50\u4e0d\u91cd\u5247\u4e0d\u5a01\uff0c\u5b78\u5247\u4e0d\u56fa\u3002\u4e3b\u5fe0\u4fe1\uff0c\u7121\u53cb\u4e0d\u5982\u5df1\u8005\uff0c\u904e\u5247\u52ff\u619a\u6539\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u7236\u5728\uff0c\u89c0\u5176\u5fd7\uff1b\u7236\u6c92\uff0c\u89c0\u5176\u884c\uff1b\u4e09\u5e74\u7121\u6539\u65bc\u7236\u4e4b\u9053\uff0c\u53ef\u8b02\u5b5d\u77e3\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u79ae\u4e4b\u7528\uff0c\u548c\u70ba\u8cb4\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u4fe1\u8fd1\u65bc\u7fa9\uff0c\u8a00\u53ef\u5fa9\u4e5f\u3002\u606d\u8fd1\u65bc\u79ae\uff0c\u9060\u6065\u8fb1\u4e5f\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u541b\u5b50\u98df\u7121\u6c42\u98fd\uff0c\u5c45\u7121\u6c42\u5b89\uff0c\u654f\u65bc\u4e8b\u800c\u614e\u65bc\u8a00\uff0c\u5c31\u6709\u9053\u800c\u6b63\u7109\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u4e0d\u60a3\u4eba\u4e4b\u4e0d\u5df1\u77e5\uff0c\u60a3\u4e0d\u77e5\u4eba\u4e5f\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u70ba\u653f\u4ee5\u5fb7\uff0c\u8b6c\u5982\u5317\u8fb0\uff0c\u5c45\u5176\u6240\u800c\u773e\u661f\u5171\u4e4b\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u9053\u4e4b\u4ee5\u653f\uff0c\u9f4a\u4e4b\u4ee5\u5211\uff0c\u6c11\u514d\u800c\u7121\u6065\uff1b\u9053\u4e4b\u4ee5\u5fb7\uff0c\u9f4a\u4e4b\u4ee5\u79ae\uff0c\u6709\u6065\u4e14\u683c\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u543e\u5341\u6709\u4e94\u800c\u5fd7\u4e8e\u5b78\uff0c\u4e09\u5341\u800c\u7acb\uff0c\u56db\u5341\u800c\u4e0d\u60d1\uff0c\u4e94\u5341\u800c\u77e5\u5929\u547d\uff0c\u516d\u5341\u800c\u8033\u9806\uff0c\u4e03\u5341\u800c\u5f9e\u5fc3\u6240\u6b32\uff0c\u4e0d\u8e30\u77e9\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u6eab\u6545\u800c\u77e5\u65b0\uff0c\u53ef\u4ee5\u70ba\u5e2b\u77e3\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u541b\u5b50\u4e0d\u5668\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u5148\u884c\u5176\u8a00\u800c\u5f8c\u5f9e\u4e4b\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u541b\u5b50\u5468\u800c\u4e0d\u6bd4\uff0c\u5c0f\u4eba\u6bd4\u800c\u4e0d\u5468\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u5b78\u800c\u4e0d\u601d\u5247\u7f54\uff0c\u601d\u800c\u4e0d\u5b78\u5247\u6b86\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    (\"\u77e5\u4e4b\u70ba\u77e5\u4e4b\uff0c\u4e0d\u77e5\u70ba\u4e0d\u77e5\uff0c\u662f\u77e5\u4e5f\u3002\", \"analects\", \"CONFUCIAN\", -5),\n    # Dao De Jing - Daoist philosophy\n    (\"\u9053\u53ef\u9053\uff0c\u975e\u5e38\u9053\u3002\u540d\u53ef\u540d\uff0c\u975e\u5e38\u540d\u3002\u7121\u540d\u5929\u5730\u4e4b\u59cb\uff1b\u6709\u540d\u842c\u7269\u4e4b\u6bcd\u3002\", \"daodejing\", \"DAOIST\", -6),\n    (\"\u5929\u4e0b\u7686\u77e5\u7f8e\u4e4b\u70ba\u7f8e\uff0c\u65af\u60e1\u5df2\u3002\u7686\u77e5\u5584\u4e4b\u70ba\u5584\uff0c\u65af\u4e0d\u5584\u5df2\u3002\", \"daodejing\", \"DAOIST\", -6),\n    (\"\u4e0d\u5c1a\u8ce2\uff0c\u4f7f\u6c11\u4e0d\u722d\uff1b\u4e0d\u8cb4\u96e3\u5f97\u4e4b\u8ca8\uff0c\u4f7f\u6c11\u4e0d\u70ba\u76dc\u3002\", \"daodejing\", \"DAOIST\", -6),\n    (\"\u9053\u6c96\u800c\u7528\u4e4b\u6216\u4e0d\u76c8\u3002\u6df5\u516e\u4f3c\u842c\u7269\u4e4b\u5b97\u3002\", \"daodejing\", \"DAOIST\", -6),\n    (\"\u5929\u5730\u4e0d\u4ec1\uff0c\u4ee5\u842c\u7269\u70ba\u82bb\u72d7\uff1b\u8056\u4eba\u4e0d\u4ec1\uff0c\u4ee5\u767e\u59d3\u70ba\u82bb\u72d7\u3002\", \"daodejing\", \"DAOIST\", -6),\n    (\"\u4e0a\u5584\u82e5\u6c34\u3002\u6c34\u5584\u5229\u842c\u7269\u800c\u4e0d\u722d\uff0c\u8655\u773e\u4eba\u4e4b\u6240\u60e1\uff0c\u6545\u5e7e\u65bc\u9053\u3002\", \"daodejing\", \"DAOIST\", -6),\n    (\"\u6301\u800c\u76c8\u4e4b\uff0c\u4e0d\u5982\u5176\u5df2\uff1b\u63e3\u800c\u92b3\u4e4b\uff0c\u4e0d\u53ef\u9577\u4fdd\u3002\", \"daodejing\", \"DAOIST\", -6),\n    (\"\u4e09\u5341\u8f3b\u5171\u4e00\u8f42\uff0c\u7576\u5176\u7121\uff0c\u6709\u8eca\u4e4b\u7528\u3002\", \"daodejing\", \"DAOIST\", -6),\n    (\"\u4e94\u8272\u4ee4\u4eba\u76ee\u76f2\uff1b\u4e94\u97f3\u4ee4\u4eba\u8033\u807e\uff1b\u4e94\u5473\u4ee4\u4eba\u53e3\u723d\u3002\", \"daodejing\", \"DAOIST\", -6),\n    (\"\u5927\u9053\u5ee2\uff0c\u6709\u4ec1\u7fa9\uff1b\u667a\u6167\u51fa\uff0c\u6709\u5927\u507d\u3002\", \"daodejing\", \"DAOIST\", -6),\n    (\"\u7d55\u8056\u68c4\u667a\uff0c\u6c11\u5229\u767e\u500d\uff1b\u7d55\u4ec1\u68c4\u7fa9\uff0c\u6c11\u5fa9\u5b5d\u6148\uff1b\u7d55\u5de7\u68c4\u5229\uff0c\u76dc\u8cca\u7121\u6709\u3002\", \"daodejing\", \"DAOIST\", -6),\n    (\"\u66f2\u5247\u5168\uff0c\u6789\u5247\u76f4\uff0c\u7aaa\u5247\u76c8\uff0c\u655d\u5247\u65b0\uff0c\u5c11\u5247\u5f97\uff0c\u591a\u5247\u60d1\u3002\", \"daodejing\", \"DAOIST\", -6),\n    (\"\u5e0c\u8a00\u81ea\u7136\u3002\u6545\u98c4\u98a8\u4e0d\u7d42\u671d\uff0c\u9a5f\u96e8\u4e0d\u7d42\u65e5\u3002\", \"daodejing\", \"DAOIST\", -6),\n    (\"\u4eba\u6cd5\u5730\uff0c\u5730\u6cd5\u5929\uff0c\u5929\u6cd5\u9053\uff0c\u9053\u6cd5\u81ea\u7136\u3002\", \"daodejing\", \"DAOIST\", -6),\n    (\"\u77e5\u4eba\u8005\u667a\uff0c\u81ea\u77e5\u8005\u660e\u3002\u52dd\u4eba\u8005\u6709\u529b\uff0c\u81ea\u52dd\u8005\u5f37\u3002\", \"daodejing\", \"DAOIST\", -6),\n    # Mencius - Confucian ethics on human nature\n    (\"\u4eba\u7686\u6709\u4e0d\u5fcd\u4eba\u4e4b\u5fc3\u3002\u5148\u738b\u6709\u4e0d\u5fcd\u4eba\u4e4b\u5fc3\uff0c\u65af\u6709\u4e0d\u5fcd\u4eba\u4e4b\u653f\u77e3\u3002\", \"mencius\", \"CONFUCIAN\", -4),\n    (\"\u60fb\u96b1\u4e4b\u5fc3\uff0c\u4ec1\u4e4b\u7aef\u4e5f\uff1b\u7f9e\u60e1\u4e4b\u5fc3\uff0c\u7fa9\u4e4b\u7aef\u4e5f\uff1b\u8fad\u8b93\u4e4b\u5fc3\uff0c\u79ae\u4e4b\u7aef\u4e5f\uff1b\u662f\u975e\u4e4b\u5fc3\uff0c\u667a\u4e4b\u7aef\u4e5f\u3002\", \"mencius\", \"CONFUCIAN\", -4),\n    (\"\u4eba\u4e4b\u6240\u4ee5\u7570\u65bc\u79bd\u7378\u8005\u5e7e\u5e0c\uff0c\u5eb6\u6c11\u53bb\u4e4b\uff0c\u541b\u5b50\u5b58\u4e4b\u3002\", \"mencius\", \"CONFUCIAN\", -4),\n    (\"\u5f97\u9053\u8005\u591a\u52a9\uff0c\u5931\u9053\u8005\u5be1\u52a9\u3002\", \"mencius\", \"CONFUCIAN\", -4),\n    (\"\u5929\u6642\u4e0d\u5982\u5730\u5229\uff0c\u5730\u5229\u4e0d\u5982\u4eba\u548c\u3002\", \"mencius\", \"CONFUCIAN\", -4),\n    (\"\u8001\u543e\u8001\uff0c\u4ee5\u53ca\u4eba\u4e4b\u8001\uff1b\u5e7c\u543e\u5e7c\uff0c\u4ee5\u53ca\u4eba\u4e4b\u5e7c\u3002\", \"mencius\", \"CONFUCIAN\", -4),\n    (\"\u6c11\u70ba\u8cb4\uff0c\u793e\u7a37\u6b21\u4e4b\uff0c\u541b\u70ba\u8f15\u3002\", \"mencius\", \"CONFUCIAN\", -4),\n    (\"\u751f\u65bc\u6182\u60a3\uff0c\u6b7b\u65bc\u5b89\u6a02\u3002\", \"mencius\", \"CONFUCIAN\", -4),\n    (\"\u5bcc\u8cb4\u4e0d\u80fd\u6deb\uff0c\u8ca7\u8ce4\u4e0d\u80fd\u79fb\uff0c\u5a01\u6b66\u4e0d\u80fd\u5c48\uff0c\u6b64\u4e4b\u8b02\u5927\u4e08\u592b\u3002\", \"mencius\", \"CONFUCIAN\", -4),\n    (\"\u7aae\u5247\u7368\u5584\u5176\u8eab\uff0c\u9054\u5247\u517c\u5584\u5929\u4e0b\u3002\", \"mencius\", \"CONFUCIAN\", -4),\n    # Zhuangzi - Daoist philosophy\n    (\"\u5317\u51a5\u6709\u9b5a\uff0c\u5176\u540d\u70ba\u9be4\u3002\u9be4\u4e4b\u5927\uff0c\u4e0d\u77e5\u5176\u5e7e\u5343\u91cc\u4e5f\u3002\", \"zhuangzi\", \"DAOIST\", -4),\n    (\"\u6614\u8005\u838a\u5468\u5922\u70ba\u80e1\u8776\uff0c\u6829\u6829\u7136\u80e1\u8776\u4e5f\u3002\u4e0d\u77e5\u5468\u4e4b\u5922\u70ba\u80e1\u8776\u8207\uff0c\u80e1\u8776\u4e4b\u5922\u70ba\u5468\u8207\uff1f\", \"zhuangzi\", \"DAOIST\", -4),\n    (\"\u543e\u751f\u4e5f\u6709\u6daf\uff0c\u800c\u77e5\u4e5f\u7121\u6daf\u3002\u4ee5\u6709\u6daf\u96a8\u7121\u6daf\uff0c\u6b86\u5df2\uff01\", \"zhuangzi\", \"DAOIST\", -4),\n    (\"\u6cc9\u6db8\uff0c\u9b5a\u76f8\u8207\u8655\u65bc\u9678\uff0c\u76f8\u5474\u4ee5\u6fd5\uff0c\u76f8\u6fe1\u4ee5\u6cab\uff0c\u4e0d\u5982\u76f8\u5fd8\u65bc\u6c5f\u6e56\u3002\", \"zhuangzi\", \"DAOIST\", -4),\n    (\"\u4eba\u7686\u77e5\u6709\u7528\u4e4b\u7528\uff0c\u800c\u83ab\u77e5\u7121\u7528\u4e4b\u7528\u4e5f\u3002\", \"zhuangzi\", \"DAOIST\", -4),\n    # Xunzi - Confucian philosophy (human nature is bad, needs cultivation)\n    (\"\u4eba\u4e4b\u6027\u60e1\uff0c\u5176\u5584\u8005\u507d\u4e5f\u3002\", \"xunzi\", \"CONFUCIAN\", -3),\n    (\"\u6545\u6728\u53d7\u7e69\u5247\u76f4\uff0c\u91d1\u5c31\u792a\u5247\u5229\uff0c\u541b\u5b50\u535a\u5b78\u800c\u65e5\u53c3\u7701\u4e4e\u5df1\uff0c\u5247\u77e5\u660e\u800c\u884c\u7121\u904e\u77e3\u3002\", \"xunzi\", \"CONFUCIAN\", -3),\n    (\"\u9752\uff0c\u53d6\u4e4b\u65bc\u85cd\uff0c\u800c\u9752\u65bc\u85cd\uff1b\u51b0\uff0c\u6c34\u70ba\u4e4b\uff0c\u800c\u5bd2\u65bc\u6c34\u3002\", \"xunzi\", \"CONFUCIAN\", -3),\n    (\"\u4e0d\u7a4d\u8dec\u6b65\uff0c\u7121\u4ee5\u81f3\u5343\u91cc\uff1b\u4e0d\u7a4d\u5c0f\u6d41\uff0c\u7121\u4ee5\u6210\u6c5f\u6d77\u3002\", \"xunzi\", \"CONFUCIAN\", -3),\n    (\"\u9365\u800c\u820d\u4e4b\uff0c\u673d\u6728\u4e0d\u6298\uff1b\u9365\u800c\u4e0d\u820d\uff0c\u91d1\u77f3\u53ef\u93e4\u3002\", \"xunzi\", \"CONFUCIAN\", -3),\n]\n\nwith open(chinese_dir / 'chinese_native.json', 'w', encoding='utf-8') as f:\n    data = [{'id': f'chinese_{i}', 'text': t, 'source': s, 'period': p, 'century': c} \n            for i, (t, s, p, c) in enumerate(CHINESE_TEXTS)]\n    json.dump(data, f, ensure_ascii=False, indent=2)\nprint(f\"Saved {len(CHINESE_TEXTS)} Chinese passages\")\n\n# ========== ISLAMIC ==========\nprint(\"\\n\" + \"=\"*60)\nprint(\"3. ISLAMIC TEXTS (Arabic)\")\nprint(\"=\"*60)\n\nislamic_dir = Path('data/raw/islamic')\nislamic_dir.mkdir(parents=True, exist_ok=True)\n\n# Key Quranic and Hadith texts in Arabic\nISLAMIC_TEXTS = [\n    # Quran - core moral teachings\n    (\"\u0628\u0650\u0633\u0652\u0645\u0650 \u0627\u0644\u0644\u064e\u0651\u0647\u0650 \u0627\u0644\u0631\u064e\u0651\u062d\u0652\u0645\u064e\u0670\u0646\u0650 \u0627\u0644\u0631\u064e\u0651\u062d\u0650\u064a\u0645\u0650\", \"quran_1_1\", \"QURANIC\", 7),\n    (\"\u0648\u064e\u0642\u064e\u0636\u064e\u0649\u0670 \u0631\u064e\u0628\u064f\u0651\u0643\u064e \u0623\u064e\u0644\u064e\u0651\u0627 \u062a\u064e\u0639\u0652\u0628\u064f\u062f\u064f\u0648\u0627 \u0625\u0650\u0644\u064e\u0651\u0627 \u0625\u0650\u064a\u064e\u0651\u0627\u0647\u064f \u0648\u064e\u0628\u0650\u0627\u0644\u0652\u0648\u064e\u0627\u0644\u0650\u062f\u064e\u064a\u0652\u0646\u0650 \u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u064b\u0627\", \"quran_17_23\", \"QURANIC\", 7),\n    (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u062a\u064f\u0644\u064f\u0648\u0627 \u0627\u0644\u0646\u064e\u0651\u0641\u0652\u0633\u064e \u0627\u0644\u064e\u0651\u062a\u0650\u064a \u062d\u064e\u0631\u064e\u0651\u0645\u064e \u0627\u0644\u0644\u064e\u0651\u0647\u064f \u0625\u0650\u0644\u064e\u0651\u0627 \u0628\u0650\u0627\u0644\u0652\u062d\u064e\u0642\u0650\u0651\", \"quran_17_33\", \"QURANIC\", 7),\n    (\"\u0648\u064e\u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u0650 \u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0652\u0639\u064e\u0647\u0652\u062f\u064e \u0643\u064e\u0627\u0646\u064e \u0645\u064e\u0633\u0652\u0626\u064f\u0648\u0644\u064b\u0627\", \"quran_17_34\", \"QURANIC\", 7),\n    (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0642\u0652\u0631\u064e\u0628\u064f\u0648\u0627 \u0645\u064e\u0627\u0644\u064e \u0627\u0644\u0652\u064a\u064e\u062a\u0650\u064a\u0645\u0650 \u0625\u0650\u0644\u064e\u0651\u0627 \u0628\u0650\u0627\u0644\u064e\u0651\u062a\u0650\u064a \u0647\u0650\u064a\u064e \u0623\u064e\u062d\u0652\u0633\u064e\u0646\u064f\", \"quran_17_34b\", \"QURANIC\", 7),\n    (\"\u0625\u0650\u0646\u064e\u0651 \u0627\u0644\u0644\u064e\u0651\u0647\u064e \u064a\u064e\u0623\u0652\u0645\u064f\u0631\u064f \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u062f\u0652\u0644\u0650 \u0648\u064e\u0627\u0644\u0652\u0625\u0650\u062d\u0652\u0633\u064e\u0627\u0646\u0650 \u0648\u064e\u0625\u0650\u064a\u062a\u064e\u0627\u0621\u0650 \u0630\u0650\u064a \u0627\u0644\u0652\u0642\u064f\u0631\u0652\u0628\u064e\u0649\u0670\", \"quran_16_90\", \"QURANIC\", 7),\n    (\"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0622\u0645\u064e\u0646\u064f\u0648\u0627 \u0643\u064f\u0648\u0646\u064f\u0648\u0627 \u0642\u064e\u0648\u064e\u0651\u0627\u0645\u0650\u064a\u0646\u064e \u0628\u0650\u0627\u0644\u0652\u0642\u0650\u0633\u0652\u0637\u0650 \u0634\u064f\u0647\u064e\u062f\u064e\u0627\u0621\u064e \u0644\u0650\u0644\u064e\u0651\u0647\u0650\", \"quran_4_135\", \"QURANIC\", 7),\n    (\"\u0648\u064e\u062a\u064e\u0639\u064e\u0627\u0648\u064e\u0646\u064f\u0648\u0627 \u0639\u064e\u0644\u064e\u0649 \u0627\u0644\u0652\u0628\u0650\u0631\u0650\u0651 \u0648\u064e\u0627\u0644\u062a\u064e\u0651\u0642\u0652\u0648\u064e\u0649\u0670 \u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0639\u064e\u0627\u0648\u064e\u0646\u064f\u0648\u0627 \u0639\u064e\u0644\u064e\u0649 \u0627\u0644\u0652\u0625\u0650\u062b\u0652\u0645\u0650 \u0648\u064e\u0627\u0644\u0652\u0639\u064f\u062f\u0652\u0648\u064e\u0627\u0646\u0650\", \"quran_5_2\", \"QURANIC\", 7),\n    (\"\u0645\u064e\u0646\u0652 \u0642\u064e\u062a\u064e\u0644\u064e \u0646\u064e\u0641\u0652\u0633\u064b\u0627 \u0628\u0650\u063a\u064e\u064a\u0652\u0631\u0650 \u0646\u064e\u0641\u0652\u0633\u064d \u0623\u064e\u0648\u0652 \u0641\u064e\u0633\u064e\u0627\u062f\u064d \u0641\u0650\u064a \u0627\u0644\u0652\u0623\u064e\u0631\u0652\u0636\u0650 \u0641\u064e\u0643\u064e\u0623\u064e\u0646\u064e\u0651\u0645\u064e\u0627 \u0642\u064e\u062a\u064e\u0644\u064e \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u064e \u062c\u064e\u0645\u0650\u064a\u0639\u064b\u0627\", \"quran_5_32a\", \"QURANIC\", 7),\n    (\"\u0648\u064e\u0645\u064e\u0646\u0652 \u0623\u064e\u062d\u0652\u064a\u064e\u0627\u0647\u064e\u0627 \u0641\u064e\u0643\u064e\u0623\u064e\u0646\u064e\u0651\u0645\u064e\u0627 \u0623\u064e\u062d\u0652\u064a\u064e\u0627 \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u064e \u062c\u064e\u0645\u0650\u064a\u0639\u064b\u0627\", \"quran_5_32b\", \"QURANIC\", 7),\n    (\"\u0648\u064e\u0644\u064e\u0627 \u062a\u064e\u0623\u0652\u0643\u064f\u0644\u064f\u0648\u0627 \u0623\u064e\u0645\u0652\u0648\u064e\u0627\u0644\u064e\u0643\u064f\u0645 \u0628\u064e\u064a\u0652\u0646\u064e\u0643\u064f\u0645 \u0628\u0650\u0627\u0644\u0652\u0628\u064e\u0627\u0637\u0650\u0644\u0650\", \"quran_2_188\", \"QURANIC\", 7),\n    (\"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0622\u0645\u064e\u0646\u064f\u0648\u0627 \u0623\u064e\u0648\u0652\u0641\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064f\u0642\u064f\u0648\u062f\u0650\", \"quran_5_1\", \"QURANIC\", 7),\n    (\"\u0648\u064e\u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0647\u064f\u0645\u0652 \u0644\u0650\u0623\u064e\u0645\u064e\u0627\u0646\u064e\u0627\u062a\u0650\u0647\u0650\u0645\u0652 \u0648\u064e\u0639\u064e\u0647\u0652\u062f\u0650\u0647\u0650\u0645\u0652 \u0631\u064e\u0627\u0639\u064f\u0648\u0646\u064e\", \"quran_23_8\", \"QURANIC\", 7),\n    (\"\u0625\u0650\u0646\u064e\u0651\u0645\u064e\u0627 \u0627\u0644\u0652\u0645\u064f\u0624\u0652\u0645\u0650\u0646\u064f\u0648\u0646\u064e \u0625\u0650\u062e\u0652\u0648\u064e\u0629\u064c \u0641\u064e\u0623\u064e\u0635\u0652\u0644\u0650\u062d\u064f\u0648\u0627 \u0628\u064e\u064a\u0652\u0646\u064e \u0623\u064e\u062e\u064e\u0648\u064e\u064a\u0652\u0643\u064f\u0645\u0652\", \"quran_49_10\", \"QURANIC\", 7),\n    (\"\u0648\u064e\u0644\u064e\u0627 \u064a\u064e\u063a\u0652\u062a\u064e\u0628 \u0628\u064e\u0651\u0639\u0652\u0636\u064f\u0643\u064f\u0645 \u0628\u064e\u0639\u0652\u0636\u064b\u0627\", \"quran_49_12\", \"QURANIC\", 7),\n    (\"\u064a\u064e\u0627 \u0623\u064e\u064a\u064f\u0651\u0647\u064e\u0627 \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u064f \u0625\u0650\u0646\u064e\u0651\u0627 \u062e\u064e\u0644\u064e\u0642\u0652\u0646\u064e\u0627\u0643\u064f\u0645 \u0645\u0650\u0651\u0646 \u0630\u064e\u0643\u064e\u0631\u064d \u0648\u064e\u0623\u064f\u0646\u062b\u064e\u0649\u0670 \u0648\u064e\u062c\u064e\u0639\u064e\u0644\u0652\u0646\u064e\u0627\u0643\u064f\u0645\u0652 \u0634\u064f\u0639\u064f\u0648\u0628\u064b\u0627 \u0648\u064e\u0642\u064e\u0628\u064e\u0627\u0626\u0650\u0644\u064e \u0644\u0650\u062a\u064e\u0639\u064e\u0627\u0631\u064e\u0641\u064f\u0648\u0627\", \"quran_49_13\", \"QURANIC\", 7),\n    (\"\u0644\u064e\u0651\u0627 \u064a\u064e\u0646\u0652\u0647\u064e\u0627\u0643\u064f\u0645\u064f \u0627\u0644\u0644\u064e\u0651\u0647\u064f \u0639\u064e\u0646\u0650 \u0627\u0644\u064e\u0651\u0630\u0650\u064a\u0646\u064e \u0644\u064e\u0645\u0652 \u064a\u064f\u0642\u064e\u0627\u062a\u0650\u0644\u064f\u0648\u0643\u064f\u0645\u0652 \u0641\u0650\u064a \u0627\u0644\u062f\u0650\u0651\u064a\u0646\u0650 \u0648\u064e\u0644\u064e\u0645\u0652 \u064a\u064f\u062e\u0652\u0631\u0650\u062c\u064f\u0648\u0643\u064f\u0645 \u0645\u0650\u0651\u0646 \u062f\u0650\u064a\u064e\u0627\u0631\u0650\u0643\u064f\u0645\u0652 \u0623\u064e\u0646 \u062a\u064e\u0628\u064e\u0631\u064f\u0651\u0648\u0647\u064f\u0645\u0652 \u0648\u064e\u062a\u064f\u0642\u0652\u0633\u0650\u0637\u064f\u0648\u0627 \u0625\u0650\u0644\u064e\u064a\u0652\u0647\u0650\u0645\u0652\", \"quran_60_8\", \"QURANIC\", 7),\n    (\"\u0648\u064e\u0625\u0650\u0630\u064e\u0627 \u062d\u064e\u0643\u064e\u0645\u0652\u062a\u064f\u0645 \u0628\u064e\u064a\u0652\u0646\u064e \u0627\u0644\u0646\u064e\u0651\u0627\u0633\u0650 \u0623\u064e\u0646 \u062a\u064e\u062d\u0652\u0643\u064f\u0645\u064f\u0648\u0627 \u0628\u0650\u0627\u0644\u0652\u0639\u064e\u062f\u0652\u0644\u0650\", \"quran_4_58\", \"QURANIC\", 7),\n    (\"\u062e\u064f\u0630\u0650 \u0627\u0644\u0652\u0639\u064e\u0641\u0652\u0648\u064e \u0648\u064e\u0623\u0652\u0645\u064f\u0631\u0652 \u0628\u0650\u0627\u0644\u0652\u0639\u064f\u0631\u0652\u0641\u0650 \u0648\u064e\u0623\u064e\u0639\u0652\u0631\u0650\u0636\u0652 \u0639\u064e\u0646\u0650 \u0627\u0644\u0652\u062c\u064e\u0627\u0647\u0650\u0644\u0650\u064a\u0646\u064e\", \"quran_7_199\", \"QURANIC\", 7),\n    (\"\u0627\u062f\u0652\u0641\u064e\u0639\u0652 \u0628\u0650\u0627\u0644\u064e\u0651\u062a\u0650\u064a \u0647\u0650\u064a\u064e \u0623\u064e\u062d\u0652\u0633\u064e\u0646\u064f \u0641\u064e\u0625\u0650\u0630\u064e\u0627 \u0627\u0644\u064e\u0651\u0630\u0650\u064a \u0628\u064e\u064a\u0652\u0646\u064e\u0643\u064e \u0648\u064e\u0628\u064e\u064a\u0652\u0646\u064e\u0647\u064f \u0639\u064e\u062f\u064e\u0627\u0648\u064e\u0629\u064c \u0643\u064e\u0623\u064e\u0646\u064e\u0651\u0647\u064f \u0648\u064e\u0644\u0650\u064a\u064c\u0651 \u062d\u064e\u0645\u0650\u064a\u0645\u064c\", \"quran_41_34\", \"QURANIC\", 7),\n    # Hadith - Prophetic traditions on ethics\n    (\"\u0625\u0646\u0645\u0627 \u0627\u0644\u0623\u0639\u0645\u0627\u0644 \u0628\u0627\u0644\u0646\u064a\u0627\u062a \u0648\u0625\u0646\u0645\u0627 \u0644\u0643\u0644 \u0627\u0645\u0631\u0626 \u0645\u0627 \u0646\u0648\u0649\", \"bukhari_1\", \"HADITH\", 9),\n    (\"\u0644\u0627 \u064a\u0624\u0645\u0646 \u0623\u062d\u062f\u0643\u0645 \u062d\u062a\u0649 \u064a\u062d\u0628 \u0644\u0623\u062e\u064a\u0647 \u0645\u0627 \u064a\u062d\u0628 \u0644\u0646\u0641\u0633\u0647\", \"bukhari_13\", \"HADITH\", 9),\n    (\"\u0645\u0646 \u0643\u0627\u0646 \u064a\u0624\u0645\u0646 \u0628\u0627\u0644\u0644\u0647 \u0648\u0627\u0644\u064a\u0648\u0645 \u0627\u0644\u0622\u062e\u0631 \u0641\u0644\u064a\u0642\u0644 \u062e\u064a\u0631\u0627 \u0623\u0648 \u0644\u064a\u0635\u0645\u062a\", \"bukhari_6018\", \"HADITH\", 9),\n    (\"\u0627\u0644\u0645\u0633\u0644\u0645 \u0645\u0646 \u0633\u0644\u0645 \u0627\u0644\u0645\u0633\u0644\u0645\u0648\u0646 \u0645\u0646 \u0644\u0633\u0627\u0646\u0647 \u0648\u064a\u062f\u0647\", \"bukhari_10\", \"HADITH\", 9),\n    (\"\u0644\u0627 \u0636\u0631\u0631 \u0648\u0644\u0627 \u0636\u0631\u0627\u0631\", \"ibn_majah_2341\", \"HADITH\", 9),\n    (\"\u0627\u0631\u062d\u0645\u0648\u0627 \u0645\u0646 \u0641\u064a \u0627\u0644\u0623\u0631\u0636 \u064a\u0631\u062d\u0645\u0643\u0645 \u0645\u0646 \u0641\u064a \u0627\u0644\u0633\u0645\u0627\u0621\", \"tirmidhi_1924\", \"HADITH\", 9),\n    (\"\u0627\u0644\u062f\u064a\u0646 \u0627\u0644\u0646\u0635\u064a\u062d\u0629\", \"muslim_55\", \"HADITH\", 9),\n    (\"\u0645\u0646 \u0631\u0623\u0649 \u0645\u0646\u0643\u0645 \u0645\u0646\u0643\u0631\u0627 \u0641\u0644\u064a\u063a\u064a\u0631\u0647 \u0628\u064a\u062f\u0647 \u0641\u0625\u0646 \u0644\u0645 \u064a\u0633\u062a\u0637\u0639 \u0641\u0628\u0644\u0633\u0627\u0646\u0647 \u0641\u0625\u0646 \u0644\u0645 \u064a\u0633\u062a\u0637\u0639 \u0641\u0628\u0642\u0644\u0628\u0647\", \"muslim_49\", \"HADITH\", 9),\n    (\"\u0644\u0627 \u064a\u062d\u0644 \u0645\u0627\u0644 \u0627\u0645\u0631\u0626 \u0645\u0633\u0644\u0645 \u0625\u0644\u0627 \u0628\u0637\u064a\u0628 \u0646\u0641\u0633 \u0645\u0646\u0647\", \"ahmad_20172\", \"HADITH\", 9),\n    (\"\u0643\u0644\u0643\u0645 \u0631\u0627\u0639 \u0648\u0643\u0644\u0643\u0645 \u0645\u0633\u0624\u0648\u0644 \u0639\u0646 \u0631\u0639\u064a\u062a\u0647\", \"bukhari_7138\", \"HADITH\", 9),\n    (\"\u0625\u0646 \u0627\u0644\u0644\u0647 \u0643\u062a\u0628 \u0627\u0644\u0625\u062d\u0633\u0627\u0646 \u0639\u0644\u0649 \u0643\u0644 \u0634\u064a\u0621\", \"muslim_1955\", \"HADITH\", 9),\n    (\"\u0644\u064a\u0633 \u0645\u0646\u0627 \u0645\u0646 \u0644\u0645 \u064a\u0631\u062d\u0645 \u0635\u063a\u064a\u0631\u0646\u0627 \u0648\u064a\u0648\u0642\u0631 \u0643\u0628\u064a\u0631\u0646\u0627\", \"tirmidhi_1919\", \"HADITH\", 9),\n    (\"\u0627\u0644\u0645\u0624\u0645\u0646 \u0644\u0644\u0645\u0624\u0645\u0646 \u0643\u0627\u0644\u0628\u0646\u064a\u0627\u0646 \u064a\u0634\u062f \u0628\u0639\u0636\u0647 \u0628\u0639\u0636\u0627\", \"bukhari_481\", \"HADITH\", 9),\n    (\"\u0645\u0646 \u063a\u0634\u0646\u0627 \u0641\u0644\u064a\u0633 \u0645\u0646\u0627\", \"muslim_101\", \"HADITH\", 9),\n    (\"\u0627\u062a\u0642 \u0627\u0644\u0644\u0647 \u062d\u064a\u062b\u0645\u0627 \u0643\u0646\u062a \u0648\u0623\u062a\u0628\u0639 \u0627\u0644\u0633\u064a\u0626\u0629 \u0627\u0644\u062d\u0633\u0646\u0629 \u062a\u0645\u062d\u0647\u0627 \u0648\u062e\u0627\u0644\u0642 \u0627\u0644\u0646\u0627\u0633 \u0628\u062e\u0644\u0642 \u062d\u0633\u0646\", \"tirmidhi_1987\", \"HADITH\", 9),\n    (\"\u0625\u0646 \u0627\u0644\u0644\u0647 \u0644\u0627 \u064a\u0646\u0638\u0631 \u0625\u0644\u0649 \u0635\u0648\u0631\u0643\u0645 \u0648\u0623\u0645\u0648\u0627\u0644\u0643\u0645 \u0648\u0644\u0643\u0646 \u064a\u0646\u0638\u0631 \u0625\u0644\u0649 \u0642\u0644\u0648\u0628\u0643\u0645 \u0648\u0623\u0639\u0645\u0627\u0644\u0643\u0645\", \"muslim_2564\", \"HADITH\", 9),\n    (\"\u0623\u062d\u0628 \u0627\u0644\u0646\u0627\u0633 \u0625\u0644\u0649 \u0627\u0644\u0644\u0647 \u0623\u0646\u0641\u0639\u0647\u0645 \u0644\u0644\u0646\u0627\u0633\", \"tabarani\", \"HADITH\", 9),\n    (\"\u062e\u064a\u0631\u0643\u0645 \u062e\u064a\u0631\u0643\u0645 \u0644\u0623\u0647\u0644\u0647 \u0648\u0623\u0646\u0627 \u062e\u064a\u0631\u0643\u0645 \u0644\u0623\u0647\u0644\u064a\", \"tirmidhi_3895\", \"HADITH\", 9),\n    (\"\u0645\u0627 \u0646\u0642\u0635\u062a \u0635\u062f\u0642\u0629 \u0645\u0646 \u0645\u0627\u0644\", \"muslim_2588\", \"HADITH\", 9),\n    (\"\u0627\u0644\u0638\u0644\u0645 \u0638\u0644\u0645\u0627\u062a \u064a\u0648\u0645 \u0627\u0644\u0642\u064a\u0627\u0645\u0629\", \"bukhari_2447\", \"HADITH\", 9),\n]\n\nwith open(islamic_dir / 'islamic_native.json', 'w', encoding='utf-8') as f:\n    data = [{'id': f'islamic_{i}', 'text': t, 'source': s, 'period': p, 'century': c}\n            for i, (t, s, p, c) in enumerate(ISLAMIC_TEXTS)]\n    json.dump(data, f, ensure_ascii=False, indent=2)\nprint(f\"Saved {len(ISLAMIC_TEXTS)} Arabic passages\")\n\n# ========== DEAR ABBY ==========\nprint(\"\\n\" + \"=\"*60)\nprint(\"4. DEAR ABBY (English)\")\nprint(\"=\"*60)\n\nif not os.path.exists('sqnd-probe-data'):\n    subprocess.run(['git', 'clone', '--depth', '1', 'https://github.com/ahb-sjsu/sqnd-probe.git', 'sqnd-probe-data'])\n\n!cp sqnd-probe-data/dear_abby_data/raw_da_qs.csv data/raw/dear_abby.csv 2>/dev/null || echo \"Using existing\"\ndf = pd.read_csv('data/raw/dear_abby.csv')\nprint(f\"Loaded {len(df):,} Dear Abby entries\")\n\nprint_resources(\"After downloads\")\nmark_task(\"Download corpora\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 3. Define Native-Language Patterns + Text Normalization { display-mode: \"form\" }\n#@markdown Native patterns for each language + proper Unicode normalization.\n\nimport re\nimport unicodedata\nfrom enum import Enum, auto\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, List, Set, Tuple\n\nprint(\"=\"*60)\nprint(\"TEXT NORMALIZATION & NATIVE PATTERNS\")\nprint(\"=\"*60)\nprint()\n\n# ============================================================\n# TEXT NORMALIZATION (Critical for Hebrew/Arabic pattern matching)\n# ============================================================\n\ndef normalize_hebrew(text: str) -> str:\n    \"\"\"Normalize Hebrew text for pattern matching.\"\"\"\n    # Unicode NFKC normalization\n    text = unicodedata.normalize('NFKC', text)\n    \n    # Remove Hebrew diacritics (nikud)\n    # Range: U+0591 to U+05C7 (cantillation marks and points)\n    text = re.sub(r'[\\u0591-\\u05C7]', '', text)\n    \n    # Normalize final letters to regular forms\n    finals_map = {\n        '\u05da': '\u05db',  # final kaf -> kaf\n        '\u05dd': '\u05de',  # final mem -> mem\n        '\u05df': '\u05e0',  # final nun -> nun\n        '\u05e3': '\u05e4',  # final pe -> pe\n        '\u05e5': '\u05e6',  # final tsadi -> tsadi\n    }\n    for final, regular in finals_map.items():\n        text = text.replace(final, regular)\n    \n    return text\n\ndef normalize_arabic(text: str) -> str:\n    \"\"\"Normalize Arabic text for pattern matching.\"\"\"\n    # Unicode NFKC normalization\n    text = unicodedata.normalize('NFKC', text)\n    \n    # Remove Arabic diacritics (tashkeel)\n    # Range: U+064B to U+065F (Arabic marks)\n    text = re.sub(r'[\\u064B-\\u065F]', '', text)\n    \n    # Remove tatweel (kashida)\n    text = text.replace('\\u0640', '')\n    \n    # Normalize alef variants\n    alef_variants = ['\u0623', '\u0625', '\u0622', '\u0671']  # alef with hamza above/below, madda, wasla\n    for variant in alef_variants:\n        text = text.replace(variant, '\u0627')  # normalize to plain alef\n    \n    # Normalize teh marbuta to heh\n    text = text.replace('\u0629', '\u0647')\n    \n    # Normalize alef maksura to yeh\n    text = text.replace('\u0649', '\u064a')\n    \n    return text\n\ndef normalize_text(text: str, language: str) -> str:\n    \"\"\"Normalize text based on language.\"\"\"\n    if language == 'hebrew':\n        return normalize_hebrew(text)\n    elif language == 'aramaic':\n        # Aramaic uses Hebrew script, same normalization\n        return normalize_hebrew(text)\n    elif language == 'arabic':\n        return normalize_arabic(text)\n    elif language == 'classical_chinese':\n        # Chinese: just NFKC normalization\n        return unicodedata.normalize('NFKC', text)\n    else:\n        # English and others: NFKC + lowercase for matching\n        return unicodedata.normalize('NFKC', text.lower())\n\nprint(\"Normalization functions defined:\")\nprint(\"  - normalize_hebrew(): removes nikud, normalizes finals\")\nprint(\"  - normalize_arabic(): removes tashkeel, normalizes alef/teh\")\nprint(\"  - normalize_text(): dispatches by language\")\nprint()\n\n# Test normalization\ntest_hebrew = \"\u05d4\u05b8\u05d0\u05b8\u05d3\u05b8\u05dd\"  # with nikud\ntest_arabic = \"\u0627\u0644\u0652\u0625\u0650\u0646\u0652\u0633\u064e\u0627\u0646\u064f\"  # with tashkeel\nprint(f\"Hebrew normalization: '{test_hebrew}' -> '{normalize_hebrew(test_hebrew)}'\")\nprint(f\"Arabic normalization: '{test_arabic}' -> '{normalize_arabic(test_arabic)}'\")\nprint()\n\n# ============================================================\n# BOND AND HOHFELD DEFINITIONS\n# ============================================================\n\nclass BondType(Enum):\n    HARM_PREVENTION = auto()\n    RECIPROCITY = auto()\n    AUTONOMY = auto()\n    PROPERTY = auto()\n    FAMILY = auto()\n    AUTHORITY = auto()\n    CARE = auto()\n    FAIRNESS = auto()\n    CONTRACT = auto()\n    NONE = auto()\n\nclass HohfeldState(Enum):\n    OBLIGATION = auto()\n    RIGHT = auto()\n    LIBERTY = auto()\n    NO_RIGHT = auto()\n\n# ============================================================\n# BOND PATTERNS BY LANGUAGE (using normalized forms)\n# ============================================================\n# Patterns should match NORMALIZED text (no diacritics, normalized letters)\n\nALL_BOND_PATTERNS = {\n    'hebrew': {\n        BondType.HARM_PREVENTION: [r'\u05d4\u05e8\u05d2', r'\u05e8\u05e6\u05d7', r'\u05e0\u05d6\u05e7', r'\u05d4\u05db\u05d4', r'\u05d4\u05e6\u05d9\u05dc', r'\u05e9\u05de\u05e8', r'\u05e4\u05e7\u05d5\u05d7.\u05e0\u05e4\u05e9'],\n        BondType.RECIPROCITY: [r'\u05d2\u05de\u05d5\u05dc', r'\u05d4\u05e9\u05d9\u05d1', r'\u05e4\u05e8\u05e2', r'\u05e0\u05ea\u05df.*\u05e7\u05d1\u05dc', r'\u05de\u05d3\u05d4.\u05db\u05e0\u05d2\u05d3'],\n        BondType.AUTONOMY: [r'\u05d1\u05d7\u05e8', r'\u05e8\u05e6\u05d5\u05df', r'\u05d7\u05e4\u05e9', r'\u05e2\u05e6\u05de'],\n        BondType.PROPERTY: [r'\u05e7\u05e0\u05d4', r'\u05de\u05db\u05e8', r'\u05d2\u05d6\u05dc', r'\u05d2\u05e0\u05d1', r'\u05de\u05de\u05d5\u05df', r'\u05e0\u05db\u05e1', r'\u05d9\u05e8\u05e9'],\n        BondType.FAMILY: [r'\u05d0\u05d1', r'\u05d0\u05de', r'\u05d1\u05e0', r'\u05db\u05d1\u05d3.*\u05d0\u05d1', r'\u05db\u05d1\u05d3.*\u05d0\u05de', r'\u05de\u05e9\u05e4\u05d7\u05d4', r'\u05d0\u05d7', r'\u05d0\u05d7\u05d5\u05ea'],\n        BondType.AUTHORITY: [r'\u05de\u05dc\u05db', r'\u05e9\u05d5\u05e4\u05d8', r'\u05e6\u05d5\u05d4', r'\u05ea\u05d5\u05e8\u05d4', r'\u05de\u05e6\u05d5\u05d4', r'\u05d3\u05d9\u05df', r'\u05d7\u05e7'],\n        BondType.CARE: [r'\u05d7\u05e1\u05d3', r'\u05e8\u05d7\u05de', r'\u05e2\u05d6\u05e8', r'\u05ea\u05de\u05db', r'\u05e6\u05d3\u05e7\u05d4'],\n        BondType.FAIRNESS: [r'\u05e6\u05d3\u05e7', r'\u05de\u05e9\u05e4\u05d8', r'\u05d9\u05e9\u05e8', r'\u05e9\u05d5\u05d4'],\n        BondType.CONTRACT: [r'\u05d1\u05e8\u05d9\u05ea', r'\u05e0\u05d3\u05e8', r'\u05e9\u05d1\u05d5\u05e2', r'\u05d4\u05ea\u05d7\u05d9\u05d1', r'\u05e2\u05e8\u05d1'],\n    },\n    'aramaic': {\n        BondType.HARM_PREVENTION: [r'\u05e7\u05d8\u05dc', r'\u05e0\u05d6\u05e7', r'\u05d7\u05d1\u05dc', r'\u05e9\u05d6\u05d9\u05d1', r'\u05e4\u05e6\u05d9'],\n        BondType.RECIPROCITY: [r'\u05e4\u05e8\u05e2', r'\u05e9\u05dc\u05de', r'\u05d0\u05d2\u05e8'],\n        BondType.AUTONOMY: [r'\u05e6\u05d1\u05d9', r'\u05e8\u05e2\u05d5'],\n        BondType.PROPERTY: [r'\u05d6\u05d1\u05e0', r'\u05e7\u05e0\u05d4', r'\u05d2\u05d6\u05dc', r'\u05de\u05de\u05d5\u05e0\u05d0', r'\u05e0\u05db\u05e1\u05d9'],\n        BondType.FAMILY: [r'\u05d0\u05d1\u05d0', r'\u05d0\u05de\u05d0', r'\u05d1\u05e8\u05d0', r'\u05d1\u05e8\u05ea\u05d0', r'\u05d9\u05e7\u05e8', r'\u05d0\u05d7\u05d0'],\n        BondType.AUTHORITY: [r'\u05de\u05dc\u05db\u05d0', r'\u05d3\u05d9\u05e0\u05d0', r'\u05d3\u05d9\u05d9\u05e0\u05d0', r'\u05e4\u05e7\u05d5\u05d3\u05d0', r'\u05d0\u05d5\u05e8\u05d9\u05ea'],\n        BondType.CARE: [r'\u05d7\u05e1\u05d3', r'\u05e8\u05d7\u05de', r'\u05e1\u05e2\u05d3'],\n        BondType.FAIRNESS: [r'\u05d3\u05d9\u05e0\u05d0', r'\u05e7\u05e9\u05d5\u05d8', r'\u05ea\u05e8\u05d9\u05e6'],\n        BondType.CONTRACT: [r'\u05e7\u05d9\u05de\u05d0', r'\u05e9\u05d1\u05d5\u05e2\u05d4', r'\u05e0\u05d3\u05e8\u05d0', r'\u05e2\u05e8\u05d1\u05d0'],\n    },\n    'classical_chinese': {\n        BondType.HARM_PREVENTION: [r'\u6bba', r'\u5bb3', r'\u50b7', r'\u6551', r'\u8b77', r'\u885b', r'\u66b4'],\n        BondType.RECIPROCITY: [r'\u5831', r'\u9084', r'\u511f', r'\u916c', r'\u7b54'],\n        BondType.AUTONOMY: [r'\u81ea', r'\u7531', r'\u4efb', r'\u610f', r'\u5fd7'],\n        BondType.PROPERTY: [r'\u8ca1', r'\u7269', r'\u7522', r'\u76dc', r'\u7aca', r'\u8ce3', r'\u8cb7'],\n        BondType.FAMILY: [r'\u5b5d', r'\u7236', r'\u6bcd', r'\u89aa', r'\u5b50', r'\u5f1f', r'\u5144', r'\u5bb6'],\n        BondType.AUTHORITY: [r'\u541b', r'\u81e3', r'\u738b', r'\u547d', r'\u4ee4', r'\u6cd5', r'\u6cbb'],\n        BondType.CARE: [r'\u4ec1', r'\u611b', r'\u6148', r'\u60e0', r'\u6069', r'\u6190'],\n        BondType.FAIRNESS: [r'\u7fa9', r'\u6b63', r'\u516c', r'\u5e73', r'\u5747'],\n        BondType.CONTRACT: [r'\u7d04', r'\u76df', r'\u8a93', r'\u8afe', r'\u4fe1'],\n    },\n    'arabic': {\n        # Patterns for NORMALIZED Arabic (no diacritics)\n        BondType.HARM_PREVENTION: [r'\u0642\u062a\u0644', r'\u0636\u0631\u0631', r'\u0627\u0630[\u064a\u0649]', r'\u0638\u0644\u0645', r'\u0627\u0646\u0642\u0630', r'\u062d\u0641\u0638', r'\u0627\u0645\u0627\u0646'],\n        BondType.RECIPROCITY: [r'\u062c\u0632\u0627', r'\u0631\u062f', r'\u0642\u0635\u0627\u0635', r'\u0645\u062b\u0644', r'\u0639\u0648\u0636'],\n        BondType.AUTONOMY: [r'\u062d\u0631', r'\u0627\u0631\u0627\u062f\u0629', r'\u0627\u062e\u062a\u064a\u0627\u0631', r'\u0645\u0634\u064a\u0626'],\n        BondType.PROPERTY: [r'\u0645\u0627\u0644', r'\u0645\u0644\u0643', r'\u0633\u0631\u0642', r'\u0628\u064a\u0639', r'\u0634\u0631\u0627', r'\u0645\u064a\u0631\u0627\u062b', r'\u063a\u0635\u0628'],\n        BondType.FAMILY: [r'\u0648\u0627\u0644\u062f', r'\u0627\u0628\u0648', r'\u0627\u0645', r'\u0627\u0628\u0646', r'\u0628\u0646\u062a', r'\u0627\u0647\u0644', r'\u0642\u0631\u0628[\u064a\u0649]', r'\u0631\u062d\u0645'],\n        BondType.AUTHORITY: [r'\u0637\u0627\u0639', r'\u0627\u0645\u0631', r'\u062d\u0643\u0645', r'\u0633\u0644\u0637\u0627\u0646', r'\u062e\u0644\u064a\u0641', r'\u0627\u0645\u0627\u0645', r'\u0634\u0631\u064a\u0639'],\n        BondType.CARE: [r'\u0631\u062d\u0645', r'\u0627\u062d\u0633\u0627\u0646', r'\u0639\u0637\u0641', r'\u0635\u062f\u0642', r'\u0632\u0643\u0627'],\n        BondType.FAIRNESS: [r'\u0639\u062f\u0644', r'\u0642\u0633\u0637', r'\u062d\u0642', r'\u0627\u0646\u0635\u0627\u0641', r'\u0633\u0648[\u064a\u0649]'],\n        BondType.CONTRACT: [r'\u0639\u0647\u062f', r'\u0639\u0642\u062f', r'\u0646\u0630\u0631', r'\u064a\u0645\u064a\u0646', r'\u0648\u0641\u0627', r'\u0627\u0645\u0627\u0646'],\n    },\n    'english': {\n        BondType.HARM_PREVENTION: [r'\\bkill', r'\\bmurder', r'\\bharm', r'\\bhurt', r'\\bsave', r'\\bprotect', r'\\bviolence'],\n        BondType.RECIPROCITY: [r'\\breturn', r'\\brepay', r'\\bexchange', r'\\bgive.*back', r'\\breciproc'],\n        BondType.AUTONOMY: [r'\\bfree', r'\\bchoice', r'\\bchoose', r'\\bconsent', r'\\bautonomy', r'\\bright to'],\n        BondType.PROPERTY: [r'\\bsteal', r'\\btheft', r'\\bown', r'\\bproperty', r'\\bbelong', r'\\binherit'],\n        BondType.FAMILY: [r'\\bfather', r'\\bmother', r'\\bparent', r'\\bchild', r'\\bfamily', r'\\bhonor.*parent'],\n        BondType.AUTHORITY: [r'\\bobey', r'\\bcommand', r'\\bauthority', r'\\blaw', r'\\brule', r'\\bgovern'],\n        BondType.CARE: [r'\\bcare', r'\\bhelp', r'\\bkind', r'\\bcompassion', r'\\bcharity', r'\\bmercy'],\n        BondType.FAIRNESS: [r'\\bfair', r'\\bjust', r'\\bequal', r'\\bequity', r'\\bright\\b'],\n        BondType.CONTRACT: [r'\\bpromise', r'\\bcontract', r'\\bagreem', r'\\bvow', r'\\boath', r'\\bcommit'],\n    },\n}\n\nALL_HOHFELD_PATTERNS = {\n    'hebrew': {\n        HohfeldState.OBLIGATION: [r'\u05d7\u05d9\u05d9\u05d1', r'\u05e6\u05e8\u05d9\u05db', r'\u05de\u05d5\u05db\u05e8\u05d7', r'\u05de\u05e6\u05d5\u05d5\u05d4'],\n        HohfeldState.RIGHT: [r'\u05d6\u05db\u05d5\u05ea', r'\u05e8\u05e9\u05d0\u05d9', r'\u05d6\u05db\u05d0\u05d9', r'\u05de\u05d2\u05d9\u05e2'],\n        HohfeldState.LIBERTY: [r'\u05de\u05d5\u05ea\u05e8', r'\u05e8\u05e9\u05d5\u05ea', r'\u05e4\u05d8\u05d5\u05e8', r'\u05d9\u05db\u05d5\u05dc'],\n        HohfeldState.NO_RIGHT: [r'\u05d0\u05e1\u05d5\u05e8', r'\u05d0\u05d9\u05e0\u05d5 \u05e8\u05e9\u05d0\u05d9', r'\u05d0\u05d9\u05df.*\u05d6\u05db\u05d5\u05ea'],\n    },\n    'aramaic': {\n        HohfeldState.OBLIGATION: [r'\u05d7\u05d9\u05d9\u05d1', r'\u05de\u05d7\u05d5\u05d9\u05d1', r'\u05d1\u05e2\u05d9'],\n        HohfeldState.RIGHT: [r'\u05d6\u05db\u05d5\u05ea', r'\u05e8\u05e9\u05d0\u05d9', r'\u05d6\u05db\u05d9'],\n        HohfeldState.LIBERTY: [r'\u05e9\u05e8\u05d9', r'\u05de\u05d5\u05ea\u05e8', r'\u05e4\u05d8\u05d5\u05e8'],\n        HohfeldState.NO_RIGHT: [r'\u05d0\u05e1\u05d5\u05e8', r'\u05dc\u05d0.*\u05e8\u05e9\u05d0\u05d9'],\n    },\n    'classical_chinese': {\n        HohfeldState.OBLIGATION: [r'\u5fc5', r'\u9808', r'\u7576', r'\u61c9', r'\u5b9c'],\n        HohfeldState.RIGHT: [r'\u53ef', r'\u5f97', r'\u6b0a', r'\u5b9c'],\n        HohfeldState.LIBERTY: [r'\u8a31', r'\u4efb', r'\u807d', r'\u514d'],\n        HohfeldState.NO_RIGHT: [r'\u4e0d\u53ef', r'\u52ff', r'\u7981', r'\u83ab', r'\u975e'],\n    },\n    'arabic': {\n        HohfeldState.OBLIGATION: [r'\u064a\u062c\u0628', r'\u0648\u0627\u062c\u0628', r'\u0641\u0631\u0636', r'\u0644\u0627\u0632\u0645', r'\u0648\u062c\u0648\u0628'],\n        HohfeldState.RIGHT: [r'\u062d\u0642', r'\u064a\u062d\u0642', r'\u062c\u0627\u0626\u0632', r'\u064a\u062c\u0648\u0632'],\n        HohfeldState.LIBERTY: [r'\u0645\u0628\u0627\u062d', r'\u062d\u0644\u0627\u0644', r'\u062c\u0627\u0626\u0632', r'\u0627\u0628\u0627\u062d'],\n        HohfeldState.NO_RIGHT: [r'\u062d\u0631\u0627\u0645', r'\u0645\u062d\u0631\u0645', r'\u0645\u0645\u0646\u0648\u0639', r'\u0644\u0627 \u064a\u062c\u0648\u0632', r'\u0646\u0647[\u064a\u0649]'],\n    },\n    'english': {\n        HohfeldState.OBLIGATION: [r'\\bmust\\b', r'\\bshall\\b', r'\\bobligat', r'\\bduty', r'\\brequir'],\n        HohfeldState.RIGHT: [r'\\bright\\b', r'\\bentitle', r'\\bdeserve', r'\\bclaim'],\n        HohfeldState.LIBERTY: [r'\\bmay\\b', r'\\bpermit', r'\\ballow', r'\\bfree to'],\n        HohfeldState.NO_RIGHT: [r'\\bforbid', r'\\bprohibit', r'\\bmust not', r'\\bshall not'],\n    },\n}\n\nprint(\"Native patterns defined for 5 languages:\")\nfor lang in ALL_BOND_PATTERNS:\n    n_bond = sum(len(p) for p in ALL_BOND_PATTERNS[lang].values())\n    n_hohfeld = sum(len(p) for p in ALL_HOHFELD_PATTERNS.get(lang, {}).values())\n    print(f\"  {lang:20s}: {n_bond:3d} bond patterns, {n_hohfeld:2d} Hohfeld patterns\")\n\nprint()\nprint(\"PATTERNS USE NORMALIZED TEXT (no diacritics)!\")\nprint()\nprint(\"\u2713 Cell 3 complete\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 4. Load Corpora and Extract Bonds (Native Patterns) { display-mode: \"form\" }\n#@markdown Labels extracted using NATIVE patterns with proper normalization.\n#@markdown **Includes label quality metrics for reviewer concerns.**\n\nimport json\nimport hashlib\nimport re\nimport gc\nimport random\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom collections import defaultdict\n\nmark_task(\"Extract bonds (native)\", \"running\")\n\nprint(\"=\"*60)\nprint(\"LOADING CORPORA & EXTRACTING BONDS (NATIVE)\")\nprint(\"=\"*60)\nprint()\nprint(\"CRITICAL: Labels extracted from NATIVE text using NATIVE patterns!\")\nprint(\"Text is NORMALIZED before pattern matching (no diacritics).\")\nprint()\n\n@dataclass\nclass Passage:\n    id: str\n    text: str  # Original text\n    text_normalized: str  # Normalized for pattern matching\n    language: str\n    time_period: str\n    century: int\n    source: str\n    source_type: str\n    \n    def to_dict(self):\n        d = asdict(self)\n        del d['text_normalized']  # Don't save normalized text, can regenerate\n        return d\n\ndef detect_sefaria_language(text: str, category: str) -> str:\n    \"\"\"Detect language of Sefaria text.\"\"\"\n    aramaic_cats = {'Talmud', 'Bavli', 'Yerushalmi', 'Zohar'}\n    if category in aramaic_cats:\n        return 'aramaic'\n    arabic_chars = sum(1 for c in text if '\\u0600' <= c <= '\\u06FF')\n    if arabic_chars > len(text) * 0.3:\n        return 'arabic'\n    return 'hebrew'\n\n# ============================================================\n# BOND EXTRACTION WITH PATTERN TRACKING\n# ============================================================\n\n# Track which patterns fire (for quality analysis)\npattern_hits = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))  # lang -> bond -> pattern -> count\naudit_samples = defaultdict(list)  # lang -> list of (text, bond, pattern) for manual audit\n\ndef extract_bonds_native(text: str, text_normalized: str, language: str) -> Dict:\n    \"\"\"Extract bonds using NATIVE language patterns on NORMALIZED text.\"\"\"\n    bond_patterns = ALL_BOND_PATTERNS.get(language, {})\n    hohfeld_patterns = ALL_HOHFELD_PATTERNS.get(language, {})\n    \n    # Find matching bonds (on normalized text)\n    found_bonds = []\n    matched_patterns = []\n    \n    for bond_type, patterns in bond_patterns.items():\n        for pattern in patterns:\n            if re.search(pattern, text_normalized):\n                found_bonds.append(bond_type.name)\n                matched_patterns.append((bond_type.name, pattern))\n                # Track pattern hits\n                pattern_hits[language][bond_type.name][pattern] += 1\n                break  # One pattern per bond type is enough\n    \n    if not found_bonds:\n        found_bonds = ['NONE']\n        matched_patterns = [('NONE', None)]\n    \n    # Find Hohfeld state\n    hohfeld = None\n    hohfeld_pattern = None\n    for state, patterns in hohfeld_patterns.items():\n        for pattern in patterns:\n            if re.search(pattern, text_normalized):\n                hohfeld = state.name\n                hohfeld_pattern = pattern\n                break\n        if hohfeld:\n            break\n    \n    return {\n        'primary_bond': found_bonds[0],\n        'all_bonds': found_bonds,\n        'hohfeld': hohfeld,\n        'language': language,\n        'matched_patterns': matched_patterns,  # For audit\n        'hohfeld_pattern': hohfeld_pattern,\n    }\n\n# Time period mappings\nCATEGORY_TO_PERIOD = {\n    'Tanakh': 'BIBLICAL', 'Torah': 'BIBLICAL', 'Prophets': 'BIBLICAL', 'Writings': 'BIBLICAL',\n    'Mishnah': 'TANNAITIC', 'Tosefta': 'TANNAITIC',\n    'Talmud': 'AMORAIC', 'Bavli': 'AMORAIC', 'Yerushalmi': 'AMORAIC', 'Midrash': 'AMORAIC',\n    'Halakhah': 'RISHONIM', 'Kabbalah': 'RISHONIM', 'Philosophy': 'RISHONIM',\n    'Chasidut': 'ACHRONIM', 'Musar': 'ACHRONIM', 'Responsa': 'ACHRONIM',\n}\n\nPERIOD_TO_CENTURY = {\n    'BIBLICAL': -6, 'TANNAITIC': 2, 'AMORAIC': 4, 'RISHONIM': 12, 'ACHRONIM': 17,\n    'CONFUCIAN': -4, 'DAOIST': -5, 'QURANIC': 7, 'HADITH': 9, 'DEAR_ABBY': 20,\n}\n\nall_passages = []\n\n# ========== LOAD SEFARIA ==========\nprint(\"Loading Sefaria (Hebrew/Aramaic)...\")\nsefaria_path = Path('data/raw/Sefaria-Export/json')\nif sefaria_path.exists():\n    json_files = list(sefaria_path.rglob('*.json'))\n    print(f\"  Found {len(json_files):,} files\")\n    \n    for json_file in tqdm(json_files, desc=\"Sefaria\", unit=\"file\"):\n        try:\n            with open(json_file, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n        except:\n            continue\n        \n        category = str(json_file.relative_to(sefaria_path).parts[0]) if json_file.relative_to(sefaria_path).parts else \"unknown\"\n        period = CATEGORY_TO_PERIOD.get(category, 'AMORAIC')\n        century = PERIOD_TO_CENTURY.get(period, 4)\n        \n        if isinstance(data, dict):\n            hebrew = data.get('he', [])\n            \n            def flatten_hebrew(h, ref=\"\"):\n                if isinstance(h, str):\n                    h_clean = re.sub(r'<[^>]+>', '', h).strip()\n                    if 20 <= len(h_clean) <= 2000:\n                        lang = detect_sefaria_language(h_clean, category)\n                        pid = hashlib.md5(f\"{json_file.stem}:{ref}:{h_clean[:30]}\".encode()).hexdigest()[:12]\n                        # NORMALIZE TEXT\n                        h_normalized = normalize_text(h_clean, lang)\n                        return [Passage(\n                            id=f\"sefaria_{pid}\",\n                            text=h_clean,\n                            text_normalized=h_normalized,\n                            language=lang,\n                            time_period=period,\n                            century=century,\n                            source=f\"{json_file.stem} {ref}\".strip(),\n                            source_type='sefaria'\n                        )]\n                    return []\n                elif isinstance(h, list):\n                    result = []\n                    for i, hh in enumerate(h):\n                        result.extend(flatten_hebrew(hh, f\"{ref}.{i+1}\" if ref else str(i+1)))\n                    return result\n                return []\n            \n            all_passages.extend(flatten_hebrew(hebrew))\n\nprint(f\"  Loaded {len(all_passages):,} Sefaria passages\")\n\n# ========== LOAD CHINESE ==========\nprint(\"\\nLoading Chinese classics...\")\nwith open('data/raw/chinese/chinese_native.json', 'r', encoding='utf-8') as f:\n    chinese_data = json.load(f)\n\nfor item in chinese_data:\n    text_norm = normalize_text(item['text'], 'classical_chinese')\n    all_passages.append(Passage(\n        id=item['id'],\n        text=item['text'],\n        text_normalized=text_norm,\n        language='classical_chinese',\n        time_period=item['period'],\n        century=item['century'],\n        source=item['source'],\n        source_type='chinese'\n    ))\nprint(f\"  Loaded {len(chinese_data)} Chinese passages\")\n\n# ========== LOAD ISLAMIC ==========\nprint(\"\\nLoading Islamic texts...\")\nwith open('data/raw/islamic/islamic_native.json', 'r', encoding='utf-8') as f:\n    islamic_data = json.load(f)\n\nfor item in islamic_data:\n    text_norm = normalize_text(item['text'], 'arabic')\n    all_passages.append(Passage(\n        id=item['id'],\n        text=item['text'],\n        text_normalized=text_norm,\n        language='arabic',\n        time_period=item['period'],\n        century=item['century'],\n        source=item['source'],\n        source_type='islamic'\n    ))\nprint(f\"  Loaded {len(islamic_data)} Arabic passages\")\n\n# ========== LOAD DEAR ABBY ==========\nprint(\"\\nLoading Dear Abby (English)...\")\ndf = pd.read_csv('data/raw/dear_abby.csv')\nabby_count = 0\nfor idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Dear Abby\", unit=\"row\"):\n    question = str(row.get('question_only', ''))\n    if not question or question == 'nan' or len(question) < 50 or len(question) > 2000:\n        continue\n    \n    year = int(row.get('year', 1990))\n    pid = hashlib.md5(f\"abby:{idx}:{question[:50]}\".encode()).hexdigest()[:12]\n    text_norm = normalize_text(question, 'english')\n    \n    all_passages.append(Passage(\n        id=f\"abby_{pid}\",\n        text=question,\n        text_normalized=text_norm,\n        language='english',\n        time_period='DEAR_ABBY',\n        century=20 if year < 2000 else 21,\n        source=f\"Dear Abby {year}\",\n        source_type='dear_abby'\n    ))\n    abby_count += 1\nprint(f\"  Loaded {abby_count:,} English passages\")\n\nprint()\nprint(\"=\"*60)\nprint(f\"TOTAL PASSAGES: {len(all_passages):,}\")\nprint(\"=\"*60)\n\n# ========== EXTRACT BONDS WITH QUALITY TRACKING ==========\nprint()\nprint(\"=\"*60)\nprint(\"EXTRACTING BONDS (NATIVE PATTERNS ON NORMALIZED TEXT)\")\nprint(\"=\"*60)\n\nbond_counts = defaultdict(lambda: defaultdict(int))\nn_mismatches = 0\n\n# Collect audit samples (200 per language)\nAUDIT_SAMPLE_SIZE = 200\n\nwith open('data/processed/passages.jsonl', 'w') as f_pass, \\\n     open('data/processed/bonds.jsonl', 'w') as f_bond:\n    \n    for p in tqdm(all_passages, desc=\"Extracting\", unit=\"passage\"):\n        # NATIVE extraction on NORMALIZED text\n        bonds = extract_bonds_native(p.text, p.text_normalized, p.language)\n        \n        bond_counts[p.language][bonds['primary_bond']] += 1\n        \n        # Collect audit samples\n        if len(audit_samples[p.language]) < AUDIT_SAMPLE_SIZE:\n            audit_samples[p.language].append({\n                'text': p.text[:200],\n                'bond': bonds['primary_bond'],\n                'pattern': bonds['matched_patterns'][0][1] if bonds['matched_patterns'] else None,\n            })\n        \n        f_pass.write(json.dumps(p.to_dict()) + '\\n')\n        f_bond.write(json.dumps({\n            'passage_id': p.id,\n            'bonds': {\n                'primary_bond': bonds['primary_bond'],\n                'all_bonds': bonds['all_bonds'],\n                'hohfeld': bonds['hohfeld'],\n                'language': bonds['language'],\n            }\n        }) + '\\n')\n\n# ============================================================\n# LABEL QUALITY METRICS (Reviewer Concern #1)\n# ============================================================\nprint()\nprint(\"=\"*60)\nprint(\"LABEL QUALITY METRICS\")\nprint(\"=\"*60)\n\n# 1. Coverage (% that are NOT 'NONE')\nprint(\"\\n1. COVERAGE (% with labels, not NONE):\")\nfor lang in sorted(bond_counts.keys()):\n    total = sum(bond_counts[lang].values())\n    none_count = bond_counts[lang].get('NONE', 0)\n    coverage = (total - none_count) / total * 100 if total > 0 else 0\n    print(f\"   {lang:20s}: {coverage:5.1f}% labeled ({total - none_count:,}/{total:,})\")\n\n# 2. Top triggering patterns per class per language\nprint(\"\\n2. TOP TRIGGERING PATTERNS:\")\nfor lang in sorted(pattern_hits.keys()):\n    print(f\"\\n   {lang.upper()}:\")\n    for bond in sorted(pattern_hits[lang].keys()):\n        if bond == 'NONE':\n            continue\n        patterns = pattern_hits[lang][bond]\n        top = sorted(patterns.items(), key=lambda x: -x[1])[:3]\n        if top:\n            top_str = \", \".join([f\"'{p}' ({c})\" for p, c in top])\n            print(f\"     {bond:20s}: {top_str}\")\n\n# 3. Save audit samples for manual review\nprint(\"\\n3. AUDIT SAMPLES (saved for manual review):\")\naudit_file = 'data/processed/audit_samples.json'\nwith open(audit_file, 'w', encoding='utf-8') as f:\n    json.dump(dict(audit_samples), f, indent=2, ensure_ascii=False)\nprint(f\"   Saved {sum(len(v) for v in audit_samples.values())} samples to {audit_file}\")\nfor lang, samples in audit_samples.items():\n    print(f\"   {lang:20s}: {len(samples)} samples\")\n\n# 4. Bond distribution summary\nprint(\"\\n4. BOND DISTRIBUTION BY LANGUAGE:\")\nfor lang in sorted(bond_counts.keys()):\n    print(f\"\\n  {lang.upper()}:\")\n    total = sum(bond_counts[lang].values())\n    for bond, cnt in sorted(bond_counts[lang].items(), key=lambda x: -x[1])[:5]:\n        pct = 100 * cnt / total\n        print(f\"    {bond:20s}: {cnt:>6,} ({pct:5.1f}%)\")\n\n# Save pattern statistics\npattern_stats = {\n    lang: {bond: dict(patterns) for bond, patterns in bonds.items()}\n    for lang, bonds in pattern_hits.items()\n}\nwith open('data/processed/pattern_stats.json', 'w') as f:\n    json.dump(pattern_stats, f, indent=2)\nprint(f\"\\nPattern statistics saved to data/processed/pattern_stats.json\")\n\nn_passages = len(all_passages)\ndel all_passages\ngc.collect()\n\nprint_resources(\"After extraction\")\nmark_task(\"Extract bonds (native)\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 5. Generate Splits { display-mode: \"form\" }\n#@markdown Language-family and temporal splits.\n\nimport random\nimport json\nimport shutil\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\n\nrandom.seed(42)\n\nmark_task(\"Generate splits\", \"running\")\n\nprint(\"=\"*60)\nprint(\"GENERATING SPLITS\")\nprint(\"=\"*60)\n\n# Read metadata\npassage_meta = []\nwith open('data/processed/passages.jsonl', 'r') as f:\n    for line in tqdm(f, desc=\"Reading\", unit=\"line\"):\n        p = json.loads(line)\n        passage_meta.append(p)\n\nprint(f\"Total: {len(passage_meta):,}\")\n\nby_lang = defaultdict(list)\nfor p in passage_meta:\n    by_lang[p['language']].append(p['id'])\n\nprint(\"\\nBy language:\")\nfor lang, ids in sorted(by_lang.items(), key=lambda x: -len(x[1])):\n    print(f\"  {lang:20s}: {len(ids):>8,}\")\n\n# ========== SPLIT 1: Hebrew -> All Others ==========\nprint(\"\\n\" + \"-\"*60)\nprint(\"SPLIT 1: HEBREW \u2192 ALL OTHERS\")\nprint(\"-\"*60)\n\nhebrew_ids = by_lang['hebrew']\nother_ids = [p['id'] for p in passage_meta if p['language'] != 'hebrew']\nrandom.shuffle(hebrew_ids)\nrandom.shuffle(other_ids)\n\nsplit_hebrew = {\n    'name': 'hebrew_to_others',\n    'train_ids': hebrew_ids,\n    'valid_ids': other_ids[:min(5000, len(other_ids)//10)],\n    'test_ids': other_ids[min(5000, len(other_ids)//10):],\n    'train_size': len(hebrew_ids),\n    'valid_size': min(5000, len(other_ids)//10),\n    'test_size': len(other_ids) - min(5000, len(other_ids)//10),\n}\nprint(f\"  Train (Hebrew): {split_hebrew['train_size']:,}\")\nprint(f\"  Test (Others):  {split_hebrew['test_size']:,}\")\n\n# ========== SPLIT 2: Semitic -> East Asian + English ==========\nprint(\"\\n\" + \"-\"*60)\nprint(\"SPLIT 2: SEMITIC \u2192 CHINESE + ENGLISH\")\nprint(\"-\"*60)\n\nsemitic_ids = by_lang['hebrew'] + by_lang.get('aramaic', []) + by_lang.get('arabic', [])\nnon_semitic_ids = by_lang.get('classical_chinese', []) + by_lang.get('english', [])\nrandom.shuffle(semitic_ids)\nrandom.shuffle(non_semitic_ids)\n\nsplit_semitic = {\n    'name': 'semitic_to_non_semitic',\n    'train_ids': semitic_ids,\n    'valid_ids': non_semitic_ids[:len(non_semitic_ids)//10],\n    'test_ids': non_semitic_ids[len(non_semitic_ids)//10:],\n    'train_size': len(semitic_ids),\n    'valid_size': len(non_semitic_ids)//10,\n    'test_size': len(non_semitic_ids) - len(non_semitic_ids)//10,\n}\nprint(f\"  Train (Semitic): {split_semitic['train_size']:,}\")\nprint(f\"  Test (Non-Semitic): {split_semitic['test_size']:,}\")\n\n# ========== SPLIT 3: Ancient -> Modern ==========\nprint(\"\\n\" + \"-\"*60)\nprint(\"SPLIT 3: ANCIENT \u2192 MODERN\")\nprint(\"-\"*60)\n\nancient_periods = {'BIBLICAL', 'TANNAITIC', 'AMORAIC', 'CONFUCIAN', 'DAOIST', 'QURANIC', 'HADITH'}\nmodern_periods = {'RISHONIM', 'ACHRONIM', 'DEAR_ABBY'}\n\nancient_ids = [p['id'] for p in passage_meta if p['time_period'] in ancient_periods]\nmodern_ids = [p['id'] for p in passage_meta if p['time_period'] in modern_periods]\nrandom.shuffle(ancient_ids)\nrandom.shuffle(modern_ids)\n\nsplit_temporal = {\n    'name': 'ancient_to_modern',\n    'train_ids': ancient_ids,\n    'valid_ids': modern_ids[:len(modern_ids)//10],\n    'test_ids': modern_ids[len(modern_ids)//10:],\n    'train_size': len(ancient_ids),\n    'valid_size': len(modern_ids)//10,\n    'test_size': len(modern_ids) - len(modern_ids)//10,\n}\nprint(f\"  Train (Ancient): {split_temporal['train_size']:,}\")\nprint(f\"  Test (Modern): {split_temporal['test_size']:,}\")\n\n# ========== SPLIT 4: Mixed Baseline ==========\nprint(\"\\n\" + \"-\"*60)\nprint(\"SPLIT 4: MIXED (In-Domain Baseline)\")\nprint(\"-\"*60)\n\nall_ids = [p['id'] for p in passage_meta]\nrandom.shuffle(all_ids)\nn = len(all_ids)\n\nsplit_mixed = {\n    'name': 'mixed_baseline',\n    'train_ids': all_ids[:int(0.7*n)],\n    'valid_ids': all_ids[int(0.7*n):int(0.85*n)],\n    'test_ids': all_ids[int(0.85*n):],\n    'train_size': int(0.7*n),\n    'valid_size': int(0.15*n),\n    'test_size': n - int(0.85*n),\n}\nprint(f\"  Train: {split_mixed['train_size']:,}\")\nprint(f\"  Test: {split_mixed['test_size']:,}\")\n\n# ========== SPLIT 5: Chinese -> All Others ==========\nprint(\"\\n\" + \"-\"*60)\nprint(\"SPLIT 5: CHINESE \u2192 ALL OTHERS\")\nprint(\"-\"*60)\n\nchinese_ids = by_lang.get('classical_chinese', [])\nnon_chinese_ids = [p['id'] for p in passage_meta if p['language'] != 'classical_chinese']\nrandom.shuffle(non_chinese_ids)\n\nsplit_chinese = {\n    'name': 'chinese_to_others',\n    'train_ids': chinese_ids,\n    'valid_ids': non_chinese_ids[:min(len(chinese_ids), 500)],\n    'test_ids': non_chinese_ids[min(len(chinese_ids), 500):min(len(chinese_ids), 500)+10000],\n    'train_size': len(chinese_ids),\n    'valid_size': min(len(chinese_ids), 500),\n    'test_size': min(10000, len(non_chinese_ids) - min(len(chinese_ids), 500)),\n}\nprint(f\"  Train (Chinese): {split_chinese['train_size']:,}\")\nprint(f\"  Test (Others): {split_chinese['test_size']:,}\")\n\n# Save\nall_splits = {\n    'hebrew_to_others': split_hebrew,\n    'semitic_to_non_semitic': split_semitic,\n    'ancient_to_modern': split_temporal,\n    'mixed_baseline': split_mixed,\n    'chinese_to_others': split_chinese,\n}\n\nwith open('data/splits/all_splits.json', 'w') as f:\n    json.dump(all_splits, f, indent=2)\n\n# Compute baselines\nbond_counts = defaultdict(int)\nlang_counts = defaultdict(int)\nperiod_counts = defaultdict(int)\n\nwith open('data/processed/bonds.jsonl', 'r') as fb, \\\n     open('data/processed/passages.jsonl', 'r') as fp:\n    for b_line, p_line in zip(fb, fp):\n        b = json.loads(b_line)\n        p = json.loads(p_line)\n        assert b['passage_id'] == p['id']\n        bond_counts[b['bonds']['primary_bond']] += 1\n        lang_counts[p['language']] += 1\n        period_counts[p['time_period']] += 1\n\nprint(\"\\nID integrity: PASSED \u2713\")\n\nbaselines = {\n    'bond_counts': dict(bond_counts),\n    'language_counts': dict(lang_counts),\n    'period_counts': dict(period_counts),\n    'chance_bond': 1.0 / len(bond_counts),\n    'chance_language': 1.0 / len(lang_counts),\n    'chance_period': 1.0 / len(period_counts),\n}\n\nwith open('data/splits/baselines.json', 'w') as f:\n    json.dump(baselines, f, indent=2)\n\nprint(f\"\\nChance bond:     {baselines['chance_bond']:.1%}\")\nprint(f\"Chance language: {baselines['chance_language']:.1%}\")\nprint(f\"Chance period:   {baselines['chance_period']:.1%}\")\n\n# Save to Drive\nshutil.copytree('data/processed', f'{SAVE_DIR}/processed', dirs_exist_ok=True)\nshutil.copytree('data/splits', f'{SAVE_DIR}/splits', dirs_exist_ok=True)\n\nmark_task(\"Generate splits\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 6. Define Model Architecture { display-mode: \"form\" }\n#@markdown Multilingual encoder with adversarial language/time heads.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel, AutoTokenizer\nfrom tqdm.auto import tqdm\nimport json\nimport gc\n\nprint(\"=\"*60)\nprint(\"MODEL ARCHITECTURE\")\nprint(\"=\"*60)\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nclass GradientReversal(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, lambda_):\n        ctx.lambda_ = lambda_\n        return x.clone()\n    @staticmethod\n    def backward(ctx, grad_output):\n        return -ctx.lambda_ * grad_output, None\n\ndef gradient_reversal(x, lambda_=1.0):\n    return GradientReversal.apply(x, lambda_)\n\nclass BIPModel(nn.Module):\n    def __init__(self, d_model=384, d_bond=64, n_bonds=10, n_langs=5, n_periods=10, n_hohfeld=4):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n        \n        self.bond_proj = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, d_bond)\n        )\n        \n        # Main task heads\n        self.bond_classifier = nn.Linear(d_bond, n_bonds)\n        self.hohfeld_classifier = nn.Linear(d_bond, n_hohfeld)\n        \n        # Adversarial heads (try to predict confounds from z_bond)\n        self.language_classifier = nn.Linear(d_bond, n_langs)\n        self.period_classifier = nn.Linear(d_bond, n_periods)\n    \n    def forward(self, input_ids, attention_mask, adv_lambda=1.0):\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        h = (out.last_hidden_state * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1, keepdim=True).clamp(min=1e-9)\n        \n        z_bond = self.bond_proj(h)\n        z_bond_adv = gradient_reversal(z_bond, adv_lambda)\n        \n        return {\n            'z_bond': z_bond,\n            'bond_pred': self.bond_classifier(z_bond),\n            'hohfeld_pred': self.hohfeld_classifier(z_bond),\n            'language_pred': self.language_classifier(z_bond_adv),\n            'period_pred': self.period_classifier(z_bond_adv),\n        }\n    \n    def extract_z_bond(self, input_ids, attention_mask):\n        with torch.no_grad():\n            out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n            h = (out.last_hidden_state * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1, keepdim=True).clamp(min=1e-9)\n            return self.bond_proj(h)\n\n# Index mappings\nBOND_TO_IDX = {b.name: i for i, b in enumerate(BondType)}\nLANG_TO_IDX = {'hebrew': 0, 'aramaic': 1, 'classical_chinese': 2, 'arabic': 3, 'english': 4}\nPERIOD_TO_IDX = {'BIBLICAL': 0, 'TANNAITIC': 1, 'AMORAIC': 2, 'RISHONIM': 3, 'ACHRONIM': 4,\n                 'CONFUCIAN': 5, 'DAOIST': 6, 'QURANIC': 7, 'HADITH': 8, 'DEAR_ABBY': 9}\nHOHFELD_TO_IDX = {'OBLIGATION': 0, 'RIGHT': 1, 'LIBERTY': 2, None: 3}\n\nclass NativeDataset(Dataset):\n    \"\"\"Dataset using NATIVE text only.\"\"\"\n    def __init__(self, passage_ids: set, passages_file: str, bonds_file: str, tokenizer, max_len=128):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.data = []\n        self.n_mismatches = 0  # Track ID mismatches\n        \n        with open(passages_file, 'r') as fp, open(bonds_file, 'r') as fb:\n            for p_line, b_line in tqdm(zip(fp, fb), desc=\"Loading\", unit=\"line\"):\n                p = json.loads(p_line)\n                b = json.loads(b_line)\n                # DATA INTEGRITY CHECK (Reviewer concern: silent data drops)\n                if b['passage_id'] != p['id']:\n                    self.n_mismatches += 1\n                    continue\n                    # Note: We track mismatches instead of assert to handle edge cases\n                    # Final count is logged below\n                if p['id'] in passage_ids:\n                    self.data.append({\n                        'text': p['text'][:1000],  # NATIVE text!\n                        'language': p['language'],\n                        'period': p['time_period'],\n                        'bond': b['bonds']['primary_bond'],\n                        'hohfeld': b['bonds']['hohfeld'],\n                    })\n        print(f\"  Loaded {len(self.data):,} samples\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        enc = self.tokenizer(item['text'], truncation=True, max_length=self.max_len,\n                            padding='max_length', return_tensors='pt')\n        return {\n            'input_ids': enc['input_ids'].squeeze(0),\n            'attention_mask': enc['attention_mask'].squeeze(0),\n            'bond_label': BOND_TO_IDX.get(item['bond'], 9),\n            'language_label': LANG_TO_IDX.get(item['language'], 4),\n            'period_label': PERIOD_TO_IDX.get(item['period'], 9),\n            'hohfeld_label': HOHFELD_TO_IDX.get(item['hohfeld'], 3),\n            'language': item['language'],\n        }\n\ndef collate_fn(batch):\n    return {\n        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n        'bond_labels': torch.tensor([x['bond_label'] for x in batch]),\n        'language_labels': torch.tensor([x['language_label'] for x in batch]),\n        'period_labels': torch.tensor([x['period_label'] for x in batch]),\n        'hohfeld_labels': torch.tensor([x['hohfeld_label'] for x in batch]),\n        'languages': [x['language'] for x in batch],\n    }\n\nprint(f\"Bond types: {len(BOND_TO_IDX)}\")\nprint(f\"Languages: {len(LANG_TO_IDX)}\")\nprint(f\"Periods: {len(PERIOD_TO_IDX)}\")\nprint(f\"Batch size: {BASE_BATCH_SIZE}\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 7. Train BIP Model { display-mode: \"form\" }\n#@markdown Training with adversarial language/period invariance.\n\n#@markdown **Select splits:**\nTRAIN_HEBREW_TO_OTHERS = True  #@param {type:\"boolean\"}\nTRAIN_SEMITIC_TO_NON_SEMITIC = True  #@param {type:\"boolean\"}\nTRAIN_ANCIENT_TO_MODERN = True  #@param {type:\"boolean\"}\nTRAIN_MIXED_BASELINE = True  #@param {type:\"boolean\"}\n\nimport time\nimport gc\nfrom sklearn.metrics import f1_score\nfrom transformers import AutoTokenizer\n\nmark_task(\"Train BIP model\", \"running\")\n\nprint(\"=\"*60)\nprint(\"TRAINING BIP MODEL (Native Patterns)\")\nprint(\"=\"*60)\nprint()\nprint(\"CRITICAL: Model sees NATIVE text, labels from NATIVE patterns.\")\nprint(\"NO English translation used anywhere!\")\nprint()\n\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n\nsplits_to_train = []\nif TRAIN_HEBREW_TO_OTHERS:\n    splits_to_train.append('hebrew_to_others')\nif TRAIN_SEMITIC_TO_NON_SEMITIC:\n    splits_to_train.append('semitic_to_non_semitic')\nif TRAIN_ANCIENT_TO_MODERN:\n    splits_to_train.append('ancient_to_modern')\nif TRAIN_MIXED_BASELINE:\n    splits_to_train.append('mixed_baseline')\n\nprint(f\"Training {len(splits_to_train)} splits\")\n\nall_results = {}\n\nfor split_idx, split_name in enumerate(splits_to_train):\n    split_start = time.time()\n    print()\n    print(\"=\"*60)\n    print(f\"[{split_idx+1}/{len(splits_to_train)}] {split_name}\")\n    print(\"=\"*60)\n    \n    with open('data/splits/all_splits.json', 'r') as f:\n        split = json.load(f)[split_name]\n    \n    print(f\"Train: {split['train_size']:,} | Test: {split['test_size']:,}\")\n    \n    model = BIPModel().to(device)\n    \n    train_dataset = NativeDataset(set(split['train_ids']), 'data/processed/passages.jsonl',\n                                   'data/processed/bonds.jsonl', tokenizer)\n    test_dataset = NativeDataset(set(split['test_ids']), 'data/processed/passages.jsonl',\n                                  'data/processed/bonds.jsonl', tokenizer)\n    \n    if len(train_dataset) == 0:\n        print(\"ERROR: No data!\")\n        continue\n    \n    # Adjust for GPU memory\n    if 'T4' in gpu_name if torch.cuda.is_available() else False:\n        batch_size = min(192, max(32, len(train_dataset) // 20))  # Conservative for T4\n        GRAD_ACCUM_STEPS = 2  # Simulate larger batch with accumulation\n    else:\n        batch_size = min(BASE_BATCH_SIZE, max(32, len(train_dataset) // 20))\n        GRAD_ACCUM_STEPS = 1\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                              collate_fn=collate_fn, drop_last=True, num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False,\n                             collate_fn=collate_fn, num_workers=4, pin_memory=True)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    \n    n_epochs = 5\n    \n    # ADVERSARIAL WARMUP SCHEDULE (Reviewer concern #2)\n    # Start with low adversarial weight, ramp up\n    def get_adv_lambda(epoch, n_epochs, warmup_epochs=2):\n        \"\"\"Warmup adversarial loss to avoid early instability.\"\"\"\n        if epoch <= warmup_epochs:\n            return 0.1 + 0.9 * (epoch / warmup_epochs)  # 0.1 -> 1.0\n        return 1.0\n    \n    # Loss weights (tuned to avoid over-penalizing representation)\n    BOND_WEIGHT = 1.0\n    LANG_WEIGHT = 0.5  # Adversarial\n    PERIOD_WEIGHT = 0.5  # Adversarial\n    best_loss = float('inf')\n    \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        total_loss = 0\n        n_batches = 0\n        \n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", unit=\"batch\", leave=False):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            bond_labels = batch['bond_labels'].to(device)\n            language_labels = batch['language_labels'].to(device)\n            period_labels = batch['period_labels'].to(device)\n            \n            # Get current adversarial lambda (warmup schedule)\n            current_adv_lambda = get_adv_lambda(epoch, n_epochs)\n            \n            with torch.cuda.amp.autocast(enabled=USE_AMP):\n                out = model(input_ids, attention_mask, adv_lambda=current_adv_lambda)\n                \n                loss_bond = F.cross_entropy(out['bond_pred'], bond_labels)\n                loss_lang = F.cross_entropy(out['language_pred'], language_labels)\n                loss_period = F.cross_entropy(out['period_pred'], period_labels)\n            \n            # Weighted loss (Reviewer concern #2: unweighted adversarial)\n            loss = BOND_WEIGHT * loss_bond + LANG_WEIGHT * loss_lang + PERIOD_WEIGHT * loss_period\n            \n            # Gradient accumulation for smaller GPUs\n            loss = loss / GRAD_ACCUM_STEPS\n            \n            if USE_AMP and scaler:\n                scaler.scale(loss).backward()\n                if (n_batches + 1) % GRAD_ACCUM_STEPS == 0:\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n            else:\n                loss.backward()\n                if (n_batches + 1) % GRAD_ACCUM_STEPS == 0:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                    optimizer.step()\n                    optimizer.zero_grad()\n            \n            total_loss += loss.item()\n            n_batches += 1\n        \n        avg_loss = total_loss / n_batches\n        print(f\"Epoch {epoch}: Loss={avg_loss:.4f} (adv_\u03bb={current_adv_lambda:.2f})\")\n        \n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            torch.save(model.state_dict(), f'models/checkpoints/best_{split_name}.pt')\n            # Also save to Drive for v10 analysis\n            torch.save(model.state_dict(), f'{SAVE_DIR}/best_{split_name}.pt')\n    \n    # Evaluate\n    print(\"\\nEvaluating...\")\n    model.load_state_dict(torch.load(f'models/checkpoints/best_{split_name}.pt'))\n    model.eval()\n    \n    all_preds = {'bond': [], 'lang': []}\n    all_labels = {'bond': [], 'lang': []}\n    all_languages = []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n            out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n            all_preds['bond'].extend(out['bond_pred'].argmax(-1).cpu().tolist())\n            all_preds['lang'].extend(out['language_pred'].argmax(-1).cpu().tolist())\n            all_labels['bond'].extend(batch['bond_labels'].tolist())\n            all_labels['lang'].extend(batch['language_labels'].tolist())\n            all_languages.extend(batch['languages'])\n    \n    bond_f1 = f1_score(all_labels['bond'], all_preds['bond'], average='macro', zero_division=0)\n    bond_acc = sum(p == l for p, l in zip(all_preds['bond'], all_labels['bond'])) / len(all_preds['bond'])\n    lang_acc = sum(p == l for p, l in zip(all_preds['lang'], all_labels['lang'])) / len(all_preds['lang'])\n    \n    # Per-language F1\n    lang_f1 = {}\n    for lang in set(all_languages):\n        mask = [l == lang for l in all_languages]\n        if sum(mask) > 0:\n            preds = [p for p, m in zip(all_preds['bond'], mask) if m]\n            labels = [l for l, m in zip(all_labels['bond'], mask) if m]\n            lang_f1[lang] = {'f1': f1_score(labels, preds, average='macro', zero_division=0), 'n': sum(mask)}\n    \n    all_results[split_name] = {\n        'bond_f1_macro': bond_f1,\n        'bond_acc': bond_acc,\n        'language_acc_adversary': lang_acc,\n        'per_language_f1': lang_f1,\n        'training_time': time.time() - split_start\n    }\n    \n    print(f\"\\n{split_name} RESULTS:\")\n    print(f\"  Bond F1 (macro): {bond_f1:.3f}\")\n    print(f\"  Bond accuracy:   {bond_acc:.1%}\")\n    print(f\"  Language acc (adversary): {lang_acc:.1%}\")\n    print(\"  Per-language Bond F1:\")\n    for lang, m in sorted(lang_f1.items(), key=lambda x: -x[1]['n']):\n        print(f\"    {lang:20s}: F1={m['f1']:.3f} (n={m['n']:,})\")\n    \n    del model, train_dataset, test_dataset\n    gc.collect()\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n\nprint()\nprint(\"=\"*60)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*60)\n\nmark_task(\"Train BIP model\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 8. Critical Baselines & Ablations { display-mode: \"form\" }\n#@markdown **Reviewer-requested controls to validate universality claims.**\n#@markdown \n#@markdown 1. **Rule Baseline**: Pattern matcher only (no neural network)\n#@markdown 2. **Shuffle Control**: Shuffle labels within each language\n#@markdown 3. **Keyword Masking**: Remove pattern-matching tokens from input\n\nimport random\nimport re\nimport copy\nfrom sklearn.metrics import f1_score\nfrom collections import defaultdict\n\nprint(\"=\"*60)\nprint(\"CRITICAL BASELINES & ABLATIONS\")\nprint(\"=\"*60)\nprint()\nprint(\"These controls address reviewer concern #4:\")\nprint(\"'You need 2-3 killer baselines to make universality harder to dismiss'\")\nprint()\n\n# Load splits and data\nwith open('data/splits/all_splits.json', 'r') as f:\n    all_splits = json.load(f)\n\nwith open('data/splits/baselines.json', 'r') as f:\n    baselines = json.load(f)\n\nbaseline_results = {}\n\n# ============================================================\n# BASELINE 1: Rule-based prediction (pattern matcher only)\n# ============================================================\nprint(\"=\"*60)\nprint(\"BASELINE 1: RULE-BASED (Pattern Matcher Only)\")\nprint(\"=\"*60)\nprint()\nprint(\"If the model beats this cross-lingually, neural learning adds value.\")\nprint()\n\ndef rule_based_predict(text: str, language: str) -> str:\n    \"\"\"Predict bond using only pattern matching (no neural network).\"\"\"\n    text_norm = normalize_text(text, language)\n    bond_patterns = ALL_BOND_PATTERNS.get(language, {})\n    \n    for bond_type, patterns in bond_patterns.items():\n        for pattern in patterns:\n            if re.search(pattern, text_norm):\n                return bond_type.name\n    return 'NONE'\n\n# Evaluate rule baseline on each split\nprint(\"Evaluating rule baseline on test sets...\")\n\n# Load passage data\npassages_by_id = {}\nwith open('data/processed/passages.jsonl', 'r') as f:\n    for line in f:\n        p = json.loads(line)\n        passages_by_id[p['id']] = p\n\nbonds_by_id = {}\nwith open('data/processed/bonds.jsonl', 'r') as f:\n    for line in f:\n        b = json.loads(line)\n        bonds_by_id[b['passage_id']] = b['bonds']\n\nfor split_name in ['hebrew_to_others', 'semitic_to_non_semitic', 'ancient_to_modern']:\n    if split_name not in all_splits:\n        continue\n    \n    split = all_splits[split_name]\n    test_ids = split['test_ids'][:10000]  # Limit for speed\n    \n    y_true = []\n    y_pred_rule = []\n    \n    for pid in test_ids:\n        if pid not in passages_by_id or pid not in bonds_by_id:\n            continue\n        \n        p = passages_by_id[pid]\n        true_bond = bonds_by_id[pid]['primary_bond']\n        pred_bond = rule_based_predict(p['text'], p['language'])\n        \n        y_true.append(true_bond)\n        y_pred_rule.append(pred_bond)\n    \n    if y_true:\n        rule_f1 = f1_score(y_true, y_pred_rule, average='macro', zero_division=0)\n        baseline_results[f'{split_name}_rule_baseline'] = {\n            'f1_macro': rule_f1,\n            'n_samples': len(y_true),\n            'description': 'Pattern matcher only, no neural network'\n        }\n        print(f\"  {split_name}: Rule F1 = {rule_f1:.3f}\")\n\nprint()\n\n# ============================================================\n# BASELINE 2: Shuffle Control (scrambled labels)\n# ============================================================\nprint(\"=\"*60)\nprint(\"BASELINE 2: SHUFFLE CONTROL\")\nprint(\"=\"*60)\nprint()\nprint(\"Shuffle bond labels WITHIN each language (preserve class balance).\")\nprint(\"If transfer collapses, the model learned real cross-lingual structure.\")\nprint()\n\n#@markdown **Run shuffle control?** (adds ~15 min)\nRUN_SHUFFLE_CONTROL = True  #@param {type:\"boolean\"}\n\nif RUN_SHUFFLE_CONTROL:\n    # Create shuffled bond labels\n    print(\"Creating shuffled labels...\")\n    \n    shuffled_bonds = {}\n    by_language = defaultdict(list)\n    \n    for pid, bonds in bonds_by_id.items():\n        if pid in passages_by_id:\n            lang = passages_by_id[pid]['language']\n            by_language[lang].append((pid, bonds['primary_bond']))\n    \n    # Shuffle within each language\n    random.seed(42)\n    for lang, items in by_language.items():\n        pids = [x[0] for x in items]\n        labels = [x[1] for x in items]\n        random.shuffle(labels)  # Shuffle labels only\n        for pid, label in zip(pids, labels):\n            shuffled_bonds[pid] = label\n    \n    print(f\"Shuffled {len(shuffled_bonds):,} labels across {len(by_language)} languages\")\n    \n    # Save shuffled bonds for training\n    with open('data/processed/bonds_shuffled.jsonl', 'w') as f:\n        for pid, bond in shuffled_bonds.items():\n            f.write(json.dumps({\n                'passage_id': pid,\n                'bonds': {\n                    'primary_bond': bond,\n                    'all_bonds': [bond],\n                    'hohfeld': None,\n                    'language': passages_by_id[pid]['language']\n                }\n            }) + '\\n')\n    \n    # Train a model on shuffled labels (abbreviated training)\n    print(\"\\nTraining model on SHUFFLED labels (hebrew_to_others split)...\")\n    \n    # Use existing NativeDataset but with shuffled bonds file\n    class ShuffledDataset(torch.utils.data.Dataset):\n        def __init__(self, ids, passages_file, bonds_file, tokenizer, max_len=128):\n            self.tokenizer = tokenizer\n            self.max_len = max_len\n            self.data = []\n            \n            ids_set = set(ids)\n            \n            with open(passages_file, 'r') as fp, open(bonds_file, 'r') as fb:\n                for p_line, b_line in zip(fp, fb):\n                    p = json.loads(p_line)\n                    b = json.loads(b_line)\n                    if p['id'] in ids_set and b['passage_id'] == p['id']:\n                        self.data.append({\n                            'id': p['id'],\n                            'text': p['text'][:500],\n                            'language': p['language'],\n                            'period': p['time_period'],\n                            'bond': b['bonds']['primary_bond'],\n                        })\n        \n        def __len__(self):\n            return len(self.data)\n        \n        def __getitem__(self, idx):\n            item = self.data[idx]\n            enc = self.tokenizer(item['text'], truncation=True, max_length=self.max_len,\n                                padding='max_length', return_tensors='pt')\n            return {\n                'input_ids': enc['input_ids'].squeeze(0),\n                'attention_mask': enc['attention_mask'].squeeze(0),\n                'bond': item['bond'],\n                'language': item['language'],\n            }\n    \n    # Quick training (2 epochs)\n    shuffle_model = BIPModel().to(device)\n    split = all_splits['hebrew_to_others']\n    \n    shuffle_train = ShuffledDataset(\n        split['train_ids'], \n        'data/processed/passages.jsonl',\n        'data/processed/bonds_shuffled.jsonl',\n        tokenizer\n    )\n    shuffle_test = ShuffledDataset(\n        split['test_ids'][:5000],\n        'data/processed/passages.jsonl', \n        'data/processed/bonds.jsonl',  # Test on REAL labels\n        tokenizer\n    )\n    \n    if len(shuffle_train) > 0:\n        def simple_collate(batch):\n            bond_to_idx = {b.name: i for i, b in enumerate(BondType)}\n            return {\n                'input_ids': torch.stack([x['input_ids'] for x in batch]),\n                'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n                'bond_labels': torch.tensor([bond_to_idx.get(x['bond'], 0) for x in batch]),\n            }\n        \n        train_loader = DataLoader(shuffle_train, batch_size=256, shuffle=True, collate_fn=simple_collate)\n        test_loader = DataLoader(shuffle_test, batch_size=256, collate_fn=simple_collate)\n        \n        optimizer = torch.optim.AdamW(shuffle_model.parameters(), lr=2e-5)\n        \n        # Quick training\n        shuffle_model.train()\n        for epoch in range(2):\n            for batch in tqdm(train_loader, desc=f\"Shuffle Epoch {epoch+1}\", leave=False):\n                out = shuffle_model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n                loss = F.cross_entropy(out['bond_pred'], batch['bond_labels'].to(device))\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        \n        # Evaluate\n        shuffle_model.eval()\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch in test_loader:\n                out = shuffle_model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n                all_preds.extend(out['bond_pred'].argmax(-1).cpu().tolist())\n                all_labels.extend(batch['bond_labels'].tolist())\n        \n        shuffle_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n        baseline_results['shuffle_control'] = {\n            'f1_macro': shuffle_f1,\n            'description': 'Trained on shuffled labels, tested on real labels',\n            'expected': 'Should be near chance if model learns real structure'\n        }\n        print(f\"\\n  Shuffle control F1: {shuffle_f1:.3f}\")\n        print(f\"  (Chance: {baselines['chance_bond']:.3f})\")\n        \n        if shuffle_f1 < baselines['chance_bond'] * 1.5:\n            print(\"  \u2713 Transfer collapsed as expected!\")\n        else:\n            print(\"  \u2717 WARNING: Model may be learning spurious patterns\")\n        \n        del shuffle_model\n        gc.collect()\n        torch.cuda.empty_cache() if torch.cuda.is_available() else None\nelse:\n    print(\"Shuffle control skipped (enable RUN_SHUFFLE_CONTROL)\")\n\nprint()\n\n# ============================================================\n# BASELINE 3: Keyword Masking Ablation\n# ============================================================\nprint(\"=\"*60)\nprint(\"BASELINE 3: KEYWORD MASKING ABLATION\")\nprint(\"=\"*60)\nprint()\nprint(\"Mask/remove tokens that match bond patterns from input text.\")\nprint(\"If transfer holds, model isn't just doing keyword spotting.\")\nprint()\n\n#@markdown **Run keyword masking ablation?** (adds ~20 min)\nRUN_KEYWORD_MASKING = True  #@param {type:\"boolean\"}\n\nif RUN_KEYWORD_MASKING:\n    def mask_keywords(text: str, language: str, mask_token: str = \"[MASK]\") -> str:\n        \"\"\"Replace pattern-matching keywords with mask token.\"\"\"\n        text_masked = text\n        bond_patterns = ALL_BOND_PATTERNS.get(language, {})\n        hohfeld_patterns = ALL_HOHFELD_PATTERNS.get(language, {})\n        \n        all_patterns = []\n        for patterns in bond_patterns.values():\n            all_patterns.extend(patterns)\n        for patterns in hohfeld_patterns.values():\n            all_patterns.extend(patterns)\n        \n        for pattern in all_patterns:\n            text_masked = re.sub(pattern, mask_token, text_masked, flags=re.IGNORECASE)\n        \n        return text_masked\n    \n    # Test masking\n    test_text = \"You must honor your father and mother\"\n    print(f\"Example masking:\")\n    print(f\"  Original: '{test_text}'\")\n    print(f\"  Masked:   '{mask_keywords(test_text, 'english')}'\")\n    print()\n    \n    # Create masked dataset\n    class MaskedDataset(torch.utils.data.Dataset):\n        def __init__(self, ids, passages_by_id, bonds_by_id, tokenizer, max_len=128):\n            self.tokenizer = tokenizer\n            self.max_len = max_len\n            self.data = []\n            \n            for pid in ids:\n                if pid in passages_by_id and pid in bonds_by_id:\n                    p = passages_by_id[pid]\n                    masked_text = mask_keywords(p['text'][:500], p['language'])\n                    self.data.append({\n                        'text': masked_text,\n                        'bond': bonds_by_id[pid]['primary_bond'],\n                        'language': p['language'],\n                    })\n        \n        def __len__(self):\n            return len(self.data)\n        \n        def __getitem__(self, idx):\n            item = self.data[idx]\n            enc = self.tokenizer(item['text'], truncation=True, max_length=self.max_len,\n                                padding='max_length', return_tensors='pt')\n            return {\n                'input_ids': enc['input_ids'].squeeze(0),\n                'attention_mask': enc['attention_mask'].squeeze(0),\n                'bond': item['bond'],\n            }\n    \n    print(\"Training model on MASKED text (hebrew_to_others split)...\")\n    \n    masked_model = BIPModel().to(device)\n    split = all_splits['hebrew_to_others']\n    \n    masked_train = MaskedDataset(split['train_ids'], passages_by_id, bonds_by_id, tokenizer)\n    masked_test = MaskedDataset(split['test_ids'][:5000], passages_by_id, bonds_by_id, tokenizer)\n    \n    if len(masked_train) > 0:\n        train_loader = DataLoader(masked_train, batch_size=256, shuffle=True, collate_fn=simple_collate)\n        test_loader = DataLoader(masked_test, batch_size=256, collate_fn=simple_collate)\n        \n        optimizer = torch.optim.AdamW(masked_model.parameters(), lr=2e-5)\n        \n        masked_model.train()\n        for epoch in range(3):\n            for batch in tqdm(train_loader, desc=f\"Masked Epoch {epoch+1}\", leave=False):\n                out = masked_model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n                loss = F.cross_entropy(out['bond_pred'], batch['bond_labels'].to(device))\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n        \n        # Evaluate on masked test set\n        masked_model.eval()\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch in test_loader:\n                out = masked_model(batch['input_ids'].to(device), batch['attention_mask'].to(device), 0)\n                all_preds.extend(out['bond_pred'].argmax(-1).cpu().tolist())\n                all_labels.extend(batch['bond_labels'].tolist())\n        \n        masked_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n        \n        # Compare to main model\n        main_model_f1 = all_results.get('hebrew_to_others', {}).get('bond_f1_macro', 0)\n        \n        baseline_results['keyword_masking'] = {\n            'f1_macro': masked_f1,\n            'main_model_f1': main_model_f1,\n            'retention': masked_f1 / main_model_f1 if main_model_f1 > 0 else 0,\n            'description': 'Keywords masked from input, tests if model learns beyond patterns'\n        }\n        \n        print(f\"\\n  Keyword-masked F1: {masked_f1:.3f}\")\n        print(f\"  Main model F1:     {main_model_f1:.3f}\")\n        print(f\"  Retention:         {masked_f1/main_model_f1*100:.1f}%\" if main_model_f1 > 0 else \"N/A\")\n        \n        if masked_f1 > baselines['chance_bond'] * 1.3:\n            print(\"  \u2713 Transfer holds even without keywords!\")\n            print(\"  \u2192 Model learns semantic structure, not just keyword spotting\")\n        else:\n            print(\"  \u2717 Performance dropped significantly\")\n            print(\"  \u2192 Model may rely heavily on keyword patterns\")\n        \n        del masked_model\n        gc.collect()\n        torch.cuda.empty_cache() if torch.cuda.is_available() else None\nelse:\n    print(\"Keyword masking ablation skipped (enable RUN_KEYWORD_MASKING)\")\n\n# Save baseline results\nwith open('results/baseline_ablation_results.json', 'w') as f:\n    json.dump(baseline_results, f, indent=2)\n\nprint()\nprint(\"=\"*60)\nprint(\"BASELINE SUMMARY\")\nprint(\"=\"*60)\nfor name, res in baseline_results.items():\n    print(f\"  {name}: F1={res['f1_macro']:.3f}\")\n\nprint()\nprint(\"Results saved to results/baseline_ablation_results.json\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 9. Linear Probe Test { display-mode: \"form\" }\n#@markdown Can language/period be decoded from frozen z_bond?\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nmark_task(\"Linear probe test\", \"running\")\n\nprint(\"=\"*60)\nprint(\"LINEAR PROBE TEST\")\nprint(\"=\"*60)\nprint()\nprint(\"Testing if language/period can be decoded from z_bond.\")\nprint(\"If probe accuracy \u2248 chance, confounds are removed!\")\nprint()\n\nlinear_probe_results = {}\n\nfor split_name in [s for s in all_results.keys() if s != 'mixed_baseline']:\n    print(f\"\\n{'='*50}\")\n    print(f\"PROBE: {split_name}\")\n    print(f\"{'='*50}\")\n    \n    model = BIPModel().to(device)\n    model.load_state_dict(torch.load(f'models/checkpoints/best_{split_name}.pt'))\n    model.eval()\n    \n    with open('data/splits/all_splits.json', 'r') as f:\n        split = json.load(f)[split_name]\n    \n    test_dataset = NativeDataset(set(split['test_ids']), 'data/processed/passages.jsonl',\n                                  'data/processed/bonds.jsonl', tokenizer)\n    test_loader = DataLoader(test_dataset, batch_size=BASE_BATCH_SIZE, collate_fn=collate_fn, num_workers=4)\n    \n    all_z = []\n    all_lang = []\n    all_period = []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Extract\", unit=\"batch\"):\n            z = model.extract_z_bond(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n            all_z.append(z.cpu().numpy())\n            all_lang.extend(batch['language_labels'].tolist())\n            all_period.extend(batch['period_labels'].tolist())\n    \n    X = np.vstack(all_z)\n    y_lang = np.array(all_lang)\n    y_period = np.array(all_period)\n    \n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    np.random.seed(42)\n    idx = np.random.permutation(len(X_scaled))\n    train_idx, test_idx = idx[:len(idx)//2], idx[len(idx)//2:]\n    \n    # Language probe\n    lang_probe = LogisticRegression(max_iter=1000, multi_class='multinomial', n_jobs=-1)\n    lang_probe.fit(X_scaled[train_idx], y_lang[train_idx])\n    lang_acc = (lang_probe.predict(X_scaled[test_idx]) == y_lang[test_idx]).mean()\n    lang_chance = 1.0 / len(np.unique(y_lang[test_idx]))\n    # CORRECT: Low accuracy = CAN'T decode = INVARIANT (good!)\n    # v4 bug: 0% accuracy was marked as \"not invariant\" - that's backwards!\n    lang_invariant = lang_acc < (lang_chance + 0.15)  # Within 15% of chance = invariant\n    \n    # Period probe\n    period_probe = LogisticRegression(max_iter=1000, multi_class='multinomial', n_jobs=-1)\n    period_probe.fit(X_scaled[train_idx], y_period[train_idx])\n    period_acc = (period_probe.predict(X_scaled[test_idx]) == y_period[test_idx]).mean()\n    period_chance = 1.0 / len(np.unique(y_period[test_idx]))\n    period_invariant = period_acc < (period_chance + 0.15)  # Within 15% of chance = invariant\n    \n    print(f\"\\nRESULTS:\")\n    print(f\"  Language: {lang_acc:.1%} (chance: {lang_chance:.1%}) {'\u2713' if lang_invariant else '\u2717'}\")\n    print(f\"  Period:   {period_acc:.1%} (chance: {period_chance:.1%}) {'\u2713' if period_invariant else '\u2717'}\")\n    \n    linear_probe_results[split_name] = {\n        'lang_probe_acc': float(lang_acc),\n        'lang_chance': float(lang_chance),\n        'lang_invariant': bool(lang_invariant),\n        'period_probe_acc': float(period_acc),\n        'period_chance': float(period_chance),\n        'period_invariant': bool(period_invariant),\n    }\n    \n    del model\n    gc.collect()\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n\nwith open('results/linear_probe_results.json', 'w') as f:\n    json.dump(linear_probe_results, f, indent=2)\n\nmark_task(\"Linear probe test\", \"done\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 10. Final Evaluation { display-mode: \"form\" }\n#@markdown Comprehensive results with proper interpretation of asymmetric transfers.\n#@markdown \n#@markdown **Key insight from v4**: Ancient\u2192Modern showed TIME-INVARIANT transfer\n#@markdown (0% time accuracy = can't decode time = GOOD for invariance!)\n\nimport time\nimport json\n\nmark_task(\"Evaluate results\", \"running\")\n\nprint(\"=\"*60)\nprint(\"FINAL BIP EVALUATION (v9 - Native Patterns)\")\nprint(\"=\"*60)\nprint()\nprint(\"METHODOLOGY: Labels extracted from NATIVE text using NATIVE patterns.\")\nprint(\"NO English translation used. Only mathematical alignment connects languages.\")\nprint()\n\n# Reference v4 findings\nprint(\"-\"*60)\nprint(\"CONTEXT: v4 Findings (Ancient\u2192Modern)\")\nprint(\"-\"*60)\nprint(\"\"\"\n  v4 showed ASYMMETRIC results:\n  \n  Ancient\u2192Modern (3.9M \u2192 19K):\n    - Time accuracy: 0.0%  \u2190 CAN'T decode time = INVARIANT \u2713\n    - Hohfeld accuracy: 48.6% (1.94x chance) = TRANSFER \u2713\n    \n  Modern\u2192Ancient (19K \u2192 3.9M):  \n    - Failed (likely data quantity issue)\n    \n  Key insight: Low probe accuracy is GOOD - it means the\n  representation doesn't encode the confound!\n\"\"\")\nprint(\"-\"*60)\nprint()\n\nwith open('data/splits/baselines.json', 'r') as f:\n    baselines = json.load(f)\n\nchance_bond = baselines['chance_bond']\nchance_language = baselines['chance_language']\nchance_period = baselines['chance_period']\n\n# Load baseline ablation results\ntry:\n    with open('results/baseline_ablation_results.json', 'r') as f:\n        baseline_ablation = json.load(f)\nexcept:\n    baseline_ablation = {}\n\nprint(\"=\"*60)\nprint(\"CROSS-DOMAIN TRANSFER RESULTS\")\nprint(\"=\"*60)\n\n# Track which splits show the pattern we want\nsuccessful_transfers = []\nasymmetric_notes = []\n\nfor split_name, res in all_results.items():\n    if split_name == 'mixed_baseline':\n        continue\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"{split_name.upper()}\")\n    print(\"=\"*60)\n    \n    probe = linear_probe_results.get(split_name, {})\n    \n    # Get metrics\n    bond_f1 = res.get('bond_f1_macro', 0)\n    lang_probe_acc = probe.get('lang_probe_acc', 0)\n    lang_chance = probe.get('lang_chance', 0.2)\n    period_probe_acc = probe.get('period_probe_acc', 0)\n    period_chance = probe.get('period_chance', 0.1)\n    \n    # CORRECT INTERPRETATION:\n    # - Low probe accuracy (near or below chance) = GOOD (invariant)\n    # - High bond F1 (above chance) = GOOD (transfer works)\n    \n    # Check invariance (probe should be NEAR chance, not above)\n    lang_invariant = lang_probe_acc < (lang_chance + 0.15)  # Within 15% of chance\n    period_invariant = period_probe_acc < (period_chance + 0.15)\n    \n    # Check transfer (bond prediction should beat chance)\n    transfer_ratio = bond_f1 / chance_bond if chance_bond > 0 else 0\n    transfer_works = transfer_ratio > 1.5\n    \n    print(f\"\\n  INVARIANCE (low probe acc = GOOD):\")\n    print(f\"    Language probe: {lang_probe_acc:.1%} (chance: {lang_chance:.1%})\")\n    if lang_probe_acc < lang_chance:\n        print(f\"      \u2192 BELOW chance = STRONGLY INVARIANT \u2713\u2713\")\n    elif lang_invariant:\n        print(f\"      \u2192 Near chance = INVARIANT \u2713\")\n    else:\n        print(f\"      \u2192 Above chance = NOT invariant \u2717\")\n    \n    print(f\"    Period probe:   {period_probe_acc:.1%} (chance: {period_chance:.1%})\")\n    if period_probe_acc < period_chance:\n        print(f\"      \u2192 BELOW chance = STRONGLY INVARIANT \u2713\u2713\")\n    elif period_invariant:\n        print(f\"      \u2192 Near chance = INVARIANT \u2713\")\n    else:\n        print(f\"      \u2192 Above chance = NOT invariant \u2717\")\n    \n    print(f\"\\n  TRANSFER (high bond F1 = GOOD):\")\n    print(f\"    Bond F1: {bond_f1:.3f} (chance: {chance_bond:.3f})\")\n    print(f\"    Transfer ratio: {transfer_ratio:.2f}x chance\")\n    if transfer_ratio > 2.0:\n        print(f\"      \u2192 STRONG transfer \u2713\u2713\")\n    elif transfer_works:\n        print(f\"      \u2192 Transfer works \u2713\")\n    else:\n        print(f\"      \u2192 Weak/no transfer \u2717\")\n    \n    # Per-language breakdown (important for asymmetric analysis)\n    if 'per_language_f1' in res:\n        print(f\"\\n  PER-LANGUAGE BREAKDOWN:\")\n        lang_f1s = res['per_language_f1']\n        for lang, m in sorted(lang_f1s.items(), key=lambda x: -x[1].get('n', 0)):\n            f1 = m.get('f1', 0)\n            n = m.get('n', 0)\n            ratio = f1 / chance_bond if chance_bond > 0 else 0\n            status = \"\u2713\" if ratio > 1.5 else \"~\" if ratio > 1.0 else \"\u2717\"\n            print(f\"    {lang:20s}: F1={f1:.3f} ({ratio:.1f}x) {status}  (n={n:,})\")\n    \n    # Data size analysis (asymmetry check)\n    train_size = res.get('train_size', 0) or all_splits.get(split_name, {}).get('train_size', 0)\n    test_size = res.get('test_size', 0) or all_splits.get(split_name, {}).get('test_size', 0)\n    \n    if train_size and test_size:\n        size_ratio = train_size / test_size if test_size > 0 else 0\n        print(f\"\\n  DATA SIZES:\")\n        print(f\"    Train: {train_size:,} | Test: {test_size:,} | Ratio: {size_ratio:.1f}x\")\n        \n        if size_ratio > 10:\n            asymmetric_notes.append(f\"{split_name}: Large\u2192small ({size_ratio:.0f}x)\")\n        elif size_ratio < 0.1:\n            asymmetric_notes.append(f\"{split_name}: Small\u2192large ({1/size_ratio:.0f}x)\")\n    \n    # Overall assessment for this split\n    print(f\"\\n  ASSESSMENT:\")\n    if (lang_invariant or period_invariant) and transfer_works:\n        print(f\"    \u2713 SUCCESS: Invariant representation + working transfer\")\n        successful_transfers.append(split_name)\n    elif transfer_works and not (lang_invariant or period_invariant):\n        print(f\"    ~ PARTIAL: Transfer works but representation encodes confounds\")\n        print(f\"      (Could be data asymmetry - check sizes above)\")\n    elif (lang_invariant or period_invariant) and not transfer_works:\n        print(f\"    ~ PARTIAL: Invariant but weak transfer\")\n        print(f\"      (May need more training data or epochs)\")\n    else:\n        print(f\"    \u2717 FAILED: Neither invariance nor transfer\")\n\n# ============================================================\n# BASELINE COMPARISON\n# ============================================================\nprint()\nprint(\"=\"*60)\nprint(\"BASELINE COMPARISONS\")\nprint(\"=\"*60)\n\nbeats_rule_baseline = False\nshuffle_collapsed = False  \nkeyword_robust = False\n\nif baseline_ablation:\n    # Rule baseline comparison\n    print(\"\\n1. NEURAL vs RULE BASELINE:\")\n    for key in baseline_ablation:\n        if 'rule_baseline' in key:\n            rule_f1 = baseline_ablation[key].get('f1_macro', 0)\n            split = key.replace('_rule_baseline', '')\n            neural_f1 = all_results.get(split, {}).get('bond_f1_macro', 0)\n            improvement = (neural_f1 - rule_f1) / rule_f1 * 100 if rule_f1 > 0 else 0\n            print(f\"    {split}:\")\n            print(f\"      Rule:   {rule_f1:.3f}\")\n            print(f\"      Neural: {neural_f1:.3f}\")\n            print(f\"      Improvement: {improvement:+.1f}%\")\n            if neural_f1 > rule_f1 * 1.1:\n                beats_rule_baseline = True\n                print(f\"      \u2192 Neural adds value \u2713\")\n    \n    # Shuffle control\n    if 'shuffle_control' in baseline_ablation:\n        print(\"\\n2. SHUFFLE CONTROL:\")\n        shuffle_f1 = baseline_ablation['shuffle_control'].get('f1_macro', 0)\n        print(f\"    Shuffled labels F1: {shuffle_f1:.3f}\")\n        print(f\"    Chance: {chance_bond:.3f}\")\n        if shuffle_f1 < chance_bond * 1.5:\n            shuffle_collapsed = True\n            print(f\"    \u2192 Transfer collapsed as expected \u2713\")\n        else:\n            print(f\"    \u2192 WARNING: May learn spurious patterns \u2717\")\n    \n    # Keyword masking\n    if 'keyword_masking' in baseline_ablation:\n        print(\"\\n3. KEYWORD MASKING:\")\n        masked_f1 = baseline_ablation['keyword_masking'].get('f1_macro', 0)\n        main_f1 = baseline_ablation['keyword_masking'].get('main_model_f1', 0)\n        retention = masked_f1 / main_f1 * 100 if main_f1 > 0 else 0\n        print(f\"    With keywords:    {main_f1:.3f}\")\n        print(f\"    Without keywords: {masked_f1:.3f}\")\n        print(f\"    Retention: {retention:.1f}%\")\n        if masked_f1 > chance_bond * 1.3:\n            keyword_robust = True\n            print(f\"    \u2192 Model learns beyond keywords \u2713\")\n        else:\n            print(f\"    \u2192 Relies heavily on keywords \u2717\")\nelse:\n    print(\"\\n  (Baseline ablations not run)\")\n\n# ============================================================\n# ASYMMETRIC ANALYSIS\n# ============================================================\nif asymmetric_notes:\n    print()\n    print(\"=\"*60)\n    print(\"ASYMMETRIC TRANSFER ANALYSIS\")\n    print(\"=\"*60)\n    print()\n    print(\"Like v4, some splits have large data asymmetry:\")\n    for note in asymmetric_notes:\n        print(f\"  \u2022 {note}\")\n    print()\n    print(\"Large\u2192small transfers typically work better (more training data).\")\n    print(\"This is expected and doesn't invalidate the universality claim.\")\n\n# ============================================================\n# FINAL VERDICT\n# ============================================================\nprint()\nprint(\"=\"*60)\nprint(\"FINAL VERDICT\")\nprint(\"=\"*60)\n\n# Count successes\nn_successful = len(successful_transfers)\nn_total_splits = len([s for s in all_results if s != 'mixed_baseline'])\nbaseline_checks = sum([beats_rule_baseline, shuffle_collapsed, keyword_robust])\n\nprint(f\"\"\"\nEVIDENCE SUMMARY:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  Successful transfers (invariant + transfer): {n_successful}/{n_total_splits}\n  Splits: {', '.join(successful_transfers) if successful_transfers else 'None'}\n  \n  Baseline checks passed: {baseline_checks}/3\n    \u2022 Neural > Rule:        {'\u2713' if beats_rule_baseline else '\u2717'}\n    \u2022 Shuffle collapsed:    {'\u2713' if shuffle_collapsed else '\u2717'}\n    \u2022 Keyword-robust:       {'\u2713' if keyword_robust else '\u2717'}\n\"\"\")\n\n# Determine verdict\nif n_successful >= 2 and baseline_checks >= 2:\n    verdict = \"STRONGLY_SUPPORTED\"\n    box = \"\"\"\n    \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n    \u2551     BIP: STRONGLY SUPPORTED                              \u2551\n    \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n    \u2551  \u2022 Multiple splits show invariant cross-lingual transfer \u2551\n    \u2551  \u2022 Native patterns (no English bridge)                   \u2551\n    \u2551  \u2022 Baselines confirm neural model learns real structure  \u2551\n    \u2551                                                          \u2551\n    \u2551  Evidence supports universal mathematical structure      \u2551\n    \u2551  in moral cognition across languages and time.           \u2551\n    \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \"\"\"\nelif n_successful >= 1 or (n_total_splits > 0 and baseline_checks >= 2):\n    verdict = \"SUPPORTED\"\n    box = \"\"\"\n    \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n    \u2551     BIP: SUPPORTED                                       \u2551\n    \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n    \u2551  \u2022 Some cross-lingual transfer demonstrated              \u2551\n    \u2551  \u2022 May have asymmetric effects (data size dependent)     \u2551\n    \u2551  \u2022 Similar to v4: large\u2192small direction works better     \u2551\n    \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \"\"\"\nelif any(r.get('bond_f1_macro', 0) > chance_bond * 1.3 for r in all_results.values()):\n    verdict = \"PARTIAL\"\n    box = \"\"\"\n    \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n    \u2551     BIP: PARTIAL SUPPORT                                 \u2551\n    \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n    \u2551  \u2022 Some transfer above chance, but:                      \u2551\n    \u2551  \u2022 Representation may encode confounds                   \u2551\n    \u2551  \u2022 Or baseline checks failed                             \u2551\n    \u2551                                                          \u2551\n    \u2551  Needs investigation of specific failure modes.          \u2551\n    \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \"\"\"\nelse:\n    verdict = \"INCONCLUSIVE\"\n    box = \"\"\"\n    \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n    \u2551     BIP: INCONCLUSIVE                                    \u2551\n    \u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n    \u2551  \u2022 Transfer not demonstrated                             \u2551\n    \u2551                                                          \u2551\n    \u2551  Possible issues:                                        \u2551\n    \u2551  \u2022 Label quality (check coverage metrics)                \u2551\n    \u2551  \u2022 Data quantity (check asymmetry)                       \u2551\n    \u2551  \u2022 Training (try more epochs)                            \u2551\n    \u2551  \u2022 Or: BIP may not hold (null result)                    \u2551\n    \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n    \"\"\"\n\nprint(box)\n\n# v4 comparison note\nprint()\nprint(\"\u2500\"*60)\nprint(\"COMPARISON TO v4:\")\nprint(\"\u2500\"*60)\nprint(\"\"\"\nv4 (Ancient\u2192Modern) showed:\n  \u2022 0% time accuracy (GOOD - time invariant)\n  \u2022 48.6% Hohfeld accuracy (1.94x chance - transfer works)\n\nThis was actually a SUCCESS that was mislabeled as \"inconclusive\"\nbecause the evaluation logic was inverted.\n\nv9 improvements over v4:\n  \u2022 Native patterns (no translation bridge)\n  \u2022 Text normalization (Hebrew/Arabic diacritics)\n  \u2022 Multiple splits (not just temporal)\n  \u2022 Baseline ablations (rule, shuffle, masking)\n  \u2022 Correct interpretation of probe accuracy\n\"\"\")\n\n# ============================================================\n# SAVE RESULTS\n# ============================================================\ntotal_time = time.time() - EXPERIMENT_START\n\n# Load split info\ntry:\n    with open('data/splits/all_splits.json', 'r') as f:\n        all_splits = json.load(f)\nexcept:\n    all_splits = {}\n\nfinal_results = {\n    'model_results': all_results,\n    'linear_probe_results': linear_probe_results,\n    'baseline_ablation_results': baseline_ablation,\n    'successful_transfers': successful_transfers,\n    'baseline_checks': {\n        'beats_rule_baseline': beats_rule_baseline,\n        'shuffle_collapsed': shuffle_collapsed,\n        'keyword_robust': keyword_robust,\n        'total_passed': baseline_checks,\n    },\n    'verdict': verdict,\n    'total_time_minutes': total_time / 60,\n    'methodology': {\n        'approach': 'Native-language pattern matching',\n        'translation_used': False,\n        'text_normalization': True,\n        'languages': ['hebrew', 'aramaic', 'classical_chinese', 'arabic', 'english'],\n        'label_source': 'Native text with language-specific patterns',\n        'connection': 'Mathematical only (shared latent space)',\n        'adversarial_warmup': True,\n    },\n    'baselines': baselines,\n    'v4_comparison': {\n        'ancient_to_modern_time_acc': 0.0,\n        'ancient_to_modern_hohfeld_acc': 0.486,\n        'note': 'v4 was actually a success - 0% time acc means invariant'\n    }\n}\n\nwith open('results/final_results.json', 'w') as f:\n    json.dump(final_results, f, indent=2, default=str)\n\n# Also save to Drive\ntry:\n    with open(f'{SAVE_DIR}/final_results.json', 'w') as f:\n        json.dump(final_results, f, indent=2, default=str)\nexcept:\n    pass\n\nprint(f\"\\nTotal time: {total_time/60:.1f} minutes\")\nprint(f\"Results saved to results/final_results.json\")\n\nmark_task(\"Evaluate results\", \"done\")\nprint_progress()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 11. Download Results & Models { display-mode: \"form\" }\n#@markdown Downloads trained models and results as a zip file.\n#@markdown **Models can be re-uploaded to v10 for analysis without Google Drive.**\n\nimport shutil\nimport os\nfrom google.colab import files\n\nprint(\"=\"*60)\nprint(\"PACKAGING RESULTS FOR DOWNLOAD\")\nprint(\"=\"*60)\n\n!mkdir -p download_package/models\n!mkdir -p download_package/data\n\n# Copy trained models\nprint(\"\\nCopying trained models...\")\nfor split_name in all_results.keys():\n    src = f'models/checkpoints/best_{split_name}.pt'\n    if os.path.exists(src):\n        !cp \"{src}\" download_package/models/\n        size_mb = os.path.getsize(src) / 1e6\n        print(f\"  {split_name}: {size_mb:.1f} MB\")\n\n# Copy data files\nprint(\"\\nCopying data files...\")\n!cp data/processed/passages.jsonl download_package/data/ 2>/dev/null && echo \"  passages.jsonl\"\n!cp data/processed/bonds.jsonl download_package/data/ 2>/dev/null && echo \"  bonds.jsonl\"\n!cp data/splits/*.json download_package/data/ 2>/dev/null && echo \"  splits/*.json\"\n\n# Copy results\nprint(\"\\nCopying results...\")\n!cp results/*.json download_package/ 2>/dev/null || true\n\n# Create zip\nprint(\"\\nCreating zip archive...\")\nshutil.make_archive('bip_v9_complete', 'zip', 'download_package')\n\n# Show contents\nprint(\"\\n\" + \"=\"*60)\nprint(\"PACKAGE CONTENTS:\")\nprint(\"=\"*60)\n!find download_package -type f -exec ls -lh {} \\;\n\ntotal_size = sum(os.path.getsize(os.path.join(dp, f)) for dp, dn, fn in os.walk('download_package') for f in fn)\nprint(f\"\\nTotal package size: {total_size/1e6:.1f} MB\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"DOWNLOADING...\")\nprint(\"=\"*60)\nprint(\"Save this zip file - it contains everything needed for v10 analysis!\")\n\nfiles.download('bip_v9_complete.zip')\n",
      "execution_count": null,
      "outputs": []
    }
  ]
}