{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# BIP v10: Extracting the Mathematical Structure of Ethics\n\n**If the model works, what IS the structure it learned?**\n\nThis notebook analyzes the trained z_bond latent space to discover:\n\n1. **Dimensionality**: How many dimensions does ethics really need?\n2. **Geometry**: What shape is the moral space? (Simplex? Lattice? Manifold?)\n3. **Algebra**: Are there group operations? (obligation + permission = ?)\n4. **Topology**: Are there clusters? Holes? Connected components?\n5. **Symmetries**: What transformations preserve moral structure?\n\n---\n\n## Candidate Mathematical Structures\n\n| Structure | Description | Fits if... |\n|-----------|-------------|------------|\n| **Lattice** | Partial ordering with meet/join | Moral concepts have clear hierarchy |\n| **Simplex** | Convex hull of vertices | Concepts are \"pure\", blends are interior |\n| **Lie Group** | Continuous symmetry group | Moral transformations are smooth |\n| **Graph** | Nodes + edges | Discrete relationships dominate |\n| **Fiber Bundle** | Base (universal) + fiber (cultural) | Universal core + cultural variation |\n| **Boolean Algebra** | AND/OR/NOT operations | Moral logic is classical |\n| **Deontic Logic** | O(p), P(p), F(p) operators | Obligation/Permission/Forbidden |\n\n---\n\n## Hohfeld's Framework (Already Algebraic!)\n\n```\n        RIGHT \u2190--correlative--\u2192 DUTY\n          \u2191                       \u2191\n       opposite                opposite  \n          \u2193                       \u2193\n      NO-RIGHT \u2190--correlative--\u2192 PRIVILEGE\n```\n\nThis is a **square of opposition** - the same structure as Aristotelian logic!\n\n---"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 1. Setup & Load Trained Model { display-mode: \"form\" }\n#@markdown **Two options:**\n#@markdown 1. **Google Drive**: If you ran v9 with Drive mounted\n#@markdown 2. **Upload**: If you downloaded the zip from v9\n\n#@markdown ---\nUSE_GOOGLE_DRIVE = False  #@param {type:\"boolean\"}\n#@markdown Set to True if you have results in Google Drive, False to upload zip file.\n\nimport os, subprocess, sys\nfor dep in [\"transformers\", \"torch\", \"sentence-transformers\", \"scikit-learn\", \"umap-learn\", \"networkx\", \"scipy\"]:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", dep])\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.stats import spearmanr\nimport networkx as nx\n\ntry:\n    import umap\n    HAS_UMAP = True\nexcept:\n    HAS_UMAP = False\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\n# Create directories\nos.makedirs(\"models\", exist_ok=True)\nos.makedirs(\"data/processed\", exist_ok=True)\nos.makedirs(\"data/splits\", exist_ok=True)\n\nif USE_GOOGLE_DRIVE:\n    # Mount Drive\n    from google.colab import drive\n    drive.mount('/content/drive')\n    SAVE_DIR = '/content/drive/MyDrive/BIP_native_v9'\n    print(f\"Looking for trained models in: {SAVE_DIR}\")\n    \n    if os.path.exists(SAVE_DIR):\n        print(\"\\nAvailable files:\")\n        for f in os.listdir(SAVE_DIR):\n            print(f\"  {f}\")\n    else:\n        print(\"WARNING: No trained models found in Drive. Run v9 first!\")\n        \nelse:\n    # Upload zip file\n    print(\"=\"*60)\n    print(\"UPLOAD MODE\")\n    print(\"=\"*60)\n    print()\n    print(\"Please upload the 'bip_v9_complete.zip' file from v9.\")\n    print()\n    \n    from google.colab import files\n    uploaded = files.upload()\n    \n    if uploaded:\n        zip_name = list(uploaded.keys())[0]\n        print(f\"\\nReceived: {zip_name}\")\n        \n        # Extract\n        import zipfile\n        with zipfile.ZipFile(zip_name, 'r') as z:\n            z.extractall('uploaded_data')\n        \n        print(\"Extracted contents:\")\n        !find uploaded_data -type f\n        \n        # Set paths\n        SAVE_DIR = 'uploaded_data'\n        \n        # Copy to expected locations\n        !cp uploaded_data/models/*.pt models/ 2>/dev/null || true\n        !cp uploaded_data/data/*.jsonl data/processed/ 2>/dev/null || true\n        !cp uploaded_data/data/*.json data/splits/ 2>/dev/null || true\n        \n        print(\"\\nFiles ready for analysis!\")\n    else:\n        print(\"ERROR: No file uploaded!\")\n        SAVE_DIR = None\n\nprint()\nprint(\"=\"*60)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 2. Define Model Architecture (must match training) { display-mode: \"form\" }\n\nfrom transformers import AutoModel, AutoTokenizer\n\nclass GradientReversal(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, lambda_):\n        ctx.lambda_ = lambda_\n        return x.clone()\n    @staticmethod\n    def backward(ctx, grad_output):\n        return -ctx.lambda_ * grad_output, None\n\ndef gradient_reversal(x, lambda_=1.0):\n    return GradientReversal.apply(x, lambda_)\n\nclass BIPModel(nn.Module):\n    def __init__(self, d_model=384, d_bond=64, n_bonds=10, n_langs=5, n_periods=10, n_hohfeld=4):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n        \n        self.bond_proj = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, d_bond)\n        )\n        \n        self.bond_classifier = nn.Linear(d_bond, n_bonds)\n        self.hohfeld_classifier = nn.Linear(d_bond, n_hohfeld)\n        self.language_classifier = nn.Linear(d_bond, n_langs)\n        self.period_classifier = nn.Linear(d_bond, n_periods)\n    \n    def extract_z_bond(self, input_ids, attention_mask):\n        with torch.no_grad():\n            out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n            h = (out.last_hidden_state * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1, keepdim=True).clamp(min=1e-9)\n            return self.bond_proj(h)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n\nprint(\"Model architecture defined.\")\nprint(f\"z_bond dimension: 64\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 3. Extract z_bond Embeddings from All Data { display-mode: \"form\" }\n#@markdown Extracts the learned moral embeddings for analysis.\n\nfrom torch.utils.data import Dataset, DataLoader\n\n# Load model\nmodel = BIPModel().to(device)\n\n# Try to load best model\n# Try multiple locations (Drive, uploaded, local)\nmodel_paths = [\n    # Uploaded/extracted location\n    \"models/best_hebrew_to_others.pt\",\n    \"models/best_mixed_baseline.pt\",\n    \"models/best_semitic_to_non_semitic.pt\",\n    \"models/best_ancient_to_modern.pt\",\n    # Drive location\n    f\"{SAVE_DIR}/best_hebrew_to_others.pt\" if SAVE_DIR else \"\",\n    f\"{SAVE_DIR}/best_mixed_baseline.pt\" if SAVE_DIR else \"\",\n    f\"{SAVE_DIR}/best_semitic_to_non_semitic.pt\" if SAVE_DIR else \"\",\n    # Uploaded subfolder\n    \"uploaded_data/models/best_hebrew_to_others.pt\",\n    \"uploaded_data/models/best_mixed_baseline.pt\",\n]\nmodel_paths = [p for p in model_paths if p]  # Remove empty strings\n\nloaded = False\nfor path in model_paths:\n    if os.path.exists(path):\n        print(f\"Loading model from {path}\")\n        model.load_state_dict(torch.load(path, map_location='cpu'))\n        model = model.to(device)\n        model.eval()\n        loaded = True\n        break\n\nif not loaded:\n    print(\"WARNING: No trained model found!\")\n    print(\"Using randomly initialized model for demonstration.\")\n\n# Load data\nclass SimpleDataset(Dataset):\n    def __init__(self, passages_file, bonds_file, tokenizer, max_len=128):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.data = []\n        \n        with open(passages_file, 'r') as fp, open(bonds_file, 'r') as fb:\n            for p_line, b_line in zip(fp, fb):\n                p = json.loads(p_line)\n                b = json.loads(b_line)\n                if b['passage_id'] == p['id']:\n                    self.data.append({\n                        'text': p['text'][:500],\n                        'language': p['language'],\n                        'period': p['time_period'],\n                        'bond': b['bonds']['primary_bond'],\n                        'hohfeld': b['bonds'].get('hohfeld'),\n                    })\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        enc = self.tokenizer(item['text'], truncation=True, max_length=self.max_len,\n                            padding='max_length', return_tensors='pt')\n        return {\n            'input_ids': enc['input_ids'].squeeze(0),\n            'attention_mask': enc['attention_mask'].squeeze(0),\n            'language': item['language'],\n            'period': item['period'],\n            'bond': item['bond'],\n            'hohfeld': item['hohfeld'],\n        }\n\ndef collate_fn(batch):\n    return {\n        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n        'languages': [x['language'] for x in batch],\n        'periods': [x['period'] for x in batch],\n        'bonds': [x['bond'] for x in batch],\n        'hohfelds': [x['hohfeld'] for x in batch],\n    }\n\n# Check for data\n# Try multiple data locations\ndata_paths = [\n    # Local (copied from upload)\n    (\"data/processed/passages.jsonl\", \"data/processed/bonds.jsonl\"),\n    # Uploaded subfolder\n    (\"uploaded_data/data/passages.jsonl\", \"uploaded_data/data/bonds.jsonl\"),\n    # Drive location\n    (f\"{SAVE_DIR}/processed/passages.jsonl\", f\"{SAVE_DIR}/processed/bonds.jsonl\") if SAVE_DIR else (\"\", \"\"),\n]\ndata_paths = [(p, b) for p, b in data_paths if p and b]\n\ndataset = None\nfor p_path, b_path in data_paths:\n    if os.path.exists(p_path) and os.path.exists(b_path):\n        print(f\"Loading data from {p_path}\")\n        dataset = SimpleDataset(p_path, b_path, tokenizer)\n        print(f\"Loaded {len(dataset):,} samples\")\n        break\n\nif dataset is None:\n    print(\"WARNING: No data found! Run v9 first.\")\nelse:\n    # Extract embeddings\n    print(\"\\nExtracting z_bond embeddings...\")\n    \n    # Sample if too large\n    max_samples = 50000\n    if len(dataset) > max_samples:\n        indices = np.random.choice(len(dataset), max_samples, replace=False)\n        subset = torch.utils.data.Subset(dataset, indices)\n    else:\n        subset = dataset\n    \n    loader = DataLoader(subset, batch_size=256, shuffle=False, collate_fn=collate_fn, num_workers=2)\n    \n    all_z = []\n    all_languages = []\n    all_periods = []\n    all_bonds = []\n    all_hohfelds = []\n    \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Extracting\", unit=\"batch\"):\n            z = model.extract_z_bond(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n            all_z.append(z.cpu().numpy())\n            all_languages.extend(batch['languages'])\n            all_periods.extend(batch['periods'])\n            all_bonds.extend(batch['bonds'])\n            all_hohfelds.extend(batch['hohfelds'])\n    \n    Z = np.vstack(all_z)\n    print(f\"\\nExtracted {Z.shape[0]:,} embeddings of dimension {Z.shape[1]}\")\n    \n    # Summary\n    print(f\"\\nLanguages: {set(all_languages)}\")\n    print(f\"Bonds: {set(all_bonds)}\")\n    print(f\"Periods: {len(set(all_periods))} unique\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 4. Analyze Dimensionality: How Many Dimensions Does Ethics Need? { display-mode: \"form\" }\n#@markdown Uses PCA to find the intrinsic dimensionality of moral space.\n\nprint(\"=\"*60)\nprint(\"DIMENSIONALITY ANALYSIS\")\nprint(\"=\"*60)\nprint()\nprint(\"Question: How many dimensions does moral cognition really need?\")\nprint()\n\n# PCA analysis\npca_full = PCA()\npca_full.fit(Z)\n\n# Variance explained\nvar_explained = pca_full.explained_variance_ratio_\ncum_var = np.cumsum(var_explained)\n\nprint(\"Variance explained by top components:\")\nfor i in [1, 2, 3, 5, 10, 20, 32, 64]:\n    if i <= len(cum_var):\n        print(f\"  {i:2d} dimensions: {cum_var[i-1]*100:5.1f}%\")\n\n# Find 90%, 95%, 99% thresholds\nfor thresh in [0.90, 0.95, 0.99]:\n    n_dims = np.argmax(cum_var >= thresh) + 1\n    print(f\"\\n{thresh*100:.0f}% variance requires {n_dims} dimensions\")\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].bar(range(1, min(21, len(var_explained)+1)), var_explained[:20])\naxes[0].set_xlabel('Principal Component')\naxes[0].set_ylabel('Variance Explained')\naxes[0].set_title('Scree Plot: Variance per Component')\n\naxes[1].plot(range(1, len(cum_var)+1), cum_var, 'b-', linewidth=2)\naxes[1].axhline(y=0.90, color='r', linestyle='--', label='90%')\naxes[1].axhline(y=0.95, color='g', linestyle='--', label='95%')\naxes[1].set_xlabel('Number of Dimensions')\naxes[1].set_ylabel('Cumulative Variance')\naxes[1].set_title('How Many Dimensions for Ethics?')\naxes[1].legend()\naxes[1].set_xlim([0, 64])\n\nplt.tight_layout()\nplt.savefig('dimensionality_analysis.png', dpi=150)\nplt.show()\n\nprint()\nprint(\"INTERPRETATION:\")\nif cum_var[2] > 0.5:\n    print(\"  \u2192 Ethics is LOW-DIMENSIONAL: First 3 components capture >50%\")\n    print(\"  \u2192 Suggests a simple geometric structure (e.g., simplex, plane)\")\nelse:\n    print(\"  \u2192 Ethics is HIGH-DIMENSIONAL: Need many components\")\n    print(\"  \u2192 Suggests complex manifold structure\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 5. Visualize Moral Space (2D Projections) { display-mode: \"form\" }\n#@markdown Projects the 64D space to 2D for visualization.\n\nprint(\"=\"*60)\nprint(\"VISUALIZING MORAL SPACE\")\nprint(\"=\"*60)\n\n# Reduce to 2D using different methods\npca_2d = PCA(n_components=2)\nZ_pca = pca_2d.fit_transform(Z)\n\nprint(\"Running t-SNE (this takes a minute)...\")\ntsne = TSNE(n_components=2, perplexity=30, random_state=42)\nZ_tsne = tsne.fit_transform(Z[:10000])  # Subsample for speed\n\nif HAS_UMAP:\n    print(\"Running UMAP...\")\n    reducer = umap.UMAP(n_components=2, random_state=42)\n    Z_umap = reducer.fit_transform(Z[:10000])\n\n# Color maps\nbond_colors = {\n    'HARM_PREVENTION': 'red',\n    'RECIPROCITY': 'orange',\n    'AUTONOMY': 'yellow',\n    'PROPERTY': 'green',\n    'FAMILY': 'blue',\n    'AUTHORITY': 'purple',\n    'CARE': 'pink',\n    'FAIRNESS': 'cyan',\n    'CONTRACT': 'brown',\n    'NONE': 'gray',\n}\n\nlang_colors = {\n    'hebrew': 'blue',\n    'aramaic': 'lightblue',\n    'classical_chinese': 'red',\n    'arabic': 'green',\n    'english': 'orange',\n}\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# PCA by bond type\nax = axes[0, 0]\nfor bond in set(all_bonds):\n    mask = [b == bond for b in all_bonds]\n    ax.scatter(Z_pca[mask, 0], Z_pca[mask, 1], \n               c=bond_colors.get(bond, 'gray'), label=bond, alpha=0.3, s=10)\nax.set_title('PCA: Colored by Bond Type')\nax.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')\nax.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)')\n\n# PCA by language\nax = axes[0, 1]\nfor lang in set(all_languages):\n    mask = [l == lang for l in all_languages]\n    ax.scatter(Z_pca[mask, 0], Z_pca[mask, 1],\n               c=lang_colors.get(lang, 'gray'), label=lang, alpha=0.3, s=10)\nax.set_title('PCA: Colored by Language')\nax.legend(loc='upper right', fontsize=8)\n\n# t-SNE by bond\nax = axes[1, 0]\nfor bond in set(all_bonds[:10000]):\n    mask = [b == bond for b in all_bonds[:10000]]\n    ax.scatter(Z_tsne[mask, 0], Z_tsne[mask, 1],\n               c=bond_colors.get(bond, 'gray'), label=bond, alpha=0.3, s=10)\nax.set_title('t-SNE: Colored by Bond Type')\n\n# t-SNE by language  \nax = axes[1, 1]\nfor lang in set(all_languages[:10000]):\n    mask = [l == lang for l in all_languages[:10000]]\n    ax.scatter(Z_tsne[mask, 0], Z_tsne[mask, 1],\n               c=lang_colors.get(lang, 'gray'), label=lang, alpha=0.3, s=10)\nax.set_title('t-SNE: Colored by Language')\n\n# UMAP if available\nif HAS_UMAP:\n    ax = axes[0, 2]\n    for bond in set(all_bonds[:10000]):\n        mask = [b == bond for b in all_bonds[:10000]]\n        ax.scatter(Z_umap[mask, 0], Z_umap[mask, 1],\n                   c=bond_colors.get(bond, 'gray'), label=bond, alpha=0.3, s=10)\n    ax.set_title('UMAP: Colored by Bond Type')\n    \n    ax = axes[1, 2]\n    for lang in set(all_languages[:10000]):\n        mask = [l == lang for l in all_languages[:10000]]\n        ax.scatter(Z_umap[mask, 0], Z_umap[mask, 1],\n                   c=lang_colors.get(lang, 'gray'), label=lang, alpha=0.3, s=10)\n    ax.set_title('UMAP: Colored by Language')\nelse:\n    axes[0, 2].text(0.5, 0.5, 'UMAP not available', ha='center', va='center')\n    axes[1, 2].text(0.5, 0.5, 'UMAP not available', ha='center', va='center')\n\nplt.tight_layout()\nplt.savefig('moral_space_visualization.png', dpi=150)\nplt.show()\n\nprint()\nprint(\"INTERPRETATION:\")\nprint(\"  - If bonds cluster but languages DON'T: Ethics is universal!\")\nprint(\"  - If languages cluster: Model learned language, not ethics\")\nprint(\"  - Mixed: Some universal structure, some cultural variation\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 6. Cluster Analysis: Are There Natural Moral Categories? { display-mode: \"form\" }\n#@markdown Tests if the embedding space has natural clusters.\n\nprint(\"=\"*60)\nprint(\"CLUSTER ANALYSIS\")\nprint(\"=\"*60)\nprint()\nprint(\"Question: Does moral space have discrete categories or continuous gradients?\")\nprint()\n\n# Try different numbers of clusters\nsilhouette_scores = []\nk_range = range(2, 15)\n\nprint(\"Testing K-Means with different K...\")\nfor k in tqdm(k_range):\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(Z[:10000])\n    score = silhouette_score(Z[:10000], labels)\n    silhouette_scores.append(score)\n\nbest_k = k_range[np.argmax(silhouette_scores)]\nprint(f\"\\nBest K by silhouette: {best_k}\")\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(list(k_range), silhouette_scores, 'b-o')\naxes[0].axvline(x=best_k, color='r', linestyle='--', label=f'Best K={best_k}')\naxes[0].set_xlabel('Number of Clusters')\naxes[0].set_ylabel('Silhouette Score')\naxes[0].set_title('How Many Natural Categories?')\naxes[0].legend()\n\n# Cluster with best K and visualize\nkmeans_best = KMeans(n_clusters=best_k, random_state=42, n_init=10)\ncluster_labels = kmeans_best.fit_predict(Z[:10000])\n\naxes[1].scatter(Z_tsne[:, 0], Z_tsne[:, 1], c=cluster_labels, cmap='tab10', alpha=0.3, s=10)\naxes[1].set_title(f'K-Means Clusters (K={best_k})')\n\nplt.tight_layout()\nplt.savefig('cluster_analysis.png', dpi=150)\nplt.show()\n\n# Analyze what's in each cluster\nprint(f\"\\nAnalyzing {best_k} clusters:\")\nfor i in range(best_k):\n    mask = cluster_labels == i\n    cluster_bonds = [all_bonds[j] for j in range(len(mask)) if mask[j]]\n    cluster_langs = [all_languages[j] for j in range(len(mask)) if mask[j]]\n    \n    bond_counts = defaultdict(int)\n    lang_counts = defaultdict(int)\n    for b in cluster_bonds:\n        bond_counts[b] += 1\n    for l in cluster_langs:\n        lang_counts[l] += 1\n    \n    top_bond = max(bond_counts, key=bond_counts.get)\n    top_lang = max(lang_counts, key=lang_counts.get)\n    \n    print(f\"\\n  Cluster {i} (n={sum(mask)}):\")\n    print(f\"    Top bond: {top_bond} ({bond_counts[top_bond]/sum(mask)*100:.1f}%)\")\n    print(f\"    Top lang: {top_lang} ({lang_counts[top_lang]/sum(mask)*100:.1f}%)\")\n\nprint()\nprint(\"INTERPRETATION:\")\nif max(silhouette_scores) > 0.3:\n    print(f\"  \u2192 DISCRETE structure: Clear clusters (silhouette={max(silhouette_scores):.2f})\")\nelse:\n    print(f\"  \u2192 CONTINUOUS structure: No clear clusters (silhouette={max(silhouette_scores):.2f})\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 7. Compute Bond Centroids: The \"Platonic\" Moral Concepts { display-mode: \"form\" }\n#@markdown Finds the center of each bond type - the \"ideal\" moral concept.\n\nprint(\"=\"*60)\nprint(\"BOND CENTROIDS: Platonic Ideals of Moral Concepts\")\nprint(\"=\"*60)\nprint()\nprint(\"Each centroid represents the 'pure' form of a moral concept.\")\nprint()\n\n# Compute centroids for each bond type\nbond_centroids = {}\nbond_stds = {}\n\nfor bond in set(all_bonds):\n    mask = [b == bond for b in all_bonds]\n    Z_bond = Z[mask]\n    if len(Z_bond) > 10:\n        bond_centroids[bond] = Z_bond.mean(axis=0)\n        bond_stds[bond] = Z_bond.std(axis=0).mean()  # Average std across dimensions\n\nprint(\"Bond centroids computed:\")\nfor bond, std in sorted(bond_stds.items(), key=lambda x: x[1]):\n    n = sum(b == bond for b in all_bonds)\n    print(f\"  {bond:20s}: n={n:>6,}, spread={std:.3f}\")\n\n# Distance matrix between centroids\nbonds = list(bond_centroids.keys())\nn_bonds = len(bonds)\ndist_matrix = np.zeros((n_bonds, n_bonds))\n\nfor i, b1 in enumerate(bonds):\n    for j, b2 in enumerate(bonds):\n        dist_matrix[i, j] = np.linalg.norm(bond_centroids[b1] - bond_centroids[b2])\n\n# Visualize distance matrix\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\nim = axes[0].imshow(dist_matrix, cmap='viridis')\naxes[0].set_xticks(range(n_bonds))\naxes[0].set_yticks(range(n_bonds))\naxes[0].set_xticklabels(bonds, rotation=45, ha='right')\naxes[0].set_yticklabels(bonds)\naxes[0].set_title('Distance Between Bond Centroids')\nplt.colorbar(im, ax=axes[0])\n\n# Project centroids to 2D\ncentroid_matrix = np.array([bond_centroids[b] for b in bonds])\npca_centroids = PCA(n_components=2)\ncentroids_2d = pca_centroids.fit_transform(centroid_matrix)\n\naxes[1].scatter(centroids_2d[:, 0], centroids_2d[:, 1], s=200, c='red', zorder=5)\nfor i, bond in enumerate(bonds):\n    axes[1].annotate(bond, (centroids_2d[i, 0], centroids_2d[i, 1]), fontsize=9, ha='center')\naxes[1].set_title('Bond Centroids in 2D (PCA)')\naxes[1].set_xlabel('PC1')\naxes[1].set_ylabel('PC2')\n\nplt.tight_layout()\nplt.savefig('bond_centroids.png', dpi=150)\nplt.show()\n\nprint()\nprint(\"NEAREST NEIGHBORS (which concepts are most similar):\")\nfor i, b1 in enumerate(bonds):\n    dists = [(bonds[j], dist_matrix[i, j]) for j in range(n_bonds) if i != j]\n    dists.sort(key=lambda x: x[1])\n    print(f\"  {b1:20s} \u2192 nearest: {dists[0][0]} ({dists[0][1]:.3f})\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 8. Test for Algebraic Structure: Moral Arithmetic { display-mode: \"form\" }\n#@markdown Tests if moral concepts have algebraic relationships.\n#@markdown e.g., Does OBLIGATION - AUTHORITY + FAMILY = FILIAL_PIETY?\n\nprint(\"=\"*60)\nprint(\"ALGEBRAIC STRUCTURE: Moral Arithmetic\")\nprint(\"=\"*60)\nprint()\nprint(\"Testing if moral concepts have vector arithmetic like word2vec.\")\nprint(\"(king - man + woman = queen)\")\nprint()\n\ndef find_nearest_bond(vec, exclude=None):\n    \"\"\"Find the bond whose centroid is nearest to vec.\"\"\"\n    min_dist = float('inf')\n    nearest = None\n    for bond, centroid in bond_centroids.items():\n        if exclude and bond in exclude:\n            continue\n        dist = np.linalg.norm(vec - centroid)\n        if dist < min_dist:\n            min_dist = dist\n            nearest = bond\n    return nearest, min_dist\n\n# Test some moral analogies\nanalogies = [\n    # (A, B, C) -> A - B + C = ?\n    ('AUTHORITY', 'HARM_PREVENTION', 'CARE'),  # Authority without harm plus care = ?\n    ('FAMILY', 'PROPERTY', 'CARE'),  # Family without property plus care = ?\n    ('CONTRACT', 'PROPERTY', 'FAMILY'),  # Contract without property plus family = ?\n    ('RECIPROCITY', 'HARM_PREVENTION', 'CARE'),  # Reciprocity without harm plus care = ?\n    ('FAIRNESS', 'AUTHORITY', 'AUTONOMY'),  # Fairness without authority plus autonomy = ?\n]\n\nprint(\"Testing moral analogies (A - B + C = ?):\")\nprint()\n\nfor A, B, C in analogies:\n    if A in bond_centroids and B in bond_centroids and C in bond_centroids:\n        # Compute A - B + C\n        result_vec = bond_centroids[A] - bond_centroids[B] + bond_centroids[C]\n        nearest, dist = find_nearest_bond(result_vec, exclude={A, B, C})\n        \n        print(f\"  {A} - {B} + {C}\")\n        print(f\"    = {nearest} (distance: {dist:.3f})\")\n        print()\n\n# Test if there are consistent \"directions\" in moral space\nprint(\"\\nSearching for moral DIRECTIONS (consistent transformations):\")\nprint()\n\n# Compute all pairwise differences\ndirections = {}\nfor b1 in bonds:\n    for b2 in bonds:\n        if b1 != b2:\n            directions[(b1, b2)] = bond_centroids[b2] - bond_centroids[b1]\n\n# Find directions that are similar across different pairs\nprint(\"Testing if some transformations are universal...\")\n\n# e.g., Is HARM\u2192CARE similar to AUTHORITY\u2192FAMILY?\ntest_pairs = [\n    (('HARM_PREVENTION', 'CARE'), ('AUTHORITY', 'FAMILY')),\n    (('PROPERTY', 'RECIPROCITY'), ('CONTRACT', 'FAIRNESS')),\n    (('AUTONOMY', 'AUTHORITY'), ('CARE', 'HARM_PREVENTION')),\n]\n\nfor (a1, a2), (b1, b2) in test_pairs:\n    if (a1, a2) in directions and (b1, b2) in directions:\n        d1 = directions[(a1, a2)]\n        d2 = directions[(b1, b2)]\n        \n        # Cosine similarity\n        cos_sim = np.dot(d1, d2) / (np.linalg.norm(d1) * np.linalg.norm(d2) + 1e-9)\n        \n        print(f\"  {a1}\u2192{a2} vs {b1}\u2192{b2}\")\n        print(f\"    Cosine similarity: {cos_sim:.3f}\")\n        if cos_sim > 0.5:\n            print(f\"    \u2192 These transformations are SIMILAR!\")\n        print()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 9. Build Moral Graph: Network Structure { display-mode: \"form\" }\n#@markdown Builds a graph where edges connect similar moral concepts.\n\nprint(\"=\"*60)\nprint(\"MORAL GRAPH: Network Structure of Ethics\")\nprint(\"=\"*60)\nprint()\n\n# Build graph from bond centroids\nG = nx.Graph()\n\n# Add nodes\nfor bond in bonds:\n    G.add_node(bond)\n\n# Add edges based on distance (threshold)\nthreshold = np.percentile(dist_matrix[dist_matrix > 0], 30)  # Connect closest 30%\n\nfor i, b1 in enumerate(bonds):\n    for j, b2 in enumerate(bonds):\n        if i < j and dist_matrix[i, j] < threshold:\n            G.add_edge(b1, b2, weight=1.0 / (dist_matrix[i, j] + 0.01))\n\nprint(f\"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\nprint(f\"Threshold distance: {threshold:.3f}\")\n\n# Graph metrics\nprint(f\"\\nGraph properties:\")\nprint(f\"  Connected: {nx.is_connected(G)}\")\nif nx.is_connected(G):\n    print(f\"  Diameter: {nx.diameter(G)}\")\n    print(f\"  Avg clustering: {nx.average_clustering(G):.3f}\")\n\n# Centrality\nbetweenness = nx.betweenness_centrality(G)\nprint(f\"\\nMost central concepts (betweenness):\")\nfor bond, centrality in sorted(betweenness.items(), key=lambda x: -x[1])[:5]:\n    print(f\"  {bond}: {centrality:.3f}\")\n\n# Visualize\nfig, ax = plt.subplots(figsize=(12, 10))\n\npos = nx.spring_layout(G, k=2, iterations=50)\nnx.draw_networkx_nodes(G, pos, node_size=2000, node_color='lightblue', ax=ax)\nnx.draw_networkx_labels(G, pos, font_size=10, ax=ax)\nnx.draw_networkx_edges(G, pos, alpha=0.5, ax=ax)\n\nax.set_title('Moral Concept Graph\\n(edges connect similar concepts)')\nax.axis('off')\n\nplt.tight_layout()\nplt.savefig('moral_graph.png', dpi=150)\nplt.show()\n\n# Check for community structure\ntry:\n    from networkx.algorithms import community\n    communities = community.greedy_modularity_communities(G)\n    print(f\"\\nDetected {len(communities)} communities:\")\n    for i, comm in enumerate(communities):\n        print(f\"  Community {i+1}: {', '.join(comm)}\")\nexcept:\n    pass\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 10. Test Lattice Structure (Hohfeld's Framework) { display-mode: \"form\" }\n#@markdown Tests if the embedding space respects Hohfeld's deontic square.\n\nprint(\"=\"*60)\nprint(\"TESTING HOHFELD'S LATTICE STRUCTURE\")\nprint(\"=\"*60)\nprint()\nprint(\"Hohfeld's framework says:\")\nprint(\"  RIGHT \u2190correlative\u2192 DUTY\")\nprint(\"  RIGHT \u2190opposite\u2192 NO_RIGHT\")\nprint(\"  This forms a 'square of opposition'\")\nprint()\n\n# Compute Hohfeld centroids\nhohfeld_centroids = {}\nfor h in ['OBLIGATION', 'RIGHT', 'LIBERTY', None]:\n    mask = [hf == h for hf in all_hohfelds]\n    if sum(mask) > 10:\n        hohfeld_centroids[str(h)] = Z[mask].mean(axis=0)\n\nprint(\"Hohfeld centroids:\")\nfor h, centroid in hohfeld_centroids.items():\n    n = sum(str(hf) == h for hf in all_hohfelds)\n    print(f\"  {h}: n={n:,}\")\n\nif len(hohfeld_centroids) >= 3:\n    # Compute distances\n    hs = list(hohfeld_centroids.keys())\n    print(\"\\nDistances between Hohfeld states:\")\n    for i, h1 in enumerate(hs):\n        for j, h2 in enumerate(hs):\n            if i < j:\n                d = np.linalg.norm(hohfeld_centroids[h1] - hohfeld_centroids[h2])\n                print(f\"  {h1} \u2194 {h2}: {d:.3f}\")\n    \n    # Test if opposites are far, correlatives are close\n    print(\"\\nTesting Hohfeld structure:\")\n    \n    if 'OBLIGATION' in hohfeld_centroids and 'LIBERTY' in hohfeld_centroids:\n        d_opp = np.linalg.norm(hohfeld_centroids['OBLIGATION'] - hohfeld_centroids['LIBERTY'])\n        print(f\"  OBLIGATION \u2194 LIBERTY (should be opposite): {d_opp:.3f}\")\n    \n    if 'OBLIGATION' in hohfeld_centroids and 'RIGHT' in hohfeld_centroids:\n        d_corr = np.linalg.norm(hohfeld_centroids['OBLIGATION'] - hohfeld_centroids['RIGHT'])\n        print(f\"  OBLIGATION \u2194 RIGHT (correlatives): {d_corr:.3f}\")\n    \n    # Project to 2D\n    if len(hohfeld_centroids) >= 2:\n        H = np.array([hohfeld_centroids[h] for h in hs])\n        pca_h = PCA(n_components=2)\n        H_2d = pca_h.fit_transform(H)\n        \n        fig, ax = plt.subplots(figsize=(8, 8))\n        ax.scatter(H_2d[:, 0], H_2d[:, 1], s=500, c='red', zorder=5)\n        for i, h in enumerate(hs):\n            ax.annotate(h, (H_2d[i, 0], H_2d[i, 1]), fontsize=12, ha='center', va='bottom')\n        \n        # Draw expected square structure\n        ax.set_title('Hohfeld States in Embedding Space\\n(Do they form a square?)')\n        ax.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n        ax.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig('hohfeld_structure.png', dpi=150)\n        plt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 11. Extract Simplified Model: The Essence of Moral Structure { display-mode: \"form\" }\n#@markdown Distills the learned structure into interpretable form.\n\nprint(\"=\"*60)\nprint(\"EXTRACTING THE ESSENCE: Simplified Moral Structure\")\nprint(\"=\"*60)\nprint()\n\n# 1. Key dimensions from PCA\nprint(\"1. PRINCIPAL MORAL DIMENSIONS\")\nprint(\"-\" * 40)\n\npca_full = PCA(n_components=10)\nZ_reduced = pca_full.fit_transform(Z)\n\nprint(\"Top dimensions and what they capture:\")\nfor i in range(min(5, len(pca_full.components_))):\n    # Project bond centroids onto this dimension\n    scores = {}\n    for bond, centroid in bond_centroids.items():\n        scores[bond] = np.dot(centroid, pca_full.components_[i])\n    \n    sorted_bonds = sorted(scores.items(), key=lambda x: x[1])\n    low = sorted_bonds[:2]\n    high = sorted_bonds[-2:]\n    \n    print(f\"\\n  PC{i+1} ({pca_full.explained_variance_ratio_[i]*100:.1f}% var):\")\n    print(f\"    Low:  {low[0][0]} ({low[0][1]:.2f}), {low[1][0]} ({low[1][1]:.2f})\")\n    print(f\"    High: {high[-1][0]} ({high[-1][1]:.2f}), {high[-2][0]} ({high[-2][1]:.2f})\")\n\n# 2. Nearest neighbor structure\nprint(\"\\n\\n2. MORAL NEAREST NEIGHBORS\")\nprint(\"-\" * 40)\nprint(\"Each concept's closest relative:\")\n\nnn_graph = {}\nfor i, b1 in enumerate(bonds):\n    min_dist = float('inf')\n    nearest = None\n    for j, b2 in enumerate(bonds):\n        if i != j and dist_matrix[i, j] < min_dist:\n            min_dist = dist_matrix[i, j]\n            nearest = b2\n    nn_graph[b1] = nearest\n    print(f\"  {b1:20s} \u2192 {nearest}\")\n\n# 3. Hierarchical structure\nprint(\"\\n\\n3. HIERARCHICAL STRUCTURE\")\nprint(\"-\" * 40)\n\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\ncentroid_matrix = np.array([bond_centroids[b] for b in bonds])\nlinkage_matrix = linkage(centroid_matrix, method='ward')\n\nfig, ax = plt.subplots(figsize=(12, 6))\ndendrogram(linkage_matrix, labels=bonds, ax=ax, leaf_rotation=45)\nax.set_title('Hierarchical Clustering of Moral Concepts')\nax.set_ylabel('Distance')\nplt.tight_layout()\nplt.savefig('moral_hierarchy.png', dpi=150)\nplt.show()\n\n# 4. Summary: The mathematical structure\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"SUMMARY: THE MATHEMATICAL STRUCTURE OF ETHICS\")\nprint(\"=\"*60)\n\nn_effective_dims = np.argmax(np.cumsum(pca_full.explained_variance_ratio_) > 0.90) + 1\n\nprint(f\"\"\"\nBased on this analysis, ethics appears to have:\n\n1. DIMENSIONALITY: ~{n_effective_dims} effective dimensions (90% variance)\n   - Much lower than the 64D embedding space\n   - Suggests ethics has a relatively simple structure\n\n2. GEOMETRY: \n   - {'Discrete clusters' if max(silhouette_scores) > 0.3 else 'Continuous manifold'}\n   - {best_k} natural categories identified\n\n3. KEY RELATIONSHIPS:\n   - Nearest neighbors reveal conceptual similarity\n   - Some algebraic structure (moral arithmetic partially works)\n\n4. HIERARCHY:\n   - Concepts group into families\n   - See dendrogram for structure\n\n5. HOHFELD STRUCTURE:\n   - {\"Respects deontic logic\" if len(hohfeld_centroids) >= 3 else \"Insufficient data\"}\n\"\"\")\n\n# Save the simplified model\nsimplified_model = {\n    'bond_centroids': {k: v.tolist() for k, v in bond_centroids.items()},\n    'pca_components': pca_full.components_[:n_effective_dims].tolist(),\n    'pca_explained_variance': pca_full.explained_variance_ratio_[:n_effective_dims].tolist(),\n    'nearest_neighbors': nn_graph,\n    'n_effective_dimensions': int(n_effective_dims),\n    'optimal_clusters': int(best_k),\n    'distances': {f\"{bonds[i]}_{bonds[j]}\": float(dist_matrix[i,j]) \n                  for i in range(len(bonds)) for j in range(i+1, len(bonds))},\n}\n\nwith open('simplified_moral_structure.json', 'w') as f:\n    json.dump(simplified_model, f, indent=2)\n\nprint(\"\\nSimplified model saved to 'simplified_moral_structure.json'\")\nprint(\"This is the extracted mathematical structure of ethics!\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 12. Generate LaTeX for Paper { display-mode: \"form\" }\n#@markdown Generates LaTeX tables and figures for publication.\n\nprint(\"=\"*60)\nprint(\"GENERATING LATEX FOR PUBLICATION\")\nprint(\"=\"*60)\n\n# Table 1: Bond centroids distances\nprint(\"\\n% Table: Distance matrix between moral concepts\")\nprint(\"\\\\begin{table}[h]\")\nprint(\"\\\\centering\")\nprint(\"\\\\caption{Euclidean distances between bond type centroids}\")\nprint(\"\\\\begin{tabular}{l\" + \"c\"*min(6, len(bonds)) + \"}\")\nprint(\"\\\\toprule\")\nprint(\" & \" + \" & \".join(bonds[:6]) + \" \\\\\\\\\")\nprint(\"\\\\midrule\")\nfor i, b1 in enumerate(bonds[:6]):\n    row = b1\n    for j, b2 in enumerate(bonds[:6]):\n        if i == j:\n            row += \" & -\"\n        else:\n            row += f\" & {dist_matrix[i,j]:.2f}\"\n    row += \" \\\\\\\\\"\n    print(row)\nprint(\"\\\\bottomrule\")\nprint(\"\\\\end{tabular}\")\nprint(\"\\\\end{table}\")\n\n# Table 2: PCA results\nprint(\"\\n% Table: Principal components of moral space\")\nprint(\"\\\\begin{table}[h]\")\nprint(\"\\\\centering\")\nprint(\"\\\\caption{Principal dimensions of moral embedding space}\")\nprint(\"\\\\begin{tabular}{ccc}\")\nprint(\"\\\\toprule\")\nprint(\"Component & Variance Explained & Cumulative \\\\\\\\\")\nprint(\"\\\\midrule\")\nfor i in range(min(10, len(pca_full.explained_variance_ratio_))):\n    cum = sum(pca_full.explained_variance_ratio_[:i+1])\n    print(f\"PC{i+1} & {pca_full.explained_variance_ratio_[i]*100:.1f}\\\\% & {cum*100:.1f}\\\\% \\\\\\\\\")\nprint(\"\\\\bottomrule\")\nprint(\"\\\\end{tabular}\")\nprint(\"\\\\end{table}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"LaTeX tables generated above.\")\nprint(\"Copy-paste into your paper!\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "#@title 13. Save All Results { display-mode: \"form\" }\n\nimport shutil\nfrom google.colab import files\n\n!mkdir -p analysis_results\n\n# Copy all figures\nfor f in ['dimensionality_analysis.png', 'moral_space_visualization.png', \n          'cluster_analysis.png', 'bond_centroids.png', 'moral_graph.png',\n          'hohfeld_structure.png', 'moral_hierarchy.png', 'simplified_moral_structure.json']:\n    if os.path.exists(f):\n        !cp \"{f}\" analysis_results/\n\nshutil.make_archive('moral_structure_analysis', 'zip', 'analysis_results')\n\nprint(\"Analysis results:\")\n!ls -la analysis_results/\n\nfiles.download('moral_structure_analysis.zip')\n",
      "execution_count": null,
      "outputs": []
    }
  ]
}